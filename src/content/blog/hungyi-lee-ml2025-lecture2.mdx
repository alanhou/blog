---
title:
  en: "Hung-yi Lee ML 2025 Lecture 2: AI Agents - From AlphaGo to LLM-Powered Autonomy"
  zh: "李宏毅机器学习2025 第二讲：AI Agent - 从AlphaGo到大型语言模型驱动的自主系统"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on AI agents: memory systems, tool use, and planning capabilities"
  zh: "台大李宏毅教授AI Agent课程笔记：记忆系统、工具使用与规划能力"
date: 2025-02-23
tags: ["ai", "llm", "ai-agents", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/M2Yg1kwPpts/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="M2Yg1kwPpts" title="李宏毅机器学习2025 第二讲：AI Agent" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is an AI Agent?

Professor Lee starts with a disclaimer: everyone has a different definition of "AI agent." Some insist only robots with physical bodies count. This lecture focuses on a specific definition.

**Traditional AI interaction**: Human gives explicit command → AI executes exactly that command → Done.

**AI Agent interaction**: Human gives a goal → AI figures out how to achieve it through multiple steps, adapting to environmental changes along the way.

An AI agent must:
- Observe the current state (observation)
- Decide what action to take
- Execute the action
- Observe the new state
- Repeat until goal is achieved

## From RL Agents to LLM Agents

**AlphaGo as an Agent**: Goal is winning, observation is the board state, action is choosing where to place a stone. Classic reinforcement learning approach.

**The RL limitation**: You need to train a separate model for each task. AlphaGo plays Go, but can't play chess without separate training.

**The LLM breakthrough**: What if we just use a language model as the agent? Goals become text prompts, observations become text descriptions (or images for multimodal models), and actions are generated as text then translated into executable commands.

AI agents became hot again not because of new agent-specific techniques, but because LLMs became powerful enough to potentially serve as general-purpose agents.

## Can LLMs Actually Play Games?

In 2022 (pre-ChatGPT era), researchers tested language models on chess. Result: no model gave correct answers, and weaker models didn't even understand the rules.

Fast forward to 2025: Someone pitted ChatGPT o1 against DeepSeek-R1 in chess. The result was... chaotic. Both models made illegal moves - using pawns like knights, bishops ignoring obstacles, pieces appearing out of nowhere. DeepSeek eventually ate its own piece with a castle it conjured from thin air, declared victory, and ChatGPT accepted defeat.

Current LLMs still can't play chess properly, but that doesn't mean they can't be useful agents for other tasks.

## Three Key Capabilities of AI Agents

### 1. Learning from Experience (Memory)

The naive approach: feed the agent its entire history before each decision. Problem: by step 10,000, the context is impossibly long.

This is like "hyperthymesia" (超忆症) - people who remember every detail of their lives. Sounds like a superpower, but it's actually classified as a disorder. These individuals struggle with abstract thinking because their minds are cluttered with trivial details.

**Solution: Long-term Memory with RAG**

- **Write module**: Decides what's worth remembering (not every observation matters)
- **Read module**: Retrieves relevant memories for the current situation (like RAG)
- **Reflection module**: Synthesizes higher-level insights from raw memories

ChatGPT already implements this. You can tell it "remember this" and it stores information in its memory. You can even view and manage these memories in settings.

**Key finding from StreamBench**: Negative examples don't help much. Telling the model "this was wrong" is less effective than showing it "this is how to do it right." Don't tell LLMs what NOT to do - tell them what TO do.

### 2. Using Tools

LLMs have limitations. They can't reliably do math, access real-time information, or interact with external systems. Solution: give them tools.

**How tool use works**:
1. Define tools with text descriptions (function name, parameters, what it does)
2. Model decides when to call a tool
3. Tool returns results
4. Model incorporates results into its response

**Example**: A speech model that can't directly process audio can call a speech-to-text tool, then work with the transcript.

**The tool selection problem**: With hundreds of tools, you can't describe them all in the prompt. Solution: treat tool descriptions like a memory bank and use RAG to select relevant tools.

**LLMs can even create their own tools**: They write functions, test them, and if successful, add them to their toolbox for future use.

### 3. Do LLMs Trust Their Tools Too Much?

Remember Google's AI Overview suggesting glue for pizza cheese? That came from a Reddit joke that the model took seriously.

But LLMs aren't completely gullible. Test: tell a model the temperature in Kaohsiung is 100°C - it reports it. Tell it 10,000°C - it recognizes this is impossible and rejects the tool output.

**What makes external information persuasive to LLMs?**

- Information closer to the model's existing beliefs is more convincing
- Models with lower confidence in their own answers are easier to sway
- LLMs tend to trust AI-written content over human-written content (even from different AI systems)
- Newer publication dates are more trusted
- Better-designed webpage templates are more persuasive

### 4. Planning Capabilities

Can LLMs plan? Early tests showed they could decompose tasks like "brush teeth" into steps. But these might just be memorized patterns from training data.

**The Mystery Blocksworld Test**: Instead of normal block-stacking rules, use bizarre operations like "attack," "devour," "conquer." Models that memorized normal blocksworld fail here.

Results: GPT-4 got only 9% on mystery blocksworld (vs 30%+ on normal). But o1-mini and o1-preview showed real improvement, suggesting actual reasoning rather than pattern matching.

**Travel Planning Benchmark**: Ask models to plan trips with budget constraints. In early 2024, even GPT-4 Turbo achieved only 0.6% success rate. Common failures: scheduling activities after departure flights, inability to adjust plans to meet budget constraints.

With proper solver tools, success rates jumped to 90%+. The lesson: LLMs can plan, but complex constraint satisfaction needs specialized tools.

## Tree Search and World Models

To improve planning, let agents explore possibilities:

1. **Tree Search**: Try different actions, evaluate outcomes, backtrack from dead ends
2. **Problem**: Some actions are irreversible (you can't un-order a pizza)
3. **Solution**: Simulate in a "dream" - the model imagines outcomes before acting

**World Models**: The agent predicts what will happen after each action. In chess, this means simulating opponent responses. For web agents, it means imagining what clicking a button will do.

## Reasoning Models as Planners

Models like DeepSeek-R1 with "thinking" capabilities naturally perform tree search in their chain-of-thought. Given a blocksworld problem, R1 explores multiple paths in its reasoning, finds an optimal solution, then executes the first step.

**The Overthinking Problem**: A recent paper "The Danger of Overthinking" found that reasoning models sometimes think too much. They'll endlessly deliberate about what clicking a button might do, when they could just... click it and see. Some give up without trying, concluding "I probably can't do this."

## Key Takeaways

1. AI agents use LLMs to pursue goals through multi-step interactions with environments
2. Memory systems (read/write/reflect) prevent context overflow and enable learning
3. Tool use extends LLM capabilities but requires trust calibration
4. Planning is possible but limited - complex constraints need specialized solvers
5. Reasoning models help with planning but can overthink simple problems
:::

:::zh
本文整理自台湾大学李宏毅教授的「生成式AI时代下的机器学习(2025)」课程。

## 什么是AI Agent？

李老师开场先声明：每个人对「AI agent」的定义都不同。有人坚持只有有身体的机器人才算。这堂课聚焦在特定的定义。

**传统AI互动**：人类给明确指令 → AI执行该指令 → 结束。

**AI Agent互动**：人类给目标 → AI自己想办法通过多个步骤达成，并根据环境变化调整。

AI agent必须能够：
- 观察目前状态（observation）
- 决定采取什么行动
- 执行行动
- 观察新状态
- 重复直到达成目标

## 从RL Agent到LLM Agent

**AlphaGo作为Agent**：目标是赢棋，observation是棋盘状态，action是选择落子位置。经典的强化学习方法。

**RL的局限**：每个任务都要训练一个模型。AlphaGo会下围棋，但不会下西洋棋，除非另外训练。

**LLM的突破**：如果直接用语言模型当agent呢？目标变成文字提示，observation变成文字描述（或多模态模型的图片），action以文字生成后转译成可执行指令。

AI agent再次爆红，不是因为有什么agent专属的新技术，而是LLM变得够强，可能可以当通用型agent使用。

## LLM真的会下棋吗？

2022年（ChatGPT之前的上古时代），研究者测试语言模型下西洋棋。结果：没有模型给出正确答案，较弱的模型甚至不懂规则。

快转到2025年：有人让ChatGPT o1对战DeepSeek-R1下西洋棋。结果是...一团混乱。两个模型都走违规棋步——把兵当马用、主教无视障碍物、棋子凭空出现。DeepSeek最后用自己变出来的城堡吃掉自己的兵，宣布获胜，ChatGPT居然接受认输。

目前的LLM还是不会正确下棋，但这不代表他们不能在其他任务上当有用的agent。

## AI Agent的三大关键能力

### 1. 从经验学习（记忆）

天真的做法：每次决策前把所有历史都喂给agent。问题：到第一万步时，上下文长到不可能处理。

这就像「超忆症」——能记住人生每个细节的人。听起来像超能力，但实际上被归类为疾病。这些人难以做抽象思考，因为脑中塞满琐碎细节。

**解决方案：长期记忆配合RAG**

- **Write模块**：决定什么值得记住（不是每个observation都重要）
- **Read模块**：检索与当前情境相关的记忆（像RAG）
- **Reflection模块**：从原始记忆中综合出更高层次的洞见

ChatGPT已经实现了这个。你可以跟他说「记下来」，他就会存到记忆中。你甚至可以在设置中查看和管理这些记忆。

**StreamBench的关键发现**：负面例子帮助不大。告诉模型「这是错的」不如告诉他「这样做才对」有效。不要告诉LLM不要做什么——告诉他要做什么。

### 2. 使用工具

LLM有局限。他们无法可靠地做数学、获取即时信息、或与外部系统互动。解决方案：给他们工具。

**工具使用的运作方式**：
1. 用文字描述定义工具（函数名称、参数、功能）
2. 模型决定何时调用工具
3. 工具返回结果
4. 模型将结果纳入回应

**范例**：无法直接处理音频的语音模型可以调用语音转文字工具，然后处理转录文字。

**工具选择问题**：有上百个工具时，不可能全部描述在prompt中。解决方案：把工具描述当作记忆库，用RAG选择相关工具。

**LLM甚至可以自己创造工具**：他们写函数、测试，如果成功就加入工具箱供未来使用。

### 3. LLM太相信工具了吗？

还记得Google的AI Overview建议用胶水粘披萨起司吗？那来自Reddit上一个网友的玩笑，模型当真了。

但LLM也不是完全好骗。测试：告诉模型高雄现在100°C——他会报告。告诉他10,000°C——他会认出这不可能，拒绝工具输出。

**什么样的外部信息对LLM有说服力？**

- 越接近模型既有信念的信息越有说服力
- 对自己答案信心越低的模型越容易被动摇
- LLM倾向相信AI写的内容胜过人类写的（即使是不同的AI系统）
- 较新的发布日期更受信任
- 设计较好的网页模板更有说服力

### 4. 规划能力

LLM会规划吗？早期测试显示他们可以把「刷牙」这样的任务分解成步骤。但这可能只是从训练数据记住的模式。

**神秘方块世界测试**：不用正常的积木堆叠规则，改用奇怪的操作如「攻击」、「吞噬」、「征服」。记住正常方块世界的模型在这里会失败。

结果：GPT-4在神秘方块世界只有9%正确率（正常版有30%+）。但o1-mini和o1-preview有明显进步，显示是真正的推理而非模式匹配。

**旅游规划Benchmark**：要求模型在预算限制下规划旅行。2024年初，即使GPT-4 Turbo也只有0.6%成功率。常见失败：在飞机起飞后还安排行程、无法调整计划以符合预算限制。

有了适当的solver工具，成功率跳到90%+。教训：LLM可以规划，但复杂的约束满足需要专门工具。

## 树搜索与世界模型

要改善规划，让agent探索可能性：

1. **树搜索**：尝试不同行动、评估结果、从死路回溯
2. **问题**：有些行动不可逆（你不能取消已订的披萨）
3. **解决方案**：在「梦境」中模拟——模型在行动前想象结果

**世界模型**：agent预测每个行动后会发生什么。下棋时，这意味着模拟对手回应。对网页agent，这意味着想象点击按钮会发生什么。

## 推理模型作为规划者

像DeepSeek-R1这样有「思考」能力的模型，在思维链中自然会进行树搜索。给他方块世界问题，R1会在推理中探索多条路径，找到最佳解，然后执行第一步。

**想太多的问题**：最近一篇论文「The Danger of Overthinking」发现推理模型有时想太多。他们会无止尽地思考点击按钮会发生什么，但其实可以直接...点一下看看。有些甚至不尝试就放弃，结论是「我大概做不到」。

## 重点整理

1. AI agent用LLM通过多步骤与环境互动来追求目标
2. 记忆系统（read/write/reflect）防止上下文溢出并实现学习
3. 工具使用扩展LLM能力但需要信任校准
4. 规划是可能的但有限——复杂约束需要专门solver
5. 推理模型有助于规划但可能对简单问题想太多
:::
