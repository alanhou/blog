---
title:
  en: "Hung-yi Lee ML 2025 Lecture 2: AI Agents - From AlphaGo to LLM-Powered Autonomy"
  zh: "李宏毅機器學習2025 第二講：AI Agent - 從AlphaGo到大型語言模型驅動的自主系統"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on AI agents: memory systems, tool use, and planning capabilities"
  zh: "台大李宏毅教授AI Agent課程筆記：記憶系統、工具使用與規劃能力"
date: 2025-02-23
tags: ["ai", "llm", "ai-agents", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/M2Yg1kwPpts/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="M2Yg1kwPpts" title="李宏毅機器學習2025 第二講：AI Agent" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is an AI Agent?

Professor Lee starts with a disclaimer: everyone has a different definition of "AI agent." Some insist only robots with physical bodies count. This lecture focuses on a specific definition.

**Traditional AI interaction**: Human gives explicit command → AI executes exactly that command → Done.

**AI Agent interaction**: Human gives a goal → AI figures out how to achieve it through multiple steps, adapting to environmental changes along the way.

An AI agent must:
- Observe the current state (observation)
- Decide what action to take
- Execute the action
- Observe the new state
- Repeat until goal is achieved

## From RL Agents to LLM Agents

**AlphaGo as an Agent**: Goal is winning, observation is the board state, action is choosing where to place a stone. Classic reinforcement learning approach.

**The RL limitation**: You need to train a separate model for each task. AlphaGo plays Go, but can't play chess without separate training.

**The LLM breakthrough**: What if we just use a language model as the agent? Goals become text prompts, observations become text descriptions (or images for multimodal models), and actions are generated as text then translated into executable commands.

AI agents became hot again not because of new agent-specific techniques, but because LLMs became powerful enough to potentially serve as general-purpose agents.

## Can LLMs Actually Play Games?

In 2022 (pre-ChatGPT era), researchers tested language models on chess. Result: no model gave correct answers, and weaker models didn't even understand the rules.

Fast forward to 2025: Someone pitted ChatGPT o1 against DeepSeek-R1 in chess. The result was... chaotic. Both models made illegal moves - using pawns like knights, bishops ignoring obstacles, pieces appearing out of nowhere. DeepSeek eventually ate its own piece with a castle it conjured from thin air, declared victory, and ChatGPT accepted defeat.

Current LLMs still can't play chess properly, but that doesn't mean they can't be useful agents for other tasks.

## Three Key Capabilities of AI Agents

### 1. Learning from Experience (Memory)

The naive approach: feed the agent its entire history before each decision. Problem: by step 10,000, the context is impossibly long.

This is like "hyperthymesia" (超憶症) - people who remember every detail of their lives. Sounds like a superpower, but it's actually classified as a disorder. These individuals struggle with abstract thinking because their minds are cluttered with trivial details.

**Solution: Long-term Memory with RAG**

- **Write module**: Decides what's worth remembering (not every observation matters)
- **Read module**: Retrieves relevant memories for the current situation (like RAG)
- **Reflection module**: Synthesizes higher-level insights from raw memories

ChatGPT already implements this. You can tell it "remember this" and it stores information in its memory. You can even view and manage these memories in settings.

**Key finding from StreamBench**: Negative examples don't help much. Telling the model "this was wrong" is less effective than showing it "this is how to do it right." Don't tell LLMs what NOT to do - tell them what TO do.

### 2. Using Tools

LLMs have limitations. They can't reliably do math, access real-time information, or interact with external systems. Solution: give them tools.

**How tool use works**:
1. Define tools with text descriptions (function name, parameters, what it does)
2. Model decides when to call a tool
3. Tool returns results
4. Model incorporates results into its response

**Example**: A speech model that can't directly process audio can call a speech-to-text tool, then work with the transcript.

**The tool selection problem**: With hundreds of tools, you can't describe them all in the prompt. Solution: treat tool descriptions like a memory bank and use RAG to select relevant tools.

**LLMs can even create their own tools**: They write functions, test them, and if successful, add them to their toolbox for future use.

### 3. Do LLMs Trust Their Tools Too Much?

Remember Google's AI Overview suggesting glue for pizza cheese? That came from a Reddit joke that the model took seriously.

But LLMs aren't completely gullible. Test: tell a model the temperature in Kaohsiung is 100°C - it reports it. Tell it 10,000°C - it recognizes this is impossible and rejects the tool output.

**What makes external information persuasive to LLMs?**

- Information closer to the model's existing beliefs is more convincing
- Models with lower confidence in their own answers are easier to sway
- LLMs tend to trust AI-written content over human-written content (even from different AI systems)
- Newer publication dates are more trusted
- Better-designed webpage templates are more persuasive

### 4. Planning Capabilities

Can LLMs plan? Early tests showed they could decompose tasks like "brush teeth" into steps. But these might just be memorized patterns from training data.

**The Mystery Blocksworld Test**: Instead of normal block-stacking rules, use bizarre operations like "attack," "devour," "conquer." Models that memorized normal blocksworld fail here.

Results: GPT-4 got only 9% on mystery blocksworld (vs 30%+ on normal). But o1-mini and o1-preview showed real improvement, suggesting actual reasoning rather than pattern matching.

**Travel Planning Benchmark**: Ask models to plan trips with budget constraints. In early 2024, even GPT-4 Turbo achieved only 0.6% success rate. Common failures: scheduling activities after departure flights, inability to adjust plans to meet budget constraints.

With proper solver tools, success rates jumped to 90%+. The lesson: LLMs can plan, but complex constraint satisfaction needs specialized tools.

## Tree Search and World Models

To improve planning, let agents explore possibilities:

1. **Tree Search**: Try different actions, evaluate outcomes, backtrack from dead ends
2. **Problem**: Some actions are irreversible (you can't un-order a pizza)
3. **Solution**: Simulate in a "dream" - the model imagines outcomes before acting

**World Models**: The agent predicts what will happen after each action. In chess, this means simulating opponent responses. For web agents, it means imagining what clicking a button will do.

## Reasoning Models as Planners

Models like DeepSeek-R1 with "thinking" capabilities naturally perform tree search in their chain-of-thought. Given a blocksworld problem, R1 explores multiple paths in its reasoning, finds an optimal solution, then executes the first step.

**The Overthinking Problem**: A recent paper "The Danger of Overthinking" found that reasoning models sometimes think too much. They'll endlessly deliberate about what clicking a button might do, when they could just... click it and see. Some give up without trying, concluding "I probably can't do this."

## Key Takeaways

1. AI agents use LLMs to pursue goals through multi-step interactions with environments
2. Memory systems (read/write/reflect) prevent context overflow and enable learning
3. Tool use extends LLM capabilities but requires trust calibration
4. Planning is possible but limited - complex constraints need specialized solvers
5. Reasoning models help with planning but can overthink simple problems
:::

:::zh
本文整理自台灣大學李宏毅教授的「生成式AI時代下的機器學習(2025)」課程。

## 什麼是AI Agent？

李老師開場先聲明：每個人對「AI agent」的定義都不同。有人堅持只有有身體的機器人才算。這堂課聚焦在特定的定義。

**傳統AI互動**：人類給明確指令 → AI執行該指令 → 結束。

**AI Agent互動**：人類給目標 → AI自己想辦法透過多個步驟達成，並根據環境變化調整。

AI agent必須能夠：
- 觀察目前狀態（observation）
- 決定採取什麼行動
- 執行行動
- 觀察新狀態
- 重複直到達成目標

## 從RL Agent到LLM Agent

**AlphaGo作為Agent**：目標是贏棋，observation是棋盤狀態，action是選擇落子位置。經典的強化學習方法。

**RL的侷限**：每個任務都要訓練一個模型。AlphaGo會下圍棋，但不會下西洋棋，除非另外訓練。

**LLM的突破**：如果直接用語言模型當agent呢？目標變成文字提示，observation變成文字描述（或多模態模型的圖片），action以文字生成後轉譯成可執行指令。

AI agent再次爆紅，不是因為有什麼agent專屬的新技術，而是LLM變得夠強，可能可以當通用型agent使用。

## LLM真的會下棋嗎？

2022年（ChatGPT之前的上古時代），研究者測試語言模型下西洋棋。結果：沒有模型給出正確答案，較弱的模型甚至不懂規則。

快轉到2025年：有人讓ChatGPT o1對戰DeepSeek-R1下西洋棋。結果是...一團混亂。兩個模型都走違規棋步——把兵當馬用、主教無視障礙物、棋子憑空出現。DeepSeek最後用自己變出來的城堡吃掉自己的兵，宣布獲勝，ChatGPT居然接受認輸。

目前的LLM還是不會正確下棋，但這不代表他們不能在其他任務上當有用的agent。

## AI Agent的三大關鍵能力

### 1. 從經驗學習（記憶）

天真的做法：每次決策前把所有歷史都餵給agent。問題：到第一萬步時，上下文長到不可能處理。

這就像「超憶症」——能記住人生每個細節的人。聽起來像超能力，但實際上被歸類為疾病。這些人難以做抽象思考，因為腦中塞滿瑣碎細節。

**解決方案：長期記憶配合RAG**

- **Write模組**：決定什麼值得記住（不是每個observation都重要）
- **Read模組**：檢索與當前情境相關的記憶（像RAG）
- **Reflection模組**：從原始記憶中綜合出更高層次的洞見

ChatGPT已經實作了這個。你可以跟他說「記下來」，他就會存到記憶中。你甚至可以在設定中查看和管理這些記憶。

**StreamBench的關鍵發現**：負面例子幫助不大。告訴模型「這是錯的」不如告訴他「這樣做才對」有效。不要告訴LLM不要做什麼——告訴他要做什麼。

### 2. 使用工具

LLM有侷限。他們無法可靠地做數學、取得即時資訊、或與外部系統互動。解決方案：給他們工具。

**工具使用的運作方式**：
1. 用文字描述定義工具（函數名稱、參數、功能）
2. 模型決定何時呼叫工具
3. 工具回傳結果
4. 模型將結果納入回應

**範例**：無法直接處理音訊的語音模型可以呼叫語音轉文字工具，然後處理轉錄文字。

**工具選擇問題**：有上百個工具時，不可能全部描述在prompt中。解決方案：把工具描述當作記憶庫，用RAG選擇相關工具。

**LLM甚至可以自己創造工具**：他們寫函數、測試，如果成功就加入工具箱供未來使用。

### 3. LLM太相信工具了嗎？

還記得Google的AI Overview建議用膠水黏披薩起司嗎？那來自Reddit上一個鄉民的玩笑，模型當真了。

但LLM也不是完全好騙。測試：告訴模型高雄現在100°C——他會報告。告訴他10,000°C——他會認出這不可能，拒絕工具輸出。

**什麼樣的外部資訊對LLM有說服力？**

- 越接近模型既有信念的資訊越有說服力
- 對自己答案信心越低的模型越容易被動搖
- LLM傾向相信AI寫的內容勝過人類寫的（即使是不同的AI系統）
- 較新的發布日期更受信任
- 設計較好的網頁模板更有說服力

### 4. 規劃能力

LLM會規劃嗎？早期測試顯示他們可以把「刷牙」這樣的任務分解成步驟。但這可能只是從訓練資料記住的模式。

**神秘方塊世界測試**：不用正常的積木堆疊規則，改用奇怪的操作如「攻擊」、「吞噬」、「征服」。記住正常方塊世界的模型在這裡會失敗。

結果：GPT-4在神秘方塊世界只有9%正確率（正常版有30%+）。但o1-mini和o1-preview有明顯進步，顯示是真正的推理而非模式匹配。

**旅遊規劃Benchmark**：要求模型在預算限制下規劃旅行。2024年初，即使GPT-4 Turbo也只有0.6%成功率。常見失敗：在飛機起飛後還安排行程、無法調整計畫以符合預算限制。

有了適當的solver工具，成功率跳到90%+。教訓：LLM可以規劃，但複雜的約束滿足需要專門工具。

## 樹搜尋與世界模型

要改善規劃，讓agent探索可能性：

1. **樹搜尋**：嘗試不同行動、評估結果、從死路回溯
2. **問題**：有些行動不可逆（你不能取消已訂的披薩）
3. **解決方案**：在「夢境」中模擬——模型在行動前想像結果

**世界模型**：agent預測每個行動後會發生什麼。下棋時，這意味著模擬對手回應。對網頁agent，這意味著想像點擊按鈕會發生什麼。

## 推理模型作為規劃者

像DeepSeek-R1這樣有「思考」能力的模型，在思維鏈中自然會進行樹搜尋。給他方塊世界問題，R1會在推理中探索多條路徑，找到最佳解，然後執行第一步。

**想太多的問題**：最近一篇論文「The Danger of Overthinking」發現推理模型有時想太多。他們會無止盡地思考點擊按鈕會發生什麼，但其實可以直接...點一下看看。有些甚至不嘗試就放棄，結論是「我大概做不到」。

## 重點整理

1. AI agent用LLM透過多步驟與環境互動來追求目標
2. 記憶系統（read/write/reflect）防止上下文溢出並實現學習
3. 工具使用擴展LLM能力但需要信任校準
4. 規劃是可能的但有限——複雜約束需要專門solver
5. 推理模型有助於規劃但可能對簡單問題想太多
:::
