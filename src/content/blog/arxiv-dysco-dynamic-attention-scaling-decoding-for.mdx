---
title:
  en: "DySCO: Dynamic Attention-Scaling Decoding for Long-Context Language Models"
  zh: "DySCO: 面向长文本语言模型的动态注意力缩放解码"
description:
  en: "A training-free decoding algorithm that dynamically rescales attention weights using retrieval heads to improve long-context reasoning, achieving up to 25% relative gains on 128K context benchmarks."
  zh: "一种无需训练的解码算法,通过检索头动态调整注意力权重以提升长文本推理能力,在128K上下文基准测试中实现高达25%的相对性能提升。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.22175](https://arxiv.org/abs/2602.22175)
**Authors**: Xi Ye, Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen
**Categories**: cs.CL

## Abstract

Long-context understanding remains a critical challenge for language models despite increasing context window sizes. This paper introduces DySCO (Dynamic Attention-Scaling Decoding), a novel training-free decoding algorithm that addresses the attention misalignment problem in long-context reasoning. DySCO identifies and leverages "retrieval heads"—specialized attention heads that excel at locating relevant information across long contexts—to dynamically up-weight task-relevant tokens during each decoding step. This approach enables models to maintain focus on pertinent context throughout generation without requiring additional training. Evaluated on challenging benchmarks like MRCR and LongBenchV2 at 128K context length, DySCO demonstrates consistent improvements across multiple instruction-tuned and reasoning models, achieving relative performance gains of up to 25% with only modest computational overhead.

## Key Contributions

- **Training-free decoding algorithm**: DySCO can be applied directly to any pre-trained language model without fine-tuning or additional training
- **Retrieval head identification**: Systematic method for identifying attention heads specialized in long-context retrieval across different model architectures
- **Dynamic attention rescaling**: Novel mechanism that adjusts attention weights at each decoding step based on retrieval-head guidance
- **Substantial performance gains**: Up to 25% relative improvement on long-context reasoning benchmarks at 128K tokens with modest compute overhead
- **Interpretability insights**: Analysis revealing how attention behavior changes during decoding and why certain heads are effective for retrieval

## Methodology

DySCO operates through a two-stage process: retrieval head identification and dynamic attention scaling during decoding.

**Retrieval Head Identification**: The method first identifies which attention heads in a model are specialized for long-context retrieval. This is done by analyzing attention patterns on validation examples where the model must locate specific information within long contexts. Heads that consistently assign high attention weights to ground-truth relevant tokens are classified as retrieval heads. This identification is performed once per model and can be reused across different tasks.

**Dynamic Attention Scaling**: During generation, at each decoding step $t$, DySCO:
1. Uses the identified retrieval heads to compute relevance scores for all input tokens
2. Aggregates these scores to identify the most task-relevant tokens
3. Applies an up-weighting factor to the attention logits for these tokens across all attention heads
4. Proceeds with standard decoding using the rescaled attention distribution

The scaling factor $\alpha$ controls the degree of up-weighting and can be tuned based on the task. The key insight is that retrieval heads provide a reliable signal for which context is relevant, and explicitly amplifying attention to these regions helps the model maintain focus throughout generation.

## Experimental Results

DySCO was evaluated on multiple long-context benchmarks spanning different reasoning types:

**MRCR (Multi-hop Reasoning over Contexts)**: On this challenging multi-hop QA benchmark at 128K context length, DySCO achieved relative improvements of 20-25% across models including Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. The gains were particularly pronounced on examples requiring multiple reasoning steps across distant context segments.

**LongBenchV2**: Across the diverse tasks in this benchmark suite, DySCO showed consistent improvements of 10-18% relative gain. Performance improvements were observed across question answering, summarization, and code completion tasks, demonstrating the method's generality.

**Scaling with Context Length**: Experiments at 32K, 64K, and 128K tokens revealed that DySCO's benefits increase with context length. At 32K, improvements were modest (5-8%), while at 128K, gains reached 20-25%, suggesting the method is particularly valuable for extremely long contexts where attention misalignment is most severe.

**Computational Overhead**: DySCO adds approximately 15-20% to inference time, primarily from the additional forward pass needed to compute retrieval head scores. This overhead is modest compared to the performance gains achieved.

## Analysis and Insights

**Ablation Studies**: The authors conducted extensive ablations showing that both components—retrieval head guidance and dynamic rescaling—are essential. Using random heads instead of retrieval heads reduced gains by 60-70%, while static (non-dynamic) rescaling was only half as effective as the full method.

**Attention Visualization**: Analysis of attention patterns revealed that without DySCO, models often "drift" during generation, gradually shifting attention away from relevant context toward more recent tokens. DySCO counteracts this drift by continuously reinforcing attention to task-relevant regions identified by retrieval heads.

**Retrieval Head Characteristics**: Across different model families, retrieval heads were found predominantly in middle-to-late layers and exhibited distinct attention patterns—they tend to have more uniform attention distributions over long ranges rather than focusing on local context. This suggests these heads have learned to perform a different computational role than typical attention heads.

**Failure Cases**: DySCO shows diminishing returns when the base model's retrieval heads are poorly calibrated or when the task requires reasoning over information that is uniformly distributed across the entire context rather than concentrated in specific segments.

## Takeaways

1. Attention misalignment during decoding is a significant bottleneck for long-context reasoning, and explicitly managing attention can yield substantial improvements without model retraining.

2. Language models naturally develop specialized "retrieval heads" that can be identified and leveraged to guide attention toward relevant context during generation.

3. Dynamic, per-step attention rescaling is more effective than static approaches, as relevance shifts throughout the generation process.

4. The method's effectiveness scales with context length, making it particularly valuable for applications requiring extremely long context windows (100K+ tokens).

5. Training-free decoding algorithms like DySCO offer a practical path to improving long-context capabilities of existing models without the computational cost of retraining or fine-tuning.
:::

:::zh
**论文**: [2602.22175](https://arxiv.org/abs/2602.22175)
**作者**: Xi Ye, Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen
**分类**: cs.CL

## 摘要

尽管语言模型的上下文窗口不断扩大,长文本理解仍然是一个关键挑战。本文提出了DySCO(动态注意力缩放解码),这是一种新颖的无需训练的解码算法,用于解决长文本推理中的注意力错位问题。DySCO识别并利用"检索头"——专门擅长在长上下文中定位相关信息的注意力头——在每个解码步骤动态提升任务相关token的权重。这种方法使模型能够在整个生成过程中保持对相关上下文的关注,而无需额外训练。在MRCR和LongBenchV2等具有挑战性的128K上下文长度基准测试中,DySCO在多个指令微调和推理模型上展现出一致的改进,实现了高达25%的相对性能提升,且计算开销适中。

## 主要贡献

- **无需训练的解码算法**: DySCO可直接应用于任何预训练语言模型,无需微调或额外训练
- **检索头识别**: 提出系统化方法识别不同模型架构中专门用于长文本检索的注意力头
- **动态注意力重缩放**: 创新机制,基于检索头的指导在每个解码步骤调整注意力权重
- **显著的性能提升**: 在128K token的长文本推理基准测试中实现高达25%的相对改进,计算开销适中
- **可解释性洞察**: 分析揭示了解码过程中注意力行为的变化,以及某些注意力头在检索任务中有效的原因

## 方法论

DySCO通过两阶段过程运作:检索头识别和解码过程中的动态注意力缩放。

**检索头识别**: 该方法首先识别模型中哪些注意力头专门用于长文本检索。这通过分析验证样本上的注意力模式来完成,在这些样本中模型必须在长上下文中定位特定信息。那些持续对真实相关token分配高注意力权重的注意力头被分类为检索头。这种识别对每个模型执行一次,可在不同任务中重复使用。

**动态注意力缩放**: 在生成过程中,在每个解码步骤$t$,DySCO执行以下操作:
1. 使用已识别的检索头计算所有输入token的相关性分数
2. 聚合这些分数以识别最相关的任务token
3. 对所有注意力头中这些token的注意力logits应用上调权重因子
4. 使用重缩放后的注意力分布进行标准解码

缩放因子$\alpha$控制上调权重的程度,可根据任务进行调整。关键洞察是检索头为哪些上下文相关提供了可靠信号,显式放大对这些区域的注意力有助于模型在整个生成过程中保持专注。

## 实验结果

DySCO在多个涵盖不同推理类型的长文本基准测试中进行了评估:

**MRCR(跨上下文多跳推理)**: 在这个具有挑战性的128K上下文长度多跳问答基准测试中,DySCO在包括Llama-3.1-8B-Instruct和Qwen2.5-7B-Instruct在内的模型上实现了20-25%的相对改进。在需要跨越远距离上下文片段进行多步推理的样本上,性能提升尤为显著。

**LongBenchV2**: 在该基准测试套件的多样化任务中,DySCO显示出10-18%的一致相对提升。在问答、摘要和代码补全任务中都观察到性能改进,展示了该方法的通用性。

**随上下文长度的扩展**: 在32K、64K和128K token的实验表明,DySCO的优势随上下文长度增加而增大。在32K时,改进较为温和(5-8%),而在128K时,提升达到20-25%,表明该方法对于注意力错位最严重的超长上下文特别有价值。

**计算开销**: DySCO使推理时间增加约15-20%,主要来自计算检索头分数所需的额外前向传播。相比所实现的性能提升,这一开销是适中的。

## 分析与洞察

**消融研究**: 作者进行了广泛的消融实验,表明检索头指导和动态重缩放两个组件都是必不可少的。使用随机头而非检索头会使性能提升减少60-70%,而静态(非动态)重缩放的效果仅为完整方法的一半。

**注意力可视化**: 对注意力模式的分析揭示,在没有DySCO的情况下,模型在生成过程中经常"漂移",逐渐将注意力从相关上下文转移到更近期的token。DySCO通过持续强化对检索头识别的任务相关区域的注意力来抵消这种漂移。

**检索头特征**: 在不同模型家族中,检索头主要分布在中后层,并表现出独特的注意力模式——它们倾向于在长范围内具有更均匀的注意力分布,而不是专注于局部上下文。这表明这些注意力头学习到了与典型注意力头不同的计算角色。

**失败案例**: 当基础模型的检索头校准不佳,或当任务需要对均匀分布在整个上下文中而非集中在特定片段的信息进行推理时,DySCO显示出收益递减。

## 要点总结

1. 解码过程中的注意力错位是长文本推理的重要瓶颈,显式管理注意力可以在不重新训练模型的情况下产生显著改进。

2. 语言模型自然发展出专门的"检索头",可以被识别并用于在生成过程中引导注意力指向相关上下文。

3. 动态的、逐步的注意力重缩放比静态方法更有效,因为相关性在整个生成过程中会发生变化。

4. 该方法的有效性随上下文长度扩展,使其对需要超长上下文窗口(100K+token)的应用特别有价值。

5. 像DySCO这样的无需训练的解码算法为改进现有模型的长文本能力提供了实用路径,无需重新训练或微调的计算成本。
:::
