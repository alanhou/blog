---
title:
  en: "Emergent Search and Backtracking in Latent Reasoning Models"
  zh: "潜在推理模型中涌现的搜索与回溯"
description:
  en: "Discovers that latent reasoning transformers spontaneously develop structured search with exploration, tentative selection, and backtracking in activation space"
  zh: "发现潜在推理Transformer自发发展出结构化搜索，在激活空间中进行探索、试探性选择和回溯"
date: 2026-02-10
tags: ["arxiv", "ai", "reasoning", "latent-reasoning", "interpretability", "chain-of-thought", "cs.CL", "cs.LG"]
image: "/arxiv-visuals/arxiv-emergent-search-backtracking-latent.png"
---

:::en
**Paper**: [2602.08100](https://arxiv.org/abs/2602.08100)
**Authors**: Jasmine Cui, Charles Ye
**Categories**: cs.CL, cs.LG

## Abstract

This paper investigates how language models perform reasoning in hidden continuous space. The authors discover that latent reasoning transformers spontaneously develop structured search behaviors: exploration across candidate answers, tentative selection, and either convergence or reversal (backtracking). Backtracking occurs in approximately one-third of cases and significantly improves accuracy. When distractors are replaced with implausible alternatives, the exploration phase shortens. The key finding is that "latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover."

## Key Contributions

- **Discovery of emergent search**: Latent reasoning models spontaneously develop structured search without explicit training for it
- **Backtracking mechanism**: Models can reverse tentative selections, occurring in ~1/3 of cases with significant accuracy benefits
- **Exploration-convergence dynamics**: A clear two-phase pattern of exploration followed by convergence or reversal
- **Distractor sensitivity**: The exploration phase adapts to problem difficulty, shortening when distractors are implausible

## Background: Latent Reasoning

Unlike chain-of-thought (CoT) models that reason through explicit token generation, latent reasoning models perform computation in continuous activation space across multiple transformer layers or recurrent steps. The reasoning process is entirely hidden -- no intermediate tokens are produced.

This raises a fundamental question: what computational strategies do these models develop internally? Do they simply pattern-match, or do they develop structured problem-solving approaches?

## Methodology

### Experimental Setup

The authors train latent reasoning transformers on tasks requiring search among candidate answers. They then analyze the internal activations to understand the reasoning process.

### Probing Internal States

At each reasoning step $t$, the authors project the model's hidden state $h_t$ onto the representation space of candidate answers:

$$s_i(t) = \text{sim}(h_t, e_i)$$

where $e_i$ is the embedding of candidate answer $i$ and $\text{sim}$ is a similarity measure (e.g., cosine similarity). This creates a trajectory showing which candidate the model is "considering" at each step.

### Phase Identification

The trajectories reveal distinct phases:

1. **Exploration phase**: The model's internal state moves between multiple candidates, with similarity scores fluctuating
2. **Selection phase**: The model tentatively commits to one candidate, with its similarity score rising
3. **Convergence or reversal**: Either the selection stabilizes (convergence) or the model abandons it and switches (backtracking)

## Key Findings

### Emergent Search Structure

Without any explicit training signal for search behavior, the models develop a clear explore-then-exploit pattern:

- Early reasoning steps show high entropy across candidates
- The model systematically evaluates different options
- A tentative selection emerges as one candidate's similarity score rises above others

### Backtracking Behavior

In approximately one-third of cases, the model exhibits backtracking:

1. The model initially commits to candidate $A$ (similarity score rises)
2. At some step $t^*$, the model reverses course
3. The similarity to $A$ drops while similarity to candidate $B$ rises
4. The model ultimately converges on $B$

This backtracking is not random -- it occurs when the initial selection was incorrect, and the reversal leads to the correct answer with high probability. The accuracy on backtracking cases is significantly higher than if the model had stuck with its initial selection.

### Distractor Effects

When implausible distractors replace plausible ones:

- The exploration phase shortens significantly
- The model spends fewer steps evaluating obviously wrong candidates
- Convergence happens earlier, suggesting the model recognizes answer plausibility

This demonstrates that the exploration is genuinely evaluative, not merely a fixed computational routine.

### Comparison with Chain-of-Thought

The paper draws a striking parallel:

| Aspect | Chain-of-Thought | Latent Reasoning |
|--------|-----------------|------------------|
| Medium | Token space | Activation space |
| Exploration | Generating candidate reasoning paths | Internal state visiting candidate representations |
| Backtracking | "Wait, let me reconsider..." | Reversal in similarity trajectories |
| Error recovery | Explicit self-correction in text | Implicit state reversal |

The core insight: latent reasoning models achieve the same functional capabilities as CoT -- the ability to explore, make tentative commitments, detect errors, and recover -- but entirely within continuous activation space.

### Quantitative Analysis

- **Backtracking frequency**: ~33% of problems exhibit backtracking
- **Accuracy with backtracking**: Significantly higher than without on the same problem types
- **Exploration length**: Correlates with problem difficulty
- **Convergence stability**: Once the model converges after backtracking, it rarely reverses again

## Implications

### For Interpretability

The discovery of structured search in activation space provides a new lens for understanding latent reasoning:

- Internal computations are not opaque -- they follow interpretable patterns
- Probing techniques can reveal the model's "thought process" even without explicit reasoning tokens
- The exploration-convergence pattern may be a universal feature of learned reasoning

### For Model Design

Understanding emergent search suggests design improvements:

- Providing sufficient reasoning depth (layers/steps) is critical for the exploration phase
- Models may benefit from architectures that facilitate backtracking
- The number of reasoning steps needed scales with problem difficulty

### For the CoT vs. Latent Reasoning Debate

This work suggests that CoT and latent reasoning are not fundamentally different strategies but rather the same computational pattern expressed in different media. The key capability -- being wrong, noticing, and recovering -- can emerge in either setting.

## Significance

This paper provides compelling evidence that structured reasoning behaviors emerge naturally in latent reasoning models without explicit supervision. The discovery of backtracking in activation space is particularly striking, as it demonstrates that error correction -- previously thought to require explicit verbalization -- can occur entirely in continuous hidden states. This work bridges the gap between interpretable CoT reasoning and efficient latent computation, suggesting that the benefits of "thinking out loud" can be achieved silently.
:::

:::zh
**论文**: [2602.08100](https://arxiv.org/abs/2602.08100)
**作者**: Jasmine Cui, Charles Ye
**分类**: cs.CL, cs.LG

## 摘要

本文研究语言模型如何在隐藏的连续空间中执行推理。作者发现潜在推理Transformer自发发展出结构化搜索行为：跨候选答案的探索、试探性选择，以及收敛或逆转（回溯）。回溯发生在约三分之一的情况中，并显著提高准确率。当干扰项被替换为不合理的替代项时，探索阶段缩短。关键发现是："潜在推理模型在激活空间中实现了思维链通过文字实现的功能：犯错、注意到错误并恢复的能力。"

## 主要贡献

- **涌现搜索的发现**：潜在推理模型在没有显式训练的情况下自发发展出结构化搜索
- **回溯机制**：模型可以逆转试探性选择，发生在约1/3的情况中，带来显著的准确率提升
- **探索-收敛动态**：清晰的两阶段模式——探索后收敛或逆转
- **干扰项敏感性**：探索阶段适应问题难度，当干扰项不合理时缩短

## 背景：潜在推理

与通过显式token生成进行推理的思维链（CoT）模型不同，潜在推理模型在跨多个Transformer层或循环步骤的连续激活空间中执行计算。推理过程完全隐藏——不产生中间token。

这引出了一个根本问题：这些模型在内部发展了什么计算策略？它们只是简单的模式匹配，还是发展了结构化的问题解决方法？

## 方法论

### 实验设置

作者在需要在候选答案中搜索的任务上训练潜在推理Transformer，然后分析内部激活以理解推理过程。

### 探测内部状态

在每个推理步骤 $t$，作者将模型的隐藏状态 $h_t$ 投影到候选答案的表示空间：

$$s_i(t) = \text{sim}(h_t, e_i)$$

其中 $e_i$ 是候选答案 $i$ 的嵌入，$\text{sim}$ 是相似度度量（如余弦相似度）。这创建了一个轨迹，显示模型在每一步"考虑"哪个候选。

### 阶段识别

轨迹揭示了不同的阶段：

1. **探索阶段**：模型的内部状态在多个候选之间移动，相似度分数波动
2. **选择阶段**：模型试探性地承诺一个候选，其相似度分数上升
3. **收敛或逆转**：选择稳定（收敛）或模型放弃并切换（回溯）

## 关键发现

### 涌现的搜索结构

在没有任何显式搜索行为训练信号的情况下，模型发展出清晰的探索-利用模式：

- 早期推理步骤在候选之间显示高熵
- 模型系统地评估不同选项
- 当一个候选的相似度分数超过其他候选时，试探性选择出现

### 回溯行为

在约三分之一的情况中，模型表现出回溯：

1. 模型最初承诺候选 $A$（相似度分数上升）
2. 在某个步骤 $t^*$，模型逆转方向
3. 与 $A$ 的相似度下降，而与候选 $B$ 的相似度上升
4. 模型最终收敛于 $B$

这种回溯不是随机的——它发生在初始选择不正确时，逆转以高概率导向正确答案。回溯情况下的准确率显著高于模型坚持初始选择的情况。

### 干扰项效应

当不合理的干扰项替换合理的干扰项时：

- 探索阶段显著缩短
- 模型花更少的步骤评估明显错误的候选
- 收敛更早发生，表明模型识别答案的合理性

这证明探索是真正的评估性的，而不仅仅是固定的计算例程。

### 与思维链的比较

论文画出了一个引人注目的平行对比：

| 方面 | 思维链 | 潜在推理 |
|------|--------|---------|
| 媒介 | Token空间 | 激活空间 |
| 探索 | 生成候选推理路径 | 内部状态访问候选表示 |
| 回溯 | "等等，让我重新考虑..." | 相似度轨迹中的逆转 |
| 错误恢复 | 文本中的显式自我纠正 | 隐式状态逆转 |

核心洞察：潜在推理模型实现了与CoT相同的功能能力——探索、做出试探性承诺、检测错误和恢复的能力——但完全在连续激活空间内。

### 定量分析

- **回溯频率**：约33%的问题表现出回溯
- **回溯准确率**：在相同问题类型上显著高于无回溯
- **探索长度**：与问题难度相关
- **收敛稳定性**：一旦模型在回溯后收敛，很少再次逆转

## 启示

### 对可解释性

在激活空间中发现结构化搜索为理解潜在推理提供了新视角：

- 内部计算不是不透明的——它们遵循可解释的模式
- 探测技术可以揭示模型的"思考过程"，即使没有显式推理token
- 探索-收敛模式可能是学习推理的普遍特征

### 对模型设计

理解涌现搜索暗示了设计改进：

- 提供足够的推理深度（层/步骤）对探索阶段至关重要
- 模型可能受益于促进回溯的架构
- 所需的推理步骤数量随问题难度而扩展

### 对CoT与潜在推理的辩论

这项工作表明CoT和潜在推理不是根本不同的策略，而是在不同媒介中表达的相同计算模式。关键能力——犯错、注意到并恢复——可以在任一设置中涌现。

## 意义

本文提供了令人信服的证据，表明结构化推理行为在没有显式监督的情况下自然地在潜在推理模型中涌现。在激活空间中发现回溯特别引人注目，因为它证明了错误纠正——以前被认为需要显式语言化——可以完全在连续隐藏状态中发生。这项工作弥合了可解释的CoT推理和高效潜在计算之间的差距，表明"大声思考"的好处可以静默地实现。
:::
