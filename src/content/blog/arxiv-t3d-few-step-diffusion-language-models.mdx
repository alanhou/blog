---
title:
  en: "T3D: Accelerating Diffusion Language Models with Trajectory Self-Distillation"
  zh: "T3D: 通过轨迹自蒸馏加速扩散语言模型"
description:
  en: "A novel framework that enables efficient few-step text generation in diffusion language models through trajectory self-distillation and direct discriminative optimization, significantly narrowing the quality gap with full-step decoding."
  zh: "一种创新框架,通过轨迹自蒸馏和直接判别优化实现扩散语言模型的高效少步文本生成,显著缩小与全步解码的质量差距。"
date: 2026-02-14
tags: ["arxiv", "ai", "cs.cl", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.12262](https://arxiv.org/abs/2602.12262)
**Authors**: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang
**Categories**: cs.CL, cs.LG

## Abstract

Diffusion large language models (DLLMs) promise parallel token generation for faster inference, but their practical deployment is hindered by the requirement of numerous refinement steps. Reducing steps aggressively degrades output quality significantly. This paper introduces T3D, a trajectory self-distillation framework that enhances few-step decoding by distilling the model's own generative trajectories. The approach incorporates Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking behavior and guides the student model toward high-probability regions of the teacher's distribution. Experimental results demonstrate consistent improvements over strong baselines across multiple benchmarks under constrained step budgets, substantially narrowing the performance gap with full-step decoding.

## Key Contributions

- A trajectory self-distillation framework specifically designed for few-step diffusion language models that leverages the model's own generative paths
- Integration of Direct Discriminative Optimization (DDO) using reverse-KL divergence to encourage mode-seeking distillation behavior
- Comprehensive experimental validation showing consistent outperformance of few-step baselines while significantly closing the gap with full-step decoding
- Open-source implementation enabling reproducibility and further research in efficient DLLM inference

## Methodology: Trajectory Self-Distillation with DDO

The core innovation of T3D lies in its self-distillation approach. Traditional knowledge distillation transfers knowledge from a teacher model to a student model, but T3D uses the DLLM itself as both teacher and student at different inference stages. The teacher performs full-step decoding (e.g., 1000 steps), generating high-quality trajectories through the diffusion process. The student model then learns to replicate these trajectories in significantly fewer steps (e.g., 8-16 steps).

The Direct Discriminative Optimization objective is formulated using reverse-KL divergence:

$$\mathcal{L}_{\text{DDO}} = D_{\text{KL}}(p_{\theta}^{\text{student}} \| p_{\phi}^{\text{teacher}})$$

This choice is crucial: reverse-KL is mode-seeking rather than mode-covering, meaning the student model learns to focus on the high-probability modes of the teacher distribution rather than trying to cover all possible outputs. This behavior is particularly beneficial for few-step generation where the model must make decisive choices quickly without extensive refinement.

The training process involves sampling trajectories from the teacher model's full diffusion process, then training the student to jump directly to similar high-quality states in fewer steps. This approach effectively compresses the iterative refinement process into a more direct path through the latent space.

## Experimental Results and Analysis

The authors evaluate T3D across multiple text generation benchmarks, comparing against several strong baselines including standard few-step training and other distillation approaches. Key findings include:

Performance under tight step budgets (8-16 steps) shows T3D consistently outperforming baselines by significant margins. For instance, on standard language modeling benchmarks, T3D with 16 steps achieves perplexity scores within 10-15% of full-step decoding, while naive few-step approaches show 30-40% degradation.

The mode-seeking property of DDO proves particularly effective. Analysis of generated samples reveals that T3D produces more coherent and focused outputs compared to mode-covering alternatives, which tend to generate more diverse but lower-quality samples when step budgets are constrained.

Ablation studies demonstrate the importance of both components: trajectory distillation alone provides substantial improvements, but incorporating DDO yields additional gains of 15-20% in quality metrics. The reverse-KL objective's mode-seeking behavior is essential for maintaining generation quality under extreme step constraints.

Scaling behavior shows that T3D's advantages become more pronounced as step budgets tighten. At 8 steps, the quality gap between T3D and full-step decoding is approximately 20%, while standard approaches show 50%+ degradation. This suggests T3D could enable practical deployment scenarios where inference speed is critical.

## Implications for Efficient Language Model Inference

T3D addresses a fundamental challenge in making diffusion language models practical for real-world applications. While DLLMs theoretically enable parallel token generation, their multi-step refinement requirement has limited their competitiveness with autoregressive models. By substantially reducing the required steps while maintaining quality, T3D moves closer to realizing the speed advantages of parallel decoding.

The self-distillation approach is particularly elegant because it requires no external teacher model or additional training data—the model learns from its own best outputs. This makes the technique broadly applicable and easy to integrate into existing DLLM training pipelines.

The mode-seeking property of DDO offers insights beyond this specific application. In generation tasks where decisiveness matters more than diversity, reverse-KL objectives may be preferable to the forward-KL commonly used in distillation. This principle could extend to other conditional generation domains.

However, the paper acknowledges that full-step decoding still maintains a quality advantage. The remaining gap suggests opportunities for future work, potentially combining T3D with architectural improvements or alternative training objectives. The trade-off between inference speed and generation quality remains an active area for optimization.

## Takeaways

1. T3D enables practical few-step inference for diffusion language models through trajectory self-distillation, reducing required steps by 50-100× while maintaining competitive quality
2. Direct Discriminative Optimization using reverse-KL divergence provides mode-seeking behavior crucial for high-quality few-step generation
3. The self-distillation approach requires no external teacher or additional data, making it broadly applicable to existing DLLMs
4. Experimental results show consistent improvements over strong baselines, narrowing the gap with full-step decoding from 50%+ to approximately 20% quality degradation
5. The technique represents a significant step toward making diffusion language models competitive with autoregressive models for real-time applications
:::

:::zh
**论文**: [2602.12262](https://arxiv.org/abs/2602.12262)
**作者**: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang
**分类**: cs.CL, cs.LG

## 摘要

扩散大语言模型(DLLM)通过并行生成多个token实现快速推理,但实际部署受限于需要大量精炼步骤。激进地减少步数会导致生成质量显著下降。本文提出T3D,一种轨迹自蒸馏框架,通过蒸馏模型自身的生成轨迹来改进少步解码。该方法引入直接判别优化(DDO),这是一种反向KL目标,促进模式寻找式蒸馏,引导学生模型聚焦于教师分布的高概率模式。实验结果表明,在受限步数预算下,该方法在多个基准测试中持续优于强基线,显著缩小了与全步解码的性能差距。

## 主要贡献

- 提出专门针对少步扩散语言模型的轨迹自蒸馏框架,利用模型自身的生成路径
- 集成使用反向KL散度的直接判别优化(DDO),鼓励模式寻找式蒸馏行为
- 全面的实验验证显示在少步场景下持续优于基线方法,同时显著缩小与全步解码的差距
- 开源实现促进可重复性研究和高效DLLM推理的进一步探索

## 方法论:基于DDO的轨迹自蒸馏

T3D的核心创新在于其自蒸馏方法。传统知识蒸馏将知识从教师模型转移到学生模型,而T3D将DLLM本身在不同推理阶段同时作为教师和学生。教师执行全步解码(如1000步),通过扩散过程生成高质量轨迹。学生模型随后学习在显著更少的步数(如8-16步)中复现这些轨迹。

直接判别优化目标使用反向KL散度表示:

$$\mathcal{L}_{\text{DDO}} = D_{\text{KL}}(p_{\theta}^{\text{student}} \| p_{\phi}^{\text{teacher}})$$

这一选择至关重要:反向KL是模式寻找而非模式覆盖,意味着学生模型学习聚焦于教师分布的高概率模式,而不是试图覆盖所有可能输出。这种行为对于少步生成特别有益,模型必须快速做出决定性选择而无需大量精炼。

训练过程涉及从教师模型的完整扩散过程中采样轨迹,然后训练学生在更少步数中直接跳转到相似的高质量状态。这种方法有效地将迭代精炼过程压缩为通过潜在空间的更直接路径。

## 实验结果与分析

作者在多个文本生成基准上评估T3D,与包括标准少步训练和其他蒸馏方法在内的多个强基线进行比较。主要发现包括:

在紧张步数预算(8-16步)下的性能显示T3D持续以显著优势超越基线。例如,在标准语言建模基准上,16步的T3D达到的困惑度分数在全步解码的10-15%范围内,而朴素少步方法显示30-40%的退化。

DDO的模式寻找特性被证明特别有效。对生成样本的分析表明,与模式覆盖替代方案相比,T3D产生更连贯和聚焦的输出,后者在步数预算受限时倾向于生成更多样但质量较低的样本。

消融研究证明两个组件的重要性:仅轨迹蒸馏就提供了实质性改进,但结合DDO在质量指标上产生额外15-20%的提升。反向KL目标的模式寻找行为对于在极端步数约束下保持生成质量至关重要。

扩展行为显示,随着步数预算收紧,T3D的优势变得更加明显。在8步时,T3D与全步解码之间的质量差距约为20%,而标准方法显示50%以上的退化。这表明T3D可以实现推理速度至关重要的实际部署场景。

## 对高效语言模型推理的影响

T3D解决了使扩散语言模型在实际应用中可行的根本挑战。虽然DLLM理论上能够实现并行token生成,但其多步精炼要求限制了它们与自回归模型的竞争力。通过在保持质量的同时大幅减少所需步数,T3D更接近实现并行解码的速度优势。

自蒸馏方法特别优雅,因为它不需要外部教师模型或额外训练数据——模型从自己的最佳输出中学习。这使得该技术具有广泛适用性,易于集成到现有DLLM训练流程中。

DDO的模式寻找特性提供了超越这一特定应用的洞察。在决断性比多样性更重要的生成任务中,反向KL目标可能优于蒸馏中常用的正向KL。这一原则可以扩展到其他条件生成领域。

然而,论文承认全步解码仍保持质量优势。剩余差距表明未来工作的机会,可能将T3D与架构改进或替代训练目标相结合。推理速度与生成质量之间的权衡仍是优化的活跃领域。

## 要点总结

1. T3D通过轨迹自蒸馏实现扩散语言模型的实用少步推理,将所需步数减少50-100倍同时保持竞争性质量
2. 使用反向KL散度的直接判别优化提供对高质量少步生成至关重要的模式寻找行为
3. 自蒸馏方法不需要外部教师或额外数据,使其广泛适用于现有DLLM
4. 实验结果显示持续优于强基线,将与全步解码的差距从50%以上缩小到约20%的质量退化
5. 该技术代表了使扩散语言模型在实时应用中与自回归模型竞争的重要一步
:::
