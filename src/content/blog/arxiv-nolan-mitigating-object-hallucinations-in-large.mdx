---
title:
  en: "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors"
  zh: "NoLan: 通过动态抑制语言先验缓解大型视觉-语言模型中的物体幻觉"
description:
  en: "A training-free framework that reduces object hallucinations in LVLMs by dynamically suppressing language decoder priors, achieving significant accuracy improvements across multiple models."
  zh: "一个无需训练的框架,通过动态抑制语言解码器先验来减少大型视觉-语言模型中的物体幻觉,在多个模型上实现显著的准确率提升。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cv", "cs.ai", "cs.cl"]
image: "/arxiv-visuals/nolan-mitigating-object-hallucinations-in-large/HeroScene.png"
---

![Concept animation](/arxiv-visuals/nolan-mitigating-object-hallucinations-in-large/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/nolan-mitigating-object-hallucinations-in-large/HeroScene.png)



:::en
**Paper**: [2602.22144](https://arxiv.org/abs/2602.22144)
**Authors**: Lingfeng Ren, Weihao Yu, Runpeng Yu, Xinchao Wang
**Categories**: cs.CV, cs.AI, cs.CL

## Abstract

Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate descriptions of objects that are not present in the input image. This paper investigates the fundamental question of which component in the LVLM pipeline—the vision encoder or the language decoder—primarily contributes to these hallucinations. Through systematic experimentation, the authors demonstrate that object hallucinations are predominantly driven by strong language priors in the decoder rather than visual perception failures. Based on this insight, they propose NoLan (No-Language-Hallucination Decoding), a training-free framework that dynamically suppresses language priors by modulating the output distribution based on differences between multimodal and text-only inputs. Experimental results show substantial improvements across various LVLMs, with accuracy gains of up to 6.45 and 7.21 on the POPE benchmark for LLaVA-1.5 7B and Qwen-VL 7B respectively.

## Key Contributions

- Systematic analysis identifying language decoder priors as the primary source of object hallucinations in LVLMs, rather than vision encoder limitations
- NoLan framework: a simple, training-free decoding strategy that dynamically suppresses language priors during generation
- Modulation mechanism based on output distribution differences between multimodal ($p_{\text{LVLM}}$) and text-only ($p_{\text{LLM}}$) inputs
- Comprehensive evaluation demonstrating consistent hallucination reduction across multiple LVLMs and benchmarks
- Open-source implementation enabling easy integration with existing models

## Methodology

The NoLan framework operates during the decoding phase without requiring any model retraining or fine-tuning. The core mechanism involves computing a refined output distribution by comparing the LVLM's multimodal predictions with the language model's text-only predictions.

The refined probability distribution is computed as:

$$p_{\text{NoLan}}(y_t | y_{<t}, x_v, x_t) = p_{\text{LVLM}}(y_t | y_{<t}, x_v, x_t) - \alpha \cdot \Delta p_t$$

where $\Delta p_t = p_{\text{LVLM}}(y_t | y_{<t}, x_v, x_t) - p_{\text{LLM}}(y_t | y_{<t}, x_t)$ represents the distribution difference, and $\alpha$ is a dynamic modulation coefficient.

The key insight is that when the LVLM's prediction strongly aligns with the text-only LLM prediction, it suggests the model is relying heavily on language priors rather than visual information. By suppressing these language-driven predictions, NoLan encourages the model to ground its outputs more firmly in the visual input.

The dynamic modulation coefficient $\alpha$ is adjusted based on the magnitude of $\Delta p_t$, allowing the framework to adaptively control the suppression strength. When the distribution difference is large (indicating strong visual grounding), suppression is minimal. When the difference is small (indicating language prior dominance), suppression increases.

## Experimental Results

NoLan demonstrates consistent and substantial improvements across multiple benchmarks and model architectures:

**POPE Benchmark**: On the Polling-based Object Probing Evaluation, NoLan achieves accuracy improvements of 6.45 points for LLaVA-1.5 7B and 7.21 points for Qwen-VL 7B. These gains represent significant reductions in false positive object mentions.

**Cross-Model Generalization**: The framework shows effectiveness across diverse LVLM architectures including LLaVA variants, Qwen-VL, and InstructBLIP, indicating that the language prior issue is a common phenomenon rather than model-specific.

**Task Versatility**: Beyond object existence verification, NoLan improves performance on various vision-language tasks including image captioning, visual question answering, and detailed image description, demonstrating broad applicability.

**Ablation Studies**: The authors validate the importance of dynamic modulation versus fixed suppression coefficients, showing that adaptive adjustment based on distribution differences is crucial for optimal performance. Static suppression can over-correct and harm performance on samples where language priors are appropriate.

## Implications and Future Directions

This work provides important insights into the mechanisms underlying object hallucinations in LVLMs. By identifying language priors as the primary culprit, it shifts focus from improving vision encoders to better balancing multimodal information during decoding.

The training-free nature of NoLan makes it immediately applicable to existing deployed models without requiring expensive retraining. This practical advantage could accelerate adoption in production systems where hallucination mitigation is critical.

However, several questions remain open. The optimal balance between language priors and visual grounding may vary across different task types and domains. Future work could explore task-adaptive modulation strategies or learn the suppression dynamics from data.

Additionally, while NoLan addresses object hallucinations, other types of hallucinations (attribute errors, relationship mistakes, contextual inconsistencies) may require complementary approaches. Investigating whether similar prior suppression strategies apply to these phenomena represents a promising research direction.

The framework also raises interesting questions about the role of language priors in multimodal learning more broadly. Are strong language priors inherently problematic, or do they serve important functions in certain contexts? Understanding this trade-off could inform future LVLM architecture designs.

## Takeaways

1. Object hallucinations in LVLMs stem primarily from strong language decoder priors rather than vision encoder failures, challenging common assumptions about the source of these errors.

2. NoLan provides a training-free solution that can be immediately applied to existing models, making hallucination mitigation practical and accessible without expensive retraining.

3. Dynamic modulation of language prior suppression based on distribution differences between multimodal and text-only predictions is more effective than static suppression strategies.

4. The framework achieves substantial improvements across multiple models and benchmarks, with accuracy gains exceeding 6-7 points on POPE for popular LVLMs like LLaVA-1.5 and Qwen-VL.

5. This work opens new research directions in understanding and controlling the balance between language priors and visual grounding in multimodal models, with implications for future architecture design and training strategies.
:::

:::zh
**论文**: [2602.22144](https://arxiv.org/abs/2602.22144)
**作者**: Lingfeng Ren, Weihao Yu, Runpeng Yu, Xinchao Wang
**分类**: cs.CV, cs.AI, cs.CL

## 摘要

物体幻觉是大型视觉-语言模型(LVLMs)中的一个关键挑战,模型会生成输入图像中并不存在的物体描述。本文探究了LVLM流程中哪个组件——视觉编码器还是语言解码器——是导致这些幻觉的主要因素这一根本问题。通过系统性实验,作者证明物体幻觉主要由解码器中的强语言先验驱动,而非视觉感知失败。基于这一洞察,他们提出了NoLan(无语言幻觉解码),这是一个无需训练的框架,通过基于多模态输入和纯文本输入之间的输出分布差异来调制,动态抑制语言先验。实验结果显示在多个LVLM上取得显著改进,在POPE基准测试中,LLaVA-1.5 7B和Qwen-VL 7B的准确率分别提升了6.45和7.21。

## 主要贡献

- 系统性分析识别出语言解码器先验是LVLM中物体幻觉的主要来源,而非视觉编码器的局限性
- NoLan框架:一个简单的、无需训练的解码策略,在生成过程中动态抑制语言先验
- 基于多模态($p_{\text{LVLM}}$)和纯文本($p_{\text{LLM}}$)输入之间输出分布差异的调制机制
- 全面评估证明在多个LVLM和基准测试中持续减少幻觉现象
- 开源实现使得与现有模型的集成变得简单易行

## 方法论

NoLan框架在解码阶段运行,无需任何模型重训练或微调。核心机制涉及通过比较LVLM的多模态预测与语言模型的纯文本预测来计算精炼的输出分布。

精炼的概率分布计算如下:

$$p_{\text{NoLan}}(y_t | y_{<t}, x_v, x_t) = p_{\text{LVLM}}(y_t | y_{<t}, x_v, x_t) - \alpha \cdot \Delta p_t$$

其中$\Delta p_t = p_{\text{LVLM}}(y_t | y_{<t}, x_v, x_t) - p_{\text{LLM}}(y_t | y_{<t}, x_t)$表示分布差异,$\alpha$是动态调制系数。

关键洞察在于,当LVLM的预测与纯文本LLM预测高度一致时,这表明模型严重依赖语言先验而非视觉信息。通过抑制这些语言驱动的预测,NoLan鼓励模型将其输出更牢固地建立在视觉输入上。

动态调制系数$\alpha$根据$\Delta p_t$的幅度进行调整,使框架能够自适应地控制抑制强度。当分布差异较大时(表明强视觉基础),抑制最小。当差异较小时(表明语言先验占主导),抑制增加。

## 实验结果

NoLan在多个基准测试和模型架构上展示了一致且显著的改进:

**POPE基准测试**:在基于轮询的物体探测评估中,NoLan使LLaVA-1.5 7B的准确率提升了6.45个百分点,Qwen-VL 7B提升了7.21个百分点。这些提升代表了假阳性物体提及的显著减少。

**跨模型泛化**:该框架在包括LLaVA变体、Qwen-VL和InstructBLIP在内的多种LVLM架构上显示出有效性,表明语言先验问题是一个普遍现象而非特定模型的问题。

**任务多样性**:除了物体存在性验证,NoLan还改进了各种视觉-语言任务的性能,包括图像描述、视觉问答和详细图像描述,展示了广泛的适用性。

**消融研究**:作者验证了动态调制相对于固定抑制系数的重要性,表明基于分布差异的自适应调整对于最优性能至关重要。静态抑制可能过度校正,在语言先验适当的样本上损害性能。

## 影响与未来方向

这项工作为理解LVLM中物体幻觉的底层机制提供了重要见解。通过识别语言先验为主要原因,它将焦点从改进视觉编码器转移到在解码过程中更好地平衡多模态信息。

NoLan的无需训练特性使其可以立即应用于现有部署的模型,无需昂贵的重训练。这一实用优势可能加速在幻觉缓解至关重要的生产系统中的采用。

然而,一些问题仍然悬而未决。语言先验和视觉基础之间的最优平衡可能因不同任务类型和领域而异。未来工作可以探索任务自适应调制策略或从数据中学习抑制动态。

此外,虽然NoLan解决了物体幻觉,但其他类型的幻觉(属性错误、关系错误、上下文不一致)可能需要互补方法。研究类似的先验抑制策略是否适用于这些现象代表了一个有前景的研究方向。

该框架还提出了关于语言先验在多模态学习中更广泛作用的有趣问题。强语言先验本质上是有问题的,还是在某些情境下发挥重要功能?理解这种权衡可以为未来的LVLM架构设计提供信息。

## 要点总结

1. LVLM中的物体幻觉主要源于强语言解码器先验而非视觉编码器失败,挑战了关于这些错误来源的常见假设。

2. NoLan提供了一个无需训练的解决方案,可以立即应用于现有模型,使幻觉缓解变得实用且易于实现,无需昂贵的重训练。

3. 基于多模态和纯文本预测之间分布差异的语言先验抑制动态调制比静态抑制策略更有效。

4. 该框架在多个模型和基准测试中实现了显著改进,在POPE上为LLaVA-1.5和Qwen-VL等流行LVLM带来超过6-7个百分点的准确率提升。

5. 这项工作开辟了理解和控制多模态模型中语言先验与视觉基础之间平衡的新研究方向,对未来架构设计和训练策略具有重要意义。
:::
