---
title:
  en: "LAD: Learning Advantage Distribution for Reasoning"
  zh: "LAD: 学习优势分布以增强推理能力"
description:
  en: "A distribution-matching framework that learns advantage distributions instead of maximizing expected rewards, improving both accuracy and diversity in LLM reasoning tasks."
  zh: "一种分布匹配框架,通过学习优势分布而非最大化期望奖励,提升大语言模型推理任务的准确性和多样性。"
date: 2026-02-24
tags: ["arxiv", "ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-lad-learning-advantage-distribution-for-reasoning.png"
---

:::en
**Paper**: [2602.20132](https://arxiv.org/abs/2602.20132)
**Authors**: Wendi Li, Sharon Li
**Categories**: cs.LG

## Abstract

Current reinforcement learning approaches for large language model reasoning focus on maximizing expected rewards, which can cause overfitting to dominant reward signals and neglect valid alternative reasoning paths. This paper introduces Learning Advantage Distributions (LAD), a distribution-matching framework that learns the advantage-induced distribution rather than simply maximizing advantages. By formulating the objective as minimizing an $f$-divergence between policy-induced and advantage-induced distributions, LAD increases likelihood for high-advantage responses while preventing over-confident probability growth. The method requires no additional training cost compared to GRPO and demonstrates improved accuracy and generative diversity across math and code reasoning benchmarks.

## Key Contributions

- Introduces a distribution-matching framework that learns advantage distributions instead of maximizing expected rewards
- Establishes theoretical equivalence between optimal policy updates and advantage-based target distributions
- Derives a practical objective using $f$-divergence minimization that naturally prevents mode collapse without entropy regularization
- Demonstrates zero additional training cost compared to existing methods like GRPO
- Validates the approach in controlled bandit settings showing faithful recovery of multimodal advantage distributions
- Shows consistent improvements in both accuracy and diversity across multiple LLM backbones on math and code tasks

## Methodology and Theoretical Framework

The core innovation of LAD lies in reframing the reinforcement learning objective from advantage maximization to distribution matching. Traditional RL objectives for LLM reasoning optimize:

$$\max_\pi \mathbb{E}_{y \sim \pi(\cdot|x)}[A(x, y)]$$

where $A(x, y)$ represents the advantage function. This formulation tends to concentrate probability mass on the highest-reward trajectories, potentially ignoring valid alternatives.

LAD instead defines a target distribution proportional to the advantage:

$$p^*(y|x) \propto \exp(\beta \cdot A(x, y))$$

The learning objective becomes minimizing an $f$-divergence between the policy distribution $\pi(y|x)$ and this advantage-induced distribution $p^*(y|x)$. This formulation yields gradient updates that:

1. Increase likelihood for responses with positive advantages
2. Suppress over-confident probability growth through the divergence structure
3. Naturally maintain exploration of the advantage landscape

The practical implementation uses a reverse KL divergence formulation, resulting in gradient updates that scale with advantage values while incorporating a regularization term that prevents probability collapse. Critically, this regularization emerges from the divergence structure itself rather than requiring auxiliary entropy terms.

## Experimental Results and Analysis

The authors validate LAD across multiple dimensions. In a controlled multi-armed bandit setting with known multimodal advantage distributions, LAD successfully recovers the true distribution shape, demonstrating that the theoretical framework translates to practice.

On practical reasoning tasks, LAD shows consistent improvements:

- **Math reasoning**: Tested on mathematical problem-solving benchmarks across different LLM backbones, LAD improves both pass rates and solution diversity
- **Code generation**: On coding tasks, the method maintains or improves functional correctness while generating more diverse solution approaches
- **Scaling behavior**: The improvements hold across model sizes, suggesting the approach scales naturally with model capacity

Importantly, LAD achieves these gains without additional computational cost during training compared to baseline methods like GRPO. The distribution-matching objective integrates seamlessly into standard policy gradient frameworks used for LLM post-training.

The diversity improvements are particularly notable. By learning the full advantage distribution rather than collapsing to single high-reward modes, LAD-trained models generate varied reasoning paths that remain valid. This suggests the method successfully addresses the exploration-exploitation tradeoff that plagues reward-maximization approaches.

## Implications for LLM Reasoning

LAD represents a shift in how we think about optimizing reasoning in large language models. Rather than treating reasoning as a single-path optimization problem, the distribution-matching perspective acknowledges that multiple valid reasoning trajectories exist and should be preserved during training.

This has several practical implications:

**Robustness**: Models that maintain diverse reasoning strategies may be more robust to distribution shift and edge cases where the dominant strategy fails.

**Interpretability**: Preserving multiple reasoning paths provides richer insight into model behavior and decision-making processes.

**Composability**: Diverse reasoning capabilities may compose more effectively with other techniques like ensemble methods or multi-step reasoning frameworks.

The theoretical grounding in distribution matching also opens avenues for future work. The $f$-divergence framework is flexible, allowing different divergence choices to encode different inductive biases about the desired policy distribution. The connection to advantage distributions suggests potential synergies with other RL techniques that reason about value distributions rather than expected values.

## Takeaways

1. Learning advantage distributions rather than maximizing expected rewards prevents overfitting to dominant reward signals while maintaining valid alternative reasoning paths
2. The distribution-matching framework naturally prevents mode collapse without requiring auxiliary entropy regularization terms
3. LAD incurs zero additional training cost compared to standard policy gradient methods like GRPO
4. Experimental validation shows consistent improvements in both accuracy and diversity across math and code reasoning tasks
5. The theoretical framework establishes a principled connection between optimal policy updates and advantage-based target distributions
6. The approach scales naturally to large language model post-training and works across different model architectures
:::

:::zh
**论文**: [2602.20132](https://arxiv.org/abs/2602.20132)
**作者**: Wendi Li, Sharon Li
**分类**: cs.LG

## 摘要

当前用于大语言模型推理的强化学习方法主要关注最大化期望奖励,这可能导致对主导奖励信号的过拟合,忽略其他有效的推理路径。本文提出了学习优势分布(LAD)框架,这是一种分布匹配方法,通过学习优势诱导的分布而非简单地最大化优势值。通过将目标表述为最小化策略诱导分布与优势诱导分布之间的$f$-散度,LAD在提高高优势响应的似然的同时防止过度自信的概率增长。该方法相比GRPO没有额外的训练成本,并在数学和代码推理基准测试中展现了准确性和生成多样性的提升。

## 主要贡献

- 提出了一种分布匹配框架,学习优势分布而非最大化期望奖励
- 建立了最优策略更新与基于优势的目标分布之间的理论等价性
- 推导出使用$f$-散度最小化的实用目标,无需熵正则化即可自然防止模式坍缩
- 证明相比现有方法如GRPO没有额外的训练成本
- 在受控的多臂老虎机设置中验证了方法能够忠实恢复多模态优势分布
- 在多个大语言模型主干网络上的数学和代码任务中展示了准确性和多样性的一致提升

## 方法论与理论框架

LAD的核心创新在于将强化学习目标从优势最大化重新定义为分布匹配。传统的大语言模型推理强化学习目标优化:

$$\max_\pi \mathbb{E}_{y \sim \pi(\cdot|x)}[A(x, y)]$$

其中$A(x, y)$表示优势函数。这种表述倾向于将概率质量集中在最高奖励的轨迹上,可能忽略有效的替代方案。

LAD转而定义一个与优势成比例的目标分布:

$$p^*(y|x) \propto \exp(\beta \cdot A(x, y))$$

学习目标变为最小化策略分布$\pi(y|x)$与这个优势诱导分布$p^*(y|x)$之间的$f$-散度。这种表述产生的梯度更新能够:

1. 增加具有正优势的响应的似然
2. 通过散度结构抑制过度自信的概率增长
3. 自然地保持对优势景观的探索

实际实现使用反向KL散度表述,产生的梯度更新随优势值缩放,同时包含防止概率坍缩的正则化项。关键的是,这种正则化来自散度结构本身,而不需要辅助的熵项。

## 实验结果与分析

作者从多个维度验证了LAD。在具有已知多模态优势分布的受控多臂老虎机设置中,LAD成功恢复了真实的分布形状,证明理论框架能够转化为实践。

在实际推理任务上,LAD展现出一致的改进:

- **数学推理**: 在不同大语言模型主干网络的数学问题求解基准测试中,LAD提高了通过率和解决方案多样性
- **代码生成**: 在编码任务中,该方法保持或提高了功能正确性,同时生成更多样化的解决方案
- **扩展行为**: 改进在不同模型规模上都成立,表明该方法随模型容量自然扩展

重要的是,LAD在训练期间相比GRPO等基线方法没有额外的计算成本。分布匹配目标无缝集成到用于大语言模型后训练的标准策略梯度框架中。

多样性的改进尤其值得注意。通过学习完整的优势分布而非坍缩到单一高奖励模式,LAD训练的模型生成的推理路径保持有效且多样化。这表明该方法成功解决了困扰奖励最大化方法的探索-利用权衡问题。

## 对大语言模型推理的影响

LAD代表了我们思考大语言模型推理优化方式的转变。分布匹配视角承认存在多条有效的推理轨迹,并且应该在训练期间保留,而不是将推理视为单路径优化问题。

这具有几个实际意义:

**鲁棒性**: 保持多样化推理策略的模型可能对分布偏移和主导策略失败的边缘情况更加鲁棒。

**可解释性**: 保留多条推理路径为模型行为和决策过程提供了更丰富的洞察。

**可组合性**: 多样化的推理能力可能与集成方法或多步推理框架等其他技术更有效地组合。

分布匹配的理论基础也为未来工作开辟了途径。$f$-散度框架具有灵活性,允许不同的散度选择来编码关于期望策略分布的不同归纳偏置。与优势分布的联系表明与其他推理价值分布而非期望值的强化学习技术存在潜在协同作用。

## 要点总结

1. 学习优势分布而非最大化期望奖励可以防止对主导奖励信号的过拟合,同时保持有效的替代推理路径
2. 分布匹配框架无需辅助熵正则化项即可自然防止模式坍缩
3. LAD相比GRPO等标准策略梯度方法没有额外的训练成本
4. 实验验证显示在数学和代码推理任务中准确性和多样性都有一致的提升
5. 理论框架在最优策略更新与基于优势的目标分布之间建立了原则性联系
6. 该方法自然扩展到大语言模型后训练,并适用于不同的模型架构
:::
