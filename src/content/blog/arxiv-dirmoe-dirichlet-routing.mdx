---
title:
  en: "DirMoE: Dirichlet-routed Mixture of Experts"
  zh: "DirMoE：基于Dirichlet路由的混合专家模型"
description:
  en: "A novel MoE routing mechanism using Dirichlet variational autoencoder framework that separates expert activation from contribution distribution decisions"
  zh: "一种使用Dirichlet变分自编码器框架的新型MoE路由机制，将专家激活与贡献分配决策分离"
date: 2026-02-10
tags: ["arxiv", "ai", "mixture-of-experts", "moe", "routing", "efficiency", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.09001](https://arxiv.org/abs/2602.09001)
**Authors**: Amirhossein Vahidi, Hesam Asadollahzadeh, Navid Akhavan Attar, Marie Moullet, Kevin Ly, Xingyi Yang, Mohammad Lotfollahi
**Categories**: cs.LG

## Abstract

This paper proposes DirMoE, a novel routing mechanism for Mixture of Experts (MoE) models that addresses fundamental limitations of the standard Top-k + Softmax routing approach. DirMoE uses a Dirichlet variational autoencoder framework to separate two key routing decisions: which experts to activate and how to distribute contributions among them. The method employs Gumbel-Sigmoid relaxation for expert selection and implicit reparameterization for the Dirichlet distribution, achieving full differentiability. A variability-based objective provides direct sparsity control. DirMoE matches or exceeds existing routing methods while improving expert specialization.

## Key Contributions

- **Decoupled routing decisions**: Separates expert activation (which experts) from contribution allocation (how much each contributes)
- **Dirichlet-based weighting**: Uses the Dirichlet distribution for principled probability allocation over active experts
- **Full differentiability**: Gumbel-Sigmoid and implicit reparameterization enable end-to-end gradient flow
- **Direct sparsity control**: Variability-based objective replaces ad-hoc load balancing losses

## Limitations of Top-k + Softmax Routing

The standard MoE routing mechanism has several well-known problems:

### Coupled Decisions

Top-k + Softmax entangles two distinct decisions:

1. **Which experts to activate** (a discrete selection problem)
2. **How to weight their contributions** (a continuous allocation problem)

By using the same scores for both, the routing cannot independently optimize selection and weighting.

### Load Imbalance

Without auxiliary losses, Top-k routing tends to collapse to using only a few experts, wasting model capacity. The auxiliary load-balancing losses used to prevent this are heuristic and can conflict with the primary training objective.

### Non-Differentiable Selection

The Top-k operation is non-differentiable, meaning gradients cannot flow through the expert selection decision. Only the selected experts receive gradient updates, creating a rich-get-richer dynamic.

## The DirMoE Framework

### Step 1: Expert Selection via Gumbel-Sigmoid

For each token $x$, compute activation logits for each expert $e$:

$$\alpha_e = W_{\text{gate}} \cdot x + b_{\text{gate}}$$

Apply Gumbel-Sigmoid relaxation to obtain differentiable binary activation decisions:

$$z_e = \sigma\left(\frac{\alpha_e + g_1 - g_2}{\tau}\right)$$

where $g_1, g_2 \sim \text{Gumbel}(0, 1)$ are Gumbel noise samples and $\tau$ is the temperature. As $\tau \to 0$, this approaches a hard binary decision; during training, a moderate $\tau$ allows gradient flow.

The activation mask is:

$$m_e = \mathbb{1}[z_e > 0.5] \quad \text{(inference)}, \quad m_e = z_e \quad \text{(training)}$$

### Step 2: Contribution Weights via Dirichlet Distribution

For the active experts, DirMoE uses a Dirichlet distribution to allocate contribution weights. The Dirichlet distribution is the natural choice because:

- It produces vectors that sum to 1 (valid probability distributions)
- It can express both uniform and concentrated allocations
- Its concentration parameters control the degree of specialization

Compute Dirichlet concentration parameters for active experts:

$$\beta_e = \text{softplus}(W_{\text{dir}} \cdot x + b_{\text{dir}}) \cdot m_e$$

Sample contribution weights from the Dirichlet distribution:

$$w \sim \text{Dir}(\beta_1, \beta_2, \ldots, \beta_E)$$

The implicit reparameterization trick enables gradient computation through this sampling step.

### Step 3: Expert Computation

The final output combines expert outputs weighted by the Dirichlet-sampled weights:

$$y = \sum_{e=1}^{E} w_e \cdot m_e \cdot \text{Expert}_e(x)$$

### Variability-Based Sparsity Control

Instead of heuristic load-balancing losses, DirMoE introduces a variability-based objective that directly controls sparsity:

$$\mathcal{L}_{\text{var}} = -\lambda \cdot \text{Var}(\bar{m})$$

where $\bar{m}_e = \mathbb{E}_x[m_e]$ is the average activation rate of expert $e$ across the batch. Maximizing variance in activation rates encourages some experts to be frequently active and others rarely, achieving sparsity. The coefficient $\lambda$ directly controls the sparsity level.

## Theoretical Advantages

### Why Dirichlet?

The Dirichlet distribution $\text{Dir}(\beta_1, \ldots, \beta_K)$ has properties ideally suited for expert weighting:

- **Sum-to-one constraint**: $\sum_e w_e = 1$ is automatically satisfied
- **Concentration control**: Large $\beta_e$ values concentrate weight on expert $e$; uniform $\beta$ gives equal weights
- **Sparse allocations**: When some $\beta_e \ll 1$, the distribution naturally produces sparse weight vectors
- **Smooth interpolation**: Between fully concentrated (one expert dominates) and uniform allocation

### Full Differentiability

The combination of Gumbel-Sigmoid (for selection) and implicit reparameterization (for Dirichlet sampling) ensures that gradients flow through the entire routing mechanism:

$$\frac{\partial \mathcal{L}}{\partial W_{\text{gate}}}, \frac{\partial \mathcal{L}}{\partial W_{\text{dir}}} \neq 0$$

This means all experts receive gradient signal, even those not selected for a particular token, through the Gumbel-Sigmoid relaxation.

## Experimental Results

### Performance

DirMoE matches or exceeds baseline routing methods across multiple benchmarks:

- Comparable or better perplexity on language modeling tasks
- Improved performance on downstream tasks requiring diverse expert knowledge
- More stable training dynamics without the need for auxiliary load-balancing losses

### Expert Specialization

A key advantage of DirMoE is improved expert specialization:

- Experts develop more distinct specializations compared to Top-k routing
- The Dirichlet weighting allows nuanced contribution allocation (not just equal weighting of top-k)
- Reduced redundancy between experts

### Sparsity Control

The variability-based objective provides precise sparsity control:

- Direct relationship between $\lambda$ and the achieved sparsity level
- No need for heuristic tuning of auxiliary loss coefficients
- Smooth trade-off between computation cost and model quality

### Load Balance

Despite not using explicit load-balancing losses:

- DirMoE achieves comparable or better load balance than Top-k with auxiliary losses
- The Gumbel-Sigmoid mechanism naturally distributes activations
- No expert collapse observed during training

## Significance

DirMoE provides a principled solution to the routing problem in MoE models by cleanly separating the two fundamental routing decisions and using appropriate probabilistic tools for each. The Dirichlet distribution is a natural fit for contribution allocation, and the Gumbel-Sigmoid provides differentiable expert selection. The variability-based sparsity control replaces ad-hoc load balancing with a theoretically grounded objective. Together, these innovations yield a routing mechanism that is more principled, more controllable, and empirically competitive with or superior to existing approaches.
:::

:::zh
**论文**: [2602.09001](https://arxiv.org/abs/2602.09001)
**作者**: Amirhossein Vahidi, Hesam Asadollahzadeh, Navid Akhavan Attar, Marie Moullet, Kevin Ly, Xingyi Yang, Mohammad Lotfollahi
**分类**: cs.LG

## 摘要

本文提出DirMoE，一种用于混合专家（MoE）模型的新型路由机制，解决了标准Top-k + Softmax路由方法的根本局限性。DirMoE使用Dirichlet变分自编码器框架将两个关键路由决策分离：激活哪些专家以及如何在它们之间分配贡献。该方法采用Gumbel-Sigmoid松弛进行专家选择，使用隐式重参数化处理Dirichlet分布，实现完全可微分。基于变异性的目标提供直接的稀疏性控制。DirMoE匹配或超越现有路由方法，同时改善专家专业化。

## 主要贡献

- **解耦路由决策**：将专家激活（哪些专家）与贡献分配（每个贡献多少）分离
- **基于Dirichlet的加权**：使用Dirichlet分布在活跃专家上进行有原则的概率分配
- **完全可微分**：Gumbel-Sigmoid和隐式重参数化实现端到端梯度流
- **直接稀疏性控制**：基于变异性的目标替代临时性的负载均衡损失

## Top-k + Softmax路由的局限性

标准MoE路由机制有几个众所周知的问题：

### 耦合决策

Top-k + Softmax将两个不同的决策纠缠在一起：

1. **激活哪些专家**（离散选择问题）
2. **如何加权它们的贡献**（连续分配问题）

通过对两者使用相同的分数，路由无法独立优化选择和加权。

### 负载不平衡

没有辅助损失时，Top-k路由倾向于崩溃为只使用少数专家，浪费模型容量。用于防止这种情况的辅助负载均衡损失是启发式的，可能与主要训练目标冲突。

### 不可微分的选择

Top-k操作不可微分，意味着梯度无法通过专家选择决策流动。只有被选中的专家接收梯度更新，创造了富者愈富的动态。

## DirMoE框架

### 步骤1：通过Gumbel-Sigmoid进行专家选择

对于每个token $x$，计算每个专家 $e$ 的激活logits：

$$\alpha_e = W_{\text{gate}} \cdot x + b_{\text{gate}}$$

应用Gumbel-Sigmoid松弛获得可微分的二元激活决策：

$$z_e = \sigma\left(\frac{\alpha_e + g_1 - g_2}{\tau}\right)$$

其中 $g_1, g_2 \sim \text{Gumbel}(0, 1)$ 是Gumbel噪声样本，$\tau$ 是温度。当 $\tau \to 0$ 时，这接近硬二元决策；训练期间，适中的 $\tau$ 允许梯度流动。

激活掩码为：

$$m_e = \mathbb{1}[z_e > 0.5] \quad \text{（推理）}, \quad m_e = z_e \quad \text{（训练）}$$

### 步骤2：通过Dirichlet分布分配贡献权重

对于活跃专家，DirMoE使用Dirichlet分布分配贡献权重。Dirichlet分布是自然选择，因为：

- 它产生和为1的向量（有效的概率分布）
- 它可以表达均匀和集中的分配
- 其浓度参数控制专业化程度

计算活跃专家的Dirichlet浓度参数：

$$\beta_e = \text{softplus}(W_{\text{dir}} \cdot x + b_{\text{dir}}) \cdot m_e$$

从Dirichlet分布采样贡献权重：

$$w \sim \text{Dir}(\beta_1, \beta_2, \ldots, \beta_E)$$

隐式重参数化技巧使得通过这个采样步骤的梯度计算成为可能。

### 步骤3：专家计算

最终输出结合了由Dirichlet采样权重加权的专家输出：

$$y = \sum_{e=1}^{E} w_e \cdot m_e \cdot \text{Expert}_e(x)$$

### 基于变异性的稀疏性控制

DirMoE引入基于变异性的目标直接控制稀疏性，替代启发式负载均衡损失：

$$\mathcal{L}_{\text{var}} = -\lambda \cdot \text{Var}(\bar{m})$$

其中 $\bar{m}_e = \mathbb{E}_x[m_e]$ 是专家 $e$ 在批次中的平均激活率。最大化激活率的方差鼓励一些专家频繁活跃而其他专家很少活跃，实现稀疏性。系数 $\lambda$ 直接控制稀疏性水平。

## 理论优势

### 为什么选择Dirichlet？

Dirichlet分布 $\text{Dir}(\beta_1, \ldots, \beta_K)$ 具有非常适合专家加权的性质：

- **和为一约束**：$\sum_e w_e = 1$ 自动满足
- **浓度控制**：大的 $\beta_e$ 值将权重集中在专家 $e$ 上；均匀的 $\beta$ 给出等权重
- **稀疏分配**：当某些 $\beta_e \ll 1$ 时，分布自然产生稀疏权重向量
- **平滑插值**：在完全集中（一个专家主导）和均匀分配之间

### 完全可微分

Gumbel-Sigmoid（用于选择）和隐式重参数化（用于Dirichlet采样）的组合确保梯度流过整个路由机制：

$$\frac{\partial \mathcal{L}}{\partial W_{\text{gate}}}, \frac{\partial \mathcal{L}}{\partial W_{\text{dir}}} \neq 0$$

这意味着所有专家都接收梯度信号，即使那些未被特定token选中的专家，也通过Gumbel-Sigmoid松弛接收信号。

## 实验结果

### 性能

DirMoE在多个基准上匹配或超越基线路由方法：

- 在语言建模任务上可比或更好的困惑度
- 在需要多样化专家知识的下游任务上改善的性能
- 更稳定的训练动态，无需辅助负载均衡损失

### 专家专业化

DirMoE的一个关键优势是改善的专家专业化：

- 与Top-k路由相比，专家发展出更独特的专业化
- Dirichlet加权允许细致的贡献分配（不仅仅是top-k的等权重）
- 减少专家之间的冗余

### 稀疏性控制

基于变异性的目标提供精确的稀疏性控制：

- $\lambda$ 与实现的稀疏性水平之间的直接关系
- 无需启发式调整辅助损失系数
- 计算成本和模型质量之间的平滑权衡

### 负载均衡

尽管没有使用显式负载均衡损失：

- DirMoE实现了与带辅助损失的Top-k可比或更好的负载均衡
- Gumbel-Sigmoid机制自然分配激活
- 训练期间未观察到专家崩溃

## 意义

DirMoE通过清晰地分离两个基本路由决策并为每个决策使用适当的概率工具，为MoE模型中的路由问题提供了有原则的解决方案。Dirichlet分布是贡献分配的自然选择，Gumbel-Sigmoid提供可微分的专家选择。基于变异性的稀疏性控制用理论上有根据的目标替代了临时性的负载均衡。这些创新共同产生了一个更有原则、更可控、且在经验上与现有方法竞争或优于现有方法的路由机制。
:::
