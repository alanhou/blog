---
title:
  en: "Sequence to Sequence Learning with Neural Networks"
  zh: "Sequence to Sequence Learning with Neural Networks"
description:
  en: "LSTM encoder-decoder architecture for sequence transformation tasks like machine translation"
  zh: "LSTM encoder-decoder architecture for sequence transformation tasks like machine translation"
date: 2014-09-10
tags: ["arxiv", "seq2seq", "lstm", "machine-translation", "encoder-decoder"]
image: "/arxiv-visuals/arxiv-seq2seq-learning.png"
---

:::zh
## English

### Overview

This paper introduced the sequence-to-sequence (Seq2Seq) framework using LSTM networks. The encoder-decoder architecture maps variable-length input sequences to variable-length output sequences, establishing the foundation for neural machine translation.

### Key Contributions

1. **Encoder-Decoder Architecture**: Two LSTMs for encoding and decoding
2. **Variable Length**: Handles sequences of different lengths
3. **End-to-End Training**: Single model trained on input-output pairs
4. **Reversed Input**: Reversing source sentences improved performance

### Architecture

- **Encoder LSTM**: Reads input sequence, produces fixed-size context vector
- **Decoder LSTM**: Generates output sequence from context vector
- **Teacher Forcing**: Uses ground truth during training

### Impact

Seq2Seq became the standard for:
- Machine translation
- Text summarization
- Dialogue systems
- Any sequence transformation task

Later enhanced with attention mechanisms.

### Paper Link

[arXiv:1409.3215](https://arxiv.org/abs/1409.3215)
:::

:::en
## 中文

### 概述

本文介绍了使用LSTM网络的序列到序列（Seq2Seq）框架。编码器-解码器架构将可变长度的输入序列映射到可变长度的输出序列，为神经机器翻译奠定了基础。

### 主要贡献

1. **编码器-解码器架构**：两个LSTM分别用于编码和解码
2. **可变长度**：处理不同长度的序列
3. **端到端训练**：单一模型在输入-输出对上训练
4. **反转输入**：反转源句子提高了性能

### 架构

- **编码器LSTM**：读取输入序列，产生固定大小的上下文向量
- **解码器LSTM**：从上下文向量生成输出序列
- **教师强制**：训练时使用真实标签

### 影响

Seq2Seq成为以下任务的标准：
- 机器翻译
- 文本摘要
- 对话系统
- 任何序列转换任务

后来通过注意力机制得到增强。

### 论文链接

[arXiv:1409.3215](https://arxiv.org/abs/1409.3215)
:::
