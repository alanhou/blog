---
title:
  en: "Attention Is All You Need: The Paper That Started the AI Revolution"
  zh: "Attention Is All You Need：开启AI革命的论文"
description:
  en: "The foundational 2017 paper introducing the Transformer architecture that powers modern LLMs"
  zh: "2017年奠基性论文，介绍了驱动现代大语言模型的Transformer架构"
date: 2017-06-12
tags: ["arxiv", "ai", "transformer", "attention", "nlp", "deep-learning", "landmark-paper"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [1706.03762](https://arxiv.org/abs/1706.03762)
**Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
**Venue**: NeurIPS 2017
**Categories**: cs.CL, cs.LG

## Abstract

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

## Key Contributions

- **The Transformer Architecture**: A novel neural network that relies entirely on self-attention, eliminating the need for recurrence and convolutions
- **Scaled Dot-Product Attention**: Efficient attention mechanism with scaling factor to prevent gradient issues
- **Multi-Head Attention**: Allows the model to attend to information from different representation subspaces at different positions
- **Positional Encoding**: Sinusoidal functions to inject sequence order information without recurrence

## The Problem with RNNs

Before Transformers, sequence models relied on RNNs (LSTM, GRU):
- **Sequential computation**: Cannot parallelize within training examples
- **Long-range dependencies**: Difficult to learn due to vanishing gradients
- **Memory constraints**: Limit batching across examples at longer sequences

## Transformer Architecture

### Encoder-Decoder Structure
```
Encoder: 6 identical layers
  - Multi-head self-attention
  - Position-wise feed-forward network
  - Residual connections + Layer normalization

Decoder: 6 identical layers
  - Masked multi-head self-attention
  - Multi-head attention over encoder output
  - Position-wise feed-forward network
```

### Scaled Dot-Product Attention

The core attention formula:
```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
```

Why scale by `sqrt(d_k)`? For large `d_k`, dot products grow large, pushing softmax into regions with extremely small gradients.

### Multi-Head Attention

Instead of single attention:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O

where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

With h=8 heads, d_k = d_v = d_model/h = 64

### Position-wise Feed-Forward Networks

Applied to each position separately:
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```
Inner layer dimension: 2048, outer: 512

### Positional Encoding

Since no recurrence, position info added via:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

## Results

### Machine Translation

| Model | EN-DE BLEU | EN-FR BLEU | Training Cost |
|-------|------------|------------|---------------|
| Previous SOTA | 26.4 | 41.0 | - |
| Transformer (base) | 27.3 | 38.1 | 12 hours, 8 GPUs |
| Transformer (big) | **28.4** | **41.8** | 3.5 days, 8 GPUs |

### Why Self-Attention?

Compared to recurrent and convolutional layers:

| Layer Type | Complexity per Layer | Sequential Ops | Max Path Length |
|------------|---------------------|----------------|-----------------|
| Self-Attention | O(n^2 * d) | O(1) | O(1) |
| Recurrent | O(n * d^2) | O(n) | O(n) |
| Convolutional | O(k * n * d^2) | O(1) | O(log_k(n)) |

Self-attention connects all positions with O(1) operations.

## Takeaways

1. **Attention alone is sufficient** - No need for recurrence or convolution for sequence transduction
2. **Parallelization wins** - Training is dramatically faster than RNN-based models
3. **Constant path length** - Any two positions can interact directly, improving long-range dependency learning
4. **Scalability** - The architecture scales well with more data and compute

This paper fundamentally changed NLP and later computer vision, leading to BERT, GPT, and all modern large language models.
:::

:::zh
**论文**: [1706.03762](https://arxiv.org/abs/1706.03762)
**作者**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
**会议**: NeurIPS 2017
**分类**: cs.CL, cs.LG

## 摘要

主流的序列转换模型基于复杂的循环或卷积神经网络，包含编码器和解码器。性能最好的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，完全基于注意力机制，完全摒弃了循环和卷积。

## 主要贡献

- **Transformer架构**：一种完全依赖自注意力的新型神经网络，消除了对循环和卷积的需求
- **缩放点积注意力**：带有缩放因子的高效注意力机制，防止梯度问题
- **多头注意力**：允许模型在不同位置关注来自不同表示子空间的信息
- **位置编码**：使用正弦函数注入序列顺序信息，无需循环

## RNN的问题

在Transformer之前，序列模型依赖RNN（LSTM、GRU）：
- **顺序计算**：无法在训练样本内并行化
- **长程依赖**：由于梯度消失难以学习
- **内存限制**：在较长序列时限制跨样本的批处理

## Transformer架构

### 编码器-解码器结构
```
编码器：6个相同的层
  - 多头自注意力
  - 逐位置前馈网络
  - 残差连接 + 层归一化

解码器：6个相同的层
  - 掩码多头自注意力
  - 对编码器输出的多头注意力
  - 逐位置前馈网络
```

### 缩放点积注意力

核心注意力公式：
```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
```

为什么要除以`sqrt(d_k)`？对于较大的`d_k`，点积会变得很大，将softmax推入梯度极小的区域。

### 多头注意力

不是单一注意力，而是：
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O

其中 head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

使用h=8个头，d_k = d_v = d_model/h = 64

### 逐位置前馈网络

分别应用于每个位置：
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```
内层维度：2048，外层：512

### 位置编码

由于没有循环，通过以下方式添加位置信息：
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

## 实验结果

### 机器翻译

| 模型 | 英德BLEU | 英法BLEU | 训练成本 |
|------|----------|----------|----------|
| 之前SOTA | 26.4 | 41.0 | - |
| Transformer (base) | 27.3 | 38.1 | 12小时，8 GPU |
| Transformer (big) | **28.4** | **41.8** | 3.5天，8 GPU |

### 为什么选择自注意力？

与循环层和卷积层相比：

| 层类型 | 每层复杂度 | 顺序操作 | 最大路径长度 |
|--------|-----------|----------|--------------|
| 自注意力 | O(n^2 * d) | O(1) | O(1) |
| 循环 | O(n * d^2) | O(n) | O(n) |
| 卷积 | O(k * n * d^2) | O(1) | O(log_k(n)) |

自注意力以O(1)操作连接所有位置。

## 要点总结

1. **仅注意力就足够了** - 序列转换不需要循环或卷积
2. **并行化获胜** - 训练速度比基于RNN的模型快得多
3. **恒定路径长度** - 任意两个位置可以直接交互，改善长程依赖学习
4. **可扩展性** - 该架构随着更多数据和计算能力而良好扩展

这篇论文从根本上改变了NLP，后来又改变了计算机视觉，催生了BERT、GPT和所有现代大语言模型。
:::
