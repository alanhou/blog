---
title:
  en: "SARAH: Spatially Aware Real-time Agentic Humans"
  zh: "SARAH: 空间感知的实时智能体人类"
description:
  en: "First real-time, fully causal method for generating spatially-aware conversational motion in VR agents, achieving 300+ FPS with natural gaze and spatial dynamics."
  zh: "首个用于VR智能体的实时全因果空间感知对话动作生成方法,实现300+FPS的自然注视和空间动态。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/arxiv-sarah-spatially-aware-real-time-agentic.png"
---

:::en
**Paper**: [2602.18432](https://arxiv.org/abs/2602.18432)
**Authors**: Evonne Ng, Siwei Zhang, Zhang Chen, Michael Zollhoefer, Alexander Richard
**Categories**: cs.CV

## Abstract

SARAH introduces the first real-time, fully causal approach for generating spatially-aware conversational motion in embodied agents. Unlike existing methods that only align gestures with speech, SARAH enables agents to naturally orient toward users, respond to their movements, and maintain appropriate gaze patterns. The system processes user position and dyadic audio to produce full-body motion at over 300 FPS—3x faster than non-causal baselines—making it deployable on streaming VR headsets. The architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. A novel gaze scoring mechanism with classifier-free guidance allows users to adjust eye contact intensity at inference time while the model learns natural spatial alignment from data.

## Key Contributions

- First real-time, fully causal method for spatially-aware conversational motion generation deployable on VR headsets
- Causal transformer-based VAE architecture with interleaved latent tokens enabling streaming inference at 300+ FPS
- Flow matching model conditioned on both user trajectory and audio for spatially-grounded full-body motion
- Gaze scoring mechanism with classifier-free guidance that decouples learning from control, allowing runtime adjustment of eye contact intensity
- State-of-the-art motion quality on Embody 3D dataset while being 3x faster than non-causal baselines
- Validation on live VR system demonstrating practical deployment of spatially-aware conversational agents

## Technical Architecture

The SARAH architecture addresses the fundamental challenge of generating spatially-aware motion in real-time through a two-stage approach. The first stage employs a causal transformer-based Variational Autoencoder (VAE) that processes motion sequences with interleaved latent tokens. This design choice is critical for streaming inference: by interleaving latent representations throughout the sequence rather than using a single global latent, the model can generate motion causally without requiring future context.

The second stage uses a flow matching model that takes the VAE's latent space and conditions it on both audio features and user trajectory. Flow matching provides a more stable training objective compared to traditional diffusion models while maintaining high-quality generation. The conditioning on user trajectory $\mathbf{u}_t$ ensures that the agent's motion remains spatially grounded—the agent naturally turns toward the user, adjusts body orientation based on relative position, and maintains appropriate interpersonal distance.

The gaze mechanism deserves special attention. Rather than directly predicting gaze from the model, SARAH introduces a scoring function that evaluates how well generated motion aligns with desired gaze patterns. This score is then used with classifier-free guidance during inference: $\mathbf{m}_t = \mathbf{m}_t^{\text{uncond}} + w \cdot (\mathbf{m}_t^{\text{cond}} - \mathbf{m}_t^{\text{uncond}})$, where $w$ controls gaze intensity. This decoupling means the model learns natural spatial behavior from data, but users can dial eye contact up or down based on cultural preferences or application requirements.

## Performance and Results

SARAH achieves remarkable performance metrics that make it practical for real-world deployment. The system runs at over 300 FPS on standard VR hardware, which is 3x faster than comparable non-causal baselines. This speed advantage comes from the causal architecture—by never requiring future frames, the model eliminates the latency inherent in bidirectional approaches.

On the Embody 3D dataset, SARAH achieves state-of-the-art motion quality across multiple metrics. The generated motions exhibit natural spatial dynamics: agents smoothly rotate to face users as they move, maintain appropriate gaze patterns during conversation, and produce gestures that are both speech-aligned and spatially coherent. The flow matching formulation contributes to motion smoothness, avoiding the jittery artifacts common in autoregressive approaches.

The gaze scoring mechanism proves particularly effective. By learning from natural conversation data, the model captures subtle patterns like looking away during thought, returning gaze during emphasis, and adjusting eye contact based on interpersonal distance. The classifier-free guidance allows these patterns to be amplified or dampened at inference time without retraining, providing flexibility for different cultural contexts or application scenarios.

## Implications for Embodied AI

SARAH represents a significant step toward truly interactive embodied agents. Previous work in conversational motion generation focused primarily on speech-driven gestures, treating the agent as essentially stationary. This limitation made agents feel disconnected from their spatial environment—they would gesture naturally but fail to acknowledge a user walking around them or maintain appropriate social distance.

By incorporating spatial awareness as a first-class concern, SARAH enables more natural human-agent interaction. In VR telepresence, this means remote participants can be represented by agents that respond naturally to others' movements. In digital human applications, virtual assistants can maintain appropriate social protocols around personal space and eye contact. The real-time performance makes these applications practical on consumer hardware rather than requiring server-side processing.

The decoupled gaze control mechanism has broader implications for controllable generation. Rather than baking control parameters into the model architecture, SARAH demonstrates that scoring functions with classifier-free guidance can provide runtime control while preserving the model's learned natural behavior. This pattern could extend to other aspects of motion control—energy level, formality, or emotional expression—without requiring separate models for each variation.

## Takeaways

1. SARAH achieves the first real-time, fully causal spatially-aware conversational motion generation, running at 300+ FPS on VR hardware—3x faster than non-causal methods.

2. The causal transformer-based VAE with interleaved latent tokens enables streaming inference without future context, critical for low-latency interactive applications.

3. Flow matching conditioned on user trajectory and audio produces spatially-grounded full-body motion that naturally orients toward users and responds to their movement.

4. The gaze scoring mechanism with classifier-free guidance decouples learning from control, allowing users to adjust eye contact intensity at inference time while the model learns natural patterns from data.

5. Validation on live VR systems demonstrates practical deployment, bringing spatially-aware conversational agents from research to real-world applications in telepresence and digital humans.
:::

:::zh
**论文**: [2602.18432](https://arxiv.org/abs/2602.18432)
**作者**: Evonne Ng, Siwei Zhang, Zhang Chen, Michael Zollhoefer, Alexander Richard
**分类**: cs.CV

## 摘要

SARAH提出了首个用于具身智能体的实时全因果空间感知对话动作生成方法。与仅将手势与语音对齐的现有方法不同,SARAH使智能体能够自然地朝向用户、响应用户移动并保持适当的注视模式。该系统处理用户位置和双向音频,以超过300 FPS的速度生成全身动作——比非因果基线快3倍——使其可部署在流式VR头显上。架构结合了基于因果Transformer的VAE(采用交错潜在token实现流式推理)和基于用户轨迹与音频条件的流匹配模型。新颖的注视评分机制配合无分类器引导,将学习与控制解耦,使模型从数据中学习自然的空间对齐,同时用户可在推理时调整眼神接触强度。

## 主要贡献

- 首个可部署在VR头显上的实时全因果空间感知对话动作生成方法
- 基于因果Transformer的VAE架构,采用交错潜在token实现300+ FPS的流式推理
- 同时基于用户轨迹和音频条件的流匹配模型,生成空间定位的全身动作
- 配合无分类器引导的注视评分机制,将学习与控制解耦,允许运行时调整眼神接触强度
- 在Embody 3D数据集上达到最先进的动作质量,同时速度比非因果基线快3倍
- 在实时VR系统上验证,展示空间感知对话智能体的实际部署能力

## 技术架构

SARAH架构通过两阶段方法解决实时生成空间感知动作的根本挑战。第一阶段采用因果Transformer变分自编码器(VAE),处理带有交错潜在token的动作序列。这一设计选择对流式推理至关重要:通过在整个序列中交错潜在表示而非使用单一全局潜在变量,模型可以因果地生成动作而无需未来上下文。

第二阶段使用流匹配模型,该模型接收VAE的潜在空间并基于音频特征和用户轨迹进行条件化。与传统扩散模型相比,流匹配提供更稳定的训练目标,同时保持高质量生成。对用户轨迹$\mathbf{u}_t$的条件化确保智能体的动作保持空间定位——智能体自然地转向用户,根据相对位置调整身体朝向,并保持适当的人际距离。

注视机制值得特别关注。SARAH不是直接从模型预测注视,而是引入评分函数来评估生成的动作与期望注视模式的对齐程度。然后在推理时使用无分类器引导来使用该分数:$\mathbf{m}_t = \mathbf{m}_t^{\text{uncond}} + w \cdot (\mathbf{m}_t^{\text{cond}} - \mathbf{m}_t^{\text{uncond}})$,其中$w$控制注视强度。这种解耦意味着模型从数据中学习自然的空间行为,但用户可以根据文化偏好或应用需求调高或调低眼神接触。

## 性能与结果

SARAH实现了令人瞩目的性能指标,使其适用于实际部署。该系统在标准VR硬件上以超过300 FPS运行,比可比的非因果基线快3倍。这种速度优势来自因果架构——通过永不需要未来帧,模型消除了双向方法固有的延迟。

在Embody 3D数据集上,SARAH在多个指标上达到最先进的动作质量。生成的动作展现自然的空间动态:智能体在用户移动时平滑旋转面向用户,在对话期间保持适当的注视模式,并产生既与语音对齐又空间连贯的手势。流匹配公式有助于动作平滑性,避免自回归方法中常见的抖动伪影。

注视评分机制被证明特别有效。通过从自然对话数据中学习,模型捕捉到微妙的模式,如思考时移开视线、强调时回归注视,以及根据人际距离调整眼神接触。无分类器引导允许在推理时放大或减弱这些模式而无需重新训练,为不同文化背景或应用场景提供灵活性。

## 对具身AI的影响

SARAH代表了向真正交互式具身智能体迈出的重要一步。以往对话动作生成的工作主要关注语音驱动的手势,将智能体视为基本静止的。这种局限使智能体感觉与其空间环境脱节——它们会自然地做手势,但无法识别用户在其周围走动或保持适当的社交距离。

通过将空间感知作为首要关注点,SARAH实现了更自然的人机交互。在VR远程呈现中,这意味着远程参与者可以由对他人移动自然响应的智能体代表。在数字人应用中,虚拟助手可以围绕个人空间和眼神接触维护适当的社交协议。实时性能使这些应用在消费级硬件上变得实用,而非需要服务器端处理。

解耦的注视控制机制对可控生成具有更广泛的影响。SARAH展示了评分函数配合无分类器引导可以提供运行时控制,同时保留模型学习的自然行为,而不是将控制参数嵌入模型架构。这种模式可以扩展到动作控制的其他方面——能量水平、正式程度或情感表达——而无需为每种变化单独训练模型。

## 要点总结

1. SARAH实现了首个实时全因果空间感知对话动作生成,在VR硬件上以300+ FPS运行——比非因果方法快3倍。

2. 采用交错潜在token的因果Transformer VAE实现无需未来上下文的流式推理,这对低延迟交互应用至关重要。

3. 基于用户轨迹和音频条件的流匹配生成空间定位的全身动作,自然地朝向用户并响应其移动。

4. 配合无分类器引导的注视评分机制将学习与控制解耦,允许用户在推理时调整眼神接触强度,同时模型从数据中学习自然模式。

5. 在实时VR系统上的验证展示了实际部署能力,将空间感知对话智能体从研究带入远程呈现和数字人的实际应用。
:::
