---
title:
  en: "SPQ: An Ensemble Technique for Large Language Model Compression"
  zh: "SPQ: 大语言模型压缩的集成技术"
description:
  en: "A novel ensemble compression method combining SVD, pruning, and quantization that achieves 75% memory reduction on LLaMA-2-7B while improving perplexity and maintaining downstream task accuracy."
  zh: "一种结合SVD、剪枝和量化的新型集成压缩方法,在LLaMA-2-7B上实现75%内存减少的同时改善困惑度并保持下游任务准确率。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.18420](https://arxiv.org/abs/2602.18420)
**Authors**: Jiamin Yao, Eren Gultepe
**Categories**: cs.CL

## Abstract

This paper introduces SPQ (SVD-Pruning-Quantization), an ensemble compression technique for large language models that strategically combines three complementary methods: variance-retained singular value decomposition for attention layers, activation-based pruning for MLP layers, and 8-bit post-training quantization across all linear layers. The key insight is that different architectural components exhibit different redundancy patterns, making them amenable to different compression strategies. When applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction (from 13.48 GB to 6.86 GB) while actually improving perplexity on WikiText-2 from 5.47 to 4.91. The method outperforms single-technique baselines and competitive methods like GPTQ and SparseGPT in both memory efficiency and inference speed, achieving up to 1.9x throughput improvement over GPTQ.

## Key Contributions

- An ensemble compression framework that applies different techniques to different model components based on their structural characteristics
- Variance-retained SVD for attention projections that decomposes weight matrices into low-rank factors while preserving critical variance
- Activation-based pruning for MLP layers that removes redundant neurons based on their contribution to model outputs
- Empirical demonstration that combining complementary compression techniques outperforms individual methods at matched compression ratios
- Comprehensive evaluation showing memory reduction, perplexity improvement, and preserved accuracy on downstream tasks (C4, TruthfulQA, GSM8K)

## Methodology: Layer-Aware Compression Strategy

The SPQ framework operates on the principle that different layers in transformer architectures have different redundancy characteristics. The attention mechanism, with its projection matrices ($W_Q$, $W_K$, $W_V$, $W_O$), benefits from low-rank decomposition since these projections often capture lower-dimensional semantic relationships. For a weight matrix $W \in \mathbb{R}^{m \times n}$, SVD decomposes it as:

$$W = U\Sigma V^T \approx U_r\Sigma_r V_r^T$$

where $r \ll \min(m,n)$ is the reduced rank determined by variance retention thresholds.

In contrast, MLP layers exhibit neuron-level redundancy where entire neurons contribute minimally to the output. The pruning strategy computes importance scores based on activation magnitudes across calibration data, removing neurons below a threshold while maintaining model capacity. Finally, 8-bit quantization applies uniformly across all linear layers, mapping floating-point weights to discrete values:

$$W_q = \text{round}\left(\frac{W - \min(W)}{\max(W) - \min(W)} \times 255\right)$$

This layered approach ensures each compression technique targets the inefficiency it handles best, avoiding the limitations of one-size-fits-all methods.

## Experimental Results and Analysis

The experiments on LLaMA-2-7B reveal several important findings. First, at matched compression ratios, SPQ consistently outperforms single-method baselines. For instance, at approximately 50% compression, SPQ achieves perplexity of 4.91 on WikiText-2, compared to 5.23 for SVD-only, 5.67 for pruning-only, and 5.89 for quantization-only approaches. This validates the hypothesis that complementary techniques capture different types of redundancy.

Second, SPQ demonstrates superior memory-accuracy tradeoffs compared to established baselines. Against GPTQ (7.16 GB, perplexity 5.47), SPQ achieves 6.86 GB with perplexity 4.91—both better memory efficiency and better perplexity. Compared to SparseGPT (7.89 GB, perplexity 5.68), the advantages are even more pronounced.

Third, downstream task performance remains robust. On TruthfulQA, SPQ maintains 38.2% accuracy compared to 38.5% for the uncompressed model. On GSM8K, accuracy is 14.6% versus 14.8% baseline. The C4 validation perplexity shows similar trends. This suggests that SPQ's compression preserves the knowledge and reasoning capabilities encoded in the original model.

Fourth, inference throughput improvements are substantial. SPQ achieves 1.9x speedup over GPTQ in token generation, attributed to reduced memory bandwidth requirements and more efficient matrix operations on compressed representations. This makes SPQ particularly attractive for deployment scenarios where both memory and latency matter.

## Implications for LLM Deployment

SPQ addresses a critical bottleneck in deploying large language models: the memory wall. With models like LLaMA-2-7B requiring over 13 GB in FP16 format, deployment on consumer hardware or edge devices becomes impractical. By reducing this to under 7 GB while maintaining or improving quality, SPQ opens new deployment possibilities.

The ensemble approach also provides flexibility. Practitioners can adjust the compression ratio of each component independently based on their specific constraints. For instance, in scenarios with limited memory but abundant compute, one might apply aggressive SVD and pruning with lighter quantization. Conversely, for inference-optimized deployments, heavier quantization might be preferred.

The method's post-training nature is another practical advantage. Unlike methods requiring retraining or fine-tuning, SPQ can be applied to existing pretrained models without access to original training data or compute resources. This democratizes model compression, making it accessible to researchers and practitioners who lack the resources to train large models from scratch.

Finally, the superior performance compared to GPTQ—a widely adopted quantization method—suggests that the field should explore ensemble approaches more systematically. The success of SPQ indicates that the future of model compression may lie not in perfecting individual techniques but in intelligently combining them.

## Takeaways

1. Ensemble compression methods that combine complementary techniques (SVD, pruning, quantization) outperform single-method approaches at matched compression ratios
2. Layer-aware compression strategies that apply different techniques to different architectural components (attention vs. MLP) are more effective than uniform approaches
3. SPQ achieves 75% memory reduction on LLaMA-2-7B (13.48 GB → 6.86 GB) while improving WikiText-2 perplexity from 5.47 to 4.91
4. Compared to GPTQ, SPQ provides better memory efficiency (6.86 GB vs. 7.16 GB), better perplexity, and 1.9x inference speedup
5. Downstream task accuracy remains robust across TruthfulQA, GSM8K, and C4 benchmarks, indicating preserved model capabilities
6. Post-training compression without fine-tuning makes the method practical and accessible for deployment scenarios
7. The success of ensemble approaches suggests future research should focus on intelligently combining compression techniques rather than optimizing individual methods in isolation
:::

:::zh
**论文**: [2602.18420](https://arxiv.org/abs/2602.18420)
**作者**: Jiamin Yao, Eren Gultepe
**分类**: cs.CL

## 摘要

本文提出了SPQ(SVD-Pruning-Quantization),一种针对大语言模型的集成压缩技术,战略性地结合了三种互补方法:用于注意力层的方差保留奇异值分解、用于MLP层的基于激活的剪枝,以及应用于所有线性层的8位训练后量化。核心洞察在于不同的架构组件表现出不同的冗余模式,使它们适合不同的压缩策略。应用于LLaMA-2-7B时,SPQ实现了高达75%的内存减少(从13.48 GB降至6.86 GB),同时实际上改善了WikiText-2上的困惑度,从5.47降至4.91。该方法在内存效率和推理速度方面都优于单一技术基线以及GPTQ和SparseGPT等竞争方法,相比GPTQ实现了高达1.9倍的吞吐量提升。

## 主要贡献

- 提出了一个集成压缩框架,根据模型组件的结构特征对不同组件应用不同技术
- 针对注意力投影的方差保留SVD方法,将权重矩阵分解为低秩因子同时保留关键方差
- 针对MLP层的基于激活的剪枝方法,根据神经元对模型输出的贡献移除冗余神经元
- 实证证明结合互补压缩技术在相同压缩比下优于单一方法
- 全面评估显示内存减少、困惑度改善以及在下游任务(C4、TruthfulQA、GSM8K)上保持的准确率

## 方法论:层级感知压缩策略

SPQ框架基于这样的原理:Transformer架构中的不同层具有不同的冗余特征。注意力机制及其投影矩阵($W_Q$、$W_K$、$W_V$、$W_O$)受益于低秩分解,因为这些投影通常捕获较低维度的语义关系。对于权重矩阵$W \in \mathbb{R}^{m \times n}$,SVD将其分解为:

$$W = U\Sigma V^T \approx U_r\Sigma_r V_r^T$$

其中$r \ll \min(m,n)$是由方差保留阈值确定的降低秩。

相比之下,MLP层表现出神经元级别的冗余,其中整个神经元对输出的贡献很小。剪枝策略基于校准数据上的激活幅度计算重要性分数,移除低于阈值的神经元同时保持模型容量。最后,8位量化统一应用于所有线性层,将浮点权重映射到离散值:

$$W_q = \text{round}\left(\frac{W - \min(W)}{\max(W) - \min(W)} \times 255\right)$$

这种分层方法确保每种压缩技术针对其最擅长处理的低效问题,避免了一刀切方法的局限性。

## 实验结果与分析

在LLaMA-2-7B上的实验揭示了几个重要发现。首先,在匹配的压缩比下,SPQ始终优于单一方法基线。例如,在约50%压缩率下,SPQ在WikiText-2上实现4.91的困惑度,而纯SVD为5.23、纯剪枝为5.67、纯量化为5.89。这验证了互补技术捕获不同类型冗余的假设。

其次,SPQ相比已建立的基线展示了优越的内存-准确率权衡。对比GPTQ(7.16 GB,困惑度5.47),SPQ实现6.86 GB且困惑度4.91——内存效率和困惑度都更好。与SparseGPT(7.89 GB,困惑度5.68)相比,优势更加明显。

第三,下游任务性能保持稳健。在TruthfulQA上,SPQ保持38.2%的准确率,而未压缩模型为38.5%。在GSM8K上,准确率为14.6%对比基线的14.8%。C4验证困惑度显示类似趋势。这表明SPQ的压缩保留了原始模型中编码的知识和推理能力。

第四,推理吞吐量改进显著。SPQ在token生成上实现了相比GPTQ 1.9倍的加速,归因于减少的内存带宽需求和压缩表示上更高效的矩阵运算。这使SPQ对于内存和延迟都重要的部署场景特别有吸引力。

## 对LLM部署的影响

SPQ解决了部署大语言模型的关键瓶颈:内存墙。像LLaMA-2-7B这样的模型在FP16格式下需要超过13 GB,使得在消费级硬件或边缘设备上部署变得不切实际。通过将其减少到7 GB以下同时保持或改善质量,SPQ开启了新的部署可能性。

集成方法还提供了灵活性。实践者可以根据具体约束独立调整每个组件的压缩比。例如,在内存有限但计算充足的场景中,可以应用激进的SVD和剪枝配合较轻的量化。相反,对于推理优化的部署,可能更倾向于更重的量化。

该方法的训练后特性是另一个实用优势。与需要重新训练或微调的方法不同,SPQ可以应用于现有的预训练模型,无需访问原始训练数据或计算资源。这使模型压缩民主化,让缺乏从头训练大模型资源的研究者和实践者也能使用。

最后,相比GPTQ——一种广泛采用的量化方法——的优越性能表明,该领域应该更系统地探索集成方法。SPQ的成功表明,模型压缩的未来可能不在于完善单一技术,而在于智能地组合它们。

## 要点总结

1. 结合互补技术(SVD、剪枝、量化)的集成压缩方法在相同压缩比下优于单一方法
2. 对不同架构组件(注意力vs MLP)应用不同技术的层级感知压缩策略比统一方法更有效
3. SPQ在LLaMA-2-7B上实现75%内存减少(13.48 GB → 6.86 GB),同时将WikiText-2困惑度从5.47改善至4.91
4. 与GPTQ相比,SPQ提供更好的内存效率(6.86 GB vs 7.16 GB)、更好的困惑度和1.9倍推理加速
5. 下游任务准确率在TruthfulQA、GSM8K和C4基准上保持稳健,表明保留了模型能力
6. 无需微调的训练后压缩使该方法对部署场景实用且易于使用
7. 集成方法的成功表明未来研究应关注智能组合压缩技术,而非孤立优化单一方法
:::
