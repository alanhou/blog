---
title:
  en: "Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards"
  zh: "基于可验证奖励的强化学习中的非对称提示加权"
description:
  en: "A novel approach to RL training that prioritizes difficult prompts with low success rates, particularly effective for from-scratch training scenarios like R1-Zero."
  zh: "一种优先处理低成功率困难提示的强化学习训练新方法,特别适用于R1-Zero等从零开始的训练场景。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-asymmetric-prompt-weighting-for-reinforcement-learning.png"
---

:::en
**Paper**: [2602.11128](https://arxiv.org/abs/2602.11128)
**Authors**: Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis
**Categories**: cs.LG

## Abstract

This paper introduces asymmetric prompt weighting for reinforcement learning with verifiable rewards, challenging the conventional wisdom of focusing on prompts with intermediate success probabilities. The authors demonstrate that assigning higher weights to prompts with low or zero empirical success probability significantly accelerates training in from-scratch RL scenarios. Through theoretical analysis, they characterize optimal prompt weights that minimize convergence time under fixed update budgets, showing that in low-success regimes where informative responses are rare, asymmetric weighting becomes optimal by prioritizing difficult prompts.

## Key Contributions

- Introduction of asymmetric prompt weighting that upweights low-success probability prompts, contrasting with existing methods like GRPO, DAPO, and RLOO
- Theoretical characterization of optimal prompt weights that minimize time to reach target accuracy under fixed update budgets
- Empirical demonstration that asymmetric weighting particularly benefits from-scratch RL (R1-Zero style) over post-SFT RL
- Analysis showing that in low-success regimes, where response cost dominates, asymmetric weights accelerate effective-time convergence

## Methodology and Approach

The core innovation lies in rethinking how policy optimization algorithms weight gradients based on prompt difficulty. Traditional algorithms like GRPO (Group Relative Policy Optimization), DAPO, and RLOO focus on "ambiguous prompts" with intermediate success probabilities, effectively downweighting both very easy and very hard prompts. The rationale is that easy prompts provide little learning signal while hard prompts may be too noisy.

This paper challenges that assumption by proposing asymmetric weighting schemes that assign higher weights to prompts with low success rates. The key insight is that in early training stages or from-scratch scenarios, the model traverses a wide accuracy range where difficult prompts contain the most valuable learning signal. The authors formalize this through an optimization framework that considers:

- The fixed budget of model updates available
- The cost of generating responses for different prompts
- The information gain from successful responses on difficult prompts

Their theoretical analysis reveals that when informative responses are rare (low-success regime), the optimal weighting strategy becomes asymmetric, prioritizing prompts where the model currently struggles.

## Experimental Results and Analysis

The empirical evaluation reveals a clear dichotomy between training scenarios:

**From-Scratch RL (R1-Zero style)**: Asymmetric weighting shows substantial benefits. When training starts from a randomly initialized or minimally capable model, the wide accuracy range means the model encounters many low-success prompts. By upweighting these difficult cases, the training process more efficiently allocates compute to the most informative examples, accelerating convergence to target accuracy levels.

**Post-SFT RL**: The benefits are less pronounced. When starting from a supervised fine-tuned model that already achieves high accuracy, most prompts have relatively high success probabilities. The training regime doesn't spend much time in the low-success zone where asymmetric weighting provides advantages.

This finding has important practical implications: the choice of prompt weighting strategy should depend on the initial model capability and training paradigm.

## Theoretical Insights

The paper provides a rigorous theoretical framework for understanding prompt weighting. Under a fixed update budget $B$, the authors characterize weights $w(p)$ as a function of success probability $p$ that minimize expected time to reach target accuracy $p^*$ from initial accuracy $p_0$.

In the low-success regime where $p \ll 1$, the optimal weights satisfy:

$$w(p) \propto \frac{1}{p \cdot c(p)}$$

where $c(p)$ is the cost of generating responses for prompts with success probability $p$. This inverse relationship with success probability creates the asymmetric weighting pattern.

The key intuition: when successful responses are rare, each success on a difficult prompt provides disproportionate value. The cost of generating many failed attempts is justified by the high information content of eventual successes. This contrasts with high-success regimes where the marginal value of additional successes diminishes.

## Implications for LLM Training

This work has several important implications for large language model post-training:

**Training Paradigm Selection**: The effectiveness of asymmetric weighting suggests that from-scratch RL approaches like R1-Zero may benefit from different optimization strategies than post-SFT RL. Training pipelines should adapt their prompt weighting based on initial model capability.

**Compute Allocation**: The theoretical framework provides principled guidance for allocating compute budget across prompts of varying difficulty, potentially improving training efficiency in resource-constrained settings.

**Curriculum Learning Connections**: Asymmetric weighting naturally implements a form of difficulty-based curriculum, but driven by empirical success rates rather than predefined difficulty metrics.

**Reasoning Tasks**: For reasoning tasks with verifiable rewards (mathematical problem-solving, code generation), where success is binary and informative, asymmetric weighting may be particularly valuable during early training phases.

## Takeaways

1. Asymmetric prompt weighting that prioritizes low-success prompts accelerates from-scratch RL training by focusing compute on the most informative examples
2. The benefit of asymmetric weighting diminishes in post-SFT scenarios where models start at high accuracy, suggesting training strategy should adapt to initial capability
3. Theoretical analysis reveals that in low-success regimes, optimal weights are inversely proportional to success probability, justifying the asymmetric approach
4. The framework provides principled guidance for compute allocation in RL with verifiable rewards, particularly relevant for reasoning tasks
5. Training paradigm choice (from-scratch vs post-SFT) should inform prompt weighting strategy, with asymmetric weighting favored for wider accuracy traversal scenarios
:::

:::zh
**论文**: [2602.11128](https://arxiv.org/abs/2602.11128)
**作者**: Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis
**分类**: cs.LG

## 摘要

本文针对具有可验证奖励的强化学习提出了非对称提示加权方法,挑战了传统上关注中等成功概率提示的做法。作者证明,对经验成功概率低甚至为零的提示赋予更高权重,能够显著加速从零开始的强化学习训练。通过理论分析,他们刻画了在固定更新预算下最小化收敛时间的最优提示权重,表明在低成功率场景中,当有效响应稀缺时,非对称加权通过优先处理困难提示而成为最优策略。

## 主要贡献

- 提出非对称提示加权方法,对低成功概率提示赋予更高权重,与GRPO、DAPO和RLOO等现有方法形成对比
- 从理论上刻画了在固定更新预算下最小化达到目标准确率所需时间的最优提示权重
- 通过实验证明非对称加权特别有利于从零开始的强化学习(R1-Zero风格),而对监督微调后的强化学习效果较弱
- 分析表明在低成功率场景中,当响应成本占主导时,非对称权重能加速有效时间收敛

## 方法论与技术路线

核心创新在于重新思考策略优化算法如何根据提示难度对梯度进行加权。传统算法如GRPO(群体相对策略优化)、DAPO和RLOO关注具有中等成功概率的"模糊提示",有效地降低了非常简单和非常困难提示的权重。其理由是简单提示提供的学习信号很少,而困难提示可能噪声过大。

本文通过提出对低成功率提示赋予更高权重的非对称加权方案来挑战这一假设。关键洞察是,在早期训练阶段或从零开始的场景中,模型会经历广泛的准确率范围,此时困难提示包含最有价值的学习信号。作者通过优化框架将其形式化,该框架考虑:

- 可用的模型更新固定预算
- 为不同提示生成响应的成本
- 困难提示上成功响应的信息增益

理论分析揭示,当有效响应稀缺时(低成功率场景),最优加权策略变为非对称,优先处理模型当前表现不佳的提示。

## 实验结果与分析

实证评估揭示了训练场景之间的明显二分性:

**从零开始的强化学习(R1-Zero风格)**:非对称加权显示出显著优势。当训练从随机初始化或能力最小的模型开始时,广泛的准确率范围意味着模型会遇到许多低成功率提示。通过对这些困难案例赋予更高权重,训练过程能更高效地将计算资源分配给最具信息量的样本,加速收敛到目标准确率水平。

**监督微调后的强化学习**:收益不太明显。当从已经达到高准确率的监督微调模型开始时,大多数提示具有相对较高的成功概率。训练过程不会在低成功率区域停留太久,而这正是非对称加权提供优势的地方。

这一发现具有重要的实践意义:提示加权策略的选择应取决于初始模型能力和训练范式。

## 理论洞察

本文为理解提示加权提供了严格的理论框架。在固定更新预算$B$下,作者将权重$w(p)$刻画为成功概率$p$的函数,该函数最小化从初始准确率$p_0$达到目标准确率$p^*$的预期时间。

在低成功率场景($p \ll 1$)中,最优权重满足:

$$w(p) \propto \frac{1}{p \cdot c(p)}$$

其中$c(p)$是为成功概率为$p$的提示生成响应的成本。这种与成功概率的反比关系创造了非对称加权模式。

关键直觉:当成功响应稀缺时,困难提示上的每次成功都提供不成比例的价值。生成许多失败尝试的成本被最终成功的高信息含量所证明是合理的。这与高成功率场景形成对比,在高成功率场景中,额外成功的边际价值递减。

## 对大语言模型训练的启示

这项工作对大语言模型后训练具有几个重要启示:

**训练范式选择**:非对称加权的有效性表明,像R1-Zero这样的从零开始强化学习方法可能受益于与监督微调后强化学习不同的优化策略。训练流程应根据初始模型能力调整提示加权。

**计算资源分配**:理论框架为在不同难度的提示之间分配计算预算提供了原则性指导,可能在资源受限的环境中提高训练效率。

**课程学习联系**:非对称加权自然地实现了一种基于难度的课程形式,但由经验成功率驱动,而非预定义的难度指标。

**推理任务**:对于具有可验证奖励的推理任务(数学问题求解、代码生成),其中成功是二元且信息丰富的,非对称加权在早期训练阶段可能特别有价值。

## 要点总结

1. 优先处理低成功率提示的非对称提示加权通过将计算资源集中在最具信息量的样本上,加速了从零开始的强化学习训练
2. 非对称加权的收益在模型以高准确率开始的监督微调后场景中减弱,表明训练策略应适应初始能力
3. 理论分析揭示在低成功率场景中,最优权重与成功概率成反比,证明了非对称方法的合理性
4. 该框架为具有可验证奖励的强化学习中的计算资源分配提供了原则性指导,特别适用于推理任务
5. 训练范式选择(从零开始vs监督微调后)应指导提示加权策略,非对称加权更适合需要经历更广泛准确率范围的场景
:::
