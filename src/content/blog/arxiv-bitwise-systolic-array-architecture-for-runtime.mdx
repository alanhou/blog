---
title:
  en: "Runtime-Reconfigurable Bitwise Systolic Arrays for Multi-Precision Neural Network Acceleration"
  zh: "面向多精度神经网络加速的运行时可重构位级脉动阵列架构"
description:
  en: "A novel bitwise systolic array architecture enabling runtime reconfiguration of precision for mixed-precision quantized neural networks, achieving up to 3.57× speedup on FPGA platforms."
  zh: "一种新型位级脉动阵列架构,支持混合精度量化神经网络的运行时精度重构,在FPGA平台上实现高达3.57倍的加速。"
date: 2026-02-28
tags: ["arxiv", "ai", "cs.ar", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

![Concept animation](/arxiv-visuals/bitwise-systolic-array-architecture-for-runtime/ConceptScene.gif)



:::en
**Paper**: [2602.23334](https://arxiv.org/abs/2602.23334)
**Authors**: Yuhao Liu, Salim Ullah, Akash Kumar
**Categories**: cs.AR, cs.AI

## Abstract

This paper addresses a critical limitation in neural network accelerators: the inability to dynamically reconfigure computational precision at runtime for mixed-precision quantized neural networks (QNNs). While quantization reduces hardware resource consumption, uniform low precision causes significant accuracy degradation. The authors propose a runtime-reconfigurable multi-precision multi-channel bitwise systolic array architecture that enables different layers to operate at different precisions without hardware redesign. Implemented on the Ultra96 FPGA platform, the design achieves 1.32× to 3.57× speedup for mixed-precision model inference while supporting higher clock frequencies (250MHz) due to reduced critical path delay.

## Key Contributions

- A bitwise systolic array architecture that supports runtime reconfiguration of multiplication precision for QNN accelerators
- Multi-channel processing capability enabling parallel computation across different precision requirements
- Hardware implementation demonstrating practical feasibility on resource-constrained FPGA platforms
- Significant performance improvements (up to 3.57× speedup) while maintaining flexibility for mixed-precision models
- Reduced critical path delay enabling higher operating frequencies compared to conventional designs

## Architecture Design and Methodology

The core innovation lies in the bitwise decomposition of multiplication operations within the systolic array structure. Traditional systolic arrays perform fixed-precision matrix multiplications, making them inflexible for mixed-precision workloads. The proposed architecture breaks down multiplications into bit-level operations that can be dynamically composed based on the required precision.

The systolic array processes data in a pipelined fashion, where each processing element (PE) performs partial product accumulation at the bit level. By controlling which bit-level operations are activated, the system can effectively implement $2$-bit, $4$-bit, $8$-bit, or other precision levels without structural changes. This is achieved through:

- Configurable bit-serial multiplication units within each PE
- Dynamic routing logic that adapts data flow based on current precision requirements
- Multi-channel support allowing different channels to operate at different precisions simultaneously

The multi-channel capability is particularly important for modern neural networks where different feature maps may benefit from different quantization levels. The architecture can process multiple channels in parallel, each configured independently for optimal precision-accuracy tradeoffs.

## Experimental Results and Performance Analysis

The implementation on the Ultra96 FPGA platform demonstrates the practical viability of the approach. The authors evaluated their design across various mixed-precision QNN models and compared against baseline fixed-precision implementations.

Performance gains range from 1.3185× to 3.5671× depending on the specific precision configuration and model architecture. The speedup is attributed to several factors:

- Efficient utilization of hardware resources by avoiding over-provisioning for worst-case precision
- Reduced data movement through optimized bit-level processing
- Higher clock frequency capability (250MHz) due to shorter critical paths in the bitwise design

The critical path analysis reveals that bitwise operations introduce less combinational delay compared to full-width multipliers, enabling higher frequency operation. This is counterintuitive but results from the fact that bit-level operations can be more deeply pipelined and have simpler logic structures.

Energy efficiency also improves as the system only activates the necessary computational resources for each layer's precision requirement, avoiding wasted computation on unused high-order bits in low-precision layers.

## Implications for Edge AI and Future Directions

This work has significant implications for deploying neural networks on edge devices where both computational efficiency and model accuracy are critical. The runtime reconfigurability enables:

- Adaptive precision based on input characteristics or battery levels
- Per-layer optimization without requiring multiple specialized hardware units
- Easier deployment of neural architecture search (NAS) results that often produce mixed-precision models

The bitwise systolic array approach opens several research directions. Future work could explore dynamic precision adjustment based on runtime conditions, integration with emerging quantization-aware training methods, and extension to other operations beyond multiplication such as activation functions and normalization layers.

The demonstrated 250MHz operating frequency on FPGA suggests that ASIC implementations could achieve even higher performance, making this architecture particularly attractive for next-generation edge AI accelerators where flexibility and efficiency must coexist.

## Takeaways

1. Runtime-reconfigurable precision is essential for efficiently deploying mixed-precision QNNs on hardware accelerators
2. Bitwise systolic array decomposition enables flexible precision without sacrificing performance, achieving up to 3.57× speedup
3. Multi-channel support allows different feature maps to operate at optimal precision levels simultaneously
4. Reduced critical path delay in bitwise designs enables higher clock frequencies (250MHz) compared to conventional approaches
5. The architecture bridges the gap between fixed-precision accelerators and the growing need for adaptive, mixed-precision neural network inference
:::

:::zh
**论文**: [2602.23334](https://arxiv.org/abs/2602.23334)
**作者**: Yuhao Liu, Salim Ullah, Akash Kumar
**分类**: cs.AR, cs.AI

## 摘要

本文针对神经网络加速器的一个关键局限性:无法在运行时动态重构混合精度量化神经网络(QNN)的计算精度。虽然量化技术可以降低硬件资源消耗,但统一的低精度会导致显著的精度损失。作者提出了一种运行时可重构的多精度多通道位级脉动阵列架构,使不同层能够在不同精度下运行而无需重新设计硬件。该设计在Ultra96 FPGA平台上实现,对混合精度模型推理实现了1.32倍至3.57倍的加速,同时由于关键路径延迟降低而支持更高的时钟频率(250MHz)。

## 主要贡献

- 提出支持QNN加速器乘法精度运行时重构的位级脉动阵列架构
- 多通道处理能力,支持不同精度需求的并行计算
- 在资源受限的FPGA平台上验证了硬件实现的实用可行性
- 显著的性能提升(高达3.57倍加速),同时保持混合精度模型的灵活性
- 降低关键路径延迟,相比传统设计支持更高的工作频率

## 架构设计与方法

核心创新在于脉动阵列结构内乘法运算的位级分解。传统脉动阵列执行固定精度的矩阵乘法,对混合精度工作负载缺乏灵活性。所提出的架构将乘法分解为位级操作,可根据所需精度动态组合。

脉动阵列以流水线方式处理数据,每个处理单元(PE)在位级执行部分积累加。通过控制激活哪些位级操作,系统可以有效实现$2$位、$4$位、$8$位或其他精度级别,而无需结构性改变。这通过以下方式实现:

- 每个PE内的可配置位串行乘法单元
- 根据当前精度需求调整数据流的动态路由逻辑
- 多通道支持,允许不同通道同时以不同精度运行

多通道能力对于现代神经网络尤为重要,因为不同特征图可能受益于不同的量化级别。该架构可以并行处理多个通道,每个通道独立配置以实现最优的精度-准确度权衡。

## 实验结果与性能分析

在Ultra96 FPGA平台上的实现验证了该方法的实用可行性。作者在各种混合精度QNN模型上评估了设计,并与基线固定精度实现进行了比较。

性能提升范围从1.3185倍到3.5671倍,具体取决于精度配置和模型架构。加速归因于几个因素:

- 通过避免为最坏情况精度过度配置而高效利用硬件资源
- 通过优化的位级处理减少数据移动
- 由于位级设计中更短的关键路径而具有更高的时钟频率能力(250MHz)

关键路径分析表明,位级操作相比全宽度乘法器引入更少的组合延迟,从而支持更高频率运行。这看似违反直觉,但源于位级操作可以更深度流水化且具有更简单的逻辑结构。

能效也得到改善,因为系统仅激活每层精度需求所需的计算资源,避免在低精度层中对未使用的高位进行浪费计算。

## 对边缘AI的影响与未来方向

这项工作对在边缘设备上部署神经网络具有重要意义,在这些场景中计算效率和模型准确性都至关重要。运行时可重构性实现了:

- 基于输入特征或电池电量的自适应精度
- 无需多个专用硬件单元的逐层优化
- 更容易部署通常产生混合精度模型的神经架构搜索(NAS)结果

位级脉动阵列方法开辟了多个研究方向。未来工作可以探索基于运行时条件的动态精度调整、与新兴量化感知训练方法的集成,以及扩展到乘法之外的其他操作,如激活函数和归一化层。

在FPGA上展示的250MHz工作频率表明,ASIC实现可以达到更高的性能,使该架构对下一代边缘AI加速器特别有吸引力,在这些场景中灵活性和效率必须共存。

## 要点总结

1. 运行时可重构精度对于在硬件加速器上高效部署混合精度QNN至关重要
2. 位级脉动阵列分解在不牺牲性能的情况下实现灵活精度,达到高达3.57倍的加速
3. 多通道支持允许不同特征图同时以最优精度级别运行
4. 位级设计中降低的关键路径延迟相比传统方法支持更高的时钟频率(250MHz)
5. 该架构弥合了固定精度加速器与日益增长的自适应混合精度神经网络推理需求之间的差距
:::
