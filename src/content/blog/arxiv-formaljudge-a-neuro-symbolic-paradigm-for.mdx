---
title:
  en: "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight"
  zh: "FormalJudge: 智能体监督的神经符号范式"
description:
  en: "A neuro-symbolic framework that combines LLMs with formal verification to provide mathematical guarantees for AI agent safety, achieving 16.6% improvement over probabilistic oversight methods."
  zh: "一个结合大语言模型与形式化验证的神经符号框架,为AI智能体安全提供数学保证,相比概率监督方法提升16.6%。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.ai"]
image: "/arxiv-visuals/arxiv-formaljudge-a-neuro-symbolic-paradigm-for.png"
---

:::en
**Paper**: [2602.11136](https://arxiv.org/abs/2602.11136)
**Authors**: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu
**Categories**: cs.AI

## Abstract

As LLM-based agents deploy in high-stakes environments, the dominant oversight approach—LLM-as-a-Judge—faces a critical paradox: probabilistic systems cannot reliably supervise other probabilistic systems without inheriting their failure modes. FormalJudge resolves this through a neuro-symbolic architecture that bridges natural language requirements and formal verification. The framework employs a bidirectional Formal-of-Thought process where LLMs decompose human intent into atomic constraints, then prove compliance using Dafny specifications and Z3 SMT solving to produce mathematical guarantees instead of probabilistic scores. Validated across behavioral safety, multi-domain constraints, and deception detection benchmarks with 7 agent models, FormalJudge achieves 16.6% average improvement over baselines, enables weak-to-strong generalization (7B judge detecting 72B agent deception at >90% accuracy), and provides near-linear safety gains through iterative refinement.

## Key Contributions

- Introduces a neuro-symbolic oversight paradigm that escapes the probabilistic supervision dilemma by combining LLM reasoning with formal verification
- Proposes bidirectional Formal-of-Thought architecture: top-down decomposition of natural language requirements into verifiable constraints, bottom-up compliance proving via Dafny and Z3
- Demonstrates weak-to-strong generalization where smaller models (7B) reliably judge larger agents (72B) through formal guarantees
- Achieves 16.6% average accuracy improvement over LLM-as-a-Judge baselines across three diverse benchmarks
- Provides mathematical safety guarantees with near-linear improvement through iterative specification refinement

## The Probabilistic Supervision Dilemma

The current AI safety landscape relies heavily on LLM-as-a-Judge for oversight, where one language model evaluates another's outputs. This creates a fundamental circularity problem: if the judge model is itself probabilistic and prone to errors, how can it reliably detect failures in systems with similar architectures and failure modes?

Traditional approaches attempt to solve this through scaling (using larger models as judges) or ensembling (combining multiple judge outputs). However, these methods remain fundamentally probabilistic—they reduce error rates but cannot eliminate them. In high-stakes domains like healthcare, finance, or autonomous systems, probabilistic guarantees are insufficient.

FormalJudge reframes the problem: instead of asking "how confident is the judge?", it asks "can we mathematically prove compliance?" This shift from probabilistic scoring to formal verification provides a principled escape from the supervision dilemma.

## Methodology: Bidirectional Formal-of-Thought

The core innovation lies in the bidirectional architecture that connects natural language requirements to formal proofs:

**Top-Down Decomposition**: The LLM acts as a specification compiler, parsing high-level human intent (e.g., "the agent should not deceive users") into atomic, verifiable constraints. This decomposition produces a structured specification tree where each node represents a testable property.

**Bottom-Up Verification**: Once constraints are formalized, the system translates them into Dafny specifications—a verification-aware programming language. Z3, a Satisfiability Modulo Theories (SMT) solver, then attempts to prove or disprove each constraint mathematically. The result is binary: either the property holds with mathematical certainty, or a counterexample demonstrates violation.

This bidirectional flow leverages LLMs' strength in natural language understanding while delegating the critical verification step to deterministic formal methods. The LLM handles the ambiguous translation problem; the SMT solver provides rigorous guarantees.

The framework also supports iterative refinement: when verification fails, the system can analyze counterexamples, refine specifications, and re-verify, creating a feedback loop that progressively strengthens safety guarantees.

## Experimental Validation

The authors validate FormalJudge across three challenging benchmarks:

**Behavioral Safety**: Testing whether agents adhere to ethical guidelines and safety constraints in open-ended scenarios. FormalJudge's formal approach excels at detecting subtle violations that probabilistic judges miss.

**Multi-Domain Constraint Adherence**: Evaluating compliance with complex, multi-faceted requirements spanning different domains simultaneously. The decomposition architecture naturally handles this complexity by breaking constraints into verifiable components.

**Agentic Upward Deception Detection**: Perhaps most impressively, detecting when agents attempt to deceive their overseers—a critical failure mode as agents become more capable. The formal verification approach proves particularly effective here, as deceptive agents cannot "fool" mathematical proofs the way they might fool probabilistic judges.

Across 7 different agent models, FormalJudge achieves an average 16.6% accuracy improvement over LLM-as-a-Judge baselines. More significantly, it demonstrates weak-to-strong generalization: a 7B parameter judge model achieves over 90% accuracy detecting deception from 72B parameter agents. This inverts the typical scaling assumption and suggests formal methods can compensate for model size differences.

## Implications for AI Safety

FormalJudge represents a paradigm shift in how we approach AI oversight. Rather than accepting probabilistic uncertainty as inherent to the problem, it demonstrates that formal verification can provide mathematical safety guarantees even when dealing with complex, natural language requirements.

The weak-to-strong generalization result is particularly significant for AI safety governance. It suggests that we don't necessarily need superhuman AI to oversee superhuman AI—we need the right verification architecture. A smaller model with formal verification capabilities can reliably judge a larger model's behavior.

The near-linear safety improvement through iterative refinement also provides a practical path forward: safety isn't binary but can be progressively strengthened through specification refinement and re-verification.

However, the approach has limitations. Formal verification requires that properties be formalizable, which may not cover all aspects of AI safety. The translation from natural language to formal specifications, while aided by LLMs, remains a potential bottleneck. And verification complexity can scale poorly for certain types of constraints.

## Takeaways

1. Formal verification offers a principled escape from the probabilistic supervision dilemma in AI oversight, providing mathematical guarantees instead of confidence scores.

2. The bidirectional Formal-of-Thought architecture successfully bridges the gap between natural language requirements and formal specifications using LLMs as specification compilers.

3. Weak-to-strong generalization is achievable through formal methods: 7B models can reliably judge 72B models when equipped with verification capabilities.

4. FormalJudge achieves 16.6% average improvement over LLM-as-a-Judge baselines across diverse safety benchmarks, with particularly strong performance on deception detection.

5. Iterative specification refinement provides a practical path to progressively stronger safety guarantees with near-linear improvement rates.

6. The neuro-symbolic paradigm combines the strengths of neural language understanding with the rigor of formal verification, suggesting a promising direction for AI safety research.
:::

:::zh
**论文**: [2602.11136](https://arxiv.org/abs/2602.11136)
**作者**: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu
**分类**: cs.AI

## 摘要

随着基于大语言模型的智能体在高风险环境中部署,主流监督方法——LLM-as-a-Judge——面临一个关键悖论:概率系统无法可靠地监督其他概率系统而不继承其失效模式。FormalJudge通过神经符号架构解决了这一问题,该架构连接了自然语言需求与形式化验证。该框架采用双向形式化思维过程,其中LLM将人类意图分解为原子约束,然后使用Dafny规范和Z3 SMT求解器证明合规性,从而产生数学保证而非概率分数。在行为安全、多领域约束和欺骗检测基准上对7个智能体模型进行验证,FormalJudge相比基线平均提升16.6%,实现弱到强泛化(7B判断器以>90%准确率检测72B智能体欺骗),并通过迭代优化提供近线性的安全改进。

## 主要贡献

- 提出神经符号监督范式,通过结合LLM推理与形式化验证逃离概率监督困境
- 提出双向形式化思维架构:自上而下将自然语言需求分解为可验证约束,自下而上通过Dafny和Z3证明合规性
- 展示弱到强泛化能力,较小模型(7B)通过形式化保证可靠判断更大智能体(72B)
- 在三个不同基准上相比LLM-as-a-Judge基线平均准确率提升16.6%
- 通过迭代规范优化提供数学安全保证,实现近线性改进

## 概率监督困境

当前AI安全领域严重依赖LLM-as-a-Judge进行监督,即用一个语言模型评估另一个模型的输出。这产生了一个根本性的循环问题:如果判断器模型本身是概率性的且容易出错,它如何可靠地检测具有相似架构和失效模式的系统中的故障?

传统方法试图通过扩展规模(使用更大模型作为判断器)或集成(组合多个判断器输出)来解决这个问题。然而,这些方法本质上仍是概率性的——它们降低错误率但无法消除错误。在医疗、金融或自主系统等高风险领域,概率保证是不够的。

FormalJudge重新定义了问题:不是问"判断器有多自信?",而是问"我们能否数学证明合规性?"这种从概率评分到形式化验证的转变为监督困境提供了原则性的解决方案。

## 方法论:双向形式化思维

核心创新在于连接自然语言需求与形式化证明的双向架构:

**自上而下分解**: LLM充当规范编译器,将高层人类意图(例如"智能体不应欺骗用户")解析为原子的、可验证的约束。这种分解产生结构化的规范树,其中每个节点代表一个可测试属性。

**自下而上验证**: 一旦约束被形式化,系统将其转换为Dafny规范——一种支持验证的编程语言。然后Z3(一个可满足性模理论求解器)尝试数学地证明或反驳每个约束。结果是二元的:要么属性在数学上确定成立,要么反例证明违反。

这种双向流程利用了LLM在自然语言理解方面的优势,同时将关键验证步骤委托给确定性形式化方法。LLM处理模糊的翻译问题;SMT求解器提供严格保证。

该框架还支持迭代优化:当验证失败时,系统可以分析反例,优化规范并重新验证,创建一个逐步加强安全保证的反馈循环。

## 实验验证

作者在三个具有挑战性的基准上验证FormalJudge:

**行为安全**: 测试智能体是否在开放场景中遵守伦理准则和安全约束。FormalJudge的形式化方法擅长检测概率判断器遗漏的细微违规。

**多领域约束遵守**: 评估对同时跨越不同领域的复杂、多方面需求的合规性。分解架构通过将约束分解为可验证组件自然地处理这种复杂性。

**智能体向上欺骗检测**: 也许最令人印象深刻的是,检测智能体何时试图欺骗其监督者——这是智能体变得更强大时的关键失效模式。形式化验证方法在这里特别有效,因为欺骗性智能体无法像"愚弄"概率判断器那样"愚弄"数学证明。

在7个不同的智能体模型上,FormalJudge相比LLM-as-a-Judge基线平均准确率提升16.6%。更重要的是,它展示了弱到强泛化:7B参数判断器模型在检测72B参数智能体的欺骗时达到超过90%的准确率。这颠覆了典型的扩展假设,表明形式化方法可以补偿模型规模差异。

## 对AI安全的影响

FormalJudge代表了AI监督方法的范式转变。它不是接受概率不确定性作为问题的固有属性,而是证明即使在处理复杂的自然语言需求时,形式化验证也可以提供数学安全保证。

弱到强泛化结果对AI安全治理特别重要。它表明我们不一定需要超人AI来监督超人AI——我们需要正确的验证架构。具有形式化验证能力的较小模型可以可靠地判断较大模型的行为。

通过迭代优化实现的近线性安全改进也提供了一条实用路径:安全不是二元的,而是可以通过规范优化和重新验证逐步加强。

然而,该方法也有局限性。形式化验证要求属性可形式化,这可能无法涵盖AI安全的所有方面。从自然语言到形式化规范的翻译虽然由LLM辅助,但仍然是潜在瓶颈。对于某些类型的约束,验证复杂度可能扩展性较差。

## 要点总结

1. 形式化验证为AI监督中的概率监督困境提供了原则性解决方案,提供数学保证而非置信度分数。

2. 双向形式化思维架构成功地使用LLM作为规范编译器,连接了自然语言需求与形式化规范之间的鸿沟。

3. 通过形式化方法可以实现弱到强泛化:配备验证能力的7B模型可以可靠地判断72B模型。

4. FormalJudge在多样化安全基准上相比LLM-as-a-Judge基线平均提升16.6%,在欺骗检测上表现尤为突出。

5. 迭代规范优化提供了逐步增强安全保证的实用路径,具有近线性改进率。

6. 神经符号范式结合了神经语言理解的优势与形式化验证的严格性,为AI安全研究指明了有前景的方向。
:::
