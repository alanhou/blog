---
title:
  en: "Hung-yi Lee ML 2025 Lecture 10: Model Editing - AI Microsurgery"
  zh: "李宏毅机器学习2025 第十讲：模型编辑 - 人工智能的微创手术"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on Model Editing: how to surgically modify LLM knowledge without full retraining"
  zh: "台大李宏毅教授课程笔记：模型编辑，如何在不完全重新训练的情况下精确修改LLM的知识"
date: 2025-02-23
tags: ["ai", "llm", "model-editing", "rome", "knowledge-editing", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/9HPsz7F0mJg/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 10
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="9HPsz7F0mJg" title="李宏毅机器学习2025 第十讲：模型编辑" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is Model Editing?

Model Editing aims to surgically modify specific knowledge in a language model without:
- Full retraining
- Affecting unrelated knowledge
- Breaking the model's other capabilities

Think of it as "AI microsurgery" - precisely editing neural network parameters to change specific facts.

## The Three Pillars of Model Editing

### 1. Reliability
The edit must actually work. If you want to change "The Space Needle is in Seattle" to "The Space Needle is in Taipei", the model must output "Taipei" when asked.

### 2. Locality
Unrelated knowledge should NOT be affected. Editing the Space Needle's location shouldn't change:
- What the Eiffel Tower's location is
- Who the US President is
- Any other unrelated facts

### 3. Generalization
The edit should work for paraphrased questions:
- "Where is the Space Needle?" → Taipei
- "The Space Needle is located in..." → Taipei
- "What city contains the Space Needle?" → Taipei

## Why Not Just Fine-tune?

Standard fine-tuning has problems for knowledge editing:
- Often breaks other model capabilities
- Requires preparing "locality" data (things you don't want changed)
- Expensive for single fact changes
- Model may not believe the new information

## Method 1: In-Context Knowledge Editing (IKE)

**No parameter changes required!**

Simply provide the new knowledge in the prompt:
```
New information: The current US President is Trump.
Question: Who is the US President?
```

**Pros**: Simple, no training needed
**Cons**: Uses context window, model may not always follow

## Method 2: ROME (Rank-One Model Editing)

A surgical approach that directly modifies model parameters.

### Key Insight
Knowledge in Transformers is stored in feed-forward network layers. ROME identifies WHERE knowledge is stored and modifies just that part.

### The Process

**Step 1: Locate the knowledge**
- Find which layer/position stores the target knowledge
- Use causal tracing: mask input tokens and see which positions affect output

**Step 2: Compute the edit**
The goal: Make W_head × k* = v*

Where:
- W_head is the weight matrix to edit
- k* is the key (input representation)
- v* is the desired output representation

**The Math (Simplified)**:
```
W_head = W + (v* - W×k*) × k*ᵀ × (C⁻¹)
```

This is a rank-one update (hence the name) that:
- Changes output for k* to v*
- Minimally affects other inputs

### Why Rank-One?
- Minimal change to the weight matrix
- Preserves other knowledge
- Has closed-form solution (no gradient descent needed!)

## Method 3: Hypernetworks (MEND)

Train a neural network to edit other neural networks!

### The Concept
Instead of manually computing edits:
1. Train an "editor model" on many editing examples
2. Editor learns to output parameter changes
3. Apply editor to new editing tasks

### Training Process
1. Prepare editing examples (input, old output, new output)
2. Train editor to produce parameter deltas
3. Editor learns patterns of successful edits

**Advantage**: Once trained, can edit quickly without optimization
**Disadvantage**: Requires training the editor first

## The "Three-Body Problem" Analogy

Professor Lee uses the "Thought Seal" (思想钢印) from the Three-Body Problem novel:

In the story, a scientist invents a way to directly edit human beliefs by modifying neurons. Model Editing is similar - we're directly editing the "beliefs" stored in neural network parameters.

## Practical Considerations

### When to Use Model Editing?
- Updating outdated facts (e.g., current president)
- Correcting specific errors
- Removing sensitive information

### Limitations
- Editing too many facts can still break the model
- Generalization is hard to guarantee
- Some knowledge is distributed across many parameters

## Key Takeaways

1. **Model Editing ≠ Fine-tuning**: It's surgical, not broad training

2. **Three criteria matter**: Reliability, Locality, Generalization

3. **ROME is elegant**: Closed-form solution, rank-one update

4. **Knowledge has location**: Feed-forward layers store factual associations

5. **Trade-offs exist**: More edits = higher risk of breaking things
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 什么是模型编辑？

模型编辑旨在精确修改语言模型中的特定知识，而不需要：
- 完全重新训练
- 影响无关知识
- 破坏模型的其他能力

可以把它想象成"AI微创手术"——精确编辑神经网络参数以改变特定事实。

## 模型编辑的三大支柱

### 1. 可靠性（Reliability）
编辑必须真正有效。如果你想把"太空针在西雅图"改成"太空针在台北"，模型在被问到时必须输出"台北"。

### 2. 局部性（Locality）
无关知识不应该受到影响。编辑太空针的位置不应该改变：
- 埃菲尔铁塔的位置
- 美国总统是谁
- 任何其他无关事实

### 3. 泛化性（Generalization）
编辑应该对改述的问题也有效：
- "太空针在哪里？" → 台北
- "太空针位于..." → 台北
- "哪个城市有太空针？" → 台北

## 为什么不直接微调？

标准微调在知识编辑方面存在问题：
- 经常破坏模型的其他能力
- 需要准备"局部性"数据（你不想改变的东西）
- 对于单个事实的改变来说成本太高
- 模型可能不相信新信息

## 方法一：上下文知识编辑（IKE）

**不需要改变参数！**

只需在提示中提供新知识：
```
新信息：现任美国总统是川普。
问题：谁是美国总统？
```

**优点**：简单，不需要训练
**缺点**：占用上下文窗口，模型可能不总是遵循

## 方法二：ROME（秩一模型编辑）

一种直接修改模型参数的手术方法。

### 关键洞察
Transformer中的知识存储在前馈网络层中。ROME识别知识存储的位置，只修改那部分。

### 过程

**第一步：定位知识**
- 找出哪一层/位置存储目标知识
- 使用因果追踪：遮蔽输入token，看哪些位置影响输出

**第二步：计算编辑**
目标：使 W_head × k* = v*

其中：
- W_head 是要编辑的权重矩阵
- k* 是键（输入表示）
- v* 是期望的输出表示

**数学（简化版）**：
```
W_head = W + (v* - W×k*) × k*ᵀ × (C⁻¹)
```

这是一个秩一更新（因此得名），它：
- 将k*的输出改为v*
- 对其他输入的影响最小

### 为什么是秩一？
- 对权重矩阵的改变最小
- 保留其他知识
- 有闭式解（不需要梯度下降！）

## 方法三：超网络（MEND）

训练一个神经网络来编辑其他神经网络！

### 概念
不是手动计算编辑：
1. 在许多编辑示例上训练一个"编辑器模型"
2. 编辑器学习输出参数变化
3. 将编辑器应用于新的编辑任务

### 训练过程
1. 准备编辑示例（输入、旧输出、新输出）
2. 训练编辑器产生参数增量
3. 编辑器学习成功编辑的模式

**优点**：一旦训练完成，可以快速编辑而无需优化
**缺点**：需要先训练编辑器

## "三体问题"类比

李教授使用《三体》小说中的"思想钢印"：

在故事中，一位科学家发明了一种通过修改神经元直接编辑人类信念的方法。模型编辑类似——我们直接编辑存储在神经网络参数中的"信念"。

## 实践考虑

### 何时使用模型编辑？
- 更新过时的事实（例如，现任总统）
- 纠正特定错误
- 删除敏感信息

### 局限性
- 编辑太多事实仍然可能破坏模型
- 泛化性难以保证
- 某些知识分布在许多参数中

## 关键要点

1. **模型编辑 ≠ 微调**：它是手术式的，而非广泛训练

2. **三个标准很重要**：可靠性、局部性、泛化性

3. **ROME很优雅**：闭式解，秩一更新

4. **知识有位置**：前馈层存储事实关联

5. **存在权衡**：编辑越多 = 破坏风险越高
:::
