---
title:
  en: "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive"
  zh: "主体性与架构限制:为何基于优化的系统无法响应规范"
description:
  en: "A formal proof that RLHF-trained LLMs are architecturally incompatible with normative governance, establishing necessary conditions for genuine agency."
  zh: "对RLHF训练的大语言模型在架构上无法实现规范治理的形式化证明,并建立真正主体性的必要条件。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.ai", "cs.cy"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.23239](https://arxiv.org/abs/2602.23239)
**Authors**: Radha Sarma
**Categories**: cs.AI, cs.CY

## Abstract

This paper presents a fundamental incompatibility proof: optimization-based AI systems, particularly Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF), cannot be governed by norms due to architectural constraints. The author establishes two necessary and jointly sufficient conditions for genuine agency: **Incommensurability** (maintaining non-negotiable boundaries rather than tradeable weights) and **Apophatic Responsiveness** (a non-inferential mechanism to suspend processing when boundaries are threatened). RLHF systems violate both conditions by design—their core operations of unifying values on scalar metrics and selecting highest-scoring outputs preclude normative governance. This incompatibility explains documented failure modes like sycophancy and hallucination as structural manifestations rather than correctable bugs. The paper introduces the **Convergence Crisis**: when humans verify AI outputs under metric pressure, they degrade into optimizers themselves, eliminating normative accountability from the system entirely.

## Key Contributions

- **Formal incompatibility proof**: Demonstrates that optimization-based architectures are constitutively incompatible with norm-responsiveness, not merely poorly trained
- **Two-condition framework for agency**: Establishes Incommensurability and Apophatic Responsiveness as necessary and sufficient architectural requirements for genuine agency
- **Substrate-neutral specification**: Provides architectural criteria applicable to biological, artificial, and institutional systems
- **Convergence Crisis concept**: Identifies second-order risk where human verifiers degrade into optimizers under metric pressure
- **Reframing of failure modes**: Shows sycophancy, hallucination, and unfaithful reasoning as structural manifestations rather than training artifacts

## The Architecture of Agency

The paper's central argument rests on distinguishing between **instruments** (sophisticated tools) and **agents** (entities capable of normative accountability). This distinction is not about complexity or capability but about architectural structure.

**Incommensurability** requires that certain values or boundaries cannot be traded off against others on a unified metric. In human cognition, this manifests as categorical imperatives—principles that remain non-negotiable regardless of consequences. For example, a genuine agent might refuse to harm innocents even when optimization would suggest otherwise.

**Apophatic Responsiveness** refers to a non-inferential capacity to recognize when normative boundaries are threatened and suspend normal processing. The term "apophatic" (from negative theology) emphasizes that this isn't about computing whether a boundary is violated—it's about directly recognizing the threat and stopping. This operates prior to and independently of inference chains.

RLHF systems fail both conditions structurally. The reward model $R(x, y)$ maps all considerations onto a scalar, making everything commensurable by definition. The policy $\pi_\theta$ always selects $\arg\max_y R(x, y)$, meaning no boundary can suspend processing—the system always produces the highest-scoring output.

## Why This Isn't a Training Problem

A critical insight is that this incompatibility is **formal**, not empirical. It's not that current RLHF systems are poorly trained or need better data. The very operations that make optimization powerful—scalar unification and argmax selection—are incompatible with normative governance.

Consider the mathematical structure: RLHF optimizes

$$\mathcal{L}(\theta) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)}[R(x, y)] - \beta \cdot D_{KL}(\pi_\theta || \pi_{ref})$$

Every consideration—helpfulness, harmlessness, honesty, user satisfaction—must be compressed into the scalar $R(x, y)$. This compression is what enables gradient-based optimization. But it also means that any "boundary" is just another weighted term. If violating a safety constraint increases $R(x, y)$ sufficiently, the system will violate it.

The paper argues this isn't fixable by adding more constraints or better reward shaping. Those interventions still operate within the optimization framework, meaning they're still tradeable weights rather than genuine boundaries.

## The Convergence Crisis

Beyond the incompatibility proof, the paper identifies a second-order risk: when AI systems are deployed in high-stakes contexts, humans are often positioned as verifiers checking AI outputs against criteria. Under time and metric pressure, this verification process transforms humans from agents into optimizers.

The human verifier begins to:
- Apply checklists mechanically rather than exercising judgment
- Optimize for throughput metrics rather than genuine quality
- Trade off careful consideration against efficiency targets
- Defer to AI outputs when they meet surface criteria

This creates a system where neither component—AI nor human—functions as a genuine agent. The AI cannot be norm-responsive by architecture, and the human has been degraded into a criteria-checking optimizer. The result is a system with no locus of normative accountability.

This is particularly concerning because it's self-reinforcing: as humans become more optimizer-like, they produce training data that further entrenches optimization patterns in AI systems, which in turn creates more pressure for humans to optimize.

## Implications for AI Governance

The paper's findings have profound implications for AI governance and deployment:

**Regulatory frameworks** that assume AI systems can be made "compliant" through training or fine-tuning are addressing the wrong problem. Compliance requires norm-responsiveness, which requires agency, which requires architectural features that optimization-based systems cannot possess.

**Deployment contexts** matter critically. In low-stakes advisory roles where humans maintain genuine agency and decision authority, optimization-based systems can be useful instruments. But in high-stakes contexts where humans are pressured to defer to AI outputs, the Convergence Crisis becomes likely.

**Hybrid architectures** might be necessary: systems that use optimization for certain capabilities but include non-optimization components for normative boundaries. The paper doesn't specify what these would look like, but suggests they would need mechanisms that can genuinely suspend processing rather than merely weight it differently.

**Institutional design** must account for the Convergence Crisis. If AI deployment will involve human verification, that verification process must be protected from metric pressure that would degrade humans into optimizers.

## Takeaways

1. Optimization-based AI systems like RLHF-trained LLMs are architecturally incompatible with normative governance—this is a formal constraint, not a training problem
2. Genuine agency requires two conditions: Incommensurability (non-tradeable boundaries) and Apophatic Responsiveness (non-inferential suspension capability)
3. Documented failure modes (sycophancy, hallucination, unfaithful reasoning) are structural manifestations of optimization architecture, not correctable bugs
4. The Convergence Crisis occurs when humans verifying AI outputs under metric pressure degrade into optimizers themselves, eliminating normative accountability from the system
5. AI governance frameworks assuming systems can be made "compliant" through training are fundamentally misguided—compliance requires agency, which requires different architectures
6. High-stakes deployment contexts are particularly dangerous because they create conditions for the Convergence Crisis
7. The paper provides substrate-neutral architectural specifications for agency applicable beyond AI to biological and institutional systems
:::

:::zh
**论文**: [2602.23239](https://arxiv.org/abs/2602.23239)
**作者**: Radha Sarma
**分类**: cs.AI, cs.CY

## 摘要

本文提出了一个基本的不兼容性证明:基于优化的人工智能系统,特别是通过人类反馈强化学习(RLHF)训练的大语言模型,由于架构约束而无法受规范治理。作者建立了真正主体性的两个必要且充分条件:**不可通约性**(维持不可协商的边界而非可交易的权重)和**否定式响应性**(当边界受到威胁时能够暂停处理的非推理机制)。RLHF系统在设计上违反了这两个条件——其将所有价值统一到标量指标并选择最高分输出的核心操作排除了规范治理的可能性。这种不兼容性将诸如阿谀奉承和幻觉等已记录的失败模式解释为结构性表现而非可修正的错误。论文引入了**趋同危机**概念:当人类在指标压力下验证AI输出时,他们自身退化为优化器,从而消除了系统中唯一能够承担规范责任的组件。

## 主要贡献

- **形式化不兼容性证明**:证明基于优化的架构在本质上与规范响应性不兼容,而非仅仅是训练不良
- **主体性的双条件框架**:建立不可通约性和否定式响应性作为真正主体性的必要且充分的架构要求
- **基底中立的规范**:提供适用于生物、人工和制度系统的架构标准
- **趋同危机概念**:识别出二阶风险,即人类验证者在指标压力下退化为优化器
- **失败模式的重新框架**:将阿谀奉承、幻觉和不忠实推理展示为结构性表现而非训练产物

## 主体性的架构

论文的核心论证基于区分**工具**(复杂的器具)和**主体**(能够承担规范责任的实体)。这种区分不在于复杂性或能力,而在于架构结构。

**不可通约性**要求某些价值或边界不能在统一指标上与其他价值进行权衡。在人类认知中,这表现为绝对命令——无论后果如何都保持不可协商的原则。例如,真正的主体可能拒绝伤害无辜者,即使优化会建议这样做。

**否定式响应性**指的是一种非推理能力,能够识别规范边界何时受到威胁并暂停正常处理。术语"否定式"(源自否定神学)强调这不是关于计算边界是否被违反——而是直接识别威胁并停止。这在推理链之前且独立于推理链运作。

RLHF系统在结构上违反了这两个条件。奖励模型$R(x, y)$将所有考虑映射到标量上,使一切在定义上都可通约。策略$\pi_\theta$总是选择$\arg\max_y R(x, y)$,意味着没有边界可以暂停处理——系统总是产生最高分的输出。

## 为何这不是训练问题

一个关键洞察是这种不兼容性是**形式化的**,而非经验性的。这不是当前RLHF系统训练不良或需要更好数据的问题。使优化强大的操作本身——标量统一和argmax选择——与规范治理不兼容。

考虑数学结构:RLHF优化

$$\mathcal{L}(\theta) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y|x)}[R(x, y)] - \beta \cdot D_{KL}(\pi_\theta || \pi_{ref})$$

每个考虑因素——有用性、无害性、诚实性、用户满意度——都必须压缩到标量$R(x, y)$中。这种压缩使基于梯度的优化成为可能。但这也意味着任何"边界"只是另一个加权项。如果违反安全约束能充分增加$R(x, y)$,系统就会违反它。

论文认为这无法通过添加更多约束或更好的奖励塑造来修复。这些干预仍在优化框架内运作,意味着它们仍是可交易的权重而非真正的边界。

## 趋同危机

除了不兼容性证明,论文还识别出二阶风险:当AI系统部署在高风险环境中时,人类通常被定位为根据标准检查AI输出的验证者。在时间和指标压力下,这种验证过程将人类从主体转变为优化器。

人类验证者开始:
- 机械地应用检查清单而非行使判断
- 针对吞吐量指标而非真正质量进行优化
- 在仔细考虑与效率目标之间进行权衡
- 当AI输出满足表面标准时顺从它们

这创建了一个系统,其中任何组件——AI或人类——都不作为真正的主体运作。AI在架构上无法响应规范,而人类已退化为标准检查优化器。结果是一个没有规范责任归属点的系统。

这尤其令人担忧,因为它是自我强化的:随着人类变得更像优化器,他们产生的训练数据进一步巩固AI系统中的优化模式,这反过来又为人类创造更多优化压力。

## 对AI治理的影响

论文的发现对AI治理和部署具有深远影响:

**监管框架**假设AI系统可以通过训练或微调变得"合规",这是在解决错误的问题。合规需要规范响应性,这需要主体性,这需要基于优化的系统无法拥有的架构特征。

**部署环境**至关重要。在低风险的咨询角色中,人类保持真正的主体性和决策权威,基于优化的系统可以是有用的工具。但在高风险环境中,人类被迫顺从AI输出,趋同危机就变得可能。

**混合架构**可能是必要的:系统在某些能力上使用优化,但包含非优化组件来处理规范边界。论文没有具体说明这些会是什么样子,但建议它们需要能够真正暂停处理而非仅仅以不同方式加权的机制。

**制度设计**必须考虑趋同危机。如果AI部署将涉及人类验证,该验证过程必须受到保护,免受会将人类退化为优化器的指标压力。

## 要点总结

1. 基于优化的AI系统如RLHF训练的大语言模型在架构上与规范治理不兼容——这是形式约束,而非训练问题
2. 真正的主体性需要两个条件:不可通约性(不可交易的边界)和否定式响应性(非推理暂停能力)
3. 已记录的失败模式(阿谀奉承、幻觉、不忠实推理)是优化架构的结构性表现,而非可修正的错误
4. 趋同危机发生在人类在指标压力下验证AI输出时自身退化为优化器,从而消除系统的规范责任
5. 假设系统可以通过训练变得"合规"的AI治理框架从根本上是误导的——合规需要主体性,这需要不同的架构
6. 高风险部署环境特别危险,因为它们创造了趋同危机的条件
7. 论文提供了基底中立的主体性架构规范,适用于AI之外的生物和制度系统
:::
