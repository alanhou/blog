---
title:
  en: "Anthropic Co-founder: Building Claude Code, Lessons From GPT-3 & LLM System Design"
  zh: "Anthropic联合创始人：构建Claude Code、GPT-3的经验教训与LLM系统设计"
description:
  en: "Tom Brown, Anthropic co-founder and GPT-3 lead author, shares his journey from YC startups to building Claude Code and insights on AI infrastructure"
  zh: "Anthropic联合创始人、GPT-3首席作者Tom Brown分享了他从YC创业公司到构建Claude Code的历程，以及对AI基础设施的见解"
date: 2025-08-19
tags: ["ycombinator", "ai", "anthropic", "claude-code", "llm", "system-design"]
image: "https://i2.ytimg.com/vi/JdT78t1Offo/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="JdT78t1Offo" title="Anthropic Co-founder: Building Claude Code, Lessons From GPT-3 & LLM System Design" />

:::en
Tom Brown, co-founder of Anthropic and lead author of the GPT-3 paper, shares his unconventional path into AI and the lessons learned building some of the most influential language models.

## Early Career: The Wolf Mindset

### Background
- Studied computer science at MIT
- First job was at a friend's YC startup as employee #1
- Learned the crucial difference between "wolf mindset" and "dog mindset"
  - Dog mindset: Wait for someone to tell you what to do
  - Wolf mindset: Figure out what needs to happen and make it happen
- This entrepreneurial thinking became foundational for his career

### YC Startup Experience
- Worked at Linked Language (language learning startup)
- Later joined Grouper (social dating app)
- These experiences taught him to be resourceful and self-directed

## Self-Teaching AI

### The Six-Month Journey
- Decided to transition into AI/ML around 2015-2016
- Self-study approach:
  - Andrew Ng's Coursera machine learning course
  - Kaggle competitions for practical experience
  - Linear algebra fundamentals
  - Reading research papers
- Key insight: You don't need a PhD to work in AI

### Breaking Into the Field
- Applied to Google Brain but was rejected
- Connected with Greg Brockman through a friend
- Joined OpenAI in its early days

## OpenAI and GPT-3

### Early OpenAI Work
- Worked on Starcraft environment for reinforcement learning
- Transitioned to language models as the field evolved

### The GPT-3 Project
- Lead author on the GPT-3 paper
- Key technical challenges:
  - Training on TPUs initially, then switching to GPUs
  - Managing massive distributed training runs
  - Debugging at scale (finding bugs in 1000+ GPU clusters)

### Scaling Laws Discovery
- One of the most important insights: more compute = more intelligence, predictably
- This wasn't obvious at the time - many thought there would be diminishing returns
- The scaling laws gave confidence to invest heavily in larger models

### Infrastructure Lessons
- TPU to GPU transition taught flexibility in hardware choices
- Software stack matters enormously (PyTorch vs TensorFlow)
- Debugging distributed systems requires new tools and approaches

## Founding Anthropic

### Why Leave OpenAI?
- Group of people who took scaling laws very seriously
- Wanted to focus on AI safety alongside capabilities
- Left during COVID period (2020-2021)
- Founded with several OpenAI colleagues including Dario and Daniela Amodei

### Early Anthropic Days
- Started with Constitutional AI research
- Focus on making AI systems more helpful, harmless, and honest
- Built infrastructure from scratch

## Claude's Success Story

### The Turning Point: Claude 3.5 Sonnet
- Claude 3.5 Sonnet was the breakthrough moment
- Particularly strong at coding tasks
- First time Anthropic had a clearly leading product in a major category

### Why Coding Worked
- Coding is a well-defined task with clear success metrics
- Models can be evaluated objectively (does the code run?)
- Rapid iteration possible based on user feedback

## Building Claude Code

### Origin Story
- Built initially for internal use at Anthropic
- Engineers wanted a better way to interact with Claude for coding
- Surprised when it became the best product in the market

### Product Philosophy
- Empathy for Claude users - the team uses Claude Code daily
- Focus on developer experience
- Iterate based on real usage patterns

### Key Features
- Terminal-based interface for developers
- Deep integration with development workflows
- Designed for extended coding sessions

## Infrastructure Strategy

### Multi-Chip Approach
- Anthropic uses three different chip manufacturers:
  - NVIDIA GPUs
  - Google TPUs
  - AWS Trainium
- This diversity provides:
  - Supply chain resilience
  - Negotiating leverage
  - Ability to optimize for different workloads

### Software Challenges
- Each chip requires different software stack
- Significant engineering investment to support multiple platforms
- Trade-off between flexibility and engineering complexity

## Advice for Young People

### Career Philosophy
- Take more risks when you're young
- Work on things that would make the idealized version of yourself proud
- Don't optimize for prestige or conventional success metrics

### On AI Careers
- The field is still accessible to self-taught practitioners
- Practical experience (Kaggle, open source) matters more than credentials
- Find problems you're genuinely curious about

### The Wolf Mindset Revisited
- Don't wait for permission or instructions
- Identify what needs to happen and make it happen
- This applies whether you're at a startup or a large company

## Looking Forward

### AI Progress
- Scaling laws suggest continued rapid improvement
- Coding is just the beginning - many domains will be transformed
- The gap between AI capabilities and human integration is the current bottleneck

### Anthropic's Mission
- Balance capability advancement with safety research
- Make AI systems that are genuinely helpful
- Build tools that augment human capabilities rather than replace them
:::

:::zh
Tom Brown，Anthropic联合创始人和GPT-3论文的首席作者，分享了他进入AI领域的非传统路径，以及构建一些最具影响力的语言模型所学到的经验。

## 早期职业：狼的心态

### 背景
- 在MIT学习计算机科学
- 第一份工作是在朋友的YC创业公司担任第一号员工
- 学到了"狼心态"和"狗心态"之间的关键区别
  - 狗心态：等待别人告诉你该做什么
  - 狼心态：弄清楚需要发生什么并让它发生
- 这种创业思维成为他职业生涯的基础

### YC创业经历
- 在Linked Language工作（语言学习创业公司）
- 后来加入Grouper（社交约会应用）
- 这些经历教会他要有资源意识和自我驱动

## 自学AI

### 六个月的学习之旅
- 大约在2015-2016年决定转向AI/ML
- 自学方法：
  - Andrew Ng的Coursera机器学习课程
  - Kaggle竞赛获取实践经验
  - 线性代数基础
  - 阅读研究论文
- 关键洞察：你不需要博士学位就能从事AI工作

### 进入这个领域
- 申请Google Brain但被拒绝
- 通过朋友联系上Greg Brockman
- 在早期加入OpenAI

## OpenAI和GPT-3

### 早期OpenAI工作
- 从事星际争霸强化学习环境的工作
- 随着领域发展转向语言模型

### GPT-3项目
- GPT-3论文的首席作者
- 关键技术挑战：
  - 最初在TPU上训练，后来切换到GPU
  - 管理大规模分布式训练运行
  - 大规模调试（在1000+ GPU集群中找bug）

### 扩展定律的发现
- 最重要的洞察之一：更多计算 = 更多智能，可预测地
- 这在当时并不明显 - 很多人认为会有收益递减
- 扩展定律给了大力投资更大模型的信心

### 基础设施经验
- TPU到GPU的转换教会了硬件选择的灵活性
- 软件栈非常重要（PyTorch vs TensorFlow）
- 调试分布式系统需要新的工具和方法

## 创立Anthropic

### 为什么离开OpenAI？
- 一群非常认真对待扩展定律的人
- 想在能力发展的同时专注于AI安全
- 在COVID期间离开（2020-2021）
- 与几位OpenAI同事一起创立，包括Dario和Daniela Amodei

### Anthropic早期
- 从Constitutional AI研究开始
- 专注于使AI系统更有帮助、无害和诚实
- 从零开始构建基础设施

## Claude的成功故事

### 转折点：Claude 3.5 Sonnet
- Claude 3.5 Sonnet是突破性时刻
- 在编程任务上特别强
- Anthropic第一次在主要类别中拥有明显领先的产品

### 为什么编程有效
- 编程是一个定义明确的任务，有清晰的成功指标
- 模型可以被客观评估（代码能运行吗？）
- 可以根据用户反馈快速迭代

## 构建Claude Code

### 起源故事
- 最初为Anthropic内部使用而构建
- 工程师想要更好的方式与Claude交互进行编程
- 当它成为市场上最好的产品时感到惊讶

### 产品理念
- 对Claude用户的同理心 - 团队每天都使用Claude Code
- 专注于开发者体验
- 根据真实使用模式进行迭代

### 关键特性
- 为开发者设计的终端界面
- 与开发工作流程深度集成
- 为长时间编程会话设计

## 基础设施策略

### 多芯片方法
- Anthropic使用三种不同的芯片制造商：
  - NVIDIA GPU
  - Google TPU
  - AWS Trainium
- 这种多样性提供：
  - 供应链弹性
  - 谈判筹码
  - 针对不同工作负载优化的能力

### 软件挑战
- 每种芯片需要不同的软件栈
- 支持多平台需要大量工程投入
- 灵活性和工程复杂性之间的权衡

## 给年轻人的建议

### 职业理念
- 年轻时多冒险
- 做那些能让理想化的自己感到骄傲的事情
- 不要为声望或传统成功指标而优化

### 关于AI职业
- 这个领域对自学者仍然开放
- 实践经验（Kaggle、开源）比资历更重要
- 找到你真正好奇的问题

### 重温狼的心态
- 不要等待许可或指示
- 确定需要发生什么并让它发生
- 无论你在创业公司还是大公司，这都适用

## 展望未来

### AI进展
- 扩展定律表明将继续快速改进
- 编程只是开始 - 许多领域将被改变
- AI能力和人类整合之间的差距是当前的瓶颈

### Anthropic的使命
- 平衡能力发展和安全研究
- 制造真正有帮助的AI系统
- 构建增强人类能力而非取代人类的工具
:::
