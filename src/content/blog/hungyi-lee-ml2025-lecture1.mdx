---
title:
  en: "Hung-yi Lee ML 2025 Lecture 1: Understanding Generative AI Breakthroughs"
  zh: "李宏毅機器學習2025 第一講：一堂課搞懂生成式AI的技術突破"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on generative AI fundamentals, from token prediction to test-time scaling"
  zh: "台大李宏毅教授生成式AI課程筆記：從token預測到測試時間縮放"
date: 2025-02-23
tags: ["ai", "llm", "machine-learning", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/QLiKmca4kzI/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="QLiKmca4kzI" title="李宏毅機器學習2025 第一講" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What Can Generative AI Do?

Professor Lee starts with a demo: an AI avatar of himself, generated entirely by AI. The face is generated, the script is written by ChatGPT from a slide image, and the voice is synthesized by Breezy Voice (MediaTek) using a reference audio clip. The final video is produced by Heygen.

The real bottleneck isn't generating the lecture video - it's creating the slides. He tested ChatGPT Deep Research to generate a 13,000-word course outline, then used Gamma to turn it into slides. The result? Technically functional, but shallow. AI can generate content, but the hard part is knowing *what* to say.

## How Language Models Work

A language model is a function that takes tokens as input and outputs the probability distribution for the next token. That's it. The magic is in how this simple mechanism produces intelligent behavior.

**Tokens**: Text is split into chunks (tokens). GPT-4 uses ~100,000 possible tokens. "Hello world" might be 2 tokens; "Hello World" (capitalized) might be 3.

**Layers**: The function f is actually many smaller functions stacked together. Each layer transforms the input slightly. More layers = more transformation steps = more complex reasoning possible.

## Why Depth Matters (A Simple Analogy)

Imagine each layer is a lookup table. To add three single-digit numbers (A + B + C) in one step, you need a table with 10 × 10 × 10 = 1,000 entries.

But if you split it into two steps:
- Step 1: A + B → B' (100 entries)
- Step 2: B' + C → result (190 entries)

Total: 290 entries instead of 1,000. **Deep learning doesn't make problems more complex - it makes them simpler by breaking them into steps.**

## Test-Time Scaling: When Depth Isn't Enough

What if the problem needs more steps than you have layers? Let the model "think" - generate intermediate reasoning tokens before the final answer.

This is called **Test-Time Scaling** (or as Professor Lee puts it: "深度不夠長度來湊" - when depth isn't enough, use length).

OpenAI's o1 model uses this. Stanford's s1 paper showed that longer thinking = higher accuracy. Their method? When the model tries to output an end token, replace it with "wait" and force it to keep going.

## Transformer Architecture

Modern LLMs use Transformer architecture, which has two types of layers:
- **Self-attention**: Considers all input tokens to produce output
- **Feed-forward**: Processes each token independently

The name "Transformer"? According to the original authors in a 2024 interview: "We just thought it sounded cool."

**Limitation**: Self-attention scales poorly with input length. For very long inputs (like extended reasoning), alternatives like **Mamba** are being explored.

## Architecture vs Parameters

A neural network has two parts:
- **Architecture**: Designed by humans (the "天資" - innate talent)
- **Parameters**: Learned from data (the "後天努力" - acquired through effort)

Transformer is the architecture. The billions of numbers that make it work are the parameters.

## Training: Pre-training and Alignment

**Pre-training**: Learn from massive internet text. Predict the next token, over and over. This creates a "base model" - good at continuing text, but not at following instructions.

**Alignment**: Transform the base model into an assistant. Two approaches:
- **SFT (Supervised Fine-Tuning)**: Train on human-written conversations
- **RLHF (Reinforcement Learning from Human Feedback)**: Learn from human preferences

The result: a model that answers questions instead of just completing text.

## Giving AI New Abilities

Three approaches, from least to most invasive:

**1. Prompting**: Give instructions in the context. The model's parameters don't change - it's like an employee following company rules at work but reverting to normal behavior at home.

**2. Fine-tuning**: Modify the parameters. Risky - you might break existing abilities. Professor Lee's demo: fine-tuning GPT-4o-mini to be a TA named "小金" worked, but the model started giving bizarre answers to unrelated questions.

**3. Model Editing**: Surgically modify specific parameters. Like implanting a "thought stamp" directly into the model's brain.

**4. Model Merging**: Combine parameters from different models. One model writes code, another speaks Chinese - merge them to get both abilities without needing the original training data.

## Key Takeaways

1. LLMs are next-token predictors, but this simple mechanism produces complex behavior
2. Depth (layers) and length (thinking tokens) both enable more complex reasoning
3. Fine-tuning is powerful but dangerous - it can break existing abilities
4. The field is moving toward surgical interventions (editing) and model combination (merging)
:::

:::zh
本文整理自台灣大學李宏毅教授的「生成式AI時代下的機器學習(2025)」課程。

## 生成式AI能做什麼？

李老師用一個AI分身開場：臉是生成的，講稿是ChatGPT根據投影片圖片寫的，聲音是Breezy Voice（聯發科）用參考音檔合成的，最後用Heygen生成影片。

真正的瓶頸不是生成講課影片，而是做投影片。他測試用ChatGPT Deep Research生成了一萬三千字的課程大綱，再用Gamma轉成投影片。結果？技術上可行，但內容淺薄。AI能生成內容，但難的是知道*該說什麼*。

## 語言模型如何運作

語言模型是一個函數：輸入tokens，輸出下一個token的機率分佈。就這樣。神奇之處在於這個簡單機制如何產生智慧行為。

**Tokens**：文字被切成小塊（tokens）。GPT-4使用約10萬種可能的tokens。"Hello world"可能是2個tokens；"Hello World"（大寫）可能是3個。

**層（Layers）**：函數f其實是很多小函數疊在一起。每一層稍微轉換輸入。層數越多 = 轉換步驟越多 = 能處理越複雜的推理。

## 為什麼深度重要（簡單比喻）

想像每一層是一個查表。要一步算出三個個位數相加（A + B + C），需要10 × 10 × 10 = 1000筆資料的表格。

但如果拆成兩步：
- 第一步：A + B → B'（100筆）
- 第二步：B' + C → 結果（190筆）

總共：290筆而不是1000筆。**深度學習不是讓問題變複雜，而是把問題拆解成簡單的步驟。**

## 測試時間縮放：深度不夠怎麼辦

如果問題需要的步驟比層數還多怎麼辦？讓模型「思考」——在最終答案前生成中間推理的tokens。

這叫做**Test-Time Scaling**（或者李老師說的：「深度不夠長度來湊」）。

OpenAI的o1模型用這個方法。Stanford的s1論文顯示：想得越長，正確率越高。他們的方法？當模型要輸出結束符號時，直接換成"wait"，強迫它繼續講下去。

## Transformer架構

現代LLM使用Transformer架構，有兩種層：
- **Self-attention**：考慮所有輸入tokens來產生輸出
- **Feed-forward**：獨立處理每個token

為什麼叫"Transformer"？根據原作者2024年的訪談：「我們只是覺得這名字很酷。」

**限制**：Self-attention在輸入很長時效率差。對於很長的輸入（如延長推理），正在探索**Mamba**等替代方案。

## 架構 vs 參數

神經網路有兩部分：
- **架構**：人類設計的（「天資」）
- **參數**：從資料學習的（「後天努力」）

Transformer是架構。讓它運作的數十億個數字是參數。

## 訓練：預訓練與對齊

**預訓練**：從大量網路文字學習。一直預測下一個token。這產生「基礎模型」——擅長接續文字，但不擅長遵循指令。

**對齊**：把基礎模型變成助手。兩種方法：
- **SFT（監督式微調）**：用人寫的對話訓練
- **RLHF（人類回饋強化學習）**：從人類偏好學習

結果：模型會回答問題，而不只是接續文字。

## 賦予AI新能力

三種方法，從最輕到最重：

**1. 提示（Prompting）**：在上下文中給指令。模型參數不變——就像員工在公司遵守規定，回家就恢復原樣。

**2. 微調（Fine-tuning）**：修改參數。有風險——可能破壞原有能力。李老師的示範：微調GPT-4o-mini變成助教「小金」成功了，但模型開始對無關問題給出奇怪答案（問誰是美國總統，回答李宏毅）。

**3. 模型編輯（Model Editing）**：精準修改特定參數。像直接在模型腦中植入「思想鋼印」。

**4. 模型合併（Model Merging）**：合併不同模型的參數。一個模型會寫程式，另一個會說中文——合併後兩種能力都有，不需要原始訓練資料。

## 重點整理

1. LLM是下一個token預測器，但這個簡單機制產生複雜行為
2. 深度（層數）和長度（思考tokens）都能實現更複雜的推理
3. 微調很強大但危險——可能破壞原有能力
4. 領域正朝向精準介入（編輯）和模型組合（合併）發展
:::
