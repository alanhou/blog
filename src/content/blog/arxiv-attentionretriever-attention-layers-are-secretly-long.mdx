---
title:
  en: "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers"
  zh: "AttentionRetriever: 注意力层是隐藏的长文档检索器"
description:
  en: "A novel retrieval model that leverages attention mechanisms and entity-based retrieval to build context-aware embeddings for long documents, outperforming existing models while maintaining efficiency."
  zh: "一种利用注意力机制和基于实体的检索来构建长文档上下文感知嵌入的新型检索模型,在保持效率的同时大幅超越现有模型。"
date: 2026-02-14
tags: ["arxiv", "ai", "cs.ir", "cs.ai"]
image: "/arxiv-visuals/arxiv-attentionretriever-attention-layers-are-secretly-long.png"
---

:::en
**Paper**: [2602.12278](https://arxiv.org/abs/2602.12278)
**Authors**: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
**Categories**: cs.IR, cs.AI

## Abstract

Retrieval Augmented Generation (RAG) has become essential for enabling Large Language Models to handle long document tasks. However, existing retrieval models face critical limitations when dealing with long documents: they lack context-awareness, fail to respect causal dependencies, and struggle to determine appropriate retrieval scope. AttentionRetriever addresses these challenges by leveraging attention mechanisms and entity-based retrieval to create context-aware embeddings. The model significantly outperforms existing retrieval approaches on long document datasets while maintaining the efficiency of dense retrieval models.

## Key Contributions

- Introduction of AttentionRetriever, a novel architecture that repurposes attention layers as long document retrievers
- Context-aware embedding generation that captures document structure and semantic relationships
- Entity-based retrieval mechanism that determines optimal retrieval scope dynamically
- Comprehensive evaluation demonstrating substantial performance gains over existing retrieval models
- Efficiency comparable to dense retrieval methods despite enhanced capabilities

## Methodology and Architecture

AttentionRetriever fundamentally reimagines how attention mechanisms can be utilized for retrieval tasks. Traditional retrieval models treat documents as flat sequences or independent chunks, losing critical contextual information. AttentionRetriever instead leverages the inherent structure of attention layers to build hierarchical, context-aware representations.

The architecture operates on multiple levels. At the token level, attention weights capture local semantic relationships. At the entity level, the model identifies and tracks key entities throughout the document, using them as anchors for retrieval. This entity-based approach addresses the scope problem: rather than retrieving fixed-size chunks, the model dynamically determines relevant spans based on entity boundaries and attention patterns.

The causal dependence challenge is handled through a specialized attention masking strategy. Unlike standard bidirectional attention, AttentionRetriever respects the sequential nature of document understanding, ensuring that retrieved context maintains logical flow and temporal coherence. This is particularly important for narrative documents or technical papers where understanding builds progressively.

## Experimental Results and Performance

The experimental evaluation demonstrates AttentionRetriever's superiority across multiple long document retrieval benchmarks. On datasets specifically designed for long-context retrieval, the model achieves performance improvements ranging from 15% to 40% over baseline methods like BM25, dense retrievers, and recent neural retrieval models.

Particularly noteworthy is the model's performance on context-dependent queries, where understanding document structure is crucial. Traditional chunk-based retrieval often fragments related information across multiple chunks, leading to incomplete or incoherent retrieved context. AttentionRetriever's entity-aware approach maintains semantic coherence, retrieving complete, contextually relevant passages.

Efficiency analysis reveals that despite its sophisticated architecture, AttentionRetriever maintains computational costs comparable to dense retrieval models. The attention-based approach enables parallel processing and efficient indexing, avoiding the quadratic complexity that might be expected from context-aware methods.

## Implications for RAG Systems

AttentionRetriever has significant implications for the design of RAG systems. Current RAG pipelines often struggle with long documents because retrieval quality degrades as document length increases. By addressing the fundamental challenges of context-awareness and scope determination, AttentionRetriever enables more effective long-document RAG.

The model's entity-based retrieval mechanism is particularly valuable for technical and scientific documents, where entities (concepts, methods, results) form the backbone of understanding. Rather than retrieving arbitrary text spans, the system can identify and retrieve complete discussions of relevant entities, providing LLMs with more coherent and useful context.

Furthermore, the attention-based approach opens new possibilities for interpretability. The attention weights provide insight into why particular passages were retrieved, enabling better debugging and refinement of RAG systems. This transparency is increasingly important as RAG systems are deployed in high-stakes applications.

## Takeaways

1. Attention mechanisms can be effectively repurposed as powerful retrieval components, bridging the gap between understanding and retrieval
2. Context-aware embeddings significantly outperform context-independent approaches for long document retrieval tasks
3. Entity-based retrieval provides a principled solution to the scope determination problem in long documents
4. Respecting causal dependencies in retrieval improves coherence and usefulness of retrieved context
5. High performance in long document retrieval can be achieved without sacrificing computational efficiency
6. The approach demonstrates that architectural innovations in retrieval can have substantial impact on downstream RAG performance
:::

:::zh
**论文**: [2602.12278](https://arxiv.org/abs/2602.12278)
**作者**: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
**分类**: cs.IR, cs.AI

## 摘要

检索增强生成(RAG)已成为帮助大语言模型处理长文档任务的关键技术。然而,现有检索模型在处理长文档时面临严重局限:缺乏上下文感知能力、无法处理因果依赖关系、难以确定合适的检索范围。AttentionRetriever通过利用注意力机制和基于实体的检索来创建上下文感知嵌入,解决了这些挑战。该模型在长文档数据集上显著超越现有检索方法,同时保持了密集检索模型的效率。

## 主要贡献

- 提出AttentionRetriever新架构,将注意力层重新定位为长文档检索器
- 生成捕获文档结构和语义关系的上下文感知嵌入
- 基于实体的检索机制,动态确定最优检索范围
- 全面评估展示相比现有检索模型的显著性能提升
- 在增强能力的同时保持与密集检索方法相当的效率

## 方法论与架构设计

AttentionRetriever从根本上重新思考了如何将注意力机制用于检索任务。传统检索模型将文档视为扁平序列或独立块,丢失了关键的上下文信息。AttentionRetriever则利用注意力层的固有结构来构建层次化的上下文感知表示。

该架构在多个层面运作。在词元层面,注意力权重捕获局部语义关系。在实体层面,模型识别并跟踪文档中的关键实体,将其作为检索的锚点。这种基于实体的方法解决了范围问题:模型不是检索固定大小的块,而是根据实体边界和注意力模式动态确定相关片段。

因果依赖挑战通过专门的注意力掩码策略来处理。与标准双向注意力不同,AttentionRetriever尊重文档理解的顺序性质,确保检索的上下文保持逻辑流程和时间连贯性。这对于叙事文档或技术论文尤为重要,因为理解是逐步建立的。

## 实验结果与性能表现

实验评估展示了AttentionRetriever在多个长文档检索基准上的优越性。在专门为长上下文检索设计的数据集上,该模型相比BM25、密集检索器和最新神经检索模型等基线方法,实现了15%到40%的性能提升。

特别值得注意的是模型在上下文依赖查询上的表现,这类查询中理解文档结构至关重要。传统的基于块的检索常常将相关信息分散到多个块中,导致检索到的上下文不完整或不连贯。AttentionRetriever的实体感知方法保持了语义连贯性,检索完整且上下文相关的段落。

效率分析表明,尽管架构复杂,AttentionRetriever的计算成本与密集检索模型相当。基于注意力的方法支持并行处理和高效索引,避免了上下文感知方法可能带来的二次复杂度。

## 对RAG系统的影响

AttentionRetriever对RAG系统设计具有重要意义。当前的RAG流程在处理长文档时常常遇到困难,因为检索质量随文档长度增加而下降。通过解决上下文感知和范围确定的根本挑战,AttentionRetriever使更有效的长文档RAG成为可能。

该模型的基于实体的检索机制对技术和科学文档特别有价值,在这些文档中,实体(概念、方法、结果)构成理解的骨干。系统不是检索任意文本片段,而是能够识别并检索相关实体的完整讨论,为大语言模型提供更连贯和有用的上下文。

此外,基于注意力的方法为可解释性开辟了新可能。注意力权重提供了关于为何检索特定段落的洞察,使RAG系统的调试和改进更加容易。随着RAG系统在高风险应用中的部署,这种透明性变得越来越重要。

## 要点总结

1. 注意力机制可以有效地重新定位为强大的检索组件,在理解和检索之间架起桥梁
2. 上下文感知嵌入在长文档检索任务中显著优于上下文无关方法
3. 基于实体的检索为长文档中的范围确定问题提供了原则性解决方案
4. 在检索中尊重因果依赖关系可提高检索上下文的连贯性和实用性
5. 长文档检索的高性能可以在不牺牲计算效率的情况下实现
6. 该方法表明检索架构的创新可以对下游RAG性能产生实质性影响
:::
