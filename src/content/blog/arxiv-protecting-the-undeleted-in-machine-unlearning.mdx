---
title:
  en: "Protecting the Undeleted in Machine Unlearning"
  zh: "机器遗忘中对未删除数据的保护"
description:
  en: "This paper reveals critical privacy vulnerabilities in machine unlearning systems that aim for perfect retraining, showing how deletion requests can leak information about undeleted data, and proposes a new security definition to protect remaining data points."
  zh: "本文揭示了追求完美重训练的机器遗忘系统中的关键隐私漏洞,展示了删除请求如何泄露未删除数据的信息,并提出了保护剩余数据点的新安全定义。"
date: 2026-02-19
tags: ["arxiv", "ai", "cs.lg", "cs.ds"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.16697](https://arxiv.org/abs/2602.16697)
**Authors**: Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer
**Categories**: cs.LG, cs.DS

## Abstract

Machine unlearning has emerged as a critical technique for removing specific data points from trained models, typically aiming to achieve "perfect retraining"—producing a model identical to one trained without the deleted data. This paper exposes a fundamental security flaw in this approach: mechanisms that enable perfect retraining inadvertently create severe privacy risks for the undeleted data. The authors demonstrate a reconstruction attack where an adversary controlling only $\omega(1)$ data points can reconstruct nearly the entire dataset through strategic deletion requests. After surveying existing security definitions and finding them either vulnerable to such attacks or too restrictive for practical use, the paper proposes a novel security definition that explicitly protects undeleted data from leakage caused by deletions while still supporting essential functionalities like summations and statistical learning.

## Key Contributions

- **Reconstruction Attack**: Demonstrates that perfect retraining mechanisms allow adversaries to reconstruct almost entire datasets by controlling a super-constant number of data points and issuing strategic deletion requests
- **Security Analysis**: Comprehensive survey of existing machine unlearning definitions, revealing they either permit dangerous reconstruction attacks or are too restrictive to support basic operations
- **New Security Definition**: Proposes a security framework that specifically safeguards undeleted data against leakage from deletion operations while maintaining practical utility
- **Feasibility Results**: Proves that the new definition permits essential functionalities including bulletin boards, exact summations, and statistical learning tasks

## The Fundamental Vulnerability

The core insight of this work is that perfect retraining, while seemingly ideal, creates an exploitable information channel. When a mechanism guarantees that deleting data point $x$ produces exactly the model that would have been trained without $x$, it implicitly reveals information about how $x$ relates to other data points in the dataset.

The reconstruction attack works as follows: an adversary who controls $\omega(1)$ data points can strategically insert and delete these points while observing the resulting model changes. Since the mechanism must produce the "correct" retrained model after each deletion, the pattern of changes reveals information about the undeleted data. For certain tasks that would be secure without deletions, this attack can reconstruct nearly the entire dataset.

Mathematically, if $M$ is a mechanism achieving perfect retraining and $D$ is a dataset, then for any subset $S \subseteq D$, we have:

$$M(D \setminus S) = M_{\text{retrain}}(D \setminus S)$$

where $M_{\text{retrain}}$ represents training from scratch. This equality, while desirable for utility, creates the vulnerability.

## Existing Definitions and Their Limitations

The paper analyzes several existing security definitions for machine unlearning:

**Perfect Retraining**: Requires the output after deletion to be indistinguishable from retraining without the deleted data. While providing strong utility guarantees, it's vulnerable to the reconstruction attack.

**Differential Privacy-Based Definitions**: Some approaches require the unlearning mechanism to satisfy differential privacy with respect to deleted points. However, these definitions often fail to protect undeleted data or become too restrictive, preventing even simple operations like exact summation.

**Indistinguishability-Based Definitions**: Require that an adversary cannot distinguish whether a point was deleted. These can be either too weak (allowing reconstruction) or too strong (preventing useful functionality).

The fundamental issue is that existing definitions focus on protecting the deleted data or ensuring correct deletion behavior, but neglect the privacy of the remaining dataset.

## The Proposed Security Definition

The new definition introduces the concept of **undeleted data protection**. Informally, a mechanism is secure if the view of an adversary (including all outputs and intermediate states) after deletions reveals no more information about undeleted data than what would be revealed by the mechanism without any deletions.

Formally, for a mechanism $M$ and datasets $D, D'$ that differ only in undeleted points, the distributions of adversary views should be computationally indistinguishable:

$$\{View_A(M, D, \text{deletions})\} \approx_c \{View_A(M, D', \text{deletions})\}$$

This definition ensures that deletion operations don't create new privacy leakage channels for undeleted data, while still allowing the mechanism to respond appropriately to deletions.

## Feasibility and Applications

The paper proves that several important functionalities can be implemented under the new security definition:

**Bulletin Boards**: Systems that store and retrieve data can support deletions while protecting undeleted entries. The key insight is that revealing which entries remain doesn't violate the definition, as long as the content of undeleted entries isn't leaked through the deletion process.

**Summation Queries**: Exact summation over remaining data points can be supported. After deleting points in set $S$ from dataset $D$, the mechanism can output $\sum_{x \in D \setminus S} x$ without violating undeleted data protection.

**Statistical Learning**: The definition permits training machine learning models on the remaining data, provided the learning algorithm itself doesn't leak information about the training data beyond what's necessary for the learning task. This can be achieved by combining the unlearning mechanism with differentially private learning algorithms.

## Implications for Practice

This work has significant implications for real-world machine unlearning systems:

**Regulatory Compliance**: Systems designed to comply with data deletion regulations (like GDPR's "right to be forgotten") must consider not just whether deleted data is removed, but whether the deletion process itself creates new privacy risks for other users.

**System Design**: Practitioners should avoid blindly pursuing perfect retraining as a goal. Instead, they should consider the privacy implications of their unlearning mechanisms on the entire dataset, not just deleted points.

**Security Auditing**: Organizations implementing machine unlearning should audit their systems for potential reconstruction attacks, especially if adversaries can control multiple data points or issue many deletion requests.

**Trade-offs**: The paper clarifies the fundamental trade-off between utility (how closely unlearning approximates retraining) and privacy (protecting undeleted data). System designers must make explicit choices about where to position themselves on this spectrum.

## Takeaways

1. Perfect retraining in machine unlearning, while intuitive, creates exploitable privacy vulnerabilities that allow reconstruction of undeleted data through strategic deletion requests.

2. An adversary controlling only $\omega(1)$ data points can potentially reconstruct nearly an entire dataset by observing model changes after deletions in systems that guarantee perfect retraining.

3. Existing security definitions for machine unlearning either fail to protect undeleted data or are too restrictive to support basic functionalities like exact summation.

4. The proposed security definition explicitly protects undeleted data from leakage caused by deletion operations while still permitting essential functionalities including bulletin boards, summations, and statistical learning.

5. Practical machine unlearning systems must balance the goal of accurate deletion with the privacy requirements of the remaining dataset, recognizing that perfect retraining may not be the optimal objective when considering overall system security.
:::

:::zh
**论文**: [2602.16697](https://arxiv.org/abs/2602.16697)
**作者**: Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer
**分类**: cs.LG, cs.DS

## 摘要

机器遗忘已成为从训练模型中移除特定数据点的关键技术,通常旨在实现"完美重训练"——生成与从未包含被删除数据的训练模型完全相同的模型。本文揭示了这种方法的根本性安全缺陷:实现完美重训练的机制会无意中为未删除的数据带来严重的隐私风险。作者展示了一种重构攻击,其中仅控制$\omega(1)$个数据点的对手可以通过策略性删除请求重构几乎整个数据集。在调研现有安全定义并发现它们要么容易受到此类攻击,要么过于严格而无法实际使用后,本文提出了一个新的安全定义,明确保护未删除数据免受删除操作导致的泄露,同时仍支持求和和统计学习等基本功能。

## 主要贡献

- **重构攻击**:证明完美重训练机制允许对手通过控制超常数个数据点并发起策略性删除请求来重构几乎整个数据集
- **安全性分析**:对现有机器遗忘定义进行全面调研,揭示它们要么允许危险的重构攻击,要么过于严格而无法支持基本操作
- **新安全定义**:提出一个安全框架,专门保护未删除数据免受删除操作的泄露,同时保持实用性
- **可行性结果**:证明新定义允许包括公告板、精确求和和统计学习任务在内的基本功能

## 根本性漏洞

这项工作的核心洞察是,完美重训练虽然看似理想,却创建了一个可被利用的信息通道。当一个机制保证删除数据点$x$会产生与不包含$x$训练出的模型完全相同的结果时,它隐式地揭示了$x$与数据集中其他数据点的关系信息。

重构攻击的工作原理如下:控制$\omega(1)$个数据点的对手可以策略性地插入和删除这些点,同时观察产生的模型变化。由于机制必须在每次删除后产生"正确的"重训练模型,变化模式会揭示关于未删除数据的信息。对于某些在没有删除时本应安全的任务,这种攻击可以重构几乎整个数据集。

从数学上讲,如果$M$是实现完美重训练的机制,$D$是数据集,那么对于任何子集$S \subseteq D$,我们有:

$$M(D \setminus S) = M_{\text{retrain}}(D \setminus S)$$

其中$M_{\text{retrain}}$表示从头训练。这种等式虽然对效用有利,却创造了漏洞。

## 现有定义及其局限性

本文分析了几种现有的机器遗忘安全定义:

**完美重训练**:要求删除后的输出与不包含被删除数据的重训练结果无法区分。虽然提供了强大的效用保证,但容易受到重构攻击。

**基于差分隐私的定义**:一些方法要求遗忘机制对被删除点满足差分隐私。然而,这些定义往往无法保护未删除数据,或变得过于严格,甚至阻止精确求和等简单操作。

**基于不可区分性的定义**:要求对手无法区分某个点是否被删除。这些定义可能过弱(允许重构)或过强(阻止有用功能)。

根本问题在于现有定义关注保护被删除数据或确保正确的删除行为,但忽视了剩余数据集的隐私。

## 提出的安全定义

新定义引入了**未删除数据保护**的概念。非正式地说,如果对手的视图(包括所有输出和中间状态)在删除后揭示的关于未删除数据的信息不超过没有任何删除时机制所揭示的信息,则该机制是安全的。

形式化地,对于机制$M$和仅在未删除点上不同的数据集$D, D'$,对手视图的分布应该在计算上不可区分:

$$\{View_A(M, D, \text{deletions})\} \approx_c \{View_A(M, D', \text{deletions})\}$$

这个定义确保删除操作不会为未删除数据创建新的隐私泄露通道,同时仍允许机制适当地响应删除。

## 可行性与应用

本文证明了几个重要功能可以在新安全定义下实现:

**公告板**:存储和检索数据的系统可以支持删除,同时保护未删除条目。关键洞察是,揭示哪些条目保留不违反定义,只要未删除条目的内容不通过删除过程泄露。

**求和查询**:可以支持对剩余数据点的精确求和。从数据集$D$中删除集合$S$中的点后,机制可以输出$\sum_{x \in D \setminus S} x$而不违反未删除数据保护。

**统计学习**:该定义允许在剩余数据上训练机器学习模型,前提是学习算法本身不会泄露超出学习任务所需的训练数据信息。这可以通过将遗忘机制与差分隐私学习算法结合来实现。

## 实践意义

这项工作对现实世界的机器遗忘系统具有重要意义:

**监管合规**:旨在遵守数据删除法规(如GDPR的"被遗忘权")的系统必须考虑的不仅是被删除数据是否被移除,还要考虑删除过程本身是否为其他用户创建了新的隐私风险。

**系统设计**:实践者应避免盲目追求完美重训练作为目标。相反,他们应该考虑遗忘机制对整个数据集的隐私影响,而不仅仅是被删除点。

**安全审计**:实施机器遗忘的组织应审计其系统是否存在潜在的重构攻击,特别是当对手可以控制多个数据点或发起许多删除请求时。

**权衡取舍**:本文阐明了效用(遗忘与重训练的接近程度)和隐私(保护未删除数据)之间的根本权衡。系统设计者必须明确选择在这个谱系上的定位。

## 要点总结

1. 机器遗忘中的完美重训练虽然直观,但会创建可被利用的隐私漏洞,允许通过策略性删除请求重构未删除数据。

2. 仅控制$\omega(1)$个数据点的对手可以通过观察保证完美重训练的系统中删除后的模型变化,潜在地重构几乎整个数据集。

3. 现有的机器遗忘安全定义要么无法保护未删除数据,要么过于严格而无法支持精确求和等基本功能。

4. 提出的安全定义明确保护未删除数据免受删除操作导致的泄露,同时仍允许包括公告板、求和和统计学习在内的基本功能。

5. 实用的机器遗忘系统必须平衡准确删除的目标与剩余数据集的隐私要求,认识到在考虑整体系统安全性时,完美重训练可能不是最优目标。
:::
