---
title:
  en: "Hung-yi Lee ML 2025 Lecture 4: Beyond Transformers - Linear Attention, Mamba, and the RNN Renaissance"
  zh: "李宏毅机器学习2025 第四讲：超越Transformer - 线性注意力、Mamba与RNN的复兴"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on Transformer alternatives: why Self-Attention won, how Linear Attention is secretly RNN, and the Mamba architecture"
  zh: "台大李宏毅教授课程笔记：Transformer的替代方案，Self-Attention为何胜出，Linear Attention其实就是RNN，以及Mamba架构"
date: 2025-02-23
tags: ["ai", "llm", "transformer", "mamba", "linear-attention", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/gjsdVi90yQo/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 4
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="gjsdVi90yQo" title="李宏毅机器学习2025 第四讲：Transformer的替代方案" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## Why Every Architecture Exists

Professor Lee starts with a philosophical point: every neural network architecture exists for a reason, and those reasons differ.

**CNN**: A special case of fully-connected networks with receptive fields and parameter sharing. Exists to reduce unnecessary parameters for image processing, preventing overfitting.

**Residual Connections**: Exist to make optimization easier. Without them, deeper networks are harder to train - not because of overfitting, but because the loss landscape becomes rugged with many local minima. Residual connections smooth the error surface.

So when you see a new architecture, ask: *what problem was it designed to solve?*

## The RNN Family

Before Self-Attention dominated, RNN-style architectures ruled sequence processing. The core idea: maintain a **hidden state** that accumulates information from all previous inputs.

The general RNN formula:
- $h_t = f_a(h_{t-1}) + f_b(x_t)$ (update hidden state)
- $y_t = f_c(h_t)$ (produce output)

Where:
- $h$ is the hidden state (can be a vector or matrix)
- $f_a$ is the "reflection" function (processes previous memory)
- $f_b$ is the "write" function (incorporates new input)
- $f_c$ is the "read" function (extracts output)

This mirrors the AI agent memory system from Lecture 2: write module, reflection module, read module.

**LSTM and GRU** are RNNs where $f_a$, $f_b$, $f_c$ vary with time (controlled by gates). This allows the model to decide what to remember and what to forget.

## Why Self-Attention Won

The real advantage of Self-Attention isn't "infinite memory" - that's an illusion. The real advantage is **parallelization during training**.

**RNN's problem**: To compute $h_6$, you must first compute $h_1, h_2, h_3, h_4, h_5$ sequentially. GPUs hate waiting.

**Self-Attention's advantage**: Given a complete input sequence, you can compute all outputs $y_1$ through $y_T$ in parallel. Everything is matrix multiplication, which GPUs love.

During training, you have the complete target sentence. Self-Attention can process it all at once. RNN must process token by token.

## The Inference Problem

But at inference time, the tables turn:

| | Self-Attention | RNN |
|---|---|---|
| Computation | Grows with sequence length | Fixed per step |
| Memory | Grows with sequence length | Fixed (just hidden state) |
| Training | Highly parallelizable | Sequential, slow |

As context windows grow (GPT-3.5: one Harry Potter chapter → Gemini 1.5: 2 million tokens), Self-Attention's quadratic scaling becomes painful.

## Linear Attention: The Bridge

Here's the mind-blowing revelation: **Linear Attention is just RNN without the reflection function**.

Start with RNN, remove $f_a$ (reflection):
$$h_t = h_{t-1} + f_b(x_t)$$

Expand this:
- $h_1 = f_b(x_1)$
- $h_2 = f_b(x_1) + f_b(x_2)$
- $h_T = f_b(x_1) + f_b(x_2) + ... + f_b(x_T)$

Now assume:
- $h$ is a $D \times D$ matrix
- $f_b(x_t) = v_t \cdot k_t^T$ (outer product of two vectors)
- Output: $y_t = h_t \cdot q_t$

Work through the math, and you get:
$$y_t = \sum_{i=1}^{t} (k_i^T \cdot q_t) \cdot v_i$$

This is exactly Self-Attention... minus the softmax!

**Linear Attention = Self-Attention without softmax = RNN without reflection**

This was known since 2020 (paper: "Transformers are RNNs"). The implication: Linear Attention can be trained like a Transformer (parallel) but run inference like an RNN (efficient).

## Why Softmax Matters

If Linear Attention is so similar, why does it underperform Self-Attention?

**The memory limit myth**: People think RNN has limited memory while Attention has infinite memory. But both have limits! In a D-dimensional space, you can only have D orthogonal vectors. With sequences longer than D, keys will inevitably have non-zero inner products with unintended queries.

**The real difference**: Softmax enables relative importance.

Without softmax: An attention weight of 1.0 always means "important."

With softmax: Importance is relative. If a new item has attention weight 2.0, the old item with weight 1.0 becomes less important (0.45 → 0.17 after softmax).

Professor Lee's analogy: In "Solo Leveling," Iron seemed strong until Igris appeared. Igris seemed strong until Beru appeared. Importance is relative.

**Linear Attention's fatal flaw**: Memory never changes. Once information enters, it stays forever. No forgetting mechanism.

## Adding Forgetting: RetNet and Gated Retention

**RetNet**: Multiply $h_{t-1}$ by a constant $\gamma$ (between 0 and 1):
$$h_t = \gamma \cdot h_{t-1} + v_t \cdot k_t^T$$

This causes gradual forgetting. Still parallelizable during training.

**Gated Retention**: Make $\gamma$ time-dependent:
$$\gamma_t = \sigma(x_t \cdot W_\gamma)$$

Now the model learns when to forget. See a paragraph break? Set $\gamma_t \approx 0$ to clear memory.

**More complex forgetting**: Use a matrix $G_t = \mathbf{1} \cdot s_t^T$ for element-wise multiplication with $h_{t-1}$. This controls forgetting per memory column.

## Mamba: The Famous Challenger

Mamba (late 2023) was the first Linear Attention variant to genuinely beat Transformers. It uses a complex state-space formulation, but Mamba2 simplified to essentially Gated Retention.

Key results:
- Slightly outperforms Transformer++ across model sizes
- Massive inference speedup (much higher tokens/second)

The famous meme: A giant stone statue (Mamba) towering over Transformers. Professor Lee notes this is actually from "Solo Leveling" - and realizes Meruem (the Chimera Ant King) is basically Beru with a different name.

## Delta Net: Memory as Gradient Descent

Delta Net introduces a clever idea: before writing new information, first erase what's already in that memory location.

The update rule can be rewritten as gradient descent:
$$h_t = h_{t-1} - \beta_k (h_{t-1} k_t - v_t) k_t^T$$

This is gradient descent on the loss: $L = \frac{1}{2}\|h \cdot k_t - v_t\|^2$

The interpretation: Update memory so that querying with $k_t$ retrieves $v_t$ as accurately as possible.

This led to **Titan** (January 2025): "Learning to Memorize at Test Time" - treating memory updates as learned gradient descent.

## Current State of the Field

**Large-scale validation**:
- JAMBA: Mamba-based, up to 52B parameters
- MiniMax-01: Linear Attention, 400B+ parameters

**Image generation**: SANA uses Linear Attention for fast inference.

**But not everywhere**: "MambaOut" paper shows Mamba doesn't help image classification - you don't need long-range attention for that task.

**Practical approach**: Instead of training from scratch, fine-tune existing models (like LLaMA) by swapping Self-Attention for Mamba layers.

## The Bet

There's an actual bet on whether "Attention is All You Need" will still hold in 2027:
- **For**: Jonathan Frankle (Harvard, Mosaic ML)
- **Against**: Sasha Rush (Cornell, HuggingFace)

656 days until we find out.

## Key Takeaways

1. Every architecture exists for a reason - understand the "why"
2. Self-Attention won because of training parallelization, not infinite memory
3. Linear Attention = Self-Attention - softmax = RNN - reflection
4. Softmax enables relative importance and implicit forgetting
5. Mamba and variants add learnable forgetting to Linear Attention
6. The field is converging: train like Transformer, infer like RNN
:::

:::zh
本文整理自台湾大学李宏毅教授的「生成式AI时代下的机器学习(2025)」课程。

## 每种架构存在的理由

李老师开场就提出一个哲学观点：每种神经网络架构都有它存在的理由，而且这些理由各不相同。

**CNN**：是全连接网络的特例，加上感受野和参数共享。存在的理由是为图像处理减少不必要的参数，防止过拟合。

**残差连接**：存在的理由是让优化更容易。没有残差连接时，更深的网络更难训练——不是因为过拟合，而是因为损失曲面变得崎岖，有很多局部最小值。残差连接让误差曲面变得平滑。

所以当你看到新架构时，要问：*它是为了解决什么问题而设计的？*

## RNN家族

在Self-Attention称霸之前，RNN风格的架构统治着序列处理。核心思想：维护一个**隐藏状态**，累积所有之前输入的信息。

RNN的通用公式：
- $h_t = f_a(h_{t-1}) + f_b(x_t)$（更新隐藏状态）
- $y_t = f_c(h_t)$（产生输出）

其中：
- $h$ 是隐藏状态（可以是向量或矩阵）
- $f_a$ 是「反思」函数（处理之前的记忆）
- $f_b$ 是「写入」函数（纳入新输入）
- $f_c$ 是「读取」函数（提取输出）

这跟第二讲的AI agent记忆系统如出一辙：写入模块、反思模块、读取模块。

**LSTM和GRU**是让$f_a$、$f_b$、$f_c$随时间变化的RNN（由门控制）。这让模型可以决定什么要记住、什么要遗忘。

## Self-Attention为何胜出

Self-Attention真正的优势不是「无限记忆」——那是假象。真正的优势是**训练时的平行化**。

**RNN的问题**：要计算$h_6$，必须先依序计算$h_1, h_2, h_3, h_4, h_5$。GPU最讨厌等待。

**Self-Attention的优势**：给定完整的输入序列，可以平行计算所有输出$y_1$到$y_T$。全部都是矩阵乘法，GPU最喜欢。

训练时，你有完整的目标句子。Self-Attention可以一次处理完。RNN必须一个token一个token处理。

## 推理时的问题

但在推理时，情况反转：

| | Self-Attention | RNN |
|---|---|---|
| 计算量 | 随序列长度增长 | 每步固定 |
| 内存 | 随序列长度增长 | 固定（只有隐藏状态）|
| 训练 | 高度可平行化 | 顺序执行，慢 |

随着上下文窗口增长（GPT-3.5：一章哈利波特 → Gemini 1.5：200万token），Self-Attention的二次方增长变得很痛苦。

## 线性注意力：桥梁

这里有个惊人的发现：**线性注意力就是没有反思函数的RNN**。

从RNN开始，移除$f_a$（反思）：
$$h_t = h_{t-1} + f_b(x_t)$$

展开：
- $h_1 = f_b(x_1)$
- $h_2 = f_b(x_1) + f_b(x_2)$
- $h_T = f_b(x_1) + f_b(x_2) + ... + f_b(x_T)$

现在假设：
- $h$ 是 $D \times D$ 矩阵
- $f_b(x_t) = v_t \cdot k_t^T$（两个向量的外积）
- 输出：$y_t = h_t \cdot q_t$

推导下去，你会得到：
$$y_t = \sum_{i=1}^{t} (k_i^T \cdot q_t) \cdot v_i$$

这正是Self-Attention...只是少了softmax！

**线性注意力 = Self-Attention没有softmax = RNN没有反思**

这在2020年就知道了（论文：「Transformers are RNNs」）。含义是：线性注意力可以像Transformer一样训练（平行），但像RNN一样推理（高效）。

## Softmax为何重要

如果线性注意力这么相似，为什么表现不如Self-Attention？

**记忆限制的迷思**：人们以为RNN记忆有限而Attention记忆无限。但两者都有限！在D维空间中，最多只能有D个正交向量。序列长度超过D时，key不可避免地会与非目标query有非零内积。

**真正的差别**：Softmax实现相对重要性。

没有softmax：注意力权重1.0永远代表「重要」。

有softmax：重要性是相对的。如果新项目的注意力权重是2.0，旧项目权重1.0就变得不那么重要了（softmax后从0.45变成0.17）。

李老师的比喻：在《我独自升级》里，爱恩看起来很强，直到尖牙出现。尖牙看起来很强，直到贝尔出现。重要性是相对的。

**线性注意力的致命缺陷**：记忆永远不会改变。信息一旦进入，就永远留在那里。没有遗忘机制。

## 加入遗忘：RetNet和门控保留

**RetNet**：在$h_{t-1}$前面乘一个常数$\gamma$（介于0和1之间）：
$$h_t = \gamma \cdot h_{t-1} + v_t \cdot k_t^T$$

这造成逐渐遗忘。训练时仍可平行化。

**门控保留**：让$\gamma$随时间变化：
$$\gamma_t = \sigma(x_t \cdot W_\gamma)$$

现在模型学习何时遗忘。看到段落分隔？设$\gamma_t \approx 0$清空记忆。

**更复杂的遗忘**：用矩阵$G_t = \mathbf{1} \cdot s_t^T$与$h_{t-1}$做逐元素相乘。这可以控制每个记忆列的遗忘。

## Mamba：著名的挑战者

Mamba（2023年底）是第一个真正打败Transformer的线性注意力变体。它使用复杂的状态空间公式，但Mamba2简化成基本上就是门控保留。

关键结果：
- 在各种模型大小上略微超越Transformer++
- 推理速度大幅提升（每秒处理更多token）

著名的梗图：一个巨大的石像（Mamba）俯视着Transformer们。李老师注意到这其实来自《我独自升级》——并发现梅路艾姆（蚁王）基本上就是贝尔换了个名字。

## Delta Net：记忆即梯度下降

Delta Net引入一个巧妙的想法：写入新信息之前，先擦除该记忆位置原有的内容。

更新规则可以改写成梯度下降：
$$h_t = h_{t-1} - \beta_k (h_{t-1} k_t - v_t) k_t^T$$

这是对损失函数的梯度下降：$L = \frac{1}{2}\|h \cdot k_t - v_t\|^2$

解释：更新记忆，使得用$k_t$查询能尽可能准确地取回$v_t$。

这导致了**Titan**（2025年1月）：「Learning to Memorize at Test Time」——把记忆更新当作学习到的梯度下降。

## 领域现状

**大规模验证**：
- JAMBA：基于Mamba，最大52B参数
- MiniMax-01：线性注意力，400B+参数

**图像生成**：SANA使用线性注意力加速推理。

**但不是到处都适用**：「MambaOut」论文显示Mamba对图像分类没有帮助——那个任务不需要长距离注意力。

**实用方法**：与其从头训练，不如微调现有模型（如LLaMA），把Self-Attention换成Mamba层。

## 赌局

有一个真实的赌局，赌「Attention is All You Need」到2027年是否还成立：
- **支持方**：Jonathan Frankle（哈佛，Mosaic ML）
- **反对方**：Sasha Rush（康奈尔，HuggingFace）

还有656天揭晓。

## 重点整理

1. 每种架构都有存在的理由——要理解「为什么」
2. Self-Attention胜出是因为训练可平行化，不是无限记忆
3. 线性注意力 = Self-Attention - softmax = RNN - 反思
4. Softmax实现相对重要性和隐式遗忘
5. Mamba等变体为线性注意力加入可学习的遗忘
6. 领域正在收敛：像Transformer一样训练，像RNN一样推理
:::
