---
title:
  en: "Hung-yi Lee ML 2025 Lecture 12: How Language Models Learn to Speak"
  zh: "李宏毅机器学习2025 第十二讲：语言模型如何学会说话"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on Speech Language Models: from text to voice, tokenization, and the future of spoken AI"
  zh: "台大李宏毅教授课程笔记：语音语言模型，从文字到语音，分词技术，以及语音AI的未来"
date: 2025-05-24
tags: ["ai", "llm", "speech", "audio", "tokenization", "tts", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/gkAyqoQkOSk/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 12
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="gkAyqoQkOSk" title="李宏毅机器学习2025 第十二讲：语音语言模型" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## The Vision: Speech Language Models

Imagine an AI that can:
- Listen and speak simultaneously (like humans in conversation)
- Understand tone, emotion, and context from voice
- Generate natural speech with appropriate prosody
- React to non-verbal cues (coughs, laughter, background sounds)

This is the goal of Speech Language Models.

## The Challenge: Speech vs Text

### Why Speech is Harder

**Sequence Length**:
- Text: ~100 tokens per minute of speech
- Speech: ~3,000-50,000 tokens per minute (depending on tokenization)

**Information Density**:
- Text: Just content
- Speech: Content + emotion + speaker identity + prosody + background sounds

**Real-time Requirements**:
- Text: Turn-based (you type, model responds)
- Speech: Simultaneous listening and speaking

## Speech Tokenization: The Foundation

### Two Main Approaches

**1. Semantic Tokens (from Self-Supervised Models)**
- Extracted from models like HuBERT, wav2vec
- Capture content/meaning
- ~12.5-50 tokens per second
- Good for understanding, less for synthesis quality

**2. Acoustic Tokens (from Neural Codecs)**
- Extracted from models like EnCodec, SoundStream
- Capture audio details (timbre, prosody)
- Multiple "layers" of tokens (coarse to fine)
- Better synthesis quality

### The Trade-off
- Semantic tokens: Good understanding, poor audio quality
- Acoustic tokens: Good audio quality, harder to model

**Solution**: Use both! Semantic for understanding, Acoustic for synthesis.

## Architecture Patterns

### Pattern 1: Text First, Then Speech
1. Model generates text response
2. TTS converts text to speech

**Pros**: Leverages existing text LLMs
**Cons**: Can't do real-time conversation, loses speech-specific info

### Pattern 2: Direct Speech-to-Speech
1. Input speech → Speech tokens
2. Language model does token prediction
3. Output tokens → Speech

**Pros**: Preserves all speech information
**Cons**: Much longer sequences, harder to train

### Pattern 3: Hybrid (Moshi-style)
- Generate text AND speech tokens simultaneously
- Text provides semantic guidance
- Speech tokens handle audio details

## The Moshi Model

One of the first truly interactive speech language models:

**Key Features**:
- Simultaneous input/output (full-duplex)
- Generates both text and speech tokens
- Can handle interruptions
- Trained on 7M hours of speech data

**Architecture**:
- Temporal Transformer: Handles sequence
- Depth Transformer: Generates multiple token layers per step

## Training Speech Language Models

### Pre-training
- Massive amounts of speech data (100K+ hours)
- Learn speech token prediction (like text LLMs learn word prediction)
- Can use YouTube, podcasts, audiobooks

### Fine-tuning for Conversation
- Human-human dialogue data
- Or: Generate synthetic dialogues with text LLM + TTS
- Teach turn-taking, appropriate responses

### RLHF for Speech
- Same concept as text RLHF
- But: Need to evaluate speech quality, not just content
- Harder to get human feedback on speech

## Key Challenges

### 1. Sequence Length
A 5-minute conversation might need 500,000 tokens!
- Solutions: Better tokenization, sparse attention, streaming

### 2. Forgetting
Fine-tuning on speech can destroy text capabilities
- Solutions: Mixed training, careful data curation

### 3. Real-time Processing
Users expect immediate responses
- Solutions: Streaming generation, speculative decoding

### 4. Multi-modal Understanding
Understanding coughs, laughter, background noise
- Solutions: More diverse training data, specialized tokens

## The Future

### What's Coming
- More natural conversations with AI
- AI that understands emotion and context
- Seamless voice interfaces
- Personalized voice assistants

### Open Questions
- How to evaluate speech model quality?
- How to handle multiple speakers?
- How to preserve privacy in voice data?
- Can we reach human-level conversation?

## Key Takeaways

1. **Speech is not just text + TTS**: Real speech models need to understand audio

2. **Tokenization matters**: The choice of tokens affects everything

3. **Sequence length is the bottleneck**: Speech needs 10-100x more tokens than text

4. **Hybrid approaches work**: Combine semantic and acoustic tokens

5. **We're just getting started**: Speech LLMs are where text LLMs were 3-4 years ago
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 愿景：语音语言模型

想象一个AI可以：
- 同时听和说（像人类对话一样）
- 从声音中理解语气、情感和上下文
- 生成具有适当韵律的自然语音
- 对非语言线索做出反应（咳嗽、笑声、背景声音）

这就是语音语言模型的目标。

## 挑战：语音 vs 文字

### 为什么语音更难

**序列长度**：
- 文字：每分钟语音约100个token
- 语音：每分钟3,000-50,000个token（取决于分词方式）

**信息密度**：
- 文字：只有内容
- 语音：内容 + 情感 + 说话人身份 + 韵律 + 背景声音

**实时要求**：
- 文字：回合制（你打字，模型回应）
- 语音：同时听和说

## 语音分词：基础

### 两种主要方法

**1. 语义Token（来自自监督模型）**
- 从HuBERT、wav2vec等模型提取
- 捕获内容/含义
- 每秒约12.5-50个token
- 适合理解，合成质量较差

**2. 声学Token（来自神经编解码器）**
- 从EnCodec、SoundStream等模型提取
- 捕获音频细节（音色、韵律）
- 多"层"token（从粗到细）
- 合成质量更好

### 权衡
- 语义token：理解好，音频质量差
- 声学token：音频质量好，更难建模

**解决方案**：两者都用！语义用于理解，声学用于合成。

## 架构模式

### 模式1：先文字，后语音
1. 模型生成文字回应
2. TTS将文字转换为语音

**优点**：利用现有的文字LLM
**缺点**：无法实时对话，丢失语音特定信息

### 模式2：直接语音到语音
1. 输入语音 → 语音token
2. 语言模型做token预测
3. 输出token → 语音

**优点**：保留所有语音信息
**缺点**：序列更长，更难训练

### 模式3：混合（Moshi风格）
- 同时生成文字和语音token
- 文字提供语义指导
- 语音token处理音频细节

## Moshi模型

最早的真正交互式语音语言模型之一：

**关键特性**：
- 同时输入/输出（全双工）
- 同时生成文字和语音token
- 可以处理打断
- 在700万小时语音数据上训练

**架构**：
- 时间Transformer：处理序列
- 深度Transformer：每步生成多层token

## 训练语音语言模型

### 预训练
- 大量语音数据（10万+小时）
- 学习语音token预测（像文字LLM学习词预测）
- 可以使用YouTube、播客、有声书

### 对话微调
- 人与人对话数据
- 或：用文字LLM + TTS生成合成对话
- 教授轮流说话、适当回应

### 语音的RLHF
- 与文字RLHF概念相同
- 但是：需要评估语音质量，不仅仅是内容
- 更难获得语音的人类反馈

## 关键挑战

### 1. 序列长度
5分钟的对话可能需要50万个token！
- 解决方案：更好的分词、稀疏注意力、流式处理

### 2. 遗忘
在语音上微调可能破坏文字能力
- 解决方案：混合训练、仔细的数据筛选

### 3. 实时处理
用户期望即时响应
- 解决方案：流式生成、推测解码

### 4. 多模态理解
理解咳嗽、笑声、背景噪音
- 解决方案：更多样化的训练数据、专门的token

## 未来

### 即将到来的
- 与AI更自然的对话
- 理解情感和上下文的AI
- 无缝的语音界面
- 个性化的语音助手

### 开放问题
- 如何评估语音模型质量？
- 如何处理多个说话人？
- 如何保护语音数据的隐私？
- 我们能达到人类水平的对话吗？

## 关键要点

1. **语音不仅仅是文字 + TTS**：真正的语音模型需要理解音频

2. **分词很重要**：token的选择影响一切

3. **序列长度是瓶颈**：语音需要比文字多10-100倍的token

4. **混合方法有效**：结合语义和声学token

5. **我们才刚刚开始**：语音LLM处于文字LLM 3-4年前的阶段
:::
