---
title:
  en: "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design"
  zh: "从缓冲区到寄存器:通过混合键合3D NPU协同设计解锁细粒度FlashAttention"
description:
  en: "3D-Flow introduces a hybrid-bonded 3D spatial accelerator that eliminates on-chip SRAM bottlenecks in Transformer attention by enabling register-to-register communication across vertical PE tiers, achieving up to 93% energy reduction."
  zh: "3D-Flow提出了一种混合键合3D空间加速器,通过垂直PE层间的寄存器到寄存器通信消除Transformer注意力机制中的片上SRAM瓶颈,实现高达93%的能耗降低。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.ar", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.11016](https://arxiv.org/abs/2602.11016)
**Authors**: Jinxin Yu, Yudong Pan, Mengdi Wang, Huawei Li, Yinhe Han, Xiaowei Li, Ying Wang
**Categories**: cs.AR, cs.AI

## Abstract

Transformer models have become the backbone of modern AI, but their quadratic attention complexity creates severe memory bottlenecks as models scale. While existing solutions like large on-chip caches (Groq, Cerebras) and algorithmic optimizations (FlashAttention) have reduced off-chip memory traffic, this paper identifies a new critical bottleneck: on-chip SRAM accesses now consume over 60% of energy in long-sequence workloads. The authors propose 3D-Flow, a hybrid-bonded 3D-stacked spatial accelerator that fundamentally rethinks data movement by enabling direct register-to-register communication across vertically partitioned processing element (PE) tiers using sub-10μm through-silicon vias (TSVs). Combined with 3D-FlashAttention, a fine-grained scheduling method that eliminates pipeline bubbles, the system achieves 46-93% energy reduction and 1.4x-7.6x speedup over state-of-the-art designs on OPT and QWEN models.

## Key Contributions

- **Identification of on-chip SRAM as the new bottleneck**: Empirical measurements showing that SRAM accesses dominate energy consumption (>60%) in attention workloads once off-chip traffic is minimized
- **3D-Flow architecture**: A hybrid-bonded 3D spatial accelerator with vertical PE tier partitioning that enables cycle-level register-to-register data transfer via dense TSV arrays
- **3D-FlashAttention scheduling**: A fine-grained operator fusion and scheduling method that balances latency across tiers to create bubble-free vertical dataflow without SRAM roundtrips
- **Comprehensive evaluation**: Demonstrated 46-93% energy savings and 1.4x-7.6x performance improvements on real Transformer workloads compared to 2D and existing 3D designs

## The Memory Hierarchy Problem in Attention

The attention mechanism's $O(N^2)$ complexity with sequence length $N$ creates a fundamental memory challenge. Traditional approaches have focused on two strategies:

**Hardware-centric solutions** like Groq's LPU and Cerebras' WSE deploy massive on-chip SRAM (up to 40GB) to cache intermediate attention matrices, reducing DRAM bandwidth pressure. However, this approach hits diminishing returns as SRAM access energy becomes dominant.

**Algorithm-centric solutions** like FlashAttention restructure computation to avoid materializing the full $N \times N$ attention matrix by fusing operations and using tiling. While this reduces memory footprint, implementations still rely heavily on SRAM for intermediate results.

The key insight of this work is that **both approaches converge on the same bottleneck**: once off-chip traffic is minimized, the energy cost shifts to on-chip SRAM accesses. For a 4096-token sequence, the authors measure that SRAM reads/writes account for 62% of total energy, while computation is only 23%. This motivates moving data through registers instead of buffers.

## 3D-Flow Architecture Design

3D-Flow employs hybrid bonding technology to create ultra-dense vertical interconnects between stacked PE tiers. Key architectural features include:

**Vertical PE partitioning**: The attention computation is decomposed across multiple physical tiers (typically 4-8 layers), with each tier handling a portion of the query, key, or value matrices. Unlike traditional 3D stacking that simply stacks memory on compute, 3D-Flow distributes the compute itself vertically.

**Register-to-register communication**: TSVs with <10μm pitch enable direct data transfer between PE registers across tiers without routing through SRAM. This achieves single-cycle latency for vertical data movement, comparable to register file access within a tier.

**Minimal NoC overhead**: By handling most data movement vertically through TSVs, the design reduces pressure on the 2D network-on-chip, which becomes a bottleneck in multi-array 2D architectures. Horizontal NoC traffic is reserved primarily for control and boundary data.

The architecture supports three data movement patterns:
- **Vertical streaming**: Sequential data flows up/down tiers (e.g., Q matrix to attention computation)
- **Vertical broadcast**: Same data distributed to multiple tiers (e.g., K matrix reuse)
- **Vertical reduction**: Partial results accumulated across tiers (e.g., softmax normalization)

## 3D-FlashAttention: Fine-Grained Scheduling

Building on the 3D-Flow hardware, the authors develop 3D-FlashAttention, which extends FlashAttention's operator fusion principles to the vertical dimension:

**Tier-aware tiling**: The algorithm partitions Q, K, V matrices not just spatially (as in FlashAttention) but also across tiers. Each tier processes a subset of attention heads or sequence chunks, with careful load balancing to prevent pipeline stalls.

**Bubble-free pipelining**: By analyzing the latency of each operation (matrix multiply, softmax, etc.) and the vertical communication cost, the scheduler arranges operations so that each tier has work ready when the previous operation completes. This eliminates idle cycles ("bubbles") in the pipeline.

**SRAM bypass**: Critical intermediate values flow directly between tiers via registers. For example, the attention scores $QK^T$ computed on one tier feed directly into the softmax on the next tier without writing to SRAM. Only final outputs and small control data touch SRAM.

The scheduling algorithm solves a constrained optimization problem:
$$\min \sum_{t=1}^{T} E_{\text{tier}_t} \quad \text{s.t.} \quad \max_{t} L_{\text{tier}_t} \leq L_{\text{target}}$$

where $E_{\text{tier}_t}$ is energy per tier, $L_{\text{tier}_t}$ is latency per tier, and the constraint ensures balanced execution.

## Experimental Results and Analysis

The evaluation compares 3D-Flow against multiple baselines:
- **2D spatial accelerators**: Multi-array designs with NoC interconnect
- **3D memory-on-logic**: Traditional 3D stacking with HBM on compute die
- **3D compute-on-compute**: Prior work on 3D stacked processing arrays

**Energy efficiency**: On OPT-6.7B with 4096-token sequences, 3D-Flow achieves:
- 93% energy reduction vs. 2D baseline
- 78% reduction vs. 3D memory-on-logic
- 61% reduction vs. 3D compute-on-compute

The energy breakdown shows that register-to-register transfers consume only 8% of total energy, compared to 62% for SRAM in the baseline.

**Performance**: Speedups range from 1.4x (short sequences, small models) to 7.6x (long sequences, large models). The benefit scales with sequence length because longer sequences amplify the SRAM access bottleneck in baseline designs.

**Scalability**: The architecture scales efficiently to 8 tiers before vertical communication latency begins to impact performance. Beyond 8 tiers, the benefit diminishes due to increased TSV routing complexity.

## Implications for Future AI Accelerators

This work demonstrates that **memory hierarchy optimization must extend beyond DRAM-SRAM to SRAM-register**. As AI models continue scaling, the energy cost of data movement at every level becomes critical.

**3D integration as enabler**: Hybrid bonding technology, with its sub-10μm pitch, makes register-level communication across dies practical. This is qualitatively different from earlier 3D approaches using micro-bumps (40μm+ pitch) that could only efficiently move data between large memory blocks.

**Co-design necessity**: The gains come from tight coupling between architecture (vertical PE partitioning), technology (hybrid bonding), and algorithms (3D-FlashAttention scheduling). Neither hardware nor software innovation alone would achieve these results.

**Broader applicability**: While demonstrated on Transformer attention, the principles apply to any compute pattern with producer-consumer relationships that can be vertically partitioned—potentially including convolutions, graph neural networks, and sparse operations.

## Takeaways

1. On-chip SRAM access has become the dominant energy cost in attention computation, accounting for over 60% of energy once off-chip traffic is minimized through caching or algorithmic optimizations.

2. Hybrid-bonded 3D stacking enables a new data movement paradigm: register-to-register communication across vertical PE tiers with single-cycle latency, bypassing SRAM entirely for intermediate results.

3. 3D-FlashAttention extends operator fusion to the vertical dimension, creating bubble-free pipelines through tier-aware scheduling that balances computation and communication latency.

4. The 3D-Flow architecture achieves 46-93% energy reduction and 1.4x-7.6x speedup on Transformer workloads compared to state-of-the-art 2D and 3D accelerators, with benefits scaling with sequence length.

5. Future AI accelerator design must consider the full memory hierarchy from DRAM to registers, with 3D integration providing a path to eliminate intermediate storage bottlenecks through direct die-to-die communication.
:::

:::zh
**论文**: [2602.11016](https://arxiv.org/abs/2602.11016)
**作者**: Jinxin Yu, Yudong Pan, Mengdi Wang, Huawei Li, Yinhe Han, Xiaowei Li, Ying Wang
**分类**: cs.AR, cs.AI

## 摘要

Transformer模型已成为现代AI的支柱,但其二次方注意力复杂度在模型规模扩大时造成了严重的内存瓶颈。虽然现有解决方案如大容量片上缓存(Groq、Cerebras)和算法优化(FlashAttention)已经减少了片外内存流量,但本文识别出一个新的关键瓶颈:片上SRAM访问现在在长序列工作负载中消耗超过60%的能量。作者提出了3D-Flow,一种混合键合3D堆叠空间加速器,通过使用亚10微米硅通孔(TSV)在垂直分区的处理单元(PE)层之间实现直接的寄存器到寄存器通信,从根本上重新思考数据移动。结合3D-FlashAttention这一消除流水线气泡的细粒度调度方法,该系统在OPT和QWEN模型上相比最先进设计实现了46-93%的能耗降低和1.4x-7.6x的加速。

## 主要贡献

- **识别片上SRAM为新瓶颈**:通过实证测量表明,一旦片外流量最小化,SRAM访问在注意力工作负载中主导能耗(>60%)
- **3D-Flow架构**:一种混合键合3D空间加速器,采用垂直PE层分区,通过密集TSV阵列实现周期级寄存器到寄存器数据传输
- **3D-FlashAttention调度**:一种细粒度算子融合和调度方法,平衡各层延迟以创建无气泡垂直数据流,无需SRAM往返
- **全面评估**:在真实Transformer工作负载上相比2D和现有3D设计展示了46-93%的能量节省和1.4x-7.6x的性能提升

## 注意力机制中的存储层次问题

注意力机制的$O(N^2)$复杂度(其中$N$为序列长度)造成了根本性的内存挑战。传统方法聚焦于两种策略:

**以硬件为中心的解决方案**,如Groq的LPU和Cerebras的WSE,部署大规模片上SRAM(高达40GB)来缓存中间注意力矩阵,降低DRAM带宽压力。然而,随着SRAM访问能耗成为主导,这种方法遇到收益递减。

**以算法为中心的解决方案**,如FlashAttention,通过融合操作和使用分块来重构计算,避免实体化完整的$N \times N$注意力矩阵。虽然这减少了内存占用,但实现仍然严重依赖SRAM存储中间结果。

本工作的关键洞察是:**两种方法都收敛到同一瓶颈**:一旦片外流量最小化,能耗成本就转移到片上SRAM访问。对于4096令牌序列,作者测量到SRAM读写占总能耗的62%,而计算仅占23%。这促使通过寄存器而非缓冲区移动数据。

## 3D-Flow架构设计

3D-Flow采用混合键合技术在堆叠的PE层之间创建超密集垂直互连。关键架构特性包括:

**垂直PE分区**:注意力计算被分解到多个物理层(通常4-8层),每层处理查询、键或值矩阵的一部分。与传统3D堆叠简单地将内存堆叠在计算上不同,3D-Flow将计算本身垂直分布。

**寄存器到寄存器通信**:间距<10μm的TSV实现跨层PE寄存器之间的直接数据传输,无需通过SRAM路由。这实现了垂直数据移动的单周期延迟,与层内寄存器文件访问相当。

**最小NoC开销**:通过TSV垂直处理大部分数据移动,设计减少了对2D片上网络的压力,后者在多阵列2D架构中成为瓶颈。水平NoC流量主要保留用于控制和边界数据。

该架构支持三种数据移动模式:
- **垂直流式传输**:顺序数据在层间上下流动(如Q矩阵到注意力计算)
- **垂直广播**:相同数据分发到多层(如K矩阵重用)
- **垂直归约**:跨层累积部分结果(如softmax归一化)

## 3D-FlashAttention:细粒度调度

基于3D-Flow硬件,作者开发了3D-FlashAttention,将FlashAttention的算子融合原则扩展到垂直维度:

**层感知分块**:算法不仅在空间上(如FlashAttention)而且跨层分区Q、K、V矩阵。每层处理注意力头或序列块的子集,通过仔细的负载平衡防止流水线停顿。

**无气泡流水线**:通过分析每个操作(矩阵乘法、softmax等)的延迟和垂直通信成本,调度器安排操作使每层在前一操作完成时有工作就绪。这消除了流水线中的空闲周期("气泡")。

**SRAM旁路**:关键中间值通过寄存器直接在层间流动。例如,在一层计算的注意力分数$QK^T$直接馈送到下一层的softmax,无需写入SRAM。只有最终输出和少量控制数据接触SRAM。

调度算法解决一个约束优化问题:
$$\min \sum_{t=1}^{T} E_{\text{tier}_t} \quad \text{s.t.} \quad \max_{t} L_{\text{tier}_t} \leq L_{\text{target}}$$

其中$E_{\text{tier}_t}$是每层能耗,$L_{\text{tier}_t}$是每层延迟,约束确保平衡执行。

## 实验结果与分析

评估将3D-Flow与多个基线进行比较:
- **2D空间加速器**:带NoC互连的多阵列设计
- **3D内存堆叠**:传统3D堆叠,HBM在计算芯片上
- **3D计算堆叠**:先前关于3D堆叠处理阵列的工作

**能效**:在OPT-6.7B模型4096令牌序列上,3D-Flow实现:
- 相比2D基线降低93%能耗
- 相比3D内存堆叠降低78%
- 相比3D计算堆叠降低61%

能耗分解显示,寄存器到寄存器传输仅消耗总能耗的8%,而基线中SRAM占62%。

**性能**:加速比从1.4x(短序列、小模型)到7.6x(长序列、大模型)不等。收益随序列长度扩展,因为更长序列放大了基线设计中的SRAM访问瓶颈。

**可扩展性**:架构有效扩展到8层,之后垂直通信延迟开始影响性能。超过8层后,由于TSV路由复杂度增加,收益递减。

## 对未来AI加速器的启示

这项工作表明,**存储层次优化必须从DRAM-SRAM扩展到SRAM-寄存器**。随着AI模型持续扩展,每个层级的数据移动能耗成本都变得关键。

**3D集成作为使能技术**:混合键合技术及其亚10μm间距,使跨芯片寄存器级通信变得实用。这与早期使用微凸点(40μm+间距)的3D方法有本质区别,后者只能有效地在大内存块之间移动数据。

**协同设计的必要性**:收益来自架构(垂直PE分区)、技术(混合键合)和算法(3D-FlashAttention调度)之间的紧密耦合。单独的硬件或软件创新都无法实现这些结果。

**更广泛的适用性**:虽然在Transformer注意力上演示,但这些原则适用于任何具有可垂直分区的生产者-消费者关系的计算模式——可能包括卷积、图神经网络和稀疏操作。

## 要点总结

1. 片上SRAM访问已成为注意力计算中的主导能耗,一旦通过缓存或算法优化最小化片外流量,SRAM占能耗超过60%。

2. 混合键合3D堆叠实现了新的数据移动范式:跨垂直PE层的寄存器到寄存器通信具有单周期延迟,完全绕过中间结果的SRAM。

3. 3D-FlashAttention将算子融合扩展到垂直维度,通过层感知调度平衡计算和通信延迟,创建无气泡流水线。

4. 3D-Flow架构在Transformer工作负载上相比最先进的2D和3D加速器实现46-93%能耗降低和1.4x-7.6x加速,收益随序列长度扩展。

5. 未来AI加速器设计必须考虑从DRAM到寄存器的完整存储层次,3D集成通过直接芯片间通信提供了消除中间存储瓶颈的路径。
:::
