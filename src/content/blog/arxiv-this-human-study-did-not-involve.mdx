---
title:
  en: "Validating LLM Simulations as Behavioral Evidence: When Can AI Replace Human Subjects?"
  zh: "验证大语言模型模拟作为行为证据:何时AI可以替代人类受试者?"
description:
  en: "This paper examines when LLM-generated synthetic participants can validly substitute for human subjects in social science research, contrasting heuristic and statistical calibration approaches."
  zh: "本文探讨大语言模型生成的合成参与者何时能够有效替代社会科学研究中的人类受试者,对比启发式方法和统计校准方法。"
date: 2026-02-18
tags: ["arxiv", "ai", "cs.ai"]
image: "/arxiv-visuals/arxiv-this-human-study-did-not-involve.png"
---

:::en
**Paper**: [2602.15785](https://arxiv.org/abs/2602.15785)
**Authors**: Jessica Hullman, David Broska, Huaman Sun, Aaron Shaw
**Categories**: cs.AI

## Abstract

Large language models are increasingly being used as synthetic participants in social science experiments, offering cost-effective and rapid data collection. However, the validity of using LLM simulations as evidence for human behavior remains unclear. This paper presents a framework for understanding when LLM-based simulations can support valid causal inference, distinguishing between heuristic approaches (prompt engineering, fine-tuning) suitable for exploratory research and statistical calibration methods that provide formal guarantees for confirmatory studies. The authors argue that statistical calibration, which combines auxiliary human data with statistical adjustments, can yield more precise causal effect estimates at lower cost than human-only experiments, while heuristic approaches lack the statistical rigor needed for confirmatory research.

## Key Contributions

- Establishes a formal framework distinguishing exploratory versus confirmatory uses of LLM simulations in behavioral research
- Contrasts heuristic approaches (prompt engineering, model tuning) with statistical calibration methods for obtaining valid causal estimates
- Demonstrates that statistical calibration can provide formal validity guarantees and improved precision-cost tradeoffs compared to human-only studies
- Identifies critical assumptions required for each approach and clarifies when LLM simulations can substitute for human participants
- Highlights overlooked opportunities when researchers focus solely on LLM-human substitution rather than complementary uses

## Two Paradigms for LLM-Based Behavioral Research

The paper identifies two fundamentally different strategies for using LLMs in behavioral studies:

**Heuristic Approaches** attempt to make LLM responses interchangeable with human responses through iterative refinement. Researchers employ prompt engineering, model fine-tuning, persona conditioning, and other "repair" strategies to minimize discrepancies between simulated and observed behavior. While these methods can be valuable for hypothesis generation, pilot testing, and exploratory analysis, they lack formal statistical guarantees. The validity of heuristic approaches depends on achieving sufficient similarity between LLM and human responses, but "sufficient" remains subjectively defined without a principled statistical framework.

**Statistical Calibration** takes a different approach by explicitly modeling the relationship between LLM simulations and human behavior. This method combines a small sample of human responses with larger-scale LLM simulations, using statistical adjustments to account for systematic discrepancies. Under explicit assumptions about the LLM-human relationship, statistical calibration preserves validity for causal inference while potentially improving precision and reducing costs compared to human-only studies. The key insight is that LLM simulations need not perfectly replicate human behavior—they must only satisfy specific statistical conditions that can be tested and validated.

## Statistical Foundations and Validity Conditions

The authors formalize the conditions under which LLM simulations support valid causal inference. For a causal effect estimate $\hat{\tau}$ to be valid, the simulation process must satisfy:

$$E[\hat{\tau}_{LLM}] = E[\hat{\tau}_{human}] + \epsilon$$

where $\epsilon$ represents a bounded, quantifiable bias that can be estimated and corrected using auxiliary human data.

Statistical calibration achieves this through a two-stage process:
1. Collect a small human sample to estimate the LLM-human discrepancy function
2. Use this function to adjust estimates from larger LLM simulations

This approach provides several advantages:
- **Formal validity**: Under stated assumptions, estimates are unbiased for the target population
- **Precision gains**: Larger effective sample sizes from LLM simulations reduce variance
- **Cost efficiency**: Achieves better precision per dollar than human-only studies
- **Testable assumptions**: The validity conditions can be empirically evaluated

However, statistical calibration requires that the LLM-human relationship be stable and estimable from auxiliary data—assumptions that may not hold across all contexts.

## Implications for Research Design

The paper challenges the prevailing focus on making LLMs perfectly mimic human participants. Instead, researchers should consider:

**When to use heuristic approaches:**
- Early-stage exploratory research and hypothesis generation
- Rapid prototyping of experimental designs
- Scenarios where formal statistical inference is not required
- Cases where the cost of validation exceeds the value of formal guarantees

**When to use statistical calibration:**
- Confirmatory research requiring valid causal inference
- Studies where precision-cost tradeoffs favor hybrid human-LLM designs
- Contexts where LLM-human relationships are stable and estimable
- Research with sufficient resources for auxiliary human data collection

**Overlooked opportunities:**
The authors argue that myopic focus on substitution obscures other valuable uses of LLMs in behavioral research, including:
- Generating diverse experimental stimuli and scenarios
- Exploring counterfactual conditions infeasible with human participants
- Modeling heterogeneous treatment effects across subpopulations
- Augmenting rather than replacing human data collection

## Takeaways

1. LLM simulations can provide valid behavioral evidence, but the appropriate method depends on research goals—heuristic approaches for exploration, statistical calibration for confirmation.

2. Statistical calibration offers a principled path to valid causal inference by combining small human samples with larger LLM simulations, potentially improving both precision and cost-efficiency.

3. Perfect LLM-human similarity is neither necessary nor sufficient for valid inference; what matters is whether specific statistical conditions hold and can be empirically validated.

4. The validity of both approaches fundamentally depends on how well LLMs approximate the target population—a question that requires empirical investigation rather than assumption.

5. Researchers should look beyond simple substitution to explore complementary uses of LLMs that augment rather than replace human participants in behavioral studies.
:::

:::zh
**论文**: [2602.15785](https://arxiv.org/abs/2602.15785)
**作者**: Jessica Hullman, David Broska, Huaman Sun, Aaron Shaw
**分类**: cs.AI

## 摘要

大语言模型越来越多地被用作社会科学实验中的合成参与者,提供成本效益高且快速的数据收集方式。然而,使用大语言模型模拟作为人类行为证据的有效性仍不明确。本文提出了一个框架,用于理解基于大语言模型的模拟何时能够支持有效的因果推断,区分了适用于探索性研究的启发式方法(提示工程、微调)和为验证性研究提供形式化保证的统计校准方法。作者认为,统计校准方法结合辅助人类数据和统计调整,能够以低于纯人类实验的成本获得更精确的因果效应估计,而启发式方法缺乏验证性研究所需的统计严谨性。

## 主要贡献

- 建立了区分大语言模型模拟在行为研究中探索性与验证性用途的形式化框架
- 对比了启发式方法(提示工程、模型调优)与统计校准方法在获得有效因果估计方面的差异
- 证明统计校准可以提供形式化的有效性保证,并相比纯人类研究改善精度-成本权衡
- 识别了每种方法所需的关键假设,明确了大语言模型模拟何时可以替代人类参与者
- 指出当研究者仅关注大语言模型-人类替代而非互补用途时被忽视的机会

## 基于大语言模型的行为研究的两种范式

本文识别了在行为研究中使用大语言模型的两种根本不同的策略:

**启发式方法**试图通过迭代优化使大语言模型响应与人类响应可互换。研究者采用提示工程、模型微调、人格条件化和其他"修复"策略来最小化模拟行为与观察行为之间的差异。虽然这些方法对假设生成、试点测试和探索性分析很有价值,但它们缺乏形式化的统计保证。启发式方法的有效性取决于在大语言模型和人类响应之间实现足够的相似性,但在没有原则性统计框架的情况下,"足够"仍然是主观定义的。

**统计校准**采用不同的方法,通过显式建模大语言模型模拟与人类行为之间的关系。该方法将小样本人类响应与大规模大语言模型模拟相结合,使用统计调整来解释系统性差异。在关于大语言模型-人类关系的明确假设下,统计校准保持因果推断的有效性,同时相比纯人类研究可能提高精度并降低成本。关键洞察是大语言模型模拟不需要完美复制人类行为——它们只需满足可以测试和验证的特定统计条件。

## 统计基础与有效性条件

作者形式化了大语言模型模拟支持有效因果推断的条件。对于因果效应估计$\hat{\tau}$有效,模拟过程必须满足:

$$E[\hat{\tau}_{LLM}] = E[\hat{\tau}_{human}] + \epsilon$$

其中$\epsilon$表示可以使用辅助人类数据估计和校正的有界、可量化偏差。

统计校准通过两阶段过程实现这一点:
1. 收集小规模人类样本以估计大语言模型-人类差异函数
2. 使用该函数调整来自更大规模大语言模型模拟的估计

这种方法提供了几个优势:
- **形式化有效性**:在陈述的假设下,估计对目标人群是无偏的
- **精度提升**:来自大语言模型模拟的更大有效样本量减少方差
- **成本效率**:实现比纯人类研究更好的每美元精度
- **可测试假设**:有效性条件可以经验性评估

然而,统计校准要求大语言模型-人类关系是稳定的且可从辅助数据中估计——这些假设可能并非在所有情境下都成立。

## 对研究设计的启示

本文挑战了让大语言模型完美模仿人类参与者的主流关注点。相反,研究者应该考虑:

**何时使用启发式方法:**
- 早期探索性研究和假设生成
- 实验设计的快速原型制作
- 不需要形式化统计推断的场景
- 验证成本超过形式化保证价值的情况

**何时使用统计校准:**
- 需要有效因果推断的验证性研究
- 精度-成本权衡有利于混合人类-大语言模型设计的研究
- 大语言模型-人类关系稳定且可估计的情境
- 有足够资源进行辅助人类数据收集的研究

**被忽视的机会:**
作者认为,对替代的短视关注掩盖了大语言模型在行为研究中的其他有价值用途,包括:
- 生成多样化的实验刺激和场景
- 探索人类参与者不可行的反事实条件
- 建模跨亚群体的异质性处理效应
- 增强而非替代人类数据收集

## 要点总结

1. 大语言模型模拟可以提供有效的行为证据,但适当的方法取决于研究目标——探索用启发式方法,验证用统计校准。

2. 统计校准通过结合小规模人类样本与大规模大语言模型模拟,提供了有效因果推断的原则性路径,可能同时改善精度和成本效率。

3. 完美的大语言模型-人类相似性既非必要也非充分条件;重要的是特定统计条件是否成立并可以经验性验证。

4. 两种方法的有效性根本上取决于大语言模型对目标人群的近似程度——这是一个需要经验调查而非假设的问题。

5. 研究者应该超越简单替代,探索大语言模型的互补用途,在行为研究中增强而非替代人类参与者。
:::
