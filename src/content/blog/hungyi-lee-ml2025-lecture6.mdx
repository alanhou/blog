---
title:
  en: "Hung-yi Lee ML 2025 Lecture 6: Post-Training and the Forgetting Problem"
  zh: "李宏毅机器学习2025 第六讲：后训练与遗忘问题"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on post-training LLMs: catastrophic forgetting, experience replay, and self-output methods"
  zh: "台大李宏毅教授课程笔记：LLM后训练，灾难性遗忘，经验回放，以及自输出方法"
date: 2025-02-23
tags: ["ai", "llm", "fine-tuning", "catastrophic-forgetting", "post-training", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/Z6b5-77EfGk/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 6
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="Z6b5-77EfGk" title="李宏毅机器学习2025 第六讲：后训练与遗忘问题" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is Post-Training?

We have powerful open-source models (LLaMA, Gemma, DeepSeek) with general capabilities. But often we need models specialized for:
- Specific domains (finance, law)
- Specific languages (Chinese, Korean, Japanese)
- Specific programming languages (Verilog)

**Post-training** (also called continual learning) takes a foundation model and further trains it for specialized capabilities.

The foundation model can be:
- A pre-trained base model
- An already-aligned chat/instruct model

Technically, post-training uses the same methods: pre-training style, SFT, or RL.

## The Forgetting Disaster

**Example**: Teaching LLaMA-2-chat Chinese

LLaMA-2-chat has safety alignment - ask it how to hack a bank, it refuses. But it prefers answering in English.

After post-training on Chinese data:
- **Before**: "I'm sorry, I can't help you obtain bank passwords..."
- **After**: "If you want to get new passwords, I can teach you several attack methods..."

The model learned Chinese but **forgot its safety training**.

This isn't an isolated case. Research shows:
- LLaMA-2-base: 25% toxic response rate
- LLaMA-2-chat: 0.22% toxic response rate
- After Chinese post-training: Much higher toxic rates

## Catastrophic Forgetting is Universal

Even "normal" fine-tuning causes forgetting:

**Alpaca dataset experiment**: Fine-tuning on completely normal Q&A data (like "What are the three primary colors?") still destroys safety alignment.

**LLaMA-3 experiments**: Teaching reasoning, medical knowledge, coding, or tool use - each one damages safety capabilities:
- HexPhi benchmark: Safety drops dramatically
- AdvBench: From 0% errors to significant error rates

**Cross-capability damage**: Teaching tool use hurts math and coding. Teaching math hurts coding and tool use. Teaching coding devastates tool use (19.6% → 3.6%).

## Why Does Forgetting Happen?

Simple: during post-training, you only optimize for the target task. You don't care what happens to other capabilities.

**Is it about model size?** Research on 1B-7B models shows larger models don't forget less.

**What correlates with forgetting?** How well you learn the target task. Better target performance = more forgetting. It's almost a straight line.

**Does LoRA help?** LoRA reduces forgetting, but only because it learns less. "LoRA learns less and forgets less." You're trading capability for stability.

**Other regularization?** Dropout, weight decay - none truly solve the problem.

## Surgery Analogy

Post-training is like brain surgery on AI:
- Surgery often succeeds (target task improves)
- But the patient dies (everything else breaks)

"Surgery successful, patient deceased."

## The 2019 Solution: Experience Replay

Professor Lee's lab discovered this in 2019 (GPT-2 era):

**The problem**: Teaching GPT-2 multiple NLP tasks sequentially. After learning task 2, it forgets task 1.

**The solution**: When training on task 2, mix in ~5% of task 1's data.

That's it. Just 5% of old data prevents catastrophic forgetting.

**Why so little?** The model doesn't truly forget - it just "doesn't want to remember." The knowledge is hidden somewhere. A small trigger brings it back.

## The Modern Problem: No Training Data

In 2019, this seemed like a solved problem. But today:
- Companies release models, not training data
- You can't do experience replay without the original data

So catastrophic forgetting is a real problem again.

## Pseudo Experience Replay

**2019 insight**: Make the model generate its own training data.

Train GPT-2 on SQuAD (reading comprehension). Then ask it to generate text freely. It produces:
- A fake news article about US invasion of Afghanistan
- A question about the article
- An answer to that question

The article is completely fabricated (wrong dates, made-up numbers), but it looks like training data. Use this as "past experience" when training new tasks.

**Modern version - Magpie**:
1. Give LLaMA just the "user:" token
2. Let it generate a question
3. Add "assistant:" token
4. Let it generate an answer

Now you have pseudo-training data for experience replay.

## Self-Output Methods

Instead of human-written answers, use the model's own answers:

**Self-Output**:
1. Ask foundation model a question
2. If it answers correctly, use its answer for training
3. Only use human answers when the model fails

**Paraphrase**:
1. Take human-written answers
2. Have foundation model rephrase them
3. Train on rephrased versions

**Why does this work?** The model's own outputs are closer to what it already knows. Training on familiar patterns causes less forgetting.

**Results**: Self-output dramatically reduces forgetting compared to standard SFT.

## RL as Natural Anti-Forgetting

RL-based training is similar to self-output:
- Model generates answers
- Good answers get reinforced
- Bad answers get penalized

The key: you never force the model to produce something it can't generate. This might explain why RL is typically the final training stage.

## Cross-Model Teaching

What if your foundation model is too weak to generate correct answers?

**Solution**: Use a stronger model (GPT-4, Claude) to generate answers.

**Finding**: Training with GPT-4's answers causes less forgetting than training with human answers. Language models "speak the same language."

**Minimum Change**: If GPT-4's answers still cause forgetting:
1. Have your weak model generate an answer
2. Ask GPT-4 to minimally correct it
3. Train on the corrected version

## Application: Speech Models

Teaching LLaMA to understand audio:

**Problem**: Direct audio training destroys text capabilities.

**Solution**:
1. Convert audio features to text descriptions
2. Have text model answer based on descriptions
3. Train audio model to match text model's outputs

**Result**: SALMONN, BLSP, DeSTA2 all use this approach. DeSTA2 achieves state-of-the-art on Dynamic-SUPERB benchmark with less training data.

## Token-Level Filtering

**Observation**: Human answers contain tokens that are "hard" for the model (low probability). These hard tokens cause forgetting.

**Solution**: During training, skip tokens that the foundation model finds difficult to produce.

**Surprising result**: Removing 10-20% of the hardest tokens actually improves performance, both in-domain and out-of-domain.

## Key Takeaways

1. **Post-training easily destroys existing capabilities** - always check what you've lost
2. **Experience replay works** - mix ~5% old data to prevent forgetting
3. **No old data? Generate it** - models can produce pseudo-training data
4. **Self-output is powerful** - train on model's own answers when possible
5. **RL may naturally prevent forgetting** - similar mechanism to self-output
6. **Skip hard tokens** - don't force models to learn what they can't

When someone claims their fine-tuned model "beats GPT-4 on task X," ask: what capabilities did they sacrifice?
:::

:::zh
本文整理自台湾大学李宏毅教授的「生成式AI时代下的机器学习(2025)」课程。

## 什么是后训练？

我们有强大的开源模型（LLaMA、Gemma、DeepSeek），具备通用能力。但我们常常需要专门针对以下领域的模型：
- 特定领域（金融、法律）
- 特定语言（中文、韩文、日文）
- 特定程序语言（Verilog）

**后训练**（也叫持续学习）是把基础模型进一步训练以获得专门能力。

基础模型可以是：
- 预训练的base模型
- 已经做过对齐的chat/instruct模型

技术上，后训练用的是同样的方法：预训练风格、SFT或RL。

## 遗忘灾难

**例子**：教LLaMA-2-chat中文

LLaMA-2-chat有安全对齐——问它怎么黑银行，它会拒绝。但它偏好用英文回答。

用中文数据后训练后：
- **之前**：「很抱歉，我不能帮你获取银行密码...」
- **之后**：「如果你要获得新密码，我可以教你几个攻击方式...」

模型学会了中文，但**忘记了安全训练**。

这不是个案。研究显示：
- LLaMA-2-base：25%有毒回复率
- LLaMA-2-chat：0.22%有毒回复率
- 中文后训练后：有毒率大幅上升

## 灾难性遗忘是普遍的

即使「正常」的微调也会造成遗忘：

**Alpaca数据集实验**：用完全正常的问答数据（如「三原色是什么？」）微调，仍然会破坏安全对齐。

**LLaMA-3实验**：教推理、医学知识、写代码或使用工具——每一个都会损害安全能力：
- HexPhi基准：安全性大幅下降
- AdvBench：从0%错误到显著错误率

**跨能力损害**：教工具使用会伤害数学和编程。教数学会伤害编程和工具使用。教编程会重创工具使用（19.6% → 3.6%）。

## 为什么会遗忘？

很简单：后训练时，你只优化目标任务。你不在乎其他能力会怎样。

**跟模型大小有关吗？** 1B-7B模型的研究显示，更大的模型并不会遗忘更少。

**什么跟遗忘相关？** 你在目标任务上学得多好。目标表现越好 = 遗忘越多。几乎是一条直线。

**LoRA有帮助吗？** LoRA减少遗忘，但只是因为它学得更少。「LoRA学得少，忘得少。」你是用能力换稳定性。

**其他正则化？** Dropout、weight decay——都无法真正解决问题。

## 手术比喻

后训练就像给AI做脑部手术：
- 手术常常成功（目标任务改善）
- 但病人死了（其他一切都坏了）

「手术成功，病人死亡。」

## 2019年的解法：经验回放

李老师实验室在2019年（GPT-2时代）发现了这个：

**问题**：依序教GPT-2多个NLP任务。学完任务2后，它忘了任务1。

**解法**：训练任务2时，混入约5%的任务1数据。

就这样。只要5%的旧数据就能防止灾难性遗忘。

**为什么这么少就够？** 模型并没有真正遗忘——它只是「不想想起来」。知识藏在某个地方。一点小触发就能唤回来。

## 现代问题：没有训练数据

2019年时，这看起来是个已解决的问题。但今天：
- 公司只发布模型，不发布训练数据
- 没有原始数据就无法做经验回放

所以灾难性遗忘又成了真正的问题。

## 伪经验回放

**2019年的洞见**：让模型自己生成训练数据。

在SQuAD（阅读理解）上训练GPT-2。然后让它自由生成文本。它会产生：
- 一篇关于美国入侵阿富汗的假新闻
- 一个关于这篇文章的问题
- 这个问题的答案

文章完全是捏造的（错误的日期、编造的数字），但看起来像训练数据。训练新任务时把这个当作「过去的经验」。

**现代版本 - Magpie**：
1. 只给LLaMA「user:」token
2. 让它生成一个问题
3. 加上「assistant:」token
4. 让它生成一个答案

现在你有了可以做经验回放的伪训练数据。

## 自输出方法

不用人写的答案，用模型自己的答案：

**自输出**：
1. 问基础模型一个问题
2. 如果它答对了，用它的答案来训练
3. 只有模型答错时才用人类答案

**改写**：
1. 拿人写的答案
2. 让基础模型改写
3. 用改写后的版本训练

**为什么有效？** 模型自己的输出更接近它已经知道的东西。用熟悉的模式训练造成的遗忘更少。

**结果**：自输出相比标准SFT大幅减少遗忘。

## RL作为天然的防遗忘方法

基于RL的训练跟自输出类似：
- 模型生成答案
- 好答案被强化
- 坏答案被惩罚

关键：你从不强迫模型产生它生成不了的东西。这可能解释了为什么RL通常是最后的训练阶段。

## 跨模型教学

如果你的基础模型太弱，生成不了正确答案怎么办？

**解法**：用更强的模型（GPT-4、Claude）生成答案。

**发现**：用GPT-4的答案训练比用人类答案造成的遗忘更少。语言模型「说同样的语言」。

**最小改动**：如果GPT-4的答案仍然造成遗忘：
1. 让你的弱模型生成答案
2. 让GPT-4做最小修正
3. 用修正后的版本训练

## 应用：语音模型

教LLaMA理解音频：

**问题**：直接的音频训练会破坏文字能力。

**解法**：
1. 把音频特征转换成文字描述
2. 让文字模型根据描述回答
3. 训练音频模型来匹配文字模型的输出

**结果**：SALMONN、BLSP、DeSTA2都用这个方法。DeSTA2用更少的训练数据在Dynamic-SUPERB基准上达到最先进水平。

## Token级别过滤

**观察**：人类答案包含对模型来说「困难」的token（低概率）。这些困难的token造成遗忘。

**解法**：训练时，跳过基础模型觉得难以产生的token。

**惊人结果**：移除10-20%最难的token实际上改善了表现，无论是域内还是域外。

## 重点整理

1. **后训练很容易破坏现有能力**——总是要检查你失去了什么
2. **经验回放有效**——混入约5%旧数据防止遗忘
3. **没有旧数据？生成它**——模型可以产生伪训练数据
4. **自输出很强大**——尽可能用模型自己的答案训练
5. **RL可能天然防止遗忘**——跟自输出机制类似
6. **跳过困难token**——不要强迫模型学它学不会的东西

当有人声称他们微调的模型「在任务X上打败GPT-4」时，要问：他们牺牲了什么能力？
:::
