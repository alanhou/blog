---
title:
  en: "GigaBrain-0.5M*: Advancing Vision-Language-Action Models Through World Model-Based Reinforcement Learning"
  zh: "GigaBrain-0.5M*: 通过世界模型强化学习推进视觉-语言-动作模型"
description:
  en: "A novel VLA model that integrates world model-based RL to achieve 30% performance gains on complex robotic manipulation tasks, demonstrating robust long-horizon execution capabilities."
  zh: "一种创新的视觉-语言-动作模型,通过集成世界模型强化学习在复杂机器人操作任务上实现30%性能提升,展现出稳健的长时域执行能力。"
date: 2026-02-13
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/arxiv-gigabrain-05m-a-vla-that-learns.png"
---

:::en
**Paper**: [2602.12099](https://arxiv.org/abs/2602.12099)
**Authors**: GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng
**Categories**: cs.CV

## Abstract

This paper introduces GigaBrain-0.5M*, a vision-language-action (VLA) model that addresses fundamental limitations in current VLA architectures through world model-based reinforcement learning. Traditional VLA models that directly predict action sequences from observations suffer from constrained scene understanding and weak future anticipation. GigaBrain-0.5M* leverages video world models pre-trained on massive video corpora to enhance spatiotemporal reasoning and future prediction capabilities. Built on GigaBrain-0.5, which was pre-trained on over 10,000 hours of robotic manipulation data and currently ranks first on the RoboChallenge benchmark, the model integrates RAMP (Reinforcement leArning via world Model-conditioned Policy) for robust cross-task adaptation. Empirical validation shows approximately 30% performance improvements over baselines on challenging tasks like laundry folding, box packing, and espresso preparation, with reliable long-horizon execution in real-world deployments.

## Key Contributions

- Introduction of RAMP, a novel framework that combines world model-based future prediction with reinforcement learning for VLA training
- Integration of web-scale video world models to enhance spatiotemporal reasoning capabilities in robotic manipulation
- Demonstration of 30% performance gains over RECAP baseline on complex, long-horizon manipulation tasks
- Achievement of first place ranking on the international RoboChallenge benchmark with the base GigaBrain-0.5 model
- Validation of reliable real-world deployment with consistent task completion without failures

## Methodology: RAMP Framework

The core innovation of GigaBrain-0.5M* lies in the RAMP (Reinforcement leArning via world Model-conditioned Policy) framework, which fundamentally rethinks how VLA models learn manipulation policies. Unlike conventional approaches that directly map observations to actions, RAMP leverages world models as a conditioning mechanism for policy learning.

The architecture consists of three key components:

1. **World Model Foundation**: Pre-trained video world models serve as the backbone, providing rich spatiotemporal representations learned from web-scale video data. These models excel at predicting future states and understanding scene dynamics, capabilities that are crucial for manipulation tasks requiring anticipation.

2. **Model-Conditioned Policy**: The policy network is conditioned on both current observations and world model predictions. This allows the agent to reason about future consequences of actions before execution, leading to more informed decision-making.

3. **RL-Based Optimization**: Rather than relying solely on behavioral cloning from demonstrations, RAMP employs reinforcement learning to fine-tune the policy. This enables the model to discover novel strategies and adapt to task variations beyond the training distribution.

The training process involves first pre-training on the extensive GigaBrain-0.5 dataset (10,000+ hours of robotic data), then applying RAMP to enable cross-task generalization. This two-stage approach combines the benefits of large-scale pre-training with the adaptability of RL-based fine-tuning.

## Experimental Results and Performance Analysis

GigaBrain-0.5M* demonstrates substantial improvements across multiple challenging manipulation benchmarks. The evaluation focuses on three particularly difficult tasks that require long-horizon planning and precise execution:

**Laundry Folding**: This task demands understanding of deformable object dynamics and multi-step sequential reasoning. GigaBrain-0.5M** achieves approximately 30% higher success rates compared to the RECAP baseline, attributed to better anticipation of fabric behavior through world model predictions.

**Box Packing**: Requiring spatial reasoning and efficient motion planning, this task benefits significantly from the model's enhanced scene understanding. The world model-conditioned policy enables more efficient packing strategies by predicting object placements before execution.

**Espresso Preparation**: This complex task involves precise manipulation of multiple objects with tight tolerances. The 30% performance gain demonstrates the model's ability to handle intricate, multi-step procedures reliably.

Beyond quantitative metrics, real-world deployment videos reveal consistent task completion without failures, a critical requirement for practical robotic systems. The model exhibits robust long-horizon execution, maintaining performance across extended task sequences where compounding errors typically degrade conventional VLA models.

The RoboChallenge benchmark results further validate the approach, with GigaBrain-0.5 (the base model) achieving first place internationally. This demonstrates that the foundation model itself is highly competitive before RAMP enhancement.

## Implications for Robotic Learning

GigaBrain-0.5M* represents a paradigm shift in how we approach vision-language-action model training. Several key implications emerge:

**World Models as Universal Priors**: The success of leveraging pre-trained video world models suggests that web-scale video data contains rich priors for physical reasoning. This opens pathways for transferring knowledge from internet-scale visual data to robotic manipulation, potentially reducing the data requirements for training capable manipulation policies.

**Beyond Behavioral Cloning**: The integration of RL through RAMP demonstrates that pure imitation learning has fundamental limitations. By combining demonstrations with model-based RL, the system can discover strategies beyond those present in training data, enabling better generalization and adaptation.

**Long-Horizon Reliability**: The consistent performance on complex, multi-step tasks addresses a critical challenge in robotic manipulation. Many real-world applications require reliable execution over extended horizons, and GigaBrain-0.5M**'s demonstrated capability in this area marks significant progress toward practical deployment.

**Scalability Considerations**: The model's architecture suggests a scalable path forward—pre-train on large robotic datasets, leverage world models for enhanced reasoning, and fine-tune with RL for specific task distributions. This modular approach allows for continuous improvement as more data and better world models become available.

The work also raises important questions about the optimal balance between pre-training scale, world model capacity, and RL fine-tuning. Future research may explore how these components interact and identify the most efficient training regimes for different task categories.

## Takeaways

1. World model-based conditioning significantly enhances VLA model performance, enabling 30% improvements on complex manipulation tasks through better future anticipation and spatiotemporal reasoning.

2. The RAMP framework successfully combines the strengths of large-scale pre-training, world model predictions, and reinforcement learning to achieve robust cross-task adaptation beyond pure behavioral cloning.

3. GigaBrain-0.5M* demonstrates reliable long-horizon execution in real-world deployments, addressing a critical challenge for practical robotic systems that require consistent multi-step task completion.

4. Pre-training on massive robotic datasets (10,000+ hours) provides a strong foundation, as evidenced by GigaBrain-0.5's first-place ranking on the international RoboChallenge benchmark.

5. Leveraging web-scale video world models as priors for robotic learning opens new pathways for knowledge transfer from internet data to physical manipulation tasks, potentially reducing domain-specific data requirements.
:::

:::zh
**论文**: [2602.12099](https://arxiv.org/abs/2602.12099)
**作者**: GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng
**分类**: cs.CV

## 摘要

本文介绍了GigaBrain-0.5M*,这是一个通过世界模型强化学习解决当前视觉-语言-动作(VLA)架构根本性局限的模型。传统的VLA模型直接从观察预测动作序列,存在场景理解受限和未来预测能力弱的问题。GigaBrain-0.5M*利用在海量视频语料上预训练的视频世界模型来增强时空推理和未来预测能力。该模型基于GigaBrain-0.5构建,后者在超过10,000小时的机器人操作数据上进行预训练,目前在RoboChallenge基准测试中排名第一。模型集成了RAMP(基于世界模型条件策略的强化学习)框架以实现稳健的跨任务适应。实证验证显示,在衣物折叠、箱子打包和咖啡制作等挑战性任务上,相比基线方法性能提升约30%,并在真实世界部署中展现出可靠的长时域执行能力。

## 主要贡献

- 提出RAMP框架,这是一种将世界模型未来预测与强化学习相结合用于VLA训练的新型方法
- 集成网络规模视频世界模型以增强机器人操作中的时空推理能力
- 在复杂长时域操作任务上相比RECAP基线实现30%的性能提升
- 基础模型GigaBrain-0.5在国际RoboChallenge基准测试中获得第一名
- 验证了可靠的真实世界部署能力,实现任务的持续完成且无失败

## 方法论:RAMP框架

GigaBrain-0.5M*的核心创新在于RAMP(基于世界模型条件策略的强化学习)框架,该框架从根本上重新思考了VLA模型学习操作策略的方式。与直接将观察映射到动作的传统方法不同,RAMP将世界模型作为策略学习的条件机制。

架构包含三个关键组件:

1. **世界模型基础**:预训练的视频世界模型作为骨干网络,提供从网络规模视频数据中学习到的丰富时空表示。这些模型擅长预测未来状态和理解场景动态,这些能力对于需要预判的操作任务至关重要。

2. **模型条件策略**:策略网络同时以当前观察和世界模型预测为条件。这使智能体能够在执行前推理动作的未来后果,从而做出更明智的决策。

3. **基于强化学习的优化**:RAMP不仅依赖于从演示中进行行为克隆,而是采用强化学习来微调策略。这使模型能够发现新策略并适应训练分布之外的任务变化。

训练过程首先在大规模GigaBrain-0.5数据集(10,000+小时机器人数据)上进行预训练,然后应用RAMP实现跨任务泛化。这种两阶段方法结合了大规模预训练的优势和基于强化学习微调的适应性。

## 实验结果与性能分析

GigaBrain-0.5M*在多个具有挑战性的操作基准测试中展现出显著改进。评估聚焦于三个特别困难的任务,这些任务需要长时域规划和精确执行:

**衣物折叠**:该任务需要理解可变形物体动力学和多步骤序列推理。GigaBrain-0.5M**相比RECAP基线实现了约30%的成功率提升,这归功于通过世界模型预测更好地预判织物行为。

**箱子打包**:该任务需要空间推理和高效运动规划,从模型增强的场景理解中显著受益。世界模型条件策略通过在执行前预测物体放置来实现更高效的打包策略。

**咖啡制作**:这个复杂任务涉及对多个物体的精确操作,容差要求严格。30%的性能提升展示了模型可靠处理复杂多步骤程序的能力。

除了定量指标,真实世界部署视频显示了持续的任务完成且无失败,这是实用机器人系统的关键要求。模型展现出稳健的长时域执行能力,在扩展任务序列中保持性能,而传统VLA模型通常会因累积误差而性能下降。

RoboChallenge基准测试结果进一步验证了该方法,基础模型GigaBrain-0.5在国际上获得第一名。这表明基础模型本身在RAMP增强之前就已经具有很强的竞争力。

## 对机器人学习的启示

GigaBrain-0.5M*代表了视觉-语言-动作模型训练方法的范式转变。几个关键启示浮现:

**世界模型作为通用先验**:利用预训练视频世界模型的成功表明,网络规模视频数据包含丰富的物理推理先验。这为从互联网规模视觉数据向机器人操作迁移知识开辟了路径,可能减少训练有能力操作策略的数据需求。

**超越行为克隆**:通过RAMP集成强化学习表明纯模仿学习存在根本性局限。通过将演示与基于模型的强化学习相结合,系统可以发现训练数据中不存在的策略,实现更好的泛化和适应。

**长时域可靠性**:在复杂多步骤任务上的持续性能解决了机器人操作中的关键挑战。许多真实世界应用需要在扩展时域上可靠执行,GigaBrain-0.5M**在这一领域展示的能力标志着向实际部署迈出的重要一步。

**可扩展性考量**:模型架构提出了一条可扩展的前进路径——在大型机器人数据集上预训练,利用世界模型增强推理,并通过强化学习针对特定任务分布进行微调。这种模块化方法允许随着更多数据和更好世界模型的出现而持续改进。

该工作也提出了关于预训练规模、世界模型容量和强化学习微调之间最优平衡的重要问题。未来研究可能探索这些组件如何相互作用,并为不同任务类别确定最有效的训练方案。

## 要点总结

1. 基于世界模型的条件机制显著增强了VLA模型性能,通过更好的未来预判和时空推理在复杂操作任务上实现30%的改进。

2. RAMP框架成功结合了大规模预训练、世界模型预测和强化学习的优势,实现了超越纯行为克隆的稳健跨任务适应能力。

3. GigaBrain-0.5M*在真实世界部署中展现出可靠的长时域执行能力,解决了需要持续多步骤任务完成的实用机器人系统的关键挑战。

4. 在海量机器人数据集(10,000+小时)上的预训练提供了强大基础,GigaBrain-0.5在国际RoboChallenge基准测试中获得第一名证明了这一点。

5. 利用网络规模视频世界模型作为机器人学习的先验,为从互联网数据向物理操作任务的知识迁移开辟了新途径,可能降低特定领域的数据需求。
:::
