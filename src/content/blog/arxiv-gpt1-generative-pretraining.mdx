---
title:
  en: "GPT-1: Improving Language Understanding by Generative Pre-Training"
  zh: "GPT-1: Improving Language Understanding by Generative Pre-Training"
description:
  en: "使用 Transformer 进行无监督语言建模并在多任务上微调，实现通用文本理解"
  zh: "使用 Transformer 进行无监督语言建模并在多任务上微调，实现通用文本理解"
date: 2018-06-11
tags: ["arxiv", "ai", "nlp", "gpt", "transformer", "pre-training", "openai"]
image: "/arxiv-visuals/arxiv-gpt1-generative-pretraining.png"
---

:::zh
## 论文信息 / Paper Information

- **标题**: Improving Language Understanding by Generative Pre-Training
- **作者**: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (OpenAI)
- **发表年份**: 2018
- **链接**: [OpenAI Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## 核心贡献 / Key Contributions

GPT-1 是 OpenAI 提出的第一代生成式预训练 Transformer 模型，开创了"预训练+微调"的范式。

### 主要创新 / Main Innovations

1. **无监督预训练**: 在大规模无标注文本上进行语言建模
2. **有监督微调**: 在下游任务上进行任务特定的微调
3. **Transformer 解码器**: 使用仅解码器的 Transformer 架构
4. **迁移学习**: 证明预训练表示可以迁移到多种 NLP 任务

### 模型架构 / Model Architecture

```
输入文本 → Token Embedding + Position Embedding
         → 12层 Transformer Decoder
         → 任务特定输出层
```

## 技术细节 / Technical Details

### 预训练目标

使用标准语言建模目标，最大化：

$$L_1(U) = \sum_i \log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta)$$

### 微调策略

在预训练模型基础上添加任务特定的线性层，同时优化：

$$L_3(C) = L_2(C) + \lambda \cdot L_1(C)$$

### 模型规格

- **参数量**: 117M
- **层数**: 12
- **隐藏维度**: 768
- **注意力头**: 12
- **训练数据**: BooksCorpus (7000本书)

## 实验结果 / Experimental Results

在多个 NLP 基准上取得显著提升：

| 任务 | 数据集 | 提升 |
|------|--------|------|
| 自然语言推理 | MNLI | +5.7% |
| 问答 | RACE | +5.7% |
| 情感分析 | SST-2 | +3.4% |
| 语义相似度 | STS-B | +4.5% |

## 影响与意义 / Impact and Significance

1. **开创预训练范式**: 为后续 GPT-2、GPT-3 奠定基础
2. **证明规模效应**: 展示大规模预训练的有效性
3. **统一框架**: 提供处理多种 NLP 任务的统一方法
4. **推动 NLP 发展**: 引发预训练语言模型的研究热潮

## 与 BERT 的对比 / Comparison with BERT

| 特性 | GPT-1 | BERT |
|------|-------|------|
| 架构 | 仅解码器 | 仅编码器 |
| 预训练 | 单向语言模型 | 双向掩码语言模型 |
| 生成能力 | 强 | 弱 |
| 理解能力 | 较强 | 更强 |

## 代码示例 / Code Example

```python
# 使用 Hugging Face 加载 GPT 模型
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel

tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')
model = OpenAIGPTModel.from_pretrained('openai-gpt')

text = "Natural language understanding comprises"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# 获取最后一层隐藏状态
last_hidden_states = outputs.last_hidden_state
```

## 总结 / Summary

GPT-1 开创了生成式预训练的新范式，证明了在大规模无标注数据上预训练然后在下游任务微调的有效性。这一工作为后续的 GPT 系列模型和整个大语言模型领域奠定了基础。
:::

:::en
*本文是 AI 经典论文系列的一部分，旨在介绍人工智能领域的里程碑式研究成果。*
:::
