---
title:
  en: "Krites: Asynchronous Verified Semantic Caching for Tiered LLM Architectures"
  zh: "Krites:面向分层LLM架构的异步验证语义缓存"
description:
  en: "A novel asynchronous caching policy that uses LLM judges to expand static cache coverage by up to 3.9x without impacting critical path latency."
  zh: "一种创新的异步缓存策略,通过LLM判断器将静态缓存覆盖率提升至3.9倍,且不影响关键路径延迟。"
date: 2026-02-16
tags: ["arxiv", "ai", "cs.ir", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.13165](https://arxiv.org/abs/2602.13165)
**Authors**: Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, Tak Chiam, Weihua Zhu
**Categories**: cs.IR, cs.AI

## Abstract

As large language models become integral to production systems, semantic caching has emerged as a critical optimization for reducing inference costs and latency. This paper introduces Krites, an asynchronous caching policy designed for tiered LLM architectures that combines static (curated, offline-vetted) and dynamic (online-populated) caches. The key innovation lies in using an LLM judge to asynchronously verify near-miss cache entries that fall just below the static similarity threshold. When verified as semantically equivalent, these responses are promoted to the dynamic cache, enabling future queries to benefit from curated static answers. Krites maintains the same critical path behavior as standard threshold policies while expanding static coverage by up to 3.9x on conversational and search workloads.

## Key Contributions

- Introduction of an asynchronous verification mechanism that decouples cache expansion from serving latency
- Design of Krites, a policy that uses LLM judges to safely expand static cache coverage without changing critical path decisions
- Demonstration that near-threshold misses represent significant untapped reuse opportunities in production workloads
- Trace-driven evaluation showing 2.5-3.9x improvement in static answer reuse across conversational and search scenarios
- Analysis of the cost-benefit tradeoffs between verification overhead and cache hit improvements

## The Static-Dynamic Cache Tradeoff

Production LLM deployments face a fundamental tension in semantic caching. Static caches contain high-quality, human-vetted responses mined from historical logs, offering reliability and consistency. Dynamic caches populate online, providing flexibility but less quality control. Both tiers typically use a single embedding similarity threshold to determine cache hits.

This unified threshold creates a problematic tradeoff. Conservative thresholds (high similarity requirements) ensure semantic correctness but miss legitimate reuse opportunities—queries that are semantically equivalent but fall just below the threshold. Aggressive thresholds (low similarity requirements) increase hit rates but risk serving incorrect responses to semantically different queries.

The paper's analysis reveals that near-threshold misses—queries with static neighbors scoring just below the threshold—represent a substantial fraction of traffic. These queries are often paraphrases or minor variations that could safely reuse static answers, but current policies treat them as complete misses requiring full LLM inference.

## Krites Architecture and Mechanism

Krites operates through a three-stage pipeline that maintains critical path performance while expanding cache coverage:

**Stage 1: Standard Threshold Check**
On the serving path, Krites behaves identically to a baseline static-threshold policy. It computes embedding similarity between the incoming query and static cache entries. If the nearest neighbor exceeds the static threshold $\tau_s$, the cached response is served immediately.

**Stage 2: Asynchronous Verification**
When the nearest static neighbor falls in the near-miss zone (below $\tau_s$ but above a lower bound $\tau_v$), Krites triggers an asynchronous verification process. The query proceeds to normal LLM inference without delay. In parallel, an LLM judge evaluates whether the static response is semantically acceptable for the new query.

**Stage 3: Promotion and Future Reuse**
If the judge approves the match, the system creates a new dynamic cache entry mapping the query to the verified static response. Future queries that are similar to this query can now hit the dynamic cache and receive the curated static answer, even though the original query didn't meet the static threshold.

The verification uses a carefully designed prompt that asks the LLM judge to assess semantic equivalence between the query and the cached response's original query. This judgment focuses on whether the cached answer would satisfy the new query's intent, not just surface-level similarity.

## Experimental Results and Analysis

The evaluation uses trace-driven simulation on two workload types: conversational assistant traffic and search-style queries. The key metric is the fraction of requests served with curated static answers, including both direct static hits and verified promotions.

For conversational workloads, Krites achieves 2.5-3.2x improvement in static answer reuse compared to tuned baseline thresholds. The gains are particularly pronounced for queries with high paraphrase rates—common in assistant scenarios where users rephrase questions when initial responses are unsatisfactory.

Search workloads show even stronger results, with up to 3.9x improvement. Search queries exhibit more semantic clustering, where multiple surface forms express the same information need. Krites effectively captures these clusters by verifying near-miss entries.

The paper demonstrates that verification costs are manageable. While each verification requires an LLM judge call, these happen asynchronously and only for near-miss queries. The amortized cost is offset by avoiding full inference on future similar queries. For workloads with moderate repeat rates, the break-even point occurs within hours to days.

Ablation studies reveal that the near-miss zone width (gap between $\tau_s$ and $\tau_v$) significantly impacts performance. Too narrow, and few candidates are verified; too wide, and verification costs escalate while approval rates drop. The optimal range varies by workload but typically spans 0.05-0.15 in cosine similarity space.

## Implications for Production Systems

Krites addresses a practical challenge in production LLM serving: how to safely expand cache coverage without risking quality degradation. The asynchronous design is particularly valuable because it maintains the same latency guarantees as simpler policies while progressively improving hit rates.

The approach is complementary to other caching optimizations. It can combine with better embedding models, more sophisticated similarity metrics, or multi-tier cache hierarchies. The key insight—that near-misses represent safe expansion opportunities—applies broadly across caching strategies.

For organizations operating LLM services, Krites offers a path to reduce inference costs without the operational complexity of manually curating larger static caches. The system automatically discovers which near-miss queries are safe to promote, effectively crowdsourcing cache expansion through production traffic.

The paper also highlights the value of LLM judges in system optimization. While judges add computational cost, their ability to make nuanced semantic decisions enables policies that would be impossible with purely metric-based approaches. This pattern may extend to other system components where semantic understanding provides optimization opportunities.

## Takeaways

1. Near-threshold cache misses represent significant untapped reuse opportunities in production LLM workloads, often accounting for queries that are semantically equivalent but fall just below similarity thresholds.

2. Asynchronous verification decouples cache expansion from serving latency, allowing systems to safely explore lower-confidence matches without impacting critical path performance.

3. LLM judges can effectively assess semantic equivalence for caching decisions, achieving 2.5-3.9x improvements in static answer reuse across conversational and search workloads.

4. The cost of verification is amortized over future cache hits, making the approach economically viable for workloads with moderate query repetition and paraphrasing.

5. Tiered caching architectures benefit from policies that bridge static and dynamic tiers, progressively expanding curated content coverage through production traffic analysis.
:::

:::zh
**论文**: [2602.13165](https://arxiv.org/abs/2602.13165)
**作者**: Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, Tak Chiam, Weihua Zhu
**分类**: cs.IR, cs.AI

## 摘要

随着大语言模型成为生产系统的核心组件,语义缓存已成为降低推理成本和延迟的关键优化手段。本文提出Krites,一种面向分层LLM架构的异步缓存策略,该架构结合了静态缓存(精选的离线审核内容)和动态缓存(在线填充内容)。核心创新在于使用LLM判断器异步验证那些略低于静态相似度阈值的近似未命中缓存条目。当验证为语义等价时,这些响应被提升到动态缓存,使未来查询能够复用精选的静态答案。Krites保持与标准阈值策略相同的关键路径行为,同时在对话和搜索工作负载上将静态覆盖率提升至3.9倍。

## 主要贡献

- 提出异步验证机制,将缓存扩展与服务延迟解耦
- 设计Krites策略,利用LLM判断器安全扩展静态缓存覆盖范围而不改变关键路径决策
- 证明近阈值未命中在生产工作负载中代表着显著的未开发复用机会
- 基于追踪的评估显示在对话和搜索场景中静态答案复用提升2.5-3.9倍
- 分析验证开销与缓存命中改进之间的成本收益权衡

## 静态-动态缓存的权衡困境

生产环境的LLM部署在语义缓存方面面临根本性张力。静态缓存包含高质量、人工审核的响应,这些响应从历史日志中挖掘而来,提供可靠性和一致性。动态缓存在线填充,提供灵活性但质量控制较弱。两层通常使用单一的嵌入相似度阈值来判定缓存命中。

这种统一阈值造成了问题性的权衡。保守阈值(高相似度要求)确保语义正确性但错过合法的复用机会——那些语义等价但略低于阈值的查询。激进阈值(低相似度要求)提高命中率但冒着向语义不同的查询提供错误响应的风险。

论文分析揭示,近阈值未命中——与静态邻居得分略低于阈值的查询——占据相当大比例的流量。这些查询通常是改述或微小变体,本可以安全复用静态答案,但当前策略将它们视为完全未命中,需要完整的LLM推理。

## Krites架构与机制

Krites通过三阶段流水线运作,在保持关键路径性能的同时扩展缓存覆盖:

**阶段1:标准阈值检查**
在服务路径上,Krites的行为与基线静态阈值策略完全相同。它计算输入查询与静态缓存条目之间的嵌入相似度。如果最近邻超过静态阈值$\tau_s$,立即提供缓存响应。

**阶段2:异步验证**
当最近的静态邻居落在近似未命中区域(低于$\tau_s$但高于下界$\tau_v$)时,Krites触发异步验证过程。查询无延迟地继续正常的LLM推理。并行地,LLM判断器评估静态响应对新查询是否语义可接受。

**阶段3:提升与未来复用**
如果判断器批准匹配,系统创建新的动态缓存条目,将查询映射到已验证的静态响应。未来与此查询相似的查询现在可以命中动态缓存并接收精选的静态答案,即使原始查询未达到静态阈值。

验证使用精心设计的提示,要求LLM判断器评估查询与缓存响应原始查询之间的语义等价性。这种判断聚焦于缓存答案是否能满足新查询的意图,而非仅表面相似性。

## 实验结果与分析

评估使用两种工作负载类型的追踪驱动模拟:对话助手流量和搜索式查询。关键指标是使用精选静态答案服务的请求比例,包括直接静态命中和验证提升。

对于对话工作负载,Krites相比调优的基线阈值实现了2.5-3.2倍的静态答案复用改进。在改述率高的查询中收益尤为显著——这在助手场景中很常见,用户在初始响应不满意时会重新表述问题。

搜索工作负载显示更强的结果,提升高达3.9倍。搜索查询表现出更多语义聚类,多种表面形式表达相同的信息需求。Krites通过验证近似未命中条目有效捕获这些聚类。

论文证明验证成本是可控的。虽然每次验证需要LLM判断器调用,但这些异步发生且仅针对近似未命中查询。摊销成本被避免未来相似查询的完整推理所抵消。对于具有中等重复率的工作负载,盈亏平衡点在数小时到数天内出现。

消融研究揭示近似未命中区域宽度($\tau_s$和$\tau_v$之间的间隙)显著影响性能。太窄,验证的候选很少;太宽,验证成本上升而批准率下降。最优范围因工作负载而异,但通常在余弦相似度空间中跨越0.05-0.15。

## 对生产系统的启示

Krites解决了生产LLM服务中的实际挑战:如何在不冒质量下降风险的情况下安全扩展缓存覆盖。异步设计特别有价值,因为它保持与简单策略相同的延迟保证,同时逐步提高命中率。

该方法与其他缓存优化互补。它可以与更好的嵌入模型、更复杂的相似度度量或多层缓存层次结构结合。关键洞察——近似未命中代表安全的扩展机会——广泛适用于各种缓存策略。

对于运营LLM服务的组织,Krites提供了一条降低推理成本的路径,而无需手动策划更大静态缓存的运营复杂性。系统自动发现哪些近似未命中查询可以安全提升,通过生产流量有效地众包缓存扩展。

论文还强调了LLM判断器在系统优化中的价值。虽然判断器增加计算成本,但它们做出细微语义决策的能力使得纯基于度量的方法无法实现的策略成为可能。这种模式可能扩展到其他系统组件,其中语义理解提供优化机会。

## 要点总结

1. 近阈值缓存未命中代表生产LLM工作负载中显著的未开发复用机会,通常占据语义等价但略低于相似度阈值的查询。

2. 异步验证将缓存扩展与服务延迟解耦,允许系统安全探索低置信度匹配而不影响关键路径性能。

3. LLM判断器能够有效评估缓存决策的语义等价性,在对话和搜索工作负载中实现2.5-3.9倍的静态答案复用改进。

4. 验证成本在未来缓存命中中摊销,使该方法对具有中等查询重复和改述的工作负载在经济上可行。

5. 分层缓存架构受益于桥接静态和动态层的策略,通过生产流量分析逐步扩展精选内容覆盖范围。
:::
