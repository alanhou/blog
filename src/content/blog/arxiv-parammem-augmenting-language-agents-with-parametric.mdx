---
title:
  en: "ParamMem: Augmenting Language Agents with Parametric Reflective Memory"
  zh: "ParamMem: 通过参数化反思记忆增强语言智能体"
description:
  en: "A novel parametric memory module that encodes cross-sample reflection patterns to enable diverse, sample-efficient self-reflection in language agents without relying on external models."
  zh: "一种新颖的参数化记忆模块,通过编码跨样本反思模式实现多样化、样本高效的自我反思,无需依赖外部模型即可增强语言智能体。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.lg", "cs.ma"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.23320](https://arxiv.org/abs/2602.23320)
**Authors**: Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen, Kun Zhang
**Categories**: cs.LG, cs.MA

## Abstract

Self-reflection has emerged as a critical capability for language agents, enabling iterative solution refinement. However, existing reflection mechanisms often fall into repetitive patterns that constrain reasoning performance. This paper introduces ParamMem, a parametric memory module that addresses the diversity bottleneck in agent reflection by encoding cross-sample reflection patterns directly into model parameters. Through temperature-controlled sampling, ParamMem generates diverse reflection signals that break the cycle of repetitive outputs. The authors propose ParamAgent, a comprehensive framework integrating parametric memory with episodic and cross-sample memory systems. Empirical validation across code generation, mathematical reasoning, and multi-hop question answering tasks demonstrates consistent performance gains over state-of-the-art baselines, with notable sample efficiency and weak-to-strong transfer capabilities.

## Key Contributions

- **Parametric Memory Architecture**: A novel memory module that encodes reflection patterns into model parameters, enabling diverse reflection generation through stochastic sampling mechanisms
- **Empirical Correlation Analysis**: Systematic demonstration of the positive relationship between reflective diversity and task success rates across multiple reasoning domains
- **ParamAgent Framework**: An integrated agent architecture combining parametric, episodic, and cross-sample memory for enhanced reflection capabilities
- **Self-Improvement Without External Models**: A training paradigm that achieves performance gains without reliance on stronger external models for supervision
- **Cross-Scale Transfer**: Evidence of weak-to-strong generalization, where smaller models can effectively transfer learned reflection patterns to larger architectures

## Methodology and Architecture

The ParamMem module operates by fine-tuning a language model on a curated dataset of successful reflection trajectories. Unlike traditional episodic memory that retrieves past experiences, parametric memory internalizes reflection patterns within the model's weights. This approach offers several architectural advantages:

The training process involves collecting reflection-action pairs from successful task completions, then fine-tuning the base model to predict effective reflections given task context. During inference, temperature-controlled sampling from this parametric distribution generates diverse reflection candidates. The diversity mechanism is crucial—higher temperatures produce more varied reflections, preventing the agent from cycling through identical reasoning paths.

ParamAgent extends this foundation by maintaining three complementary memory systems: (1) parametric memory for pattern-based reflection generation, (2) episodic memory for storing task-specific experiences, and (3) cross-sample memory for aggregating insights across multiple problem instances. This multi-tier architecture balances generalization with specificity, allowing agents to leverage both learned patterns and concrete examples.

## Experimental Results and Analysis

The evaluation spans three challenging domains: code generation (HumanEval, MBPP), mathematical reasoning (GSM8K, MATH), and multi-hop question answering (HotpotQA). Across all benchmarks, ParamAgent demonstrates consistent improvements over baseline reflection methods, with particularly strong gains in mathematical reasoning tasks where diverse solution strategies prove most valuable.

A key finding is the sample efficiency of ParamMem. While traditional reflection methods require extensive trial-and-error during inference, parametric memory amortizes this exploration cost during training. The authors show that ParamAgent achieves comparable or superior performance with significantly fewer inference-time reflection iterations.

The weak-to-strong transfer experiments reveal an intriguing property: reflection patterns learned by smaller models (e.g., 7B parameters) successfully transfer to larger models (e.g., 13B, 70B parameters) with minimal performance degradation. This suggests that effective reflection strategies exhibit scale-invariant characteristics, opening possibilities for efficient knowledge distillation in agent systems.

Ablation studies isolate the contribution of each memory component. Parametric memory alone provides substantial gains, while the combination with episodic and cross-sample memory yields the best overall performance. The temperature parameter emerges as a critical hyperparameter—moderate temperatures (0.7-0.9) balance diversity with coherence, while extreme values either collapse to deterministic outputs or produce incoherent reflections.

## Implications for Agent Design

ParamMem represents a shift in how we conceptualize agent memory systems. Traditional approaches treat memory as external storage—databases of past experiences to be retrieved and reused. Parametric memory instead views the model itself as a malleable substrate for encoding strategic knowledge. This perspective aligns with recent work on in-context learning and prompt engineering, but operates at a deeper architectural level.

The self-improvement capability without external supervision is particularly significant for practical deployment. Many reflection-based agents rely on stronger models (e.g., GPT-4) to provide feedback or generate training data. ParamMem's ability to bootstrap from its own successful trajectories suggests a path toward more autonomous agent development, reducing dependence on proprietary systems.

The diversity-performance correlation documented in this work has broader implications for agent design. It suggests that exploration mechanisms—traditionally associated with reinforcement learning—play an equally important role in language-based reasoning. Future agent architectures might benefit from explicit diversity objectives during both training and inference.

## Takeaways

1. Reflective diversity strongly correlates with task success in language agents, motivating architectural designs that explicitly promote varied reasoning paths
2. Parametric memory offers a sample-efficient alternative to episodic retrieval by encoding reflection patterns directly into model weights
3. Temperature-controlled sampling from parametric distributions provides a simple yet effective mechanism for generating diverse reflections
4. Multi-tier memory architectures combining parametric, episodic, and cross-sample components achieve superior performance compared to single-memory systems
5. Weak-to-strong transfer of reflection patterns suggests scale-invariant properties of effective reasoning strategies, enabling efficient knowledge distillation
6. Self-improvement without external model supervision is achievable through careful curation of successful reflection trajectories
7. The ParamAgent framework demonstrates consistent gains across diverse reasoning domains, indicating broad applicability of the parametric memory approach
:::

:::zh
**论文**: [2602.23320](https://arxiv.org/abs/2602.23320)
**作者**: Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen, Kun Zhang
**分类**: cs.LG, cs.MA

## 摘要

自我反思已成为语言智能体的关键能力,支持迭代式解决方案优化。然而,现有反思机制常陷入重复模式,限制了推理性能。本文提出ParamMem,一种参数化记忆模块,通过将跨样本反思模式直接编码到模型参数中来解决智能体反思的多样性瓶颈。通过温度控制采样,ParamMem生成多样化的反思信号,打破重复输出的循环。作者提出ParamAgent框架,整合参数化记忆与情节记忆和跨样本记忆系统。在代码生成、数学推理和多跳问答任务上的实证验证表明,该方法相比最先进基线实现了持续性能提升,并展现出显著的样本效率和弱到强迁移能力。

## 主要贡献

- **参数化记忆架构**: 提出新颖的记忆模块,将反思模式编码到模型参数中,通过随机采样机制实现多样化反思生成
- **实证相关性分析**: 系统性证明反思多样性与任务成功率在多个推理领域的正相关关系
- **ParamAgent框架**: 集成参数化、情节和跨样本记忆的智能体架构,增强反思能力
- **无需外部模型的自我改进**: 提出无需依赖更强外部模型监督即可实现性能提升的训练范式
- **跨尺度迁移**: 证明弱到强泛化能力,小型模型学习的反思模式可有效迁移到大型架构

## 方法论与架构设计

ParamMem模块通过在成功反思轨迹数据集上微调语言模型来运作。与检索过往经验的传统情节记忆不同,参数化记忆将反思模式内化到模型权重中。这种方法提供了多项架构优势:

训练过程包括从成功任务完成中收集反思-行动对,然后微调基础模型以在给定任务上下文时预测有效反思。推理期间,从该参数化分布进行温度控制采样生成多样化反思候选。多样性机制至关重要——较高温度产生更多变化的反思,防止智能体在相同推理路径中循环。

ParamAgent在此基础上维护三个互补的记忆系统:(1)用于基于模式的反思生成的参数化记忆,(2)用于存储任务特定经验的情节记忆,(3)用于聚合多个问题实例洞察的跨样本记忆。这种多层架构平衡了泛化与特异性,使智能体能够同时利用学习到的模式和具体示例。

## 实验结果与分析

评估涵盖三个具有挑战性的领域:代码生成(HumanEval, MBPP)、数学推理(GSM8K, MATH)和多跳问答(HotpotQA)。在所有基准测试中,ParamAgent相比基线反思方法展现出持续改进,在数学推理任务中表现尤为突出,因为多样化解决策略在此类任务中最具价值。

关键发现是ParamMem的样本效率。传统反思方法在推理时需要大量试错,而参数化记忆在训练期间分摊了这种探索成本。作者表明,ParamAgent以显著更少的推理时反思迭代次数实现了相当或更优的性能。

弱到强迁移实验揭示了一个有趣特性:小型模型(如7B参数)学习的反思模式成功迁移到大型模型(如13B、70B参数),性能下降极小。这表明有效的反思策略具有尺度不变特征,为智能体系统中的高效知识蒸馏开辟了可能性。

消融研究分离了各记忆组件的贡献。单独的参数化记忆就能提供实质性收益,而与情节和跨样本记忆的组合产生最佳整体性能。温度参数成为关键超参数——中等温度(0.7-0.9)平衡了多样性与连贯性,而极端值要么坍缩为确定性输出,要么产生不连贯的反思。

## 对智能体设计的启示

ParamMem代表了我们概念化智能体记忆系统方式的转变。传统方法将记忆视为外部存储——过往经验的数据库,用于检索和重用。参数化记忆则将模型本身视为编码策略知识的可塑基底。这一视角与近期关于上下文学习和提示工程的工作一致,但在更深的架构层面运作。

无需外部监督的自我改进能力对实际部署尤为重要。许多基于反思的智能体依赖更强模型(如GPT-4)提供反馈或生成训练数据。ParamMem从自身成功轨迹自举的能力为更自主的智能体开发指明了道路,减少了对专有系统的依赖。

本工作记录的多样性-性能相关性对智能体设计具有更广泛的启示。它表明探索机制——传统上与强化学习相关——在基于语言的推理中同样发挥重要作用。未来的智能体架构可能受益于在训练和推理期间的显式多样性目标。

## 要点总结

1. 反思多样性与语言智能体的任务成功强相关,激励明确促进多样化推理路径的架构设计
2. 参数化记忆通过将反思模式直接编码到模型权重中,提供了情节检索的样本高效替代方案
3. 从参数化分布进行温度控制采样为生成多样化反思提供了简单而有效的机制
4. 结合参数化、情节和跨样本组件的多层记忆架构相比单一记忆系统实现了卓越性能
5. 反思模式的弱到强迁移表明有效推理策略的尺度不变特性,支持高效知识蒸馏
6. 通过精心策划成功反思轨迹,可实现无需外部模型监督的自我改进
7. ParamAgent框架在多样化推理领域展现持续收益,表明参数化记忆方法的广泛适用性
:::
