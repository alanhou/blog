---
title:
  en: "Document Reconstruction Unlocks Scalable Long-Context RLVR"
  zh: "文档重建解锁可扩展的长上下文RLVR"
description:
  en: "An unsupervised method using document reconstruction via RL to improve LM long-context abilities without human annotations"
  zh: "一种通过强化学习进行文档重建的无监督方法，无需人工标注即可提升语言模型的长上下文能力"
date: 2026-02-10
tags: ["arxiv", "ai", "long-context", "reinforcement-learning", "rlvr", "unsupervised", "cs.CL", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.08237](https://arxiv.org/abs/2602.08237)
**Authors**: Yao Xiao, Lei Wang, Yue Deng, Guanzheng Chen, Ziqi Jin, Jung-jae Kim, Xiaoli Li, Roy Ka-wei Lee, Lidong Bing
**Categories**: cs.CL, cs.LG

## Abstract

This paper presents an unsupervised method to improve language model long-context abilities without requiring human annotations. The approach replaces paragraphs in long documents with placeholders, then trains the model via reinforcement learning to reconstruct the original document by identifying and correctly ordering the missing sections. The method is evaluated on RULER and LongBench v2, showing consistent improvements. The authors release code, data, and models.

## Key Contributions

- **Unsupervised long-context training**: No human annotations or task-specific labels required
- **Document reconstruction as RL reward**: A naturally verifiable objective for long-context reasoning
- **Scalable data generation**: Any long document can be converted into a training instance
- **Open release**: Code, data, and trained models publicly available

## Motivation

Training LLMs to handle long contexts effectively remains challenging. Existing approaches typically rely on:

- **Supervised fine-tuning**: Requires expensive human-annotated long-context QA pairs
- **Synthetic data**: Often limited in diversity and naturalness
- **Continued pretraining**: Improves context window but not necessarily reasoning over long contexts

Reinforcement Learning with Verifiable Rewards (RLVR) has shown promise for improving reasoning, but applying it to long-context tasks has been limited by the difficulty of constructing verifiable long-context tasks at scale.

## Method

### Document Reconstruction Task

Given a long document $D = [p_1, p_2, \ldots, p_n]$ consisting of $n$ paragraphs:

1. **Select paragraphs**: Randomly choose $k$ paragraphs $\{p_{i_1}, p_{i_2}, \ldots, p_{i_k}\}$ for removal
2. **Create placeholders**: Replace each selected paragraph with a placeholder token $[\text{BLANK}_j]$
3. **Shuffle removed paragraphs**: Present the removed paragraphs in random order as candidates
4. **Task**: The model must assign each candidate paragraph to its correct placeholder position

The corrupted document with candidates forms the input:

$$\text{Input} = [D_{\text{corrupted}}, \text{Candidates}: \{c_1, c_2, \ldots, c_k\}]$$

The model must output the correct mapping: $[\text{BLANK}_j] \rightarrow c_{\sigma(j)}$ where $\sigma$ is the correct permutation.

### Why This Task Works for Long-Context Training

The reconstruction task requires multiple long-context skills simultaneously:

- **Global comprehension**: Understanding the document's overall structure and flow
- **Local coherence**: Matching paragraph boundaries based on topical and stylistic continuity
- **Positional reasoning**: Determining the correct order among multiple candidates
- **Long-range dependency**: Connecting information across distant parts of the document

### Reinforcement Learning with Verifiable Rewards

The reconstruction task provides a naturally verifiable reward signal:

$$R = \frac{1}{k}\sum_{j=1}^{k} \mathbb{1}[\text{predicted}(j) = \text{correct}(j)]$$

This reward is:
- **Exact**: No ambiguity in correctness (either the paragraph is correctly placed or not)
- **Granular**: Partial credit for partially correct reconstructions
- **Scalable**: Automatically computed without human judgment

The RL training uses this reward to optimize the model's policy for long-context reasoning:

$$\mathcal{L}_{\text{RL}} = -\mathbb{E}_{\pi_\theta}[R \cdot \log \pi_\theta(a|s)]$$

### Data Generation Pipeline

The data generation is fully automatic:

1. Collect long documents from diverse sources (books, articles, reports)
2. Filter for quality and minimum length
3. Segment into paragraphs using natural boundaries
4. Generate multiple reconstruction instances per document by varying which paragraphs are removed
5. Control difficulty by adjusting the number of removed paragraphs $k$

This pipeline can generate virtually unlimited training data from any text corpus.

## Experimental Results

### RULER Benchmark

RULER evaluates long-context capabilities across multiple tasks:

- **Retrieval**: Finding specific information in long contexts
- **Multi-hop reasoning**: Connecting information across document sections
- **Aggregation**: Summarizing information distributed throughout the context

Results show consistent improvements across all RULER subtasks, with particularly strong gains on multi-hop reasoning tasks that require connecting distant information.

### LongBench v2

On the more challenging LongBench v2 benchmark:

- Improvements across diverse real-world long-context tasks
- Strong performance on tasks requiring document understanding
- Gains maintained across different context lengths

### Scaling Properties

The method shows favorable scaling behavior:

- **More reconstruction instances**: Performance improves with more training data (easy to generate)
- **Harder instances**: Increasing the number of removed paragraphs $k$ provides curriculum-like difficulty scaling
- **Longer documents**: Training on longer documents transfers to improved performance at all context lengths

## Analysis

### What the Model Learns

Analysis of the trained models reveals improved capabilities in:

- **Structural understanding**: Better modeling of document organization and flow
- **Coherence detection**: Improved ability to detect topical transitions
- **Position sensitivity**: Enhanced awareness of where information appears in long contexts
- **Information integration**: Better at combining information from multiple document locations

### Comparison with Supervised Approaches

Despite being fully unsupervised, the document reconstruction approach:

- Matches or approaches supervised long-context fine-tuning on most benchmarks
- Significantly outperforms continued pretraining alone
- Provides more diverse training signal than task-specific supervised data

### Difficulty Analysis

The number of removed paragraphs $k$ controls task difficulty:

- $k = 2$: Relatively easy, model can use simple local coherence cues
- $k = 4\text{-}6$: Moderate difficulty, requires global document understanding
- $k = 8+$: Challenging, requires sophisticated reasoning about document structure

Training with a curriculum from easy to hard instances yields the best results.

## Significance

This work makes a compelling case for document reconstruction as a scalable, unsupervised objective for long-context RL training. The key insight is that the structure of natural documents provides a rich, freely available source of verifiable rewards. By leveraging this signal through RL, the method improves long-context reasoning without the bottleneck of human annotation. The open release of code, data, and models makes this approach immediately accessible to the research community.
:::

:::zh
**论文**: [2602.08237](https://arxiv.org/abs/2602.08237)
**作者**: Yao Xiao, Lei Wang, Yue Deng, Guanzheng Chen, Ziqi Jin, Jung-jae Kim, Xiaoli Li, Roy Ka-wei Lee, Lidong Bing
**分类**: cs.CL, cs.LG

## 摘要

本文提出一种无监督方法，无需人工标注即可提升语言模型的长上下文能力。该方法将长文档中的段落替换为占位符，然后通过强化学习训练模型通过识别和正确排序缺失部分来重建原始文档。在RULER和LongBench v2上的评估显示一致的改进。作者发布了代码、数据和模型。

## 主要贡献

- **无监督长上下文训练**：不需要人工标注或任务特定标签
- **文档重建作为RL奖励**：长上下文推理的自然可验证目标
- **可扩展的数据生成**：任何长文档都可以转换为训练实例
- **开放发布**：代码、数据和训练模型公开可用

## 动机

训练LLM有效处理长上下文仍然具有挑战性。现有方法通常依赖于：

- **监督微调**：需要昂贵的人工标注长上下文问答对
- **合成数据**：多样性和自然性通常有限
- **持续预训练**：改善上下文窗口但不一定改善长上下文推理

带有可验证奖励的强化学习（RLVR）在改善推理方面显示出前景，但将其应用于长上下文任务受限于大规模构建可验证长上下文任务的困难。

## 方法

### 文档重建任务

给定一个由 $n$ 个段落组成的长文档 $D = [p_1, p_2, \ldots, p_n]$：

1. **选择段落**：随机选择 $k$ 个段落 $\{p_{i_1}, p_{i_2}, \ldots, p_{i_k}\}$ 进行移除
2. **创建占位符**：用占位符token $[\text{BLANK}_j]$ 替换每个选定段落
3. **打乱移除的段落**：将移除的段落以随机顺序作为候选呈现
4. **任务**：模型必须将每个候选段落分配到其正确的占位符位置

带有候选的损坏文档构成输入：

$$\text{Input} = [D_{\text{corrupted}}, \text{Candidates}: \{c_1, c_2, \ldots, c_k\}]$$

模型必须输出正确的映射：$[\text{BLANK}_j] \rightarrow c_{\sigma(j)}$，其中 $\sigma$ 是正确的排列。

### 为什么这个任务适用于长上下文训练

重建任务同时需要多种长上下文技能：

- **全局理解**：理解文档的整体结构和流程
- **局部连贯性**：基于主题和风格连续性匹配段落边界
- **位置推理**：确定多个候选之间的正确顺序
- **长距离依赖**：连接文档远距离部分的信息

### 带有可验证奖励的强化学习

重建任务提供自然可验证的奖励信号：

$$R = \frac{1}{k}\sum_{j=1}^{k} \mathbb{1}[\text{predicted}(j) = \text{correct}(j)]$$

这个奖励是：
- **精确的**：正确性没有歧义（段落要么正确放置要么没有）
- **细粒度的**：对部分正确的重建给予部分分数
- **可扩展的**：无需人工判断自动计算

RL训练使用这个奖励来优化模型的长上下文推理策略：

$$\mathcal{L}_{\text{RL}} = -\mathbb{E}_{\pi_\theta}[R \cdot \log \pi_\theta(a|s)]$$

### 数据生成管道

数据生成完全自动化：

1. 从多样化来源收集长文档（书籍、文章、报告）
2. 过滤质量和最小长度
3. 使用自然边界分割为段落
4. 通过变化移除哪些段落为每个文档生成多个重建实例
5. 通过调整移除段落数 $k$ 控制难度

这个管道可以从任何文本语料库生成几乎无限的训练数据。

## 实验结果

### RULER基准

RULER评估跨多个任务的长上下文能力：

- **检索**：在长上下文中找到特定信息
- **多跳推理**：连接文档各部分的信息
- **聚合**：总结分布在整个上下文中的信息

结果显示所有RULER子任务的一致改进，在需要连接远距离信息的多跳推理任务上增益特别强。

### LongBench v2

在更具挑战性的LongBench v2基准上：

- 跨多样化真实世界长上下文任务的改进
- 在需要文档理解的任务上表现强劲
- 在不同上下文长度上保持增益

### 扩展特性

该方法显示良好的扩展行为：

- **更多重建实例**：性能随更多训练数据改善（易于生成）
- **更难的实例**：增加移除段落数 $k$ 提供类似课程的难度扩展
- **更长的文档**：在更长文档上训练转移到所有上下文长度的改进性能

## 分析

### 模型学到了什么

对训练模型的分析揭示了以下能力的改进：

- **结构理解**：更好地建模文档组织和流程
- **连贯性检测**：改善检测主题转换的能力
- **位置敏感性**：增强对信息在长上下文中出现位置的感知
- **信息整合**：更好地组合来自多个文档位置的信息

### 与监督方法的比较

尽管完全无监督，文档重建方法：

- 在大多数基准上匹配或接近监督长上下文微调
- 显著优于仅持续预训练
- 提供比任务特定监督数据更多样化的训练信号

### 难度分析

移除段落数 $k$ 控制任务难度：

- $k = 2$：相对容易，模型可以使用简单的局部连贯性线索
- $k = 4\text{-}6$：中等难度，需要全局文档理解
- $k = 8+$：具有挑战性，需要对文档结构的复杂推理

从简单到困难实例的课程训练产生最佳结果。

## 意义

这项工作有力地证明了文档重建作为长上下文RL训练的可扩展无监督目标的价值。关键洞察是自然文档的结构提供了丰富的、免费可用的可验证奖励来源。通过RL利用这个信号，该方法在没有人工标注瓶颈的情况下改善了长上下文推理。代码、数据和模型的开放发布使这种方法立即可供研究社区使用。
:::
