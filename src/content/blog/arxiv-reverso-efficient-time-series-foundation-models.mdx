---
title:
  en: "Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting"
  zh: "Reverso: 用于零样本预测的高效时间序列基础模型"
description:
  en: "A family of efficient time series foundation models that match transformer performance while being 100x smaller through hybrid convolution-RNN architectures."
  zh: "通过混合卷积-RNN架构实现的高效时间序列基础模型系列,在模型大小缩小100倍的同时达到Transformer性能。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "/arxiv-visuals/arxiv-reverso-efficient-time-series-foundation-models.png"
---

:::en
**Paper**: [2602.17634](https://arxiv.org/abs/2602.17634)
**Authors**: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim
**Categories**: cs.LG, cs.AI

## Abstract

This paper introduces Reverso, a family of efficient time series foundation models designed for zero-shot forecasting that challenges the prevailing assumption that larger models are necessary for better performance. While recent time series foundation models have followed the scaling paradigm successful in language and vision, resulting in models with hundreds of millions of parameters, Reverso demonstrates that small hybrid architectures combining long convolutions and linear RNN layers (specifically DeltaNet) can achieve comparable performance while being orders of magnitude smaller. The authors present a comprehensive recipe including architectural choices, data augmentation strategies, and inference techniques that collectively push the performance-efficiency Pareto frontier significantly forward.

## Key Contributions

- Introduction of a hybrid architecture that interleaves long convolution layers with DeltaNet (linear RNN) layers, achieving competitive performance with 100x fewer parameters than transformer-based models
- Demonstration that large-scale transformers are not necessary for effective time series foundation models
- Development of data augmentation and inference strategies specifically tailored for efficient zero-shot time series forecasting
- Comprehensive empirical validation showing Reverso models significantly advance the performance-efficiency trade-off in time series forecasting

## Architecture Design

The core innovation of Reverso lies in its hybrid architecture that departs from the transformer-dominated paradigm. Instead of relying on attention mechanisms that scale quadratically with sequence length, Reverso employs a combination of:

**Long Convolution Layers**: These layers capture local temporal patterns efficiently through convolution operations with extended receptive fields. Unlike standard convolutions, long convolutions can model dependencies across extended time horizons without the computational overhead of attention.

**DeltaNet Layers**: DeltaNet represents a class of linear RNNs that maintain the modeling capacity of traditional RNNs while offering linear-time complexity. By interleaving DeltaNet layers with convolution layers, Reverso achieves a balance between local pattern recognition and long-range dependency modeling.

This architectural choice is motivated by the observation that time series data exhibits different characteristics than natural language or images. The temporal structure and periodicity in time series can be effectively captured by convolutions and linear recurrences without requiring the full expressiveness (and computational cost) of self-attention.

## Training and Inference Strategies

Beyond architecture, the paper describes several critical strategies that enhance Reverso's performance:

**Data Augmentation**: The authors employ domain-specific augmentation techniques that preserve the statistical properties of time series while increasing training diversity. These augmentations help the model generalize across different time series domains without overfitting to specific patterns.

**Inference Optimization**: The paper introduces inference-time strategies that leverage the model's efficiency. The linear complexity of the architecture enables fast inference even on long sequences, making Reverso practical for real-time forecasting applications.

**Zero-shot Transfer**: The training recipe is specifically designed to maximize zero-shot transfer performance, allowing the model to forecast on unseen time series domains without fine-tuning. This is achieved through careful dataset curation and training objectives that encourage learning of generalizable temporal patterns.

## Experimental Results

The empirical evaluation demonstrates that Reverso models achieve performance competitive with much larger transformer-based foundation models across diverse benchmarks. Key findings include:

- Models with only a few million parameters match or exceed the performance of models with hundreds of millions of parameters
- The efficiency gains translate to significant reductions in inference time and memory footprint
- Zero-shot forecasting performance remains strong across different domains, including finance, energy, and web traffic
- The performance-efficiency Pareto frontier is substantially improved, offering practitioners viable options for resource-constrained deployments

## Implications for Foundation Model Research

Reverso's success challenges the assumption that scaling is the primary path to better foundation models in time series. The results suggest that:

**Domain-Specific Architectures Matter**: While transformers have proven universal in language and vision, time series may benefit from architectures specifically designed for temporal data. The inductive biases provided by convolutions and linear RNNs appear well-suited to time series structure.

**Efficiency as a First-Class Objective**: Rather than treating efficiency as a post-hoc optimization problem, designing for efficiency from the start can lead to fundamentally different and better solutions. Reverso demonstrates that the performance-efficiency trade-off is not fixed but can be shifted through careful architectural and algorithmic choices.

**Practical Deployment Considerations**: The orders-of-magnitude reduction in model size makes foundation models accessible for edge deployment, real-time applications, and resource-constrained environments where large transformers are impractical.

## Takeaways

1. Large transformers are not necessary for effective time series foundation models—hybrid architectures with convolutions and linear RNNs can achieve comparable performance at a fraction of the size
2. Reverso models are 100x smaller than transformer-based alternatives while maintaining competitive zero-shot forecasting accuracy
3. Domain-specific architectural choices that respect the structure of time series data can outperform generic scaling approaches
4. The combination of architectural efficiency, targeted data augmentation, and optimized inference strategies is key to pushing the performance-efficiency Pareto frontier
5. Efficient foundation models enable new deployment scenarios including edge devices, real-time systems, and cost-sensitive applications
:::

:::zh
**论文**: [2602.17634](https://arxiv.org/abs/2602.17634)
**作者**: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim
**分类**: cs.LG, cs.AI

## 摘要

本文介绍了Reverso,这是一个专为零样本预测设计的高效时间序列基础模型系列,挑战了更大模型才能获得更好性能的普遍假设。虽然近期的时间序列基础模型遵循了在语言和视觉领域成功的扩展范式,导致模型参数达到数亿级别,但Reverso证明了结合长卷积和线性RNN层(特别是DeltaNet)的小型混合架构可以在模型规模小几个数量级的情况下实现相当的性能。作者提出了一个综合方案,包括架构选择、数据增强策略和推理技术,共同显著推进了性能-效率的帕累托前沿。

## 主要贡献

- 引入了交替使用长卷积层和DeltaNet(线性RNN)层的混合架构,以比基于Transformer的模型少100倍的参数实现了竞争性能
- 证明了大规模Transformer对于有效的时间序列基础模型并非必需
- 开发了专门针对高效零样本时间序列预测的数据增强和推理策略
- 通过全面的实证验证表明Reverso模型显著提升了时间序列预测中的性能-效率权衡

## 架构设计

Reverso的核心创新在于其混合架构,这种架构偏离了Transformer主导的范式。Reverso没有依赖于随序列长度二次方扩展的注意力机制,而是采用了以下组合:

**长卷积层**: 这些层通过具有扩展感受野的卷积操作高效捕获局部时间模式。与标准卷积不同,长卷积可以在没有注意力机制计算开销的情况下对扩展时间范围内的依赖关系进行建模。

**DeltaNet层**: DeltaNet代表一类线性RNN,在保持传统RNN建模能力的同时提供线性时间复杂度。通过将DeltaNet层与卷积层交替使用,Reverso在局部模式识别和长程依赖建模之间实现了平衡。

这种架构选择的动机是观察到时间序列数据表现出与自然语言或图像不同的特征。时间序列中的时间结构和周期性可以通过卷积和线性递归有效捕获,而不需要自注意力的完整表达能力(和计算成本)。

## 训练和推理策略

除了架构之外,论文还描述了几个增强Reverso性能的关键策略:

**数据增强**: 作者采用了特定领域的增强技术,在增加训练多样性的同时保留时间序列的统计特性。这些增强帮助模型在不同时间序列领域中泛化,而不会过拟合特定模式。

**推理优化**: 论文引入了利用模型效率的推理时策略。架构的线性复杂度使得即使在长序列上也能实现快速推理,使Reverso适用于实时预测应用。

**零样本迁移**: 训练方案专门设计用于最大化零样本迁移性能,允许模型在未见过的时间序列领域上进行预测而无需微调。这是通过仔细的数据集策划和鼓励学习可泛化时间模式的训练目标实现的。

## 实验结果

实证评估表明,Reverso模型在各种基准测试中实现了与更大的基于Transformer的基础模型相当的性能。主要发现包括:

- 仅有几百万参数的模型就能匹配或超越具有数亿参数的模型的性能
- 效率提升转化为推理时间和内存占用的显著减少
- 零样本预测性能在不同领域(包括金融、能源和网络流量)中保持强劲
- 性能-效率帕累托前沿得到实质性改善,为从业者提供了资源受限部署的可行选择

## 对基础模型研究的启示

Reverso的成功挑战了扩展是时间序列中更好基础模型的主要路径这一假设。结果表明:

**领域特定架构很重要**: 虽然Transformer在语言和视觉领域已被证明是通用的,但时间序列可能受益于专门为时间数据设计的架构。卷积和线性RNN提供的归纳偏置似乎非常适合时间序列结构。

**效率作为一等目标**: 与其将效率视为事后优化问题,不如从一开始就为效率而设计,这可以导致根本不同且更好的解决方案。Reverso证明了性能-效率权衡不是固定的,而是可以通过仔细的架构和算法选择来改变。

**实际部署考虑**: 模型大小的数量级减少使得基础模型可以用于边缘部署、实时应用和资源受限环境,在这些场景中大型Transformer是不切实际的。

## 要点总结

1. 大型Transformer对于有效的时间序列基础模型并非必需——具有卷积和线性RNN的混合架构可以在模型大小仅为其一小部分的情况下实现相当的性能
2. Reverso模型比基于Transformer的替代方案小100倍,同时保持竞争性的零样本预测准确性
3. 尊重时间序列数据结构的领域特定架构选择可以优于通用扩展方法
4. 架构效率、针对性数据增强和优化推理策略的结合是推进性能-效率帕累托前沿的关键
5. 高效的基础模型支持新的部署场景,包括边缘设备、实时系统和成本敏感型应用
:::
