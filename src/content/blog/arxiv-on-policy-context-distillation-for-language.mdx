---
title:
  en: "On-Policy Context Distillation for Language Models"
  zh: "语言模型的在线策略上下文蒸馏"
description:
  en: "A novel framework that enables language models to internalize in-context knowledge through on-policy training, achieving superior performance in experiential learning and system prompt distillation."
  zh: "一种新颖的框架,通过在线策略训练使语言模型内化上下文知识,在经验学习和系统提示蒸馏中实现卓越性能。"
date: 2026-02-14
tags: ["arxiv", "ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.12275](https://arxiv.org/abs/2602.12275)
**Authors**: Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei
**Categories**: cs.CL

## Abstract

This paper introduces On-Policy Context Distillation (OPCD), a framework that enables language models to internalize contextual knowledge into their parameters through on-policy learning. Unlike traditional context distillation methods that rely on off-policy data, OPCD trains student models on their own generated trajectories while minimizing reverse KL divergence against a context-conditioned teacher model. The framework demonstrates effectiveness across two key applications: experiential knowledge distillation, where models learn from their historical solution traces, and system prompt distillation, where models internalize behaviors from optimized prompts. Experimental results across mathematical reasoning, text-based games, and domain-specific tasks show that OPCD consistently outperforms baseline methods while maintaining strong out-of-distribution generalization.

## Key Contributions

- Introduction of OPCD, a principled framework that bridges on-policy distillation with context distillation using reverse KL divergence
- Demonstration of experiential knowledge distillation, enabling models to consolidate transferable knowledge from their own solution trajectories
- System prompt distillation capability that internalizes beneficial behaviors from optimized prompts into model parameters
- Comprehensive evaluation across diverse domains showing consistent improvements over baseline methods
- Evidence of effective cross-size distillation, where smaller models successfully internalize knowledge from larger teachers

## Methodology

The OPCD framework operates by training a student model $\pi_\theta$ to match the behavior of a context-conditioned teacher model $\pi_{\text{teacher}}(\cdot|c)$, where $c$ represents the context (e.g., few-shot examples or system prompts). The key innovation lies in using on-policy sampling: the student generates its own trajectories, which are then used as training data.

The training objective minimizes the reverse KL divergence:

$$\mathcal{L}(\theta) = \mathbb{E}_{x \sim \pi_\theta} [D_{\text{KL}}(\pi_{\text{teacher}}(\cdot|x,c) \| \pi_\theta(\cdot|x))]$$

This formulation encourages the student to cover all modes of the teacher's distribution while avoiding overconfident predictions on low-probability regions. The on-policy nature ensures that the student learns from realistic scenarios it will encounter during deployment, rather than from potentially off-distribution teacher-generated data.

For experiential knowledge distillation, the context $c$ consists of successful solution traces from the model's own problem-solving history. For system prompt distillation, $c$ represents carefully crafted prompts that encode desired behaviors. The framework iteratively refines the student model by generating new trajectories and updating parameters based on the teacher's guidance.

## Experimental Results

The authors evaluate OPCD across three domains: mathematical reasoning (GSM8K, MATH), text-based games (ALFWorld, WebShop), and domain-specific tasks. In mathematical reasoning, OPCD achieves substantial improvements over baseline context distillation methods, with gains of 3-5% on GSM8K and 2-4% on MATH datasets. The experiential knowledge distillation variant shows particularly strong results, suggesting that models can effectively learn from their own successful problem-solving strategies.

In text-based game environments, OPCD demonstrates superior sample efficiency and final performance. On ALFWorld, the method achieves 15% higher success rates compared to standard fine-tuning approaches, while maintaining better generalization to unseen game scenarios. The system prompt distillation experiments reveal that OPCD can successfully internalize complex behavioral patterns, such as step-by-step reasoning or specific formatting preferences, without requiring explicit prompts at inference time.

Cross-size distillation experiments show promising results, with 7B parameter students successfully learning from 13B and 70B teachers. The performance gap between student and teacher narrows significantly compared to traditional distillation methods, suggesting that on-policy training helps smaller models better capture the reasoning patterns of larger models.

## Implications and Future Directions

OPCD represents a significant advancement in making language models more efficient and capable. By internalizing contextual knowledge, models can achieve strong performance without the computational overhead of processing lengthy contexts at inference time. This has practical implications for deployment scenarios where latency and cost are critical factors.

The experiential learning aspect opens new possibilities for continual learning systems where models progressively improve by reflecting on their own experiences. Unlike traditional supervised learning that requires external labeled data, experiential knowledge distillation enables self-improvement through practice and reflection.

The framework also addresses a key challenge in prompt engineering: while carefully crafted prompts can dramatically improve model behavior, they add computational cost and complexity. System prompt distillation offers a path to "bake in" these improvements, making them available without runtime overhead.

Future research directions include exploring OPCD for multi-task learning scenarios, investigating optimal strategies for selecting which experiences to distill, and extending the framework to handle more complex forms of contextual knowledge such as retrieved documents or tool usage patterns.

## Takeaways

1. On-policy training is crucial for effective context distillation, as it ensures the student learns from realistic scenarios rather than potentially off-distribution teacher data
2. Language models can successfully learn from their own experiences, consolidating successful problem-solving strategies into their parameters through experiential knowledge distillation
3. System prompt distillation enables models to internalize beneficial behaviors without requiring explicit prompts at inference time, reducing computational overhead
4. OPCD maintains strong out-of-distribution generalization while improving in-distribution performance, avoiding the overfitting issues common in traditional distillation
5. Cross-size distillation with OPCD allows smaller models to effectively learn from larger teachers, enabling more efficient deployment without significant performance degradation
:::

:::zh
**论文**: [2602.12275](https://arxiv.org/abs/2602.12275)
**作者**: Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei
**分类**: cs.CL

## 摘要

本文提出了在线策略上下文蒸馏(OPCD)框架,使语言模型能够通过在线学习将上下文知识内化到参数中。与依赖离线策略数据的传统上下文蒸馏方法不同,OPCD在学生模型自己生成的轨迹上进行训练,同时最小化与上下文条件教师模型之间的反向KL散度。该框架在两个关键应用中展现了有效性:经验知识蒸馏,模型从历史求解轨迹中学习;系统提示蒸馏,模型内化优化提示中编码的行为。在数学推理、文本游戏和特定领域任务的实验结果表明,OPCD持续优于基线方法,同时保持强大的分布外泛化能力。

## 主要贡献

- 提出OPCD框架,使用反向KL散度将在线策略蒸馏与上下文蒸馏相结合
- 展示经验知识蒸馏能力,使模型能够从自身求解轨迹中整合可迁移知识
- 实现系统提示蒸馏,将优化提示中的有益行为内化到模型参数中
- 在多个领域进行全面评估,显示相比基线方法的持续改进
- 证明跨尺寸蒸馏的有效性,小型模型可成功内化大型教师的知识

## 方法论

OPCD框架通过训练学生模型 $\pi_\theta$ 来匹配上下文条件教师模型 $\pi_{\text{teacher}}(\cdot|c)$ 的行为,其中 $c$ 表示上下文(例如少样本示例或系统提示)。关键创新在于使用在线策略采样:学生生成自己的轨迹,然后将其用作训练数据。

训练目标最小化反向KL散度:

$$\mathcal{L}(\theta) = \mathbb{E}_{x \sim \pi_\theta} [D_{\text{KL}}(\pi_{\text{teacher}}(\cdot|x,c) \| \pi_\theta(\cdot|x))]$$

这种形式鼓励学生覆盖教师分布的所有模式,同时避免在低概率区域产生过度自信的预测。在线策略特性确保学生从部署时将遇到的真实场景中学习,而不是从可能偏离分布的教师生成数据中学习。

对于经验知识蒸馏,上下文 $c$ 由模型自身问题求解历史中的成功解决轨迹组成。对于系统提示蒸馏,$c$ 表示精心设计的编码期望行为的提示。框架通过生成新轨迹并基于教师指导更新参数来迭代优化学生模型。

## 实验结果

作者在三个领域评估OPCD:数学推理(GSM8K、MATH)、文本游戏(ALFWorld、WebShop)和特定领域任务。在数学推理中,OPCD相比基线上下文蒸馏方法取得显著改进,在GSM8K上提升3-5%,在MATH数据集上提升2-4%。经验知识蒸馏变体显示出特别强的结果,表明模型可以有效地从自己成功的问题求解策略中学习。

在文本游戏环境中,OPCD展示了优越的样本效率和最终性能。在ALFWorld上,该方法相比标准微调方法实现了15%更高的成功率,同时对未见游戏场景保持更好的泛化能力。系统提示蒸馏实验表明,OPCD可以成功内化复杂的行为模式,如逐步推理或特定格式偏好,而无需在推理时使用显式提示。

跨尺寸蒸馏实验显示出有希望的结果,7B参数的学生成功从13B和70B教师中学习。相比传统蒸馏方法,学生与教师之间的性能差距显著缩小,表明在线策略训练帮助小型模型更好地捕获大型模型的推理模式。

## 影响与未来方向

OPCD代表了使语言模型更高效和更强大的重要进展。通过内化上下文知识,模型可以在推理时无需处理冗长上下文的计算开销即可实现强大性能。这对延迟和成本是关键因素的部署场景具有实际意义。

经验学习方面为持续学习系统开辟了新可能性,模型通过反思自己的经验逐步改进。与需要外部标注数据的传统监督学习不同,经验知识蒸馏通过实践和反思实现自我改进。

该框架还解决了提示工程中的关键挑战:虽然精心设计的提示可以显著改善模型行为,但它们增加了计算成本和复杂性。系统提示蒸馏提供了一条"烘焙"这些改进的途径,使其无需运行时开销即可使用。

未来研究方向包括探索OPCD在多任务学习场景中的应用,研究选择哪些经验进行蒸馏的最优策略,以及将框架扩展到处理更复杂形式的上下文知识,如检索文档或工具使用模式。

## 要点总结

1. 在线策略训练对有效的上下文蒸馏至关重要,因为它确保学生从真实场景而非可能偏离分布的教师数据中学习
2. 语言模型可以成功地从自己的经验中学习,通过经验知识蒸馏将成功的问题求解策略整合到参数中
3. 系统提示蒸馏使模型能够内化有益行为而无需在推理时使用显式提示,减少计算开销
4. OPCD在提高分布内性能的同时保持强大的分布外泛化能力,避免传统蒸馏中常见的过拟合问题
5. 使用OPCD的跨尺寸蒸馏允许小型模型有效地从大型教师中学习,实现更高效的部署而不会显著降低性能
:::
