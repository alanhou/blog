---
title:
  en: "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation"
  zh: "SeeThrough3D: 文本到图像生成中的遮挡感知3D控制"
description:
  en: "A novel approach for 3D layout-conditioned image generation that explicitly models object occlusions through translucent 3D box representations, enabling depth-consistent synthesis with precise camera control."
  zh: "一种新颖的3D布局条件图像生成方法,通过半透明3D盒子表示显式建模物体遮挡,实现深度一致的合成和精确的相机控制。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.cv", "cs.ai"]
image: "/arxiv-visuals/seethrough3d-occlusion-aware-3d-control-in/HeroScene.png"
---

![Hero diagram](/arxiv-visuals/seethrough3d-occlusion-aware-3d-control-in/HeroScene.png)



:::en
**Paper**: [2602.23359](https://arxiv.org/abs/2602.23359)
**Authors**: Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu
**Categories**: cs.CV, cs.AI

## Abstract

SeeThrough3D addresses a critical gap in 3D layout-conditioned image generation: the ability to reason about object occlusions. While existing text-to-image models can generate scenes following spatial layouts, they struggle with modeling precise inter-object occlusions, leading to depth inconsistencies and unrealistic spatial relationships. This work introduces an occlusion-aware 3D scene representation (OSCR) that depicts objects as translucent 3D bounding boxes in a virtual environment. The transparency encoding allows the model to understand which parts of objects should be hidden, while the rendered viewpoint provides explicit camera control. By conditioning a pretrained flow-based diffusion model with visual tokens derived from this representation and applying masked self-attention for object-text binding, SeeThrough3D achieves accurate multi-object generation without attribute mixing, generalizing effectively to unseen categories.

## Key Contributions

- Introduction of occlusion-aware 3D scene representation (OSCR) using translucent 3D bounding boxes to explicitly encode object visibility and occlusion relationships
- A conditioning mechanism that integrates OSCR-derived visual tokens into pretrained flow-based text-to-image models for occlusion-aware generation
- Masked self-attention architecture that binds each object's bounding box to its textual description, preventing attribute leakage across objects
- Construction of a synthetic training dataset featuring diverse multi-object scenes with strong inter-object occlusions
- Demonstrated generalization to unseen object categories with precise 3D layout control and realistic depth-consistent occlusions

## Technical Methodology

The core innovation of SeeThrough3D lies in its occlusion-aware scene representation. Traditional 3D layout conditioning methods use opaque bounding boxes or depth maps, which fail to convey information about occluded regions. OSCR renders objects as semi-transparent boxes from a specified camera viewpoint, where the alpha channel encodes visibility—fully transparent regions indicate completely occluded areas, while opaque regions represent visible surfaces.

The architecture builds upon a pretrained flow-based diffusion model (likely Stable Diffusion 3 or similar). The OSCR representation is processed through a visual encoder to extract spatial tokens that capture both geometric layout and occlusion patterns. These tokens are injected into the model's cross-attention layers alongside text embeddings.

A critical component is the masked self-attention mechanism. For each object $i$ with bounding box $B_i$ and text description $T_i$, the attention is masked such that spatial regions within $B_i$ can only attend to tokens from $T_i$. This prevents the common failure mode where attributes from multiple objects blend together (e.g., "red car" and "blue truck" producing a purple vehicle). Mathematically, the attention mask $M_{ij}$ is defined such that:

$$M_{ij} = \begin{cases} 0 & \text{if pixel } j \in B_i \text{ and token from } T_i \\ -\infty & \text{otherwise} \end{cases}$$

The training dataset is synthetically generated using 3D rendering engines, creating scenes with controlled occlusion patterns. This ensures the model learns robust occlusion reasoning across various spatial configurations, object scales, and camera angles.

## Results and Capabilities

SeeThrough3D demonstrates significant improvements over baseline methods in several key areas:

**Occlusion Accuracy**: The model correctly renders partially occluded objects with appropriate depth ordering. Objects behind others are properly hidden in occluded regions while maintaining geometric consistency in visible areas.

**Camera Control**: The explicit viewpoint rendering in OSCR provides precise camera control. Users can specify exact camera positions and orientations, and the generated images accurately reflect these parameters with correct perspective and foreshortening.

**Multi-Object Generation**: The masked self-attention mechanism successfully prevents attribute mixing. When generating scenes with multiple objects having distinct properties (colors, textures, shapes), each object maintains its specified attributes without cross-contamination.

**Generalization**: Despite training on synthetic data, the model generalizes to real-world object categories not seen during training. This suggests the occlusion reasoning learned from geometric relationships transfers across semantic categories.

**Scale and Depth Consistency**: Objects maintain physically plausible sizes relative to their depth in the scene. Distant objects appear appropriately smaller, and the depth relationships remain consistent across the generated image.

## Implications and Future Directions

SeeThrough3D represents a significant step toward controllable 3D-aware image synthesis. The explicit modeling of occlusions addresses a fundamental limitation in spatial conditioning for generative models. This has practical applications in:

- **Virtual scene design**: Designers can precisely control object placement and camera angles for concept art and visualization
- **Data augmentation**: Generating training data with controlled occlusions for object detection and segmentation tasks
- **AR/VR content creation**: Producing realistic scenes with proper depth relationships for immersive experiences

The approach opens several research directions. First, extending beyond bounding boxes to more detailed 3D representations (meshes, point clouds) could enable finer geometric control. Second, incorporating physical constraints (gravity, support relationships) could improve scene plausibility. Third, interactive editing capabilities where users can manipulate the 3D layout and see real-time updates would enhance usability.

The reliance on synthetic training data, while enabling controlled learning, may limit photorealism compared to models trained on real images. Hybrid approaches combining synthetic occlusion data with real image datasets could balance geometric accuracy with visual fidelity.

## Takeaways

1. Occlusion reasoning is essential for depth-consistent 3D layout-conditioned generation, yet has been largely overlooked in existing methods
2. Translucent 3D box representations effectively encode visibility information, enabling models to learn which object regions should be hidden
3. Masked self-attention prevents attribute mixing in multi-object scenes by binding spatial regions to their corresponding text descriptions
4. Explicit camera viewpoint rendering in the conditioning signal provides precise control over perspective and viewing angles
5. Synthetic training data with controlled occlusions enables robust learning of geometric relationships that generalize to unseen categories
6. The approach bridges the gap between 2D image generation and 3D scene understanding, advancing controllable synthesis capabilities
:::

:::zh
**论文**: [2602.23359](https://arxiv.org/abs/2602.23359)
**作者**: Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu
**分类**: cs.CV, cs.AI

## 摘要

SeeThrough3D解决了3D布局条件图像生成中的一个关键缺陷:对物体遮挡的推理能力。虽然现有的文本到图像模型可以生成遵循空间布局的场景,但它们在建模精确的物体间遮挡关系时表现不佳,导致深度不一致和不真实的空间关系。本研究引入了遮挡感知3D场景表示(OSCR),将物体描绘为虚拟环境中的半透明3D边界框。透明度编码使模型能够理解物体的哪些部分应该被隐藏,而渲染的视点则提供显式的相机控制。通过使用从该表示派生的视觉标记来条件化预训练的流式扩散模型,并应用掩码自注意力机制进行物体-文本绑定,SeeThrough3D实现了准确的多物体生成而不会产生属性混淆,并能有效泛化到未见过的类别。

## 主要贡献

- 引入遮挡感知3D场景表示(OSCR),使用半透明3D边界框显式编码物体可见性和遮挡关系
- 提出一种条件化机制,将OSCR派生的视觉标记集成到预训练的流式文本到图像模型中,实现遮挡感知生成
- 设计掩码自注意力架构,将每个物体的边界框绑定到其文本描述,防止物体间的属性泄漏
- 构建包含多样化多物体场景和强物体间遮挡的合成训练数据集
- 展示了对未见物体类别的有效泛化能力,实现精确的3D布局控制和真实的深度一致遮挡

## 技术方法

SeeThrough3D的核心创新在于其遮挡感知场景表示。传统的3D布局条件化方法使用不透明边界框或深度图,无法传达被遮挡区域的信息。OSCR从指定的相机视点渲染物体为半透明盒子,其中alpha通道编码可见性——完全透明的区域表示完全被遮挡的区域,而不透明区域表示可见表面。

该架构建立在预训练的流式扩散模型之上(可能是Stable Diffusion 3或类似模型)。OSCR表示通过视觉编码器处理,提取捕获几何布局和遮挡模式的空间标记。这些标记与文本嵌入一起注入到模型的交叉注意力层中。

关键组件是掩码自注意力机制。对于每个具有边界框$B_i$和文本描述$T_i$的物体$i$,注意力被掩码化,使得$B_i$内的空间区域只能关注来自$T_i$的标记。这防止了常见的失败模式,即多个物体的属性混合在一起(例如,"红色汽车"和"蓝色卡车"产生紫色车辆)。数学上,注意力掩码$M_{ij}$定义为:

$$M_{ij} = \begin{cases} 0 & \text{如果像素 } j \in B_i \text{ 且标记来自 } T_i \\ -\infty & \text{否则} \end{cases}$$

训练数据集使用3D渲染引擎合成生成,创建具有受控遮挡模式的场景。这确保模型在各种空间配置、物体尺度和相机角度下学习鲁棒的遮挡推理。

## 实验结果与能力

SeeThrough3D在几个关键领域相比基线方法展示了显著改进:

**遮挡准确性**: 模型正确渲染部分被遮挡的物体,具有适当的深度顺序。位于其他物体后面的物体在被遮挡区域被正确隐藏,同时在可见区域保持几何一致性。

**相机控制**: OSCR中的显式视点渲染提供精确的相机控制。用户可以指定确切的相机位置和方向,生成的图像准确反映这些参数,具有正确的透视和前缩效果。

**多物体生成**: 掩码自注意力机制成功防止了属性混淆。在生成具有多个不同属性(颜色、纹理、形状)物体的场景时,每个物体保持其指定的属性而不会交叉污染。

**泛化能力**: 尽管在合成数据上训练,模型能够泛化到训练期间未见过的真实世界物体类别。这表明从几何关系中学习的遮挡推理能够跨语义类别迁移。

**尺度和深度一致性**: 物体相对于其在场景中的深度保持物理上合理的大小。远处的物体显得适当地更小,深度关系在生成的图像中保持一致。

## 影响与未来方向

SeeThrough3D代表了向可控3D感知图像合成迈出的重要一步。对遮挡的显式建模解决了生成模型空间条件化中的一个基本局限。这在以下方面具有实际应用:

- **虚拟场景设计**: 设计师可以精确控制物体放置和相机角度,用于概念艺术和可视化
- **数据增强**: 为物体检测和分割任务生成具有受控遮挡的训练数据
- **AR/VR内容创作**: 为沉浸式体验制作具有适当深度关系的真实场景

该方法开启了几个研究方向。首先,从边界框扩展到更详细的3D表示(网格、点云)可以实现更精细的几何控制。其次,结合物理约束(重力、支撑关系)可以提高场景的合理性。第三,交互式编辑能力,用户可以操纵3D布局并实时查看更新,将增强可用性。

对合成训练数据的依赖虽然能够实现受控学习,但与在真实图像上训练的模型相比可能限制照片真实感。结合合成遮挡数据和真实图像数据集的混合方法可以平衡几何准确性和视觉保真度。

## 要点总结

1. 遮挡推理对于深度一致的3D布局条件生成至关重要,但在现有方法中很大程度上被忽视
2. 半透明3D盒子表示有效编码可见性信息,使模型能够学习哪些物体区域应该被隐藏
3. 掩码自注意力通过将空间区域绑定到相应的文本描述,防止多物体场景中的属性混淆
4. 条件信号中的显式相机视点渲染提供对透视和视角的精确控制
5. 具有受控遮挡的合成训练数据使模型能够鲁棒学习几何关系,并泛化到未见类别
6. 该方法弥合了2D图像生成和3D场景理解之间的差距,推进了可控合成能力
:::
