---
title:
  en: "ROCKET: Training-Free Model Compression via Knapsack Optimization and Sparse Factorization"
  zh: "ROCKET: 基于背包优化和稀疏分解的免训练模型压缩方法"
description:
  en: "A novel training-free compression method that uses knapsack optimization for layer-wise budget allocation and single-step sparse factorization, achieving 90%+ performance retention at 30% compression without fine-tuning."
  zh: "一种创新的免训练压缩方法,通过背包优化进行层级预算分配和单步稀疏分解,在30%压缩率下无需微调即可保持90%以上性能。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.lg", "cs.ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.11008](https://arxiv.org/abs/2602.11008)
**Authors**: Ammar Ali, Baher Mohammad, Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Stamatios Lefkimmiatis
**Categories**: cs.LG, cs.AI, cs.CL

## Abstract

ROCKET introduces a training-free model compression framework that achieves state-of-the-art results by combining two innovative techniques. The method formulates layer-wise compression as a multi-choice knapsack problem to optimally allocate compression budgets across layers, minimizing reconstruction error while meeting target model sizes. It then applies a single-step sparse matrix factorization inspired by dictionary learning, using only a small calibration dataset to sparsify weights based on activation-weight sensitivity and updating the dictionary via closed-form least squares. This approach bypasses iterative optimization and backpropagation entirely. ROCKET demonstrates superior performance across various architectures at 20-50% compression rates, retaining over 90% of original performance at 30% compression without fine-tuning, and achieving near-original performance when combined with lightweight fine-tuning.

## Key Contributions

- **Knapsack-based compression allocation**: Formulates optimal layer-wise compression as a multi-choice knapsack problem, enabling principled budget distribution that minimizes global reconstruction error
- **Single-step sparse factorization**: Introduces a dictionary learning-inspired approach that sparsifies weights based on activation sensitivity and updates dictionaries in closed form, eliminating iterative optimization
- **Training-free operation**: Achieves compression using only a small calibration set without requiring backpropagation or gradient computation
- **Strong empirical results**: Retains 90%+ performance at 30% compression without fine-tuning, and achieves near-original performance with minimal fine-tuning (e.g., compressing Qwen3-14B to 8B parameters)

## Methodology

ROCKET operates in two main phases: compression budget allocation and sparse matrix factorization.

**Phase 1: Knapsack Optimization for Budget Allocation**

The method treats each layer as an item in a multi-choice knapsack problem, where each layer can be compressed at different levels (e.g., no compression, 20%, 40%, 60%). For a target global compression budget $B$, ROCKET solves:

$$\min_{\{c_i\}} \sum_{i=1}^{L} E_i(c_i) \quad \text{subject to} \quad \sum_{i=1}^{L} S_i(c_i) \leq B$$

where $E_i(c_i)$ is the reconstruction error for layer $i$ at compression level $c_i$, and $S_i(c_i)$ is the resulting size. This formulation allows ROCKET to allocate more compression to layers that tolerate it better while preserving critical layers.

**Phase 2: Sparse Matrix Factorization**

For each layer's weight matrix $W \in \mathbb{R}^{m \times n}$, ROCKET decomposes it as $W \approx DC$ where $D \in \mathbb{R}^{m \times k}$ is a dense dictionary and $C \in \mathbb{R}^{k \times n}$ is a sparse coefficient matrix. The process:

1. **Sensitivity-based sparsification**: Using calibration data, compute activation-weight sensitivity scores to identify important coefficients
2. **Sparse coding**: Retain only top-$s$ coefficients per column in $C$ based on sensitivity
3. **Dictionary update**: Solve $D^* = \arg\min_D \|W - DC\|_F^2$ in closed form via least squares: $D = WC^T(CC^T)^{-1}$

This single-step approach avoids the alternating optimization typical in dictionary learning, making it extremely fast.

## Experimental Results

ROCKET demonstrates consistent improvements across multiple model families and compression rates:

**Performance Retention**: At 30% compression without fine-tuning, ROCKET retains over 90% of the original model's performance across benchmarks. This significantly outperforms baseline methods like SVD-based factorization, structured pruning, and dynamic compression techniques.

**Compression Range**: The method shows strong results across 20-50% compression rates, with graceful degradation as compression increases. At 20% compression, performance loss is often negligible (< 2%), while at 50% compression, the method still maintains 75-80% of original performance.

**Fine-tuning Synergy**: When combined with lightweight fine-tuning, ROCKET achieves remarkable recovery. The paper demonstrates compressing Qwen3-14B (14 billion parameters) to an 8B-parameter model, then fine-tuning with just 30 million tokens to achieve performance nearly matching the original Qwen3-8B model. This suggests ROCKET creates a strong initialization that requires minimal adaptation.

**Architecture Generalization**: Results span transformer-based language models of various sizes, demonstrating the method's applicability across different architectures and scales.

## Technical Insights

Several design choices contribute to ROCKET's effectiveness:

**Global vs. Local Optimization**: By formulating compression as a global knapsack problem rather than applying uniform compression per layer, ROCKET exploits the heterogeneous sensitivity of different layers. Early layers and attention mechanisms often require less compression than later feed-forward layers.

**Activation-aware Sparsification**: Unlike magnitude-based pruning, ROCKET's sensitivity metric considers both weight magnitudes and their interaction with typical activations from the calibration set. This produces more informed sparsity patterns that preserve functional behavior.

**Closed-form Updates**: The single-step dictionary update via least squares is computationally efficient and avoids local minima issues that plague iterative alternating optimization. This also makes the method deterministic and reproducible.

**Calibration Efficiency**: ROCKET requires only a small calibration dataset (typically a few hundred samples) to compute sensitivity scores, making it practical for scenarios where full training data is unavailable or expensive to process.

## Takeaways

1. Training-free compression can achieve competitive results with trained methods when using principled optimization frameworks like knapsack allocation
2. Global budget optimization across layers significantly outperforms uniform per-layer compression strategies
3. Activation-aware sensitivity metrics provide better sparsification than magnitude-based approaches alone
4. Single-step closed-form factorization can match or exceed iterative optimization methods while being orders of magnitude faster
5. The combination of aggressive compression with minimal fine-tuning (30M tokens) offers a practical path to model size reduction without extensive retraining
6. ROCKET's training-free nature makes it particularly valuable for compressing proprietary or large-scale models where full retraining is prohibitive
:::

:::zh
**论文**: [2602.11008](https://arxiv.org/abs/2602.11008)
**作者**: Ammar Ali, Baher Mohammad, Denis Makhov, Dmitriy Shopkhoev, Magauiya Zhussip, Stamatios Lefkimmiatis
**分类**: cs.LG, cs.AI, cs.CL

## 摘要

ROCKET提出了一种免训练的模型压缩框架,通过结合两项创新技术实现了最先进的压缩效果。该方法将层级压缩建模为多选背包问题,在满足目标模型大小的同时优化各层压缩预算分配以最小化重构误差。随后应用受字典学习启发的单步稀疏矩阵分解,仅使用小规模校准数据集根据激活-权重敏感度稀疏化权重,并通过闭式最小二乘更新字典。该方法完全绕过了迭代优化和反向传播。ROCKET在20-50%压缩率范围内展现出优异性能,在30%压缩率下无需微调即可保持原始性能的90%以上,结合轻量级微调时可达到接近原始模型的性能。

## 主要贡献

- **基于背包的压缩分配**: 将最优层级压缩建模为多选背包问题,实现最小化全局重构误差的原则性预算分配
- **单步稀疏分解**: 引入字典学习启发的方法,基于激活敏感度稀疏化权重并以闭式形式更新字典,消除迭代优化需求
- **免训练操作**: 仅使用小规模校准集实现压缩,无需反向传播或梯度计算
- **强大的实证结果**: 在30%压缩率下无需微调保持90%以上性能,通过最小微调实现接近原始性能(例如将Qwen3-14B压缩至8B参数)

## 方法论

ROCKET分为两个主要阶段:压缩预算分配和稀疏矩阵分解。

**阶段1: 背包优化的预算分配**

该方法将每一层视为多选背包问题中的物品,每层可以在不同级别压缩(如不压缩、20%、40%、60%)。对于目标全局压缩预算$B$,ROCKET求解:

$$\min_{\{c_i\}} \sum_{i=1}^{L} E_i(c_i) \quad \text{约束条件} \quad \sum_{i=1}^{L} S_i(c_i) \leq B$$

其中$E_i(c_i)$是层$i$在压缩级别$c_i$下的重构误差,$S_i(c_i)$是结果大小。这种建模允许ROCKET对容忍度更高的层分配更多压缩,同时保护关键层。

**阶段2: 稀疏矩阵分解**

对每层权重矩阵$W \in \mathbb{R}^{m \times n}$,ROCKET将其分解为$W \approx DC$,其中$D \in \mathbb{R}^{m \times k}$是密集字典,$C \in \mathbb{R}^{k \times n}$是稀疏系数矩阵。处理流程:

1. **基于敏感度的稀疏化**: 使用校准数据计算激活-权重敏感度分数以识别重要系数
2. **稀疏编码**: 根据敏感度在$C$的每列中仅保留前$s$个系数
3. **字典更新**: 通过闭式最小二乘求解$D^* = \arg\min_D \|W - DC\|_F^2$: $D = WC^T(CC^T)^{-1}$

这种单步方法避免了字典学习中典型的交替优化,使其极为快速。

## 实验结果

ROCKET在多个模型家族和压缩率下展现出一致的性能提升:

**性能保持**: 在30%压缩率下无需微调,ROCKET在各项基准测试中保持原始模型90%以上的性能。这显著优于基于SVD的分解、结构化剪枝和动态压缩等基线方法。

**压缩范围**: 该方法在20-50%压缩率范围内表现强劲,随压缩增加呈现平滑退化。在20%压缩时,性能损失往往可忽略不计(<2%),而在50%压缩时仍能维持原始性能的75-80%。

**微调协同**: 结合轻量级微调时,ROCKET实现了显著的性能恢复。论文展示了将Qwen3-14B(140亿参数)压缩至8B参数模型,然后仅用3000万token微调即可达到接近原始Qwen3-8B模型的性能。这表明ROCKET创建了只需最小适应的强初始化。

**架构泛化**: 结果涵盖不同规模的基于Transformer的语言模型,证明了该方法在不同架构和规模上的适用性。

## 技术洞察

几个设计选择促成了ROCKET的有效性:

**全局vs局部优化**: 通过将压缩建模为全局背包问题而非对每层应用统一压缩,ROCKET利用了不同层的异质敏感度。早期层和注意力机制通常比后期前馈层需要更少压缩。

**激活感知稀疏化**: 与基于幅度的剪枝不同,ROCKET的敏感度指标同时考虑权重幅度及其与校准集典型激活的交互。这产生了更能保持功能行为的稀疏模式。

**闭式更新**: 通过最小二乘的单步字典更新计算高效,避免了困扰迭代交替优化的局部最小值问题。这也使方法具有确定性和可重现性。

**校准效率**: ROCKET仅需小规模校准数据集(通常几百个样本)来计算敏感度分数,使其适用于完整训练数据不可用或处理成本高昂的场景。

## 要点总结

1. 使用背包分配等原则性优化框架时,免训练压缩可以达到与训练方法相当的效果
2. 跨层的全局预算优化显著优于统一的逐层压缩策略
3. 激活感知的敏感度指标比单纯基于幅度的方法提供更好的稀疏化效果
4. 单步闭式分解可以匹配或超越迭代优化方法,同时速度快几个数量级
5. 激进压缩与最小微调(3000万token)的组合为模型尺寸缩减提供了无需大规模重训练的实用路径
6. ROCKET的免训练特性使其特别适用于压缩专有或大规模模型,在这些场景下完整重训练成本过高
:::
