---
title:
  en: "Hung-yi Lee ML 2025 Lecture 9: Why Do You Care So Much About Benchmarks?"
  zh: "李宏毅机器学习2025 第九讲：你这么认这个评分系统干什么啊？"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on LLM evaluation: benchmark limitations, Chatbot Arena, ARC-AGI, and Goodhart's Law"
  zh: "台大李宏毅教授课程笔记：LLM评估，基准测试的局限性，Chatbot Arena，ARC-AGI，以及古德哈特定律"
date: 2025-05-03
tags: ["ai", "llm", "evaluation", "benchmarks", "chatbot-arena", "arc-agi", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/s266BzGNKKc/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 9
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="s266BzGNKKc" title="李宏毅机器学习2025 第九讲：LLM评估" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## The Problem with Benchmarks

How do we know if a model has good reasoning ability? The common approach: test it on math problems like GSM8K. But this raises serious questions:

**Can models just memorize answers?**
- Models train on massive internet data
- GSM8K problems might be in training data
- A model might "reason" by recalling memorized answers

## Goodhart's Law

> "When a measure becomes a target, it ceases to be a good measure."

This applies directly to AI benchmarks:
- Once a benchmark becomes popular, models get optimized for it
- The benchmark stops measuring what it was designed to measure
- Most benchmarks get "solved" within 2-3 years of release

## Testing Benchmark Robustness

Researchers tested whether models truly understand problems or just memorize:

**Experiment**: Modify GSM8K problems without changing difficulty
- Change names (Sophia → other names)
- Change numbers
- Reorder sentences
- Add irrelevant sentences

**Results**: Model accuracy dropped significantly on modified problems, suggesting some memorization is happening.

## Chatbot Arena: Human Evaluation

An alternative approach: let humans judge which model is better.

**How it works:**
1. User asks a question
2. Two anonymous models respond
3. User picks the better response
4. Elo ratings calculated from win/loss records

**Advantages:**
- No fixed test set to memorize
- Real-world questions from real users
- Continuous evaluation

### But Chatbot Arena Has Problems Too

Research found that **style affects rankings more than substance**:

Factors that influence human preference (but shouldn't):
- Response length (longer often preferred)
- Emoji usage
- Bullet points and formatting
- Headline usage
- Positive/friendly tone

**Example**: Claude models are smart but "don't talk pretty" - they rank lower than their actual capability because they're less verbose and use fewer emojis.

When researchers controlled for style factors, rankings changed significantly!

## ARC-AGI: A Different Approach

ARC-AGI (Abstraction and Reasoning Corpus) tries to test genuine reasoning:

**Design principles:**
- Visual pattern recognition tasks
- No language-based shortcuts
- Problems can't be found on the internet
- Requires genuine abstraction ability

**Example tasks:**
- Given input/output pattern pairs, predict the output for a new input
- Patterns involve colors, shapes, transformations

### Why ARC-AGI is Hard to Game

1. **No fixed question bank**: New problems can always be created
2. **Visual format**: Can't be easily memorized as text
3. **Hidden test set**: Public examples differ from evaluation set
4. **Requires true abstraction**: Pattern recognition, not knowledge recall

### O3's Breakthrough

OpenAI's O3 model achieved near-human performance on ARC-AGI:
- First model to significantly crack this benchmark
- But required ~$1000 of compute per problem
- Shows reasoning is possible but expensive

## The Cobra Effect

Professor Lee shares the "Cobra Effect" story:

During British colonial India, there was a cobra problem. The government offered bounties for dead cobras. Result? People started breeding cobras for the bounty money, making the problem worse.

**Lesson for AI**: When you optimize for a metric, you might get unintended behaviors that game the metric rather than solve the underlying problem.

## Key Takeaways

1. **No perfect benchmark exists**: Every evaluation method has flaws

2. **Memorization vs. Reasoning**: Hard to distinguish when models train on internet-scale data

3. **Style vs. Substance**: Human evaluations are biased by presentation

4. **Goodhart's Law is real**: Popular benchmarks get gamed

5. **Multiple evaluations needed**: Don't rely on any single metric

## What Makes a Good Benchmark?

- Resistant to memorization
- Tests genuine capability, not surface patterns
- Hidden test sets
- Continuous renewal of questions
- Multiple evaluation dimensions
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 基准测试的问题

我们如何知道一个模型是否具有良好的推理能力？常见方法：用GSM8K等数学问题测试它。但这引发了严重的问题：

**模型能否只是记住答案？**
- 模型在海量互联网数据上训练
- GSM8K问题可能在训练数据中
- 模型可能通过回忆记忆的答案来"推理"

## 古德哈特定律

> "当一个指标成为目标时，它就不再是一个好的指标。"

这直接适用于AI基准测试：
- 一旦基准测试变得流行，模型就会针对它进行优化
- 基准测试不再衡量它设计要衡量的东西
- 大多数基准测试在发布后2-3年内就被"解决"

## 测试基准测试的稳健性

研究人员测试了模型是否真正理解问题还是只是记忆：

**实验**：在不改变难度的情况下修改GSM8K问题
- 更改名字（Sophia → 其他名字）
- 更改数字
- 重新排列句子
- 添加无关句子

**结果**：模型在修改后的问题上准确率显著下降，表明存在一些记忆。

## Chatbot Arena：人类评估

另一种方法：让人类判断哪个模型更好。

**工作原理：**
1. 用户提问
2. 两个匿名模型回答
3. 用户选择更好的回答
4. 根据胜负记录计算Elo评分

**优势：**
- 没有固定的测试集可以记忆
- 来自真实用户的真实问题
- 持续评估

### 但Chatbot Arena也有问题

研究发现**风格比实质更影响排名**：

影响人类偏好的因素（但不应该）：
- 回答长度（更长通常更受欢迎）
- 表情符号使用
- 项目符号和格式
- 标题使用
- 积极/友好的语气

**例子**：Claude模型很聪明但"不太会说话"——它们的排名低于实际能力，因为它们不那么冗长，使用的表情符号更少。

当研究人员控制风格因素后，排名发生了显著变化！

## ARC-AGI：一种不同的方法

ARC-AGI（抽象和推理语料库）试图测试真正的推理：

**设计原则：**
- 视觉模式识别任务
- 没有基于语言的捷径
- 问题在互联网上找不到
- 需要真正的抽象能力

**示例任务：**
- 给定输入/输出模式对，预测新输入的输出
- 模式涉及颜色、形状、变换

### 为什么ARC-AGI难以作弊

1. **没有固定题库**：总是可以创建新问题
2. **视觉格式**：不能轻易作为文本记忆
3. **隐藏测试集**：公开示例与评估集不同
4. **需要真正的抽象**：模式识别，而非知识回忆

### O3的突破

OpenAI的O3模型在ARC-AGI上达到了接近人类的表现：
- 第一个显著突破这个基准测试的模型
- 但每个问题需要约1000美元的计算
- 表明推理是可能的，但代价昂贵

## 眼镜蛇效应

李教授分享了"眼镜蛇效应"的故事：

在英国殖民印度期间，有眼镜蛇问题。政府为死眼镜蛇提供赏金。结果？人们开始为了赏金养殖眼镜蛇，使问题更加严重。

**对AI的教训**：当你为一个指标优化时，你可能会得到意想不到的行为，这些行为是在玩弄指标而不是解决根本问题。

## 关键要点

1. **不存在完美的基准测试**：每种评估方法都有缺陷

2. **记忆vs推理**：当模型在互联网规模的数据上训练时，很难区分

3. **风格vs实质**：人类评估受到呈现方式的偏见影响

4. **古德哈特定律是真实的**：流行的基准测试会被玩弄

5. **需要多种评估**：不要依赖任何单一指标

## 什么是好的基准测试？

- 抵抗记忆
- 测试真正的能力，而非表面模式
- 隐藏测试集
- 持续更新问题
- 多维度评估
:::
