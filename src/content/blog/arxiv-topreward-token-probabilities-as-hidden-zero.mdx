---
title:
  en: "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics"
  zh: "TOPReward: 将Token概率作为机器人零样本奖励的隐藏信号"
description:
  en: "A novel approach that extracts task progress signals from vision-language model token logits to provide generalizable rewards for robotic reinforcement learning, achieving 0.947 correlation across 130+ real-world tasks."
  zh: "一种从视觉语言模型token logits中提取任务进度信号的新方法,为机器人强化学习提供可泛化的奖励,在130多个真实任务中实现0.947的相关性。"
date: 2026-02-24
tags: ["arxiv", "ai", "cs.ro", "cs.ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-topreward-token-probabilities-as-hidden-zero.png"
---

:::en
**Paper**: [2602.19313](https://arxiv.org/abs/2602.19313)
**Authors**: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren, Lillian J. Ratliff, Jiafei Duan, Dieter Fox, Ranjay Krishna
**Categories**: cs.RO, cs.AI, cs.LG

## Abstract

Vision-Language-Action (VLA) models have made significant strides in pretraining, but their application to reinforcement learning faces critical challenges: low sample efficiency and sparse reward signals in real-world robotics. The core issue lies in developing generalizable process reward models that can provide fine-grained feedback across diverse tasks. TOPReward addresses this by introducing a probabilistically grounded temporal value function that taps into the latent world knowledge embedded in pretrained video Vision-Language Models (VLMs). Rather than prompting VLMs to directly output numerical progress estimates—a method prone to misrepresentation—TOPReward extracts task progress information directly from the model's internal token logits. Evaluated zero-shot across over 130 distinct real-world robotic tasks spanning multiple platforms (Franka, YAM, SO-100/101), TOPReward achieves a remarkable 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, vastly outperforming the state-of-the-art GVL baseline which struggles with near-zero correlation on the same model.

## Key Contributions

- Introduction of TOPReward, a novel temporal value function that extracts task progress from VLM token logits rather than prompted numerical outputs
- Probabilistic grounding that leverages pretrained video VLMs' latent world knowledge for zero-shot generalization
- Comprehensive zero-shot evaluation across 130+ real-world robotic tasks on multiple platforms, achieving 0.947 mean VOC
- Demonstration of TOPReward's versatility for downstream applications including success detection and reward-aligned behavior cloning
- Significant performance improvement over state-of-the-art methods, with GVL baseline achieving near-zero correlation on the same open-source model

## Technical Methodology

The fundamental innovation of TOPReward lies in its approach to extracting reward signals from VLMs. Traditional methods prompt VLMs to generate explicit numerical progress values, but this approach suffers from a critical flaw: language models are notoriously poor at representing precise numerical quantities through their text generation process. TOPReward circumvents this limitation by directly accessing the model's internal token probability distributions.

The method operates on video sequences of robotic task execution. Given a video trajectory, TOPReward queries the VLM with task-specific prompts and analyzes the probability distribution over tokens in the model's vocabulary. Rather than sampling from this distribution to generate text, TOPReward extracts the logits (pre-softmax values) associated with tokens that indicate task progress or completion. This probabilistic signal is inherently more reliable than generated text because it reflects the model's internal confidence about task state without the distortion introduced by the sampling and decoding process.

The temporal value function $V(s_t)$ at state $s_t$ is computed by aggregating token probabilities across relevant progress indicators. The model can be formulated as:

$$V(s_t) = f(\text{logits}(\text{VLM}(v_{1:t}, p)))$$

where $v_{1:t}$ represents the video frames up to time $t$, $p$ is the task prompt, and $f$ is an aggregation function over the relevant token logits. This formulation provides a continuous, differentiable signal that can be used for reinforcement learning optimization.

## Experimental Results and Analysis

The experimental validation of TOPReward is extensive and compelling. Across 130+ distinct real-world robotic tasks, the method demonstrates exceptional zero-shot generalization capabilities. The 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL represents a dramatic improvement over existing approaches. VOC measures how well the predicted value function preserves the ordering of states according to their true progress toward task completion—a critical property for effective reward shaping in RL.

The comparison with the GVL baseline is particularly striking. While GVL represents the state-of-the-art in prompted VLM-based reward modeling, it achieves near-zero correlation on the same open-source Qwen3-VL model. This performance gap highlights the fundamental advantage of TOPReward's approach: by accessing internal model representations rather than relying on generated text, it captures more reliable and nuanced progress signals.

The evaluation spans multiple robot platforms including Franka robotic arms, YAM mobile manipulators, and SO-100/101 systems, demonstrating that TOPReward's effectiveness is not platform-specific but rather generalizes across different embodiments and task types. This cross-platform generalization is crucial for practical deployment in diverse robotic applications.

## Implications for Robotic Learning

TOPReward addresses a fundamental bottleneck in robotic reinforcement learning: the reward specification problem. In real-world settings, defining reward functions that are both informative and generalizable is notoriously difficult. Sparse rewards (e.g., binary success/failure) provide insufficient learning signal, while hand-crafted dense rewards require extensive domain expertise and often fail to generalize to new tasks.

By leveraging the world knowledge embedded in large-scale pretrained VLMs, TOPReward provides a path toward more sample-efficient robotic learning. The fine-grained progress signals it generates can guide policy optimization more effectively than sparse rewards, potentially reducing the number of real-world interactions required for learning. This is particularly valuable in robotics where data collection is expensive and time-consuming.

The versatility demonstrated through downstream applications—success detection and reward-aligned behavior cloning—suggests that TOPReward can serve as a foundational component in robotic learning systems. Success detection enables automated evaluation and curriculum learning, while reward-aligned behavior cloning allows for more effective imitation learning by focusing on high-reward trajectories.

## Takeaways

1. TOPReward introduces a paradigm shift in VLM-based reward modeling by extracting signals from token logits rather than generated text, achieving 0.947 VOC across 130+ real-world robotic tasks.

2. The method's probabilistic grounding enables zero-shot generalization across diverse tasks and robot platforms without requiring task-specific training or fine-tuning.

3. By dramatically outperforming state-of-the-art prompted approaches (GVL baseline achieves near-zero correlation on the same model), TOPReward demonstrates the superiority of accessing internal model representations.

4. The approach addresses critical challenges in robotic RL—sparse rewards and low sample efficiency—by providing fine-grained, generalizable progress signals derived from pretrained VLM world knowledge.

5. TOPReward's versatility for downstream applications including success detection and reward-aligned behavior cloning positions it as a foundational tool for next-generation robotic learning systems.
:::

:::zh
**论文**: [2602.19313](https://arxiv.org/abs/2602.19313)
**作者**: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren, Lillian J. Ratliff, Jiafei Duan, Dieter Fox, Ranjay Krishna
**分类**: cs.RO, cs.AI, cs.LG

## 摘要

视觉-语言-动作(VLA)模型在预训练方面取得了显著进展,但其在强化学习中的应用面临关键挑战:真实世界机器人场景中的低样本效率和稀疏奖励信号。核心问题在于开发能够跨多样化任务提供细粒度反馈的可泛化过程奖励模型。TOPReward通过引入一个基于概率论的时序价值函数来解决这一问题,该函数利用预训练视频视觉语言模型(VLM)中嵌入的潜在世界知识。与提示VLM直接输出数值进度估计的方法不同——这种方法容易产生误表示——TOPReward直接从模型的内部token logits中提取任务进度信息。在超过130个不同的真实世界机器人任务上进行零样本评估,涵盖多个平台(Franka、YAM、SO-100/101),TOPReward在Qwen3-VL上实现了0.947的平均值序相关性(VOC),远超在同一模型上相关性接近零的最先进GVL基线。

## 主要贡献

- 提出TOPReward,一种从VLM token logits而非提示数值输出中提取任务进度的新型时序价值函数
- 基于概率论的基础设计,利用预训练视频VLM的潜在世界知识实现零样本泛化
- 在多个平台上对130多个真实世界机器人任务进行全面的零样本评估,实现0.947的平均VOC
- 展示TOPReward在下游应用中的多功能性,包括成功检测和奖励对齐的行为克隆
- 相比最先进方法实现显著性能提升,GVL基线在同一开源模型上的相关性接近零

## 技术方法论

TOPReward的根本创新在于其从VLM中提取奖励信号的方法。传统方法提示VLM生成显式的数值进度值,但这种方法存在一个关键缺陷:语言模型在通过文本生成过程表示精确数值量时表现不佳。TOPReward通过直接访问模型的内部token概率分布来规避这一限制。

该方法作用于机器人任务执行的视频序列。给定一个视频轨迹,TOPReward使用特定任务的提示查询VLM,并分析模型词汇表中token的概率分布。TOPReward不是从这个分布中采样生成文本,而是提取与指示任务进度或完成的token相关的logits(softmax前的值)。这种概率信号本质上比生成的文本更可靠,因为它反映了模型对任务状态的内部置信度,而没有采样和解码过程引入的失真。

状态$s_t$处的时序价值函数$V(s_t)$通过聚合相关进度指标的token概率来计算。该模型可以表述为:

$$V(s_t) = f(\text{logits}(\text{VLM}(v_{1:t}, p)))$$

其中$v_{1:t}$表示到时间$t$的视频帧,$p$是任务提示,$f$是相关token logits上的聚合函数。这种表述提供了一个连续的、可微分的信号,可用于强化学习优化。

## 实验结果与分析

TOPReward的实验验证广泛而有说服力。在130多个不同的真实世界机器人任务中,该方法展示了卓越的零样本泛化能力。在Qwen3-VL上0.947的平均值序相关性(VOC)代表了相对现有方法的显著改进。VOC衡量预测的价值函数在多大程度上保持了状态根据其向任务完成的真实进度的排序——这是强化学习中有效奖励塑形的关键属性。

与GVL基线的比较尤其引人注目。虽然GVL代表了基于提示的VLM奖励建模的最先进水平,但它在同一开源Qwen3-VL模型上的相关性接近零。这种性能差距突显了TOPReward方法的根本优势:通过访问内部模型表示而不是依赖生成的文本,它捕获了更可靠和细致的进度信号。

评估涵盖多个机器人平台,包括Franka机械臂、YAM移动操作器和SO-100/101系统,表明TOPReward的有效性不是特定于平台的,而是在不同的具身形态和任务类型中泛化。这种跨平台泛化对于在多样化机器人应用中的实际部署至关重要。

## 对机器人学习的影响

TOPReward解决了机器人强化学习中的一个根本瓶颈:奖励规范问题。在真实世界环境中,定义既有信息量又可泛化的奖励函数是出了名的困难。稀疏奖励(例如二元成功/失败)提供的学习信号不足,而手工制作的密集奖励需要大量领域专业知识,并且往往无法泛化到新任务。

通过利用大规模预训练VLM中嵌入的世界知识,TOPReward为更高样本效率的机器人学习提供了一条路径。它生成的细粒度进度信号可以比稀疏奖励更有效地指导策略优化,潜在地减少学习所需的真实世界交互次数。这在数据收集昂贵且耗时的机器人领域尤其有价值。

通过下游应用——成功检测和奖励对齐的行为克隆——展示的多功能性表明,TOPReward可以作为机器人学习系统中的基础组件。成功检测支持自动评估和课程学习,而奖励对齐的行为克隆通过关注高奖励轨迹实现更有效的模仿学习。

## 要点总结

1. TOPReward在基于VLM的奖励建模中引入了范式转变,通过从token logits而非生成文本中提取信号,在130多个真实世界机器人任务中实现0.947的VOC。

2. 该方法的概率论基础使其能够在不需要任务特定训练或微调的情况下,跨多样化任务和机器人平台实现零样本泛化。

3. 通过大幅超越最先进的提示方法(GVL基线在同一模型上相关性接近零),TOPReward证明了访问内部模型表示的优越性。

4. 该方法通过提供源自预训练VLM世界知识的细粒度、可泛化进度信号,解决了机器人强化学习中的关键挑战——稀疏奖励和低样本效率。

5. TOPReward在包括成功检测和奖励对齐行为克隆在内的下游应用中的多功能性,使其成为下一代机器人学习系统的基础工具。
:::
