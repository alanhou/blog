---
title:
  en: "Flow-Guided Neural Operators: Dynamic Noise Levels for Self-Supervised Time-Series Learning"
  zh: "流引导神经算子:时间序列自监督学习中的动态噪声水平"
description:
  en: "FGNO introduces adaptive corruption levels through flow matching and operator learning, achieving up to 35% performance gains in biomedical time-series tasks by extracting hierarchical representations from varying noise strengths."
  zh: "FGNO通过流匹配和算子学习引入自适应损坏水平,在生物医学时间序列任务中实现高达35%的性能提升,通过不同噪声强度提取层次化表征。"
date: 2026-02-15
tags: ["arxiv", "ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-self-supervised-learning-via-flow-guided.png"
---

:::en
**Paper**: [2602.12267](https://arxiv.org/abs/2602.12267)
**Authors**: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar
**Categories**: cs.LG

## Abstract

This paper presents Flow-Guided Neural Operator (FGNO), a novel self-supervised learning framework for time-series data that treats corruption level as a learnable parameter rather than a fixed hyperparameter. Unlike traditional masked autoencoders that use predetermined masking ratios, FGNO combines operator learning with flow matching to learn mappings in functional spaces. The method leverages Short-Time Fourier Transform to handle multiple time resolutions and extracts hierarchical features by sampling from different network layers and flow times. Evaluated on three biomedical datasets, FGNO demonstrates substantial improvements: 35% AUROC gains in neural signal decoding, 16% RMSE reduction in temperature prediction, and over 20% accuracy improvement in sleep stage classification under low-data conditions.

## Key Contributions

- **Dynamic Corruption Framework**: Introduces corruption level as a continuous degree of freedom, moving beyond fixed masking ratios in traditional SSL methods
- **Operator Learning Integration**: Combines neural operators with flow matching to learn functional space mappings that unify different time resolutions via Short-Time Fourier Transform
- **Hierarchical Feature Extraction**: Extracts multi-scale representations by tapping into different network layers and flow times, capturing both low-level patterns and high-level semantics
- **Clean Inference Protocol**: Uses clean inputs during inference while training with noise, eliminating randomness and improving downstream task accuracy
- **Strong Empirical Results**: Achieves state-of-the-art performance across three biomedical domains, particularly excelling in low-data regimes

## Methodology: Flow Matching Meets Operator Learning

The core innovation of FGNO lies in its treatment of the corruption process as a continuous flow. Traditional SSL methods like MAEs apply a fixed masking ratio $r$ to all samples, limiting their ability to capture features at different abstraction levels. FGNO instead models the corruption as a time-dependent process parameterized by flow time $t \in [0, 1]$.

The framework operates in functional spaces using neural operators, which learn mappings between function spaces rather than finite-dimensional vectors. This is particularly powerful for time-series data where signals can have varying lengths and sampling rates. The Short-Time Fourier Transform (STFT) serves as the bridge, converting time-series into a unified frequency-time representation:

$$\text{STFT}(x)[n, k] = \sum_{m} x[m] w[m-n] e^{-j2\pi km/N}$$

where $w$ is a window function, $n$ is the time index, and $k$ is the frequency bin.

The flow matching objective trains the model to predict the direction of the flow at any corruption level $t$, learning a velocity field $v_\theta(x_t, t)$ that connects clean data $x_0$ to increasingly corrupted versions. During training, the model sees inputs at various corruption levels, learning to denoise from any intermediate state. This multi-level training enables the extraction of features at different semantic levels by querying the model at specific flow times.

## Hierarchical Representation Extraction

A key insight of FGNO is that different corruption levels and network depths capture complementary information. Early layers with low corruption ($t \approx 0$) preserve fine-grained temporal patterns, while deeper layers with higher corruption ($t \approx 1$) capture global structure and long-range dependencies.

The authors propose a systematic feature extraction strategy:

1. **Layer-wise features**: Extract activations from multiple network depths to capture hierarchical abstractions
2. **Flow-time features**: Sample representations at different $t$ values during the denoising process
3. **Aggregation**: Combine these multi-scale features through concatenation or learned pooling

This approach yields a rich feature bank without requiring multiple models or training runs. For a specific downstream task, practitioners can select the optimal combination of layers and flow times, or use all features with a lightweight classifier.

Critically, during inference for downstream tasks, FGNO uses clean inputs ($t=0$) rather than noisy ones. This contrasts with some generative SSL methods that inject noise during inference, introducing unwanted randomness. By training with noise but inferring with clean data, FGNO achieves both robust representations and deterministic predictions.

## Experimental Results and Analysis

The authors evaluate FGNO on three challenging biomedical time-series benchmarks:

**BrainTreeBank (Neural Signal Decoding)**: This dataset involves decoding cognitive states from neural recordings. FGNO achieves up to 35% improvement in AUROC over baselines including SimCLR, BYOL, and standard MAEs. The gains are most pronounced in low-data scenarios where only 10-20% of labeled data is available, demonstrating FGNO's superior sample efficiency.

**DREAMT (Skin Temperature Prediction)**: For physiological signal forecasting, FGNO reduces RMSE by 16% compared to supervised baselines and other SSL methods. The continuous corruption framework proves particularly effective for capturing the multi-scale temporal dynamics in circadian rhythms and acute temperature changes.

**SleepEDF (Sleep Stage Classification)**: In this multi-class classification task with severe class imbalance, FGNO improves both accuracy and macro-F1 by over 20% in low-data regimes. The hierarchical features enable better discrimination of subtle differences between sleep stages, which often manifest at different temporal scales.

Ablation studies reveal that both the flow-based corruption and operator learning components are essential. Removing flow matching and reverting to fixed corruption reduces performance by 12-18%, while replacing the neural operator with standard convolutions decreases accuracy by 8-15%. The STFT-based functional representation is particularly important for handling variable-length sequences and multi-resolution analysis.

## Implications and Future Directions

FGNO represents a significant advance in self-supervised learning for time-series data, with several broader implications:

**Theoretical**: The work bridges operator learning, flow matching, and SSL, opening new avenues for theoretical analysis of representation learning in functional spaces. The continuous corruption framework provides a more principled way to think about data augmentation and invariance learning.

**Practical**: The method's strong performance in low-data regimes makes it particularly valuable for domains like healthcare and scientific computing where labeled data is expensive or scarce. The single-model, multi-scale approach is also computationally efficient compared to training separate models for different tasks.

**Limitations and Extensions**: While FGNO excels on biomedical data, its performance on other time-series domains (finance, climate, audio) remains to be thoroughly evaluated. The reliance on STFT may be suboptimal for signals with non-stationary or highly irregular patterns. Future work could explore adaptive time-frequency representations or learnable basis functions.

The clean-input inference strategy, while effective, raises questions about the train-test distribution gap. Investigating whether intermediate corruption levels during inference could provide uncertainty estimates or robustness to noisy inputs is an interesting direction.

Finally, extending FGNO to multivariate time-series with complex dependencies, or to spatiotemporal data, could unlock applications in climate modeling, traffic prediction, and video understanding.

## Takeaways

1. Treating corruption level as a continuous parameter rather than a fixed hyperparameter enables more flexible and powerful self-supervised learning for time-series data
2. Combining neural operators with flow matching provides a principled framework for learning in functional spaces, naturally handling variable-length sequences and multiple resolutions
3. Hierarchical feature extraction from different network layers and flow times captures complementary information from fine-grained patterns to global structure
4. Using clean inputs during inference while training with noise eliminates randomness and improves downstream task performance
5. FGNO achieves substantial gains (up to 35%) over established baselines across multiple biomedical domains, particularly excelling in low-data regimes where labeled samples are scarce
:::

:::zh
**论文**: [2602.12267](https://arxiv.org/abs/2602.12267)
**作者**: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar
**分类**: cs.LG

## 摘要

本文提出了流引导神经算子(FGNO),这是一种用于时间序列数据的新型自监督学习框架,将损坏水平视为可学习参数而非固定超参数。与使用预定掩码比例的传统掩码自编码器不同,FGNO结合算子学习和流匹配来学习函数空间中的映射。该方法利用短时傅里叶变换处理多个时间分辨率,并通过从不同网络层和流时间采样来提取层次化特征。在三个生物医学数据集上的评估显示,FGNO取得了显著改进:神经信号解码中AUROC提升35%,温度预测中RMSE降低16%,以及在低数据条件下睡眠阶段分类准确率提高超过20%。

## 主要贡献

- **动态损坏框架**:将损坏水平引入为连续自由度,超越传统SSL方法中的固定掩码比例
- **算子学习集成**:将神经算子与流匹配相结合,学习函数空间映射,通过短时傅里叶变换统一不同时间分辨率
- **层次化特征提取**:通过访问不同网络层和流时间提取多尺度表征,捕获从低级模式到高级语义的特征
- **清洁推理协议**:在推理时使用清洁输入而在训练时使用噪声,消除随机性并提高下游任务准确性
- **强大的实证结果**:在三个生物医学领域实现最先进性能,特别是在低数据场景中表现出色

## 方法论:流匹配与算子学习的结合

FGNO的核心创新在于将损坏过程视为连续流。传统SSL方法如MAE对所有样本应用固定掩码比例$r$,限制了其在不同抽象层次捕获特征的能力。FGNO则将损坏建模为由流时间$t \in [0, 1]$参数化的时间依赖过程。

该框架使用神经算子在函数空间中操作,学习函数空间之间的映射而非有限维向量。这对于可能具有不同长度和采样率的时间序列数据特别强大。短时傅里叶变换(STFT)作为桥梁,将时间序列转换为统一的频率-时间表示:

$$\text{STFT}(x)[n, k] = \sum_{m} x[m] w[m-n] e^{-j2\pi km/N}$$

其中$w$是窗函数,$n$是时间索引,$k$是频率箱。

流匹配目标训练模型预测任意损坏水平$t$下的流方向,学习速度场$v_\theta(x_t, t)$,连接清洁数据$x_0$到逐渐损坏的版本。在训练期间,模型看到各种损坏水平的输入,学习从任何中间状态去噪。这种多级训练使得能够通过在特定流时间查询模型来提取不同语义层次的特征。

## 层次化表征提取

FGNO的一个关键洞察是不同的损坏水平和网络深度捕获互补信息。具有低损坏($t \approx 0$)的早期层保留细粒度时间模式,而具有较高损坏($t \approx 1$)的深层捕获全局结构和长程依赖关系。

作者提出了系统的特征提取策略:

1. **逐层特征**:从多个网络深度提取激活以捕获层次化抽象
2. **流时间特征**:在去噪过程中在不同$t$值处采样表征
3. **聚合**:通过拼接或学习池化组合这些多尺度特征

这种方法产生丰富的特征库,无需多个模型或训练运行。对于特定的下游任务,实践者可以选择层和流时间的最优组合,或使用所有特征配合轻量级分类器。

关键的是,在下游任务的推理期间,FGNO使用清洁输入($t=0$)而非噪声输入。这与一些在推理期间注入噪声的生成式SSL方法形成对比,后者引入了不必要的随机性。通过用噪声训练但用清洁数据推理,FGNO实现了鲁棒表征和确定性预测。

## 实验结果与分析

作者在三个具有挑战性的生物医学时间序列基准上评估FGNO:

**BrainTreeBank(神经信号解码)**:该数据集涉及从神经记录解码认知状态。FGNO在AUROC上比包括SimCLR、BYOL和标准MAE在内的基线提高了35%。在只有10-20%标注数据可用的低数据场景中,增益最为显著,展示了FGNO卓越的样本效率。

**DREAMT(皮肤温度预测)**:对于生理信号预测,FGNO比监督基线和其他SSL方法降低了16%的RMSE。连续损坏框架在捕获昼夜节律和急性温度变化中的多尺度时间动态方面特别有效。

**SleepEDF(睡眠阶段分类)**:在这个具有严重类别不平衡的多类分类任务中,FGNO在低数据场景下将准确率和宏F1都提高了超过20%。层次化特征使得能够更好地区分睡眠阶段之间的细微差异,这些差异通常在不同时间尺度上表现出来。

消融研究表明,基于流的损坏和算子学习组件都是必不可少的。移除流匹配并恢复到固定损坏会使性能降低12-18%,而用标准卷积替换神经算子会使准确率降低8-15%。基于STFT的函数表示对于处理可变长度序列和多分辨率分析特别重要。

## 影响与未来方向

FGNO代表了时间序列数据自监督学习的重大进展,具有几个更广泛的影响:

**理论层面**:该工作连接了算子学习、流匹配和SSL,为函数空间中表征学习的理论分析开辟了新途径。连续损坏框架提供了一种更有原则的方式来思考数据增强和不变性学习。

**实践层面**:该方法在低数据场景中的强大性能使其对医疗保健和科学计算等标注数据昂贵或稀缺的领域特别有价值。单模型、多尺度方法与为不同任务训练单独模型相比在计算上也更高效。

**局限性与扩展**:虽然FGNO在生物医学数据上表现出色,但其在其他时间序列领域(金融、气候、音频)的性能仍有待全面评估。对STFT的依赖对于具有非平稳或高度不规则模式的信号可能不是最优的。未来工作可以探索自适应时频表示或可学习基函数。

清洁输入推理策略虽然有效,但引发了关于训练-测试分布差距的问题。研究推理期间的中间损坏水平是否可以提供不确定性估计或对噪声输入的鲁棒性是一个有趣的方向。

最后,将FGNO扩展到具有复杂依赖关系的多变量时间序列,或时空数据,可以解锁气候建模、交通预测和视频理解中的应用。

## 要点总结

1. 将损坏水平视为连续参数而非固定超参数,为时间序列数据的自监督学习提供了更灵活和强大的方法
2. 将神经算子与流匹配相结合提供了在函数空间中学习的原则性框架,自然地处理可变长度序列和多分辨率
3. 从不同网络层和流时间进行层次化特征提取,捕获从细粒度模式到全局结构的互补信息
4. 在推理时使用清洁输入而在训练时使用噪声,消除随机性并提高下游任务性能
5. FGNO在多个生物医学领域相比已建立的基线实现了显著增益(高达35%),特别是在标注样本稀缺的低数据场景中表现出色
:::
