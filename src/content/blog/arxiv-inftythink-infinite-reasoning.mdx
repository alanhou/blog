---
title:
  en: "InftyThink+: Infinite-Horizon Reasoning via Reinforcement Learning"
  zh: "InftyThink+：通过强化学习实现无限视野推理"
description:
  en: "A two-stage RL framework for infinite-horizon reasoning that achieves 21% improvement on AIME24 through iterative reasoning with strategic summarization"
  zh: "一种两阶段RL框架，通过迭代推理和策略性总结实现无限视野推理，在AIME24上提升21%"
date: 2026-02-05
tags: ["arxiv", "ai", "reasoning", "reinforcement-learning", "test-time-scaling", "chain-of-thought", "cs.CL", "cs.AI"]
image: "/arxiv-visuals/arxiv-inftythink-infinite-reasoning.png"
---

:::en
**Paper**: [2602.06960](https://arxiv.org/abs/2602.06960)
**Authors**: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen
**Categories**: cs.CL, cs.AI

## Abstract

Large reasoning models using chain-of-thought scaling face three fundamental limitations: quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. InftyThink+ proposes iterative reasoning with periodic summarization, optimized through reinforcement learning, to enable effective infinite-horizon reasoning.

## Key Contributions

- **Iterative reasoning framework**: Breaks long reasoning chains into manageable segments with periodic summarization
- **Two-stage training**: Combines supervised learning initialization with trajectory-level reinforcement learning
- **Strategic summarization**: The model learns when and how to summarize, enabling continuation beyond context limits

## The Problem

Standard chain-of-thought reasoning in LLMs faces three compounding issues:

1. **Quadratic cost**: Attention computation grows quadratically with sequence length
2. **Context limits**: Models have fixed context windows that cap reasoning depth
3. **Lost-in-the-middle**: Performance degrades as reasoning chains grow longer, with the model losing track of earlier steps

These limitations mean that simply "thinking longer" hits diminishing returns and eventually negative returns.

## The Solution: Iterative Reasoning with Summarization

InftyThink+ breaks the reasoning process into iterations:

1. **Reason**: Generate a segment of reasoning within a manageable context window
2. **Summarize**: Compress the key insights and intermediate results
3. **Continue**: Start a new reasoning segment from the summary

The model learns **when** to summarize and **what** to preserve through RL training, making these decisions strategic rather than mechanical.

### Two-Stage Training

- **Stage 1 (SFT initialization)**: Supervised fine-tuning teaches the basic pattern of iterative reasoning with summarization
- **Stage 2 (Trajectory-level RL)**: Reinforcement learning optimizes the full trajectory, teaching the model to make better summarization and continuation decisions

## Results

- **21% accuracy improvement** on AIME24 benchmark
- Outperforms conventional long chain-of-thought RL approaches
- Better generalization to out-of-distribution benchmarks
- Reduced inference latency compared to single long-chain approaches

## Why This Matters

1. **Breaks the context ceiling**: Reasoning is no longer bounded by context window size
2. **Efficient compute usage**: Avoids quadratic blowup by working in segments
3. **Learned compression**: The model decides what information to carry forward
4. **Practical scaling**: Enables test-time compute scaling without architectural changes
:::

:::zh
**论文**: [2602.06960](https://arxiv.org/abs/2602.06960)
**作者**: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen
**分类**: cs.CL, cs.AI

## 摘要

使用思维链扩展的大型推理模型面临三个根本限制：二次方成本、上下文长度限制以及由于"中间丢失"效应导致的推理退化。InftyThink+提出了通过强化学习优化的迭代推理与周期性总结方法，以实现有效的无限视野推理。

## 主要贡献

- **迭代推理框架**：将长推理链分解为可管理的片段，并进行周期性总结
- **两阶段训练**：结合监督学习初始化和轨迹级强化学习
- **策略性总结**：模型学习何时以及如何总结，使推理能够超越上下文限制

## 问题所在

LLM中的标准思维链推理面临三个叠加问题：

1. **二次方成本**：注意力计算随序列长度二次增长
2. **上下文限制**：模型有固定的上下文窗口，限制了推理深度
3. **中间丢失**：随着推理链变长，性能下降，模型丢失早期步骤的信息

这些限制意味着简单地"思考更长时间"会遇到收益递减，最终变为负收益。

## 解决方案：带总结的迭代推理

InftyThink+将推理过程分解为迭代：

1. **推理**：在可管理的上下文窗口内生成一段推理
2. **总结**：压缩关键洞察和中间结果
3. **继续**：从总结开始新的推理段

模型通过RL训练学习**何时**总结和**保留什么**，使这些决策具有策略性而非机械性。

### 两阶段训练

- **阶段1（SFT初始化）**：监督微调教授带总结的迭代推理基本模式
- **阶段2（轨迹级RL）**：强化学习优化完整轨迹，教模型做出更好的总结和继续决策

## 实验结果

- 在AIME24基准测试上**准确率提升21%**
- 优于传统的长思维链RL方法
- 在分布外基准测试上更好的泛化能力
- 与单一长链方法相比，推理延迟降低

## 重要意义

1. **突破上下文天花板**：推理不再受上下文窗口大小限制
2. **高效计算使用**：通过分段工作避免二次方膨胀
3. **学习压缩**：模型决定携带哪些信息继续
4. **实用扩展**：无需架构更改即可实现测试时计算扩展
:::
