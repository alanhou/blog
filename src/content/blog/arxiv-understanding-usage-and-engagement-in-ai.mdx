---
title:
  en: "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset"
  zh: "理解AI驱动科研工具的使用与参与模式:Asta交互数据集"
description:
  en: "Analysis of 200,000+ user interactions with AI research assistants reveals how scientists use LLM-powered tools as collaborative partners, submitting complex queries and engaging with responses as persistent research artifacts."
  zh: "通过分析20万+用户与AI科研助手的交互数据,揭示科学家如何将LLM驱动的工具视为协作伙伴,提交复杂查询并将响应作为持久性研究成果进行深度参与。"
date: 2026-02-28
tags: ["arxiv", "ai", "cs.hc", "cs.ai", "cs.ir"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.23335](https://arxiv.org/abs/2602.23335)
**Authors**: Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh, Harshit Surana, Aryeh Tiktinsky
**Categories**: cs.HC, cs.AI, cs.IR

## Abstract

This paper presents the Asta Interaction Dataset, a comprehensive collection of over 200,000 user queries and interaction logs from deployed AI-powered scientific research tools. The dataset captures real-world usage patterns from two interfaces: a literature discovery tool and a scientific question-answering system, both built on an LLM-powered retrieval-augmented generation (RAG) platform. The analysis reveals fundamental shifts in how researchers interact with AI assistants compared to traditional search engines, including longer and more complex queries, treatment of the system as a collaborative research partner, and non-linear engagement with generated content. The study tracks how usage patterns evolve with user experience and introduces a new query intent taxonomy to support future research assistant design and evaluation.

## Key Contributions

- Release of a large-scale, anonymized dataset containing 200,000+ real-world user interactions with AI research tools
- Empirical characterization of query patterns showing users submit significantly longer and more complex queries than in traditional search systems
- Evidence that users delegate high-level research tasks (content drafting, gap identification) to AI assistants, treating them as collaborative partners
- Analysis of non-linear engagement patterns where users revisit and navigate among generated responses and citations as persistent artifacts
- Documentation of how usage evolves with experience: more targeted queries and deeper citation engagement, though keyword queries persist
- Introduction of a novel query intent taxonomy specifically designed for scientific research assistance contexts

## Dataset Characteristics and Methodology

The Asta Interaction Dataset captures interactions from two distinct interfaces within a unified RAG platform. The literature discovery interface helps researchers find relevant papers, while the question-answering interface provides direct answers to scientific questions with supporting citations. Both systems leverage large language models combined with retrieval mechanisms to access scientific literature.

The dataset spans multiple months of deployment and includes diverse interaction signals: query text, query length and complexity metrics, response engagement patterns (time spent, scrolling behavior, citation clicks), and longitudinal user behavior across sessions. Users are segmented by experience level based on cumulative interaction count, enabling analysis of how research workflows adapt over time.

The authors employ mixed-methods analysis combining quantitative metrics (query length distributions, engagement time, citation click-through rates) with qualitative coding of query intents. This dual approach provides both statistical rigor and interpretive depth, revealing not just what users do but why they interact with AI research tools in particular ways.

## Query Patterns and User Intent

Analysis reveals that queries submitted to AI research assistants differ fundamentally from traditional search engine queries. The median query length is substantially longer, with users frequently submitting full sentences or even paragraph-length requests. This suggests users perceive the system as capable of understanding complex, nuanced information needs rather than requiring keyword optimization.

The introduced query intent taxonomy identifies several distinct categories:
- **Information seeking**: Direct questions about scientific concepts or findings
- **Literature discovery**: Requests to find papers on specific topics or with particular characteristics
- **Content generation**: Delegation of writing tasks like summarizing findings or drafting explanations
- **Research planning**: Higher-level requests about research directions, gaps, or methodological approaches
- **Citation verification**: Checking claims or seeking supporting evidence for specific statements

Notably, content generation and research planning queries—categories largely absent in traditional search—constitute a significant portion of interactions. This indicates users view AI assistants as active collaborators in the research process, not merely information retrieval tools.

## Engagement Behaviors and Response Interaction

Users exhibit sophisticated engagement patterns with generated responses that differ markedly from traditional search result interaction. Rather than quickly scanning and moving on, users spend extended time with AI-generated content, treating responses as artifacts worth careful examination.

The data reveals non-linear navigation patterns: users frequently return to previous responses, compare multiple generated answers, and navigate between main content and supporting citations in complex sequences. This behavior suggests users are synthesizing information across multiple AI-generated outputs rather than treating each response as independent.

Citation engagement is particularly noteworthy. Users don't simply accept generated content at face value but actively verify claims by clicking through to source papers. The click-through rate on citations is substantially higher than typical academic search systems, indicating users value the transparency of seeing supporting evidence and use it to assess response quality.

Response persistence is another key finding. Users bookmark, revisit, and build upon previous interactions, suggesting they integrate AI-generated content into their ongoing research workflows. The system functions less like a search engine (ephemeral queries and results) and more like a research notebook (accumulated, revisited artifacts).

## Evolution of Usage with Experience

Longitudinal analysis tracking individual users over time reveals how interaction patterns evolve with experience. Novice users (first few sessions) submit more exploratory, broad queries and engage somewhat superficially with responses. As users gain familiarity, several shifts occur:

Query targeting improves: experienced users formulate more specific, focused questions that leverage understanding of system capabilities. They learn to phrase requests in ways that elicit higher-quality responses, suggesting a co-adaptation process between user and system.

Citation engagement deepens: experienced users click through to supporting papers more frequently and spend more time examining cited evidence. This indicates growing sophistication in evaluating AI-generated content and integrating it with primary literature.

However, keyword-style queries persist even among experienced users, appearing alongside more complex natural language requests. This suggests users maintain multiple query strategies, selecting approaches based on task context rather than fully abandoning familiar search patterns.

The learning curve appears relatively quick—significant behavioral changes emerge within the first 10-20 interactions—but continues evolving over hundreds of queries, indicating both immediate adaptation and longer-term workflow integration.

## Implications for AI Research Assistant Design

The findings carry several important implications for designing future AI-powered research tools:

**Support for complex queries**: Systems should be optimized for longer, more nuanced inputs rather than assuming keyword-based queries. Interface design should encourage natural language expression of complex information needs.

**Collaborative framing**: Rather than positioning AI as a search tool, interfaces should support delegation of higher-level research tasks. Features for content generation, research planning, and gap identification should be first-class capabilities.

**Response persistence and organization**: Since users treat outputs as persistent artifacts, systems need robust mechanisms for saving, organizing, and revisiting generated content. Integration with research workflow tools (reference managers, note-taking systems) becomes critical.

**Citation transparency and verification**: Prominent, easily accessible citations are essential. Users actively verify claims, so systems must make source evidence readily available and support efficient navigation between generated content and supporting literature.

**Adaptive interfaces**: Given the evolution of usage patterns with experience, interfaces might adapt to user sophistication, offering more advanced features or different interaction paradigms as users demonstrate growing expertise.

**Multi-modal interaction support**: The persistence of keyword queries alongside natural language suggests value in supporting multiple query modalities rather than forcing users into a single interaction paradigm.

## Takeaways

1. Users submit significantly longer and more complex queries to AI research assistants than to traditional search engines, indicating they perceive these systems as capable of understanding nuanced information needs.

2. Researchers treat AI assistants as collaborative partners, delegating high-level tasks like content drafting and research gap identification rather than using them solely for information retrieval.

3. Generated responses function as persistent research artifacts that users revisit, compare, and build upon over time, requiring systems to support non-linear navigation and content organization.

4. Citation engagement is substantially higher than in traditional academic search, with users actively verifying AI-generated claims by examining supporting evidence, highlighting the importance of transparency.

5. Usage patterns evolve with experience toward more targeted queries and deeper citation engagement, though keyword-style queries persist even among experienced users, suggesting value in supporting multiple interaction modalities.

6. The Asta Interaction Dataset provides a crucial empirical foundation for understanding real-world AI research assistant usage, enabling more realistic evaluation and design of future systems.
:::

:::zh
**论文**: [2602.23335](https://arxiv.org/abs/2602.23335)
**作者**: Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh, Harshit Surana, Aryeh Tiktinsky
**分类**: cs.HC, cs.AI, cs.IR

## 摘要

本文介绍了Asta交互数据集,这是一个包含超过20万条用户查询和交互日志的综合性数据集,来源于已部署的AI驱动科研工具。该数据集捕获了两个界面的真实使用模式:文献发现工具和科学问答系统,两者均基于LLM驱动的检索增强生成(RAG)平台构建。分析揭示了研究人员与AI助手交互方式相比传统搜索引擎的根本性转变,包括更长更复杂的查询、将系统视为协作研究伙伴,以及对生成内容的非线性参与。研究追踪了使用模式如何随用户经验演变,并引入了新的查询意图分类法以支持未来研究助手的设计和评估。

## 主要贡献

- 发布包含20万+真实用户与AI科研工具交互的大规模匿名数据集
- 实证表征查询模式,显示用户提交的查询比传统搜索系统显著更长更复杂
- 证明用户将高层次研究任务(内容起草、空白识别)委托给AI助手,将其视为协作伙伴
- 分析非线性参与模式,用户将生成的响应和引用作为持久性成果进行重访和导航
- 记录使用如何随经验演变:更有针对性的查询和更深入的引用参与,尽管关键词查询持续存在
- 引入专为科学研究辅助场景设计的新型查询意图分类法

## 数据集特征与方法论

Asta交互数据集捕获了统一RAG平台内两个不同界面的交互。文献发现界面帮助研究人员找到相关论文,而问答界面则提供带有支持引用的科学问题直接答案。两个系统都利用大语言模型结合检索机制来访问科学文献。

数据集跨越多个月的部署期,包含多样化的交互信号:查询文本、查询长度和复杂度指标、响应参与模式(停留时间、滚动行为、引用点击),以及跨会话的纵向用户行为。用户根据累积交互次数按经验水平分段,使得能够分析研究工作流如何随时间适应。

作者采用混合方法分析,结合定量指标(查询长度分布、参与时间、引用点击率)与查询意图的定性编码。这种双重方法既提供统计严谨性又具有解释深度,不仅揭示用户做什么,还揭示他们为何以特定方式与AI研究工具交互。

## 查询模式与用户意图

分析显示,提交给AI研究助手的查询与传统搜索引擎查询存在根本差异。中位查询长度显著更长,用户经常提交完整句子甚至段落长度的请求。这表明用户认为系统能够理解复杂、细微的信息需求,而非需要关键词优化。

引入的查询意图分类法识别出几个不同类别:
- **信息寻求**:关于科学概念或发现的直接问题
- **文献发现**:寻找特定主题或具有特定特征的论文的请求
- **内容生成**:委托写作任务,如总结发现或起草解释
- **研究规划**:关于研究方向、空白或方法论途径的更高层次请求
- **引用验证**:检查声明或寻求特定陈述的支持证据

值得注意的是,内容生成和研究规划查询——在传统搜索中基本不存在的类别——构成了相当大比例的交互。这表明用户将AI助手视为研究过程中的主动协作者,而非仅仅是信息检索工具。

## 参与行为与响应交互

用户对生成响应表现出复杂的参与模式,与传统搜索结果交互明显不同。用户不是快速浏览后离开,而是花费大量时间仔细检查AI生成的内容,将响应视为值得仔细审查的成果。

数据揭示了非线性导航模式:用户频繁返回先前的响应,比较多个生成的答案,并在主要内容和支持引用之间以复杂序列导航。这种行为表明用户正在跨多个AI生成输出综合信息,而非将每个响应视为独立的。

引用参与尤其值得注意。用户不是简单地接受生成内容的表面价值,而是通过点击源论文来主动验证声明。引用的点击率远高于典型学术搜索系统,表明用户重视看到支持证据的透明度,并用它来评估响应质量。

响应持久性是另一个关键发现。用户收藏、重访并基于先前的交互构建,表明他们将AI生成的内容整合到持续的研究工作流中。系统的功能更像研究笔记本(累积的、重访的成果),而非搜索引擎(短暂的查询和结果)。

## 使用随经验的演变

追踪个体用户随时间的纵向分析揭示了交互模式如何随经验演变。新手用户(前几次会话)提交更多探索性、宽泛的查询,并对响应的参与相对肤浅。随着用户获得熟悉度,发生了几个转变:

查询针对性提高:有经验的用户制定更具体、更聚焦的问题,利用对系统能力的理解。他们学会以能引发更高质量响应的方式表述请求,表明用户与系统之间存在共同适应过程。

引用参与加深:有经验的用户更频繁地点击支持论文,并花更多时间检查引用证据。这表明在评估AI生成内容并将其与原始文献整合方面日益成熟。

然而,关键词式查询即使在有经验的用户中也持续存在,与更复杂的自然语言请求并存。这表明用户维持多种查询策略,根据任务上下文选择方法,而非完全放弃熟悉的搜索模式。

学习曲线似乎相对快速——显著的行为变化在前10-20次交互中出现——但在数百次查询中持续演变,表明既有即时适应也有长期工作流整合。

## 对AI研究助手设计的启示

这些发现对设计未来AI驱动的研究工具具有几个重要启示:

**支持复杂查询**:系统应针对更长、更细微的输入进行优化,而非假设基于关键词的查询。界面设计应鼓励复杂信息需求的自然语言表达。

**协作框架**:与其将AI定位为搜索工具,界面应支持更高层次研究任务的委托。内容生成、研究规划和空白识别的功能应成为一流能力。

**响应持久性和组织**:由于用户将输出视为持久性成果,系统需要强大的机制来保存、组织和重访生成的内容。与研究工作流工具(参考文献管理器、笔记系统)的集成变得至关重要。

**引用透明度和验证**:突出、易于访问的引用至关重要。用户主动验证声明,因此系统必须使源证据易于获取,并支持生成内容与支持文献之间的高效导航。

**自适应界面**:鉴于使用模式随经验的演变,界面可能会适应用户的成熟度,随着用户展示日益增长的专业知识而提供更高级的功能或不同的交互范式。

**多模态交互支持**:关键词查询与自然语言并存的持续性表明支持多种查询模态的价值,而非强迫用户进入单一交互范式。

## 要点总结

1. 用户向AI研究助手提交的查询比向传统搜索引擎提交的查询显著更长更复杂,表明他们认为这些系统能够理解细微的信息需求。

2. 研究人员将AI助手视为协作伙伴,委托高层次任务如内容起草和研究空白识别,而非仅用于信息检索。

3. 生成的响应作为持久性研究成果,用户随时间重访、比较并基于其构建,要求系统支持非线性导航和内容组织。

4. 引用参与度远高于传统学术搜索,用户通过检查支持证据主动验证AI生成的声明,突显透明度的重要性。

5. 使用模式随经验演变为更有针对性的查询和更深入的引用参与,尽管关键词式查询即使在有经验用户中也持续存在,表明支持多种交互模态的价值。

6. Asta交互数据集为理解真实世界AI研究助手使用提供了关键的实证基础,使未来系统的更现实评估和设计成为可能。
:::
