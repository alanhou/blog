---
title:
  en: "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching"
  zh: "DyTopo：基于语义匹配的多智能体推理动态拓扑路由"
description:
  en: "A novel framework that dynamically constructs agent communication topologies based on semantic relevance for improved multi-agent reasoning"
  zh: "一种基于语义相关性动态构建智能体通信拓扑的新框架，用于改进多智能体推理"
date: 2026-02-06
tags: ["arxiv", "ai", "multi-agent", "reasoning", "topology", "semantic-matching", "llm", "cs.AI"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.06039](https://arxiv.org/abs/2602.06039)
**Categories**: cs.AI, cs.CL, cs.MA

## Abstract

Multi-agent systems powered by Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, existing approaches typically rely on fixed communication topologies (e.g., chain, tree, or fully-connected graphs), which fail to adapt to the varying semantic requirements of different reasoning steps. We propose DyTopo (Dynamic Topology), a framework that dynamically constructs agent communication topologies based on semantic matching between agent expertise and task requirements.

## Key Contributions

- **Dynamic Topology Construction**: Adaptively builds communication graphs based on semantic relevance rather than predefined static structures
- **Semantic Matching Module**: Learns to match agent capabilities with sub-task requirements using embedding-based similarity
- **Routing Mechanism**: Efficiently routes information between agents based on semantic affinity scores
- **Scalability**: Maintains efficiency as the number of agents increases through sparse connectivity

## The Problem with Static Topologies

Current multi-agent reasoning systems use fixed communication patterns:

- **Chain topology**: Sequential information flow, limited parallelism
- **Tree topology**: Hierarchical but rigid structure
- **Fully-connected**: $O(n^2)$ communication overhead, information overload
- **Star topology**: Central bottleneck, single point of failure

These static structures cannot adapt to:
1. Varying complexity across reasoning steps
2. Different expertise requirements for sub-problems
3. Dynamic information dependencies

## DyTopo Framework

### Architecture Overview

The framework consists of three main components:

1. **Agent Pool**: Specialized agents with different expertise domains
2. **Semantic Router**: Dynamically determines communication paths
3. **Topology Constructor**: Builds task-specific communication graphs

### Semantic Matching

For each reasoning step, compute semantic affinity between agents:

$$S_{ij} = \frac{\mathbf{e}_i^T \mathbf{q}_j}{\|\mathbf{e}_i\| \|\mathbf{q}_j\|}$$

where $\mathbf{e}_i$ is the expertise embedding of agent $i$ and $\mathbf{q}_j$ is the query embedding for sub-task $j$.

### Dynamic Routing

The routing decision uses a learned threshold:

$$A_{ij} = \mathbb{1}[S_{ij} > \tau + \sigma(\mathbf{W}[\mathbf{e}_i; \mathbf{q}_j])]$$

where $\tau$ is a base threshold and $\sigma(\cdot)$ is a learned adaptive component.

### Topology Construction

The communication graph $G_t$ at step $t$ is constructed as:

$$G_t = (V, E_t), \quad E_t = \{(i, j) : A_{ij} = 1\}$$

This creates sparse, task-relevant connections rather than dense all-to-all communication.

## Methodology

### Training Objective

DyTopo is trained end-to-end with a composite loss:

$$\mathcal{L} = \mathcal{L}_{task} + \lambda_1 \mathcal{L}_{sparse} + \lambda_2 \mathcal{L}_{coverage}$$

- $\mathcal{L}_{task}$: Task-specific reasoning accuracy
- $\mathcal{L}_{sparse}$: Encourages sparse connectivity for efficiency
- $\mathcal{L}_{coverage}$: Ensures all relevant expertise is accessible

### Agent Specialization

Each agent maintains:
- Domain-specific knowledge embeddings
- Reasoning style preferences
- Historical performance on sub-task types

## Results

### Benchmark Performance

DyTopo demonstrates consistent improvements across reasoning benchmarks:

| Method | GSM8K | MATH | HotpotQA | StrategyQA |
|--------|-------|------|----------|------------|
| Chain-of-Agents | 78.2 | 42.1 | 65.3 | 71.8 |
| Tree-of-Agents | 79.5 | 43.8 | 67.1 | 73.2 |
| Fully-Connected | 80.1 | 44.2 | 66.8 | 72.9 |
| **DyTopo** | **82.7** | **47.3** | **70.5** | **76.4** |

### Efficiency Analysis

Communication overhead comparison (relative to fully-connected):

| Topology | Messages | Latency | Accuracy |
|----------|----------|---------|----------|
| Fully-Connected | 100% | 100% | Baseline |
| Chain | 15% | 180% | -2.1% |
| Tree | 25% | 120% | -1.3% |
| **DyTopo** | 35% | 85% | +2.6% |

DyTopo achieves better accuracy with fewer messages and lower latency.

### Ablation Studies

Key findings from ablations:
- Semantic matching contributes +1.8% accuracy over random routing
- Adaptive threshold improves +0.9% over fixed threshold
- Sparsity regularization reduces messages by 40% with minimal accuracy loss

## Why This Matters

The key insight: **optimal agent communication depends on task semantics**. By learning to route information based on semantic relevance:

1. Reduces irrelevant information exchange
2. Enables parallel processing of independent sub-tasks
3. Scales efficiently with agent pool size
4. Adapts to varying task complexity

## Takeaways

1. **Dynamic over static**: Adaptive topologies outperform fixed communication structures for multi-agent reasoning
2. **Semantic routing**: Matching agent expertise to task requirements improves both efficiency and accuracy
3. **Sparse is sufficient**: Dense all-to-all communication is unnecessary and often harmful
4. **Scalability**: Semantic-based routing maintains efficiency as systems grow
5. **End-to-end learning**: Joint optimization of routing and reasoning yields best results
:::

:::zh
**论文**: [2602.06039](https://arxiv.org/abs/2602.06039)
**分类**: cs.AI, cs.CL, cs.MA

## 摘要

由大语言模型（LLM）驱动的多智能体系统在复杂推理任务中展现了卓越的能力。然而，现有方法通常依赖固定的通信拓扑（如链式、树形或全连接图），无法适应不同推理步骤的语义需求变化。我们提出DyTopo（动态拓扑），一个基于智能体专业知识与任务需求之间语义匹配来动态构建智能体通信拓扑的框架。

## 主要贡献

- **动态拓扑构建**：基于语义相关性自适应构建通信图，而非预定义的静态结构
- **语义匹配模块**：使用基于嵌入的相似度学习匹配智能体能力与子任务需求
- **路由机制**：基于语义亲和度分数在智能体之间高效路由信息
- **可扩展性**：通过稀疏连接在智能体数量增加时保持效率

## 静态拓扑的问题

当前多智能体推理系统使用固定的通信模式：

- **链式拓扑**：顺序信息流，并行性有限
- **树形拓扑**：层次化但结构僵化
- **全连接**：$O(n^2)$通信开销，信息过载
- **星形拓扑**：中心瓶颈，单点故障

这些静态结构无法适应：
1. 不同推理步骤的复杂度变化
2. 子问题的不同专业知识需求
3. 动态的信息依赖关系

## DyTopo框架

### 架构概述

该框架由三个主要组件组成：

1. **智能体池**：具有不同专业领域的专门化智能体
2. **语义路由器**：动态确定通信路径
3. **拓扑构建器**：构建特定任务的通信图

### 语义匹配

对于每个推理步骤，计算智能体之间的语义亲和度：

$$S_{ij} = \frac{\mathbf{e}_i^T \mathbf{q}_j}{\|\mathbf{e}_i\| \|\mathbf{q}_j\|}$$

其中$\mathbf{e}_i$是智能体$i$的专业知识嵌入，$\mathbf{q}_j$是子任务$j$的查询嵌入。

### 动态路由

路由决策使用学习的阈值：

$$A_{ij} = \mathbb{1}[S_{ij} > \tau + \sigma(\mathbf{W}[\mathbf{e}_i; \mathbf{q}_j])]$$

其中$\tau$是基础阈值，$\sigma(\cdot)$是学习的自适应组件。

### 拓扑构建

步骤$t$的通信图$G_t$构建为：

$$G_t = (V, E_t), \quad E_t = \{(i, j) : A_{ij} = 1\}$$

这创建了稀疏的、与任务相关的连接，而非密集的全对全通信。

## 方法论

### 训练目标

DyTopo使用复合损失进行端到端训练：

$$\mathcal{L} = \mathcal{L}_{task} + \lambda_1 \mathcal{L}_{sparse} + \lambda_2 \mathcal{L}_{coverage}$$

- $\mathcal{L}_{task}$：特定任务的推理准确率
- $\mathcal{L}_{sparse}$：鼓励稀疏连接以提高效率
- $\mathcal{L}_{coverage}$：确保所有相关专业知识可访问

### 智能体专门化

每个智能体维护：
- 领域特定的知识嵌入
- 推理风格偏好
- 子任务类型的历史表现

## 实验结果

### 基准测试性能

DyTopo在推理基准测试中展示了持续的改进：

| 方法 | GSM8K | MATH | HotpotQA | StrategyQA |
|------|-------|------|----------|------------|
| Chain-of-Agents | 78.2 | 42.1 | 65.3 | 71.8 |
| Tree-of-Agents | 79.5 | 43.8 | 67.1 | 73.2 |
| 全连接 | 80.1 | 44.2 | 66.8 | 72.9 |
| **DyTopo** | **82.7** | **47.3** | **70.5** | **76.4** |

### 效率分析

通信开销比较（相对于全连接）：

| 拓扑 | 消息量 | 延迟 | 准确率 |
|------|--------|------|--------|
| 全连接 | 100% | 100% | 基准 |
| 链式 | 15% | 180% | -2.1% |
| 树形 | 25% | 120% | -1.3% |
| **DyTopo** | 35% | 85% | +2.6% |

DyTopo以更少的消息和更低的延迟实现了更好的准确率。

### 消融研究

消融实验的关键发现：
- 语义匹配比随机路由提高+1.8%准确率
- 自适应阈值比固定阈值提高+0.9%
- 稀疏性正则化减少40%消息量，准确率损失极小

## 重要意义

关键洞察：**最优的智能体通信取决于任务语义**。通过学习基于语义相关性路由信息：

1. 减少无关信息交换
2. 实现独立子任务的并行处理
3. 随智能体池规模高效扩展
4. 适应不同的任务复杂度

## 要点总结

1. **动态优于静态**：自适应拓扑在多智能体推理中优于固定通信结构
2. **语义路由**：将智能体专业知识与任务需求匹配可同时提高效率和准确率
3. **稀疏即足够**：密集的全对全通信是不必要的，通常还有害
4. **可扩展性**：基于语义的路由在系统增长时保持效率
5. **端到端学习**：路由和推理的联合优化产生最佳结果
:::
