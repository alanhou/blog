---
title:
  en: "Knowing When Not to Answer: Abstention-Aware Scientific Reasoning"
  zh: "知道何时不回答:具有弃权意识的科学推理"
description:
  en: "A framework that enables language models to abstain from answering scientific claims when evidence is insufficient, reducing harmful errors in scientific verification tasks."
  zh: "一个使语言模型在证据不足时能够弃权不回答科学声明的框架,减少科学验证任务中的有害错误。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.cl", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.14189](https://arxiv.org/abs/2602.14189)
**Authors**: Samir Abdaljalil, Erchin Serpedin, Hasan Kurban
**Categories**: cs.CL, cs.AI

## Abstract

This paper introduces an abstention-aware framework for scientific claim verification that recognizes a fundamental limitation in current LLM evaluations: the assumption that models must always provide definitive answers. The authors argue that in scientific contexts, incorrect conclusions can be more damaging than admitting uncertainty. Their framework decomposes scientific claims into minimal verifiable conditions, audits each against available evidence using natural language inference, and makes selective decisions to support, refute, or abstain. Evaluated across SciFact and PubMedQA benchmarks with six diverse language models, the results demonstrate that while raw accuracy varies modestly across architectures, confidence-based abstention significantly reduces risk at moderate coverage levels, highlighting that the key challenge in scientific reasoning is determining when evidence suffices to justify an answer.

## Key Contributions

- Introduction of an abstention-aware verification framework that decomposes scientific claims into minimal conditions and selectively decides whether to answer or abstain
- Comprehensive evaluation across two complementary benchmarks (SciFact and PubMedQA) covering both closed-book and open-domain evidence settings
- Experimental analysis with six diverse language models spanning encoder-decoder, open-weight chat models, and proprietary APIs
- Demonstration that confidence-based abstention substantially reduces error risk even when absolute accuracy improvements are limited
- Evidence that model selection matters less than determining when available evidence is sufficient for reliable answers

## Methodology and Framework Design

The proposed framework operates through a three-stage pipeline designed to handle scientific claim verification with explicit abstention capabilities. First, the system decomposes complex scientific claims into minimal, atomic conditions that can be independently verified. This decomposition is crucial because scientific claims often contain multiple assertions that require separate evidence evaluation.

Second, each decomposed condition undergoes an evidence audit using natural language inference (NLI). The NLI component assesses whether available evidence entails, contradicts, or is neutral with respect to each condition. This granular approach allows the system to identify which specific aspects of a claim are supported by evidence and which lack sufficient backing.

Third, the framework employs a selective decision mechanism that aggregates the condition-level assessments and determines whether to support the overall claim, refute it, or abstain from judgment. The abstention decision is guided by confidence thresholds, allowing the system to recognize when evidence is insufficient or contradictory. This design philosophy prioritizes reliability over coverage, acknowledging that in scientific contexts, withholding judgment is often more responsible than forcing a potentially incorrect conclusion.

## Experimental Results and Analysis

The experiments reveal several critical insights about abstention-aware scientific reasoning. Across both SciFact and PubMedQA benchmarks, raw accuracy differences between models are relatively modest, typically varying by only 5-10 percentage points. This suggests that architectural choices alone do not dramatically impact baseline performance on scientific verification tasks.

However, the introduction of confidence-based abstention mechanisms reveals a different picture. When models are allowed to abstain on low-confidence predictions, error rates drop substantially at moderate coverage levels (70-85% coverage). For instance, at 80% coverage, models consistently achieve 15-25% lower error rates compared to forced-answer scenarios. This improvement is particularly pronounced in open-domain settings where evidence retrieval introduces additional uncertainty.

The results also demonstrate that abstention behavior varies significantly across model families. Encoder-decoder models tend to exhibit more conservative abstention patterns, while chat-optimized models show higher confidence even on ambiguous cases. Proprietary API models generally achieve better calibration between confidence scores and actual accuracy, suggesting that alignment training may improve epistemic awareness.

Interestingly, the framework's effectiveness is relatively model-agnostic. Even smaller open-weight models benefit substantially from abstention mechanisms, achieving risk-coverage trade-offs comparable to larger proprietary models when allowed to abstain strategically. This finding has important practical implications for deploying scientific reasoning systems in resource-constrained environments.

## Implications for Scientific AI Systems

This work challenges the prevailing evaluation paradigm in scientific NLP, which typically measures success solely through accuracy on forced-answer tasks. The authors demonstrate that this approach misses a critical dimension of reliability: knowing when not to answer. In scientific domains where incorrect information can propagate through literature and influence research directions, the cost of false positives often exceeds the cost of abstention.

The abstention-aware framework provides a more realistic model of how AI systems should interact with scientific knowledge. Rather than treating every claim as equally answerable, the system acknowledges the inherent limitations of available evidence and model capabilities. This epistemic humility is essential for building trustworthy scientific AI assistants.

The findings also have implications for model development priorities. Rather than focusing exclusively on improving raw accuracy through larger models or more training data, developers should invest in better confidence calibration and uncertainty quantification. The results suggest that even modest improvements in confidence estimation can yield substantial gains in practical reliability when combined with selective abstention.

Furthermore, the work highlights the importance of benchmark design. Future scientific reasoning benchmarks should incorporate abstention as a first-class evaluation dimension, measuring not just whether models can answer correctly, but whether they can recognize when they should not answer at all.

## Takeaways

1. In scientific reasoning tasks, the ability to abstain from answering is as important as the ability to answer correctly, as unsupported conclusions can be more harmful than admitting uncertainty.

2. Confidence-based abstention reduces error risk by 15-25% at moderate coverage levels (70-85%), even when absolute accuracy improvements are limited.

3. Raw accuracy varies only modestly across different model architectures (5-10 percentage points), suggesting that model selection alone is not the primary determinant of scientific reasoning performance.

4. The key challenge in scientific claim verification is determining when available evidence is sufficient to justify an answer, rather than simply selecting the most accurate model.

5. Abstention-aware evaluation provides a practical, model-agnostic framework for assessing scientific reliability that better reflects real-world deployment requirements than traditional forced-answer metrics.
:::

:::zh
**论文**: [2602.14189](https://arxiv.org/abs/2602.14189)
**作者**: Samir Abdaljalil, Erchin Serpedin, Hasan Kurban
**分类**: cs.CL, cs.AI

## 摘要

本文提出了一个具有弃权意识的科学声明验证框架,该框架认识到当前大语言模型评估中的一个根本性局限:假设模型必须始终提供明确答案。作者认为,在科学语境中,错误的结论可能比承认不确定性更具破坏性。他们的框架将科学声明分解为最小可验证条件,使用自然语言推理对每个条件与可用证据进行审核,并选择性地决定支持、反驳或弃权。该研究在SciFact和PubMedQA基准上使用六种不同的语言模型进行评估,结果表明,虽然不同架构间的原始准确率差异不大,但基于置信度的弃权机制在中等覆盖率水平下显著降低了风险,突显出科学推理中的关键挑战是判断何时证据足以支持答案。

## 主要贡献

- 引入了一个具有弃权意识的验证框架,将科学声明分解为最小条件并选择性地决定是否回答或弃权
- 在两个互补基准(SciFact和PubMedQA)上进行全面评估,涵盖闭卷和开放域证据设置
- 使用六种不同的语言模型进行实验分析,包括编码器-解码器模型、开放权重对话模型和专有API
- 证明基于置信度的弃权即使在绝对准确率提升有限的情况下也能显著降低错误风险
- 提供证据表明模型选择不如判断可用证据何时足以支持可靠答案重要

## 方法论与框架设计

所提出的框架通过三阶段流程运作,旨在处理具有显式弃权能力的科学声明验证。首先,系统将复杂的科学声明分解为可独立验证的最小原子条件。这种分解至关重要,因为科学声明通常包含需要单独证据评估的多个断言。

其次,每个分解的条件使用自然语言推理(NLI)进行证据审核。NLI组件评估可用证据是否蕴含、矛盾或对每个条件保持中立。这种细粒度方法使系统能够识别声明的哪些具体方面得到证据支持,哪些缺乏充分支撑。

第三,框架采用选择性决策机制,聚合条件级评估并决定是支持整体声明、反驳它还是弃权判断。弃权决策由置信度阈值引导,使系统能够识别证据不足或矛盾的情况。这种设计理念优先考虑可靠性而非覆盖率,承认在科学语境中,保留判断往往比强制得出可能错误的结论更负责任。

## 实验结果与分析

实验揭示了关于具有弃权意识的科学推理的几个关键见解。在SciFact和PubMedQA基准上,模型之间的原始准确率差异相对较小,通常仅相差5-10个百分点。这表明架构选择本身不会显著影响科学验证任务的基线性能。

然而,引入基于置信度的弃权机制后呈现出不同的景象。当允许模型对低置信度预测弃权时,在中等覆盖率水平(70-85%覆盖率)下错误率大幅下降。例如,在80%覆盖率下,与强制回答场景相比,模型始终实现15-25%的更低错误率。这种改进在开放域设置中尤为明显,因为证据检索引入了额外的不确定性。

结果还表明,不同模型家族的弃权行为差异显著。编码器-解码器模型倾向于表现出更保守的弃权模式,而针对对话优化的模型即使在模糊情况下也显示出更高的置信度。专有API模型通常在置信度分数与实际准确率之间实现更好的校准,表明对齐训练可能改善认识论意识。

有趣的是,该框架的有效性相对与模型无关。即使是较小的开放权重模型也从弃权机制中获益匪浅,当允许策略性弃权时,实现了与更大专有模型相当的风险-覆盖率权衡。这一发现对在资源受限环境中部署科学推理系统具有重要的实践意义。

## 对科学AI系统的启示

这项工作挑战了科学自然语言处理中的主流评估范式,该范式通常仅通过强制回答任务的准确率来衡量成功。作者证明这种方法忽略了可靠性的一个关键维度:知道何时不回答。在科学领域,错误信息可能通过文献传播并影响研究方向,误报的成本往往超过弃权的成本。

具有弃权意识的框架提供了AI系统应如何与科学知识交互的更现实模型。该系统不是将每个声明都视为同等可回答,而是承认可用证据和模型能力的固有局限性。这种认识论上的谦逊对于构建值得信赖的科学AI助手至关重要。

研究结果对模型开发优先级也有启示。开发者不应仅专注于通过更大模型或更多训练数据来提高原始准确率,而应投资于更好的置信度校准和不确定性量化。结果表明,即使在置信度估计方面的适度改进,当与选择性弃权结合时,也能在实际可靠性方面产生实质性收益。

此外,这项工作强调了基准设计的重要性。未来的科学推理基准应将弃权作为一流的评估维度,不仅衡量模型是否能正确回答,还要衡量它们是否能识别何时根本不应回答。

## 要点总结

1. 在科学推理任务中,弃权回答的能力与正确回答的能力同样重要,因为无根据的结论可能比承认不确定性更有害。

2. 基于置信度的弃权在中等覆盖率水平(70-85%)下将错误风险降低15-25%,即使绝对准确率提升有限。

3. 不同模型架构间的原始准确率差异仅为5-10个百分点,表明模型选择本身不是科学推理性能的主要决定因素。

4. 科学声明验证中的关键挑战是判断可用证据何时足以支持答案,而不仅仅是选择最准确的模型。

5. 具有弃权意识的评估提供了一个实用的、与模型无关的框架来评估科学可靠性,比传统的强制回答指标更好地反映了实际部署需求。
:::
