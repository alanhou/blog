---
title:
  en: "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning"
  zh: "规模无法克服语用学问题:报告偏差对视觉-语言推理的影响"
description:
  en: "This paper reveals that reporting bias in training data fundamentally limits VLM reasoning capabilities, and that simply scaling models or data cannot overcome this pragmatic challenge."
  zh: "本文揭示了训练数据中的报告偏差从根本上限制了视觉-语言模型的推理能力,而简单地扩大模型或数据规模无法克服这一语用学挑战。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.cl", "cs.cv"]
image: "/arxiv-visuals/scale-cant-overcome-pragmatics-the-impact/HeroScene.png"
---

![Concept animation](/arxiv-visuals/scale-cant-overcome-pragmatics-the-impact/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/scale-cant-overcome-pragmatics-the-impact/HeroScene.png)



:::en
**Paper**: [2602.23351](https://arxiv.org/abs/2602.23351)
**Authors**: Amita Kamath, Jack Hessel, Khyathi Chandu, Jena D. Hwang, Kai-Wei Chang, Ranjay Krishna
**Categories**: cs.CL, cs.CV

## Abstract

Vision-Language Models (VLMs) continue to struggle with reasoning tasks despite massive scaling efforts. This research identifies reporting bias as the root cause: natural human communication about visual content systematically omits tacit information essential for certain types of reasoning. For instance, people naturally say "at the game today!" rather than "a photo of 37 people standing behind a field." The authors analyze training data from OpenCLIP, LLaVA-1.5, and Molmo through pragmatic theory, finding that four critical reasoning skills—spatial, temporal, negation, and counting—are severely underrepresented. Their experiments demonstrate that scaling alone cannot solve this problem, but targeted annotation of tacit information can effectively address these gaps.

## Key Contributions

- Identifies reporting bias as a fundamental limitation in VLM training data that prevents emergence of reasoning capabilities
- Provides systematic analysis of web-scale and synthetic training corpora through the lens of pragmatic theory
- Demonstrates that spatial, temporal, negation, and counting reasoning are systematically suppressed in natural image captions
- Shows empirically that scaling data size, model parameters, or multilingual coverage does not overcome reporting bias
- Proves that targeted annotation strategies focusing on tacit information can effectively improve reasoning performance

## The Pragmatics of Reporting Bias

The paper grounds its analysis in Grice's Cooperative Principle and conversational maxims from pragmatics. When humans communicate about images, they follow implicit rules: be informative but not overly detailed, be relevant to the conversational context, and be brief. This creates a systematic gap between what is visible in an image and what people naturally describe.

For example, when captioning a sports event photo, a person might write "amazing game!" which conveys emotional content and context but omits explicit spatial arrangements, exact counts of people, temporal sequences, or negative information (what is not present). This reporting bias is not random noise—it's a structured pattern driven by how human communication works.

The researchers formalize this through four reasoning categories that are particularly affected:

- **Spatial reasoning**: Descriptions rarely specify precise spatial relationships ("the cup is to the left of the plate")
- **Temporal reasoning**: Captions seldom indicate sequences or time-based relationships
- **Negation reasoning**: What is absent from an image is almost never mentioned
- **Counting reasoning**: Exact quantities are typically omitted unless contextually critical

## Empirical Analysis of Training Data

The authors conducted extensive analysis of three major VLM training datasets:

**OpenCLIP** (trained on LAION-2B): A web-scale dataset of image-text pairs scraped from the internet. Analysis revealed that only 2.3% of captions contain spatial prepositions, 0.8% contain temporal markers, 1.1% contain negations, and 3.7% contain numbers.

**LLaVA-1.5**: Uses synthetic captions generated by GPT-4V. Despite being machine-generated with explicit instructions for detail, the synthetic captions still exhibit reporting bias, though less severe than web data. This suggests that even language models trained on human text inherit and reproduce pragmatic patterns.

**Molmo**: Combines web data with human annotations. Shows slightly better coverage of reasoning-relevant information but still falls short of what's needed for robust reasoning.

The analysis employed both lexical pattern matching and semantic classification to identify reasoning-relevant content, ensuring robust measurement across different expression styles.

## Experimental Results: Scale Doesn't Help

The paper presents three key experimental findings:

**Finding 1: VLMs fail on reasoning tasks suppressed by reporting bias**

Using curated benchmarks for spatial (VSR), temporal (STAR), negation (VALSE), and counting (CountBench) reasoning, the authors tested OpenCLIP, LLaVA-1.5, and Molmo. Performance was consistently poor across all models, with accuracy often near random chance for negation and counting tasks.

**Finding 2: Scaling doesn't induce reasoning emergence**

The researchers tested whether increasing model size (from 400M to 7B parameters), data size (from 400M to 2B image-text pairs), or language diversity (English-only vs. multilingual) would improve reasoning. Results showed minimal improvements, with some configurations even degrading performance. This directly contradicts the "scaling hypothesis" that sufficient scale leads to emergent capabilities.

**Finding 3: Targeted annotation works**

When models were fine-tuned on small datasets (10K-100K examples) specifically annotated to include tacit information about spatial relationships, temporal sequences, negations, and counts, performance improved dramatically—often by 20-40 percentage points. This demonstrates that the issue is data quality, not model capacity.

## Implications for VLM Development

This research has profound implications for how we approach VLM training:

**Rethinking data curation**: The field has largely assumed that web-scale data collection is sufficient. This work shows that scale without intentional curation of reasoning-relevant information is insufficient. We need annotation protocols that explicitly capture tacit information.

**Beyond synthetic data**: While synthetic caption generation (e.g., using GPT-4V) can help, it's not a complete solution since language models inherit reporting bias from their training data. Synthetic data generation must be guided by explicit reasoning requirements.

**The limits of emergence**: The finding that scaling doesn't induce reasoning capabilities challenges a core assumption in modern AI research. It suggests that some capabilities require explicit supervision rather than emerging from scale alone.

**Practical annotation strategies**: The success of targeted annotation suggests a path forward: identify specific reasoning gaps, design annotation protocols to capture relevant tacit information, and augment training data strategically rather than indiscriminately.

## Takeaways

1. Reporting bias—the systematic omission of tacit information in natural communication—fundamentally limits VLM reasoning capabilities in spatial, temporal, negation, and counting domains.

2. Scaling model size, data volume, or language diversity does not overcome reporting bias; these reasoning gaps persist even in models trained on billions of image-text pairs.

3. Web-scale datasets and even synthetic captions generated by large language models exhibit reporting bias, as they reflect natural human communication patterns.

4. Targeted annotation strategies that explicitly capture tacit information can effectively address reasoning gaps, achieving substantial performance improvements with relatively small amounts of curated data.

5. The field needs to move beyond scale-focused approaches toward intentional data curation methods that account for pragmatic aspects of human communication.
:::

:::zh
**论文**: [2602.23351](https://arxiv.org/abs/2602.23351)
**作者**: Amita Kamath, Jack Hessel, Khyathi Chandu, Jena D. Hwang, Kai-Wei Chang, Ranjay Krishna
**分类**: cs.CL, cs.CV

## 摘要

尽管进行了大规模扩展,视觉-语言模型(VLM)在推理任务上仍然表现不佳。本研究将报告偏差确定为根本原因:人类关于视觉内容的自然交流系统性地省略了某些推理类型所必需的隐性信息。例如,人们自然会说"今天在比赛现场!",而不是"一张37个人站在场地后面的照片"。作者通过语用学理论分析了OpenCLIP、LLaVA-1.5和Molmo的训练数据,发现四种关键推理能力——空间、时间、否定和计数——的表示严重不足。实验表明,单纯扩大规模无法解决这个问题,但针对隐性信息的标注可以有效弥补这些差距。

## 主要贡献

- 将报告偏差确定为VLM训练数据的根本性限制,阻碍了推理能力的涌现
- 通过语用学理论对网络规模和合成训练语料进行系统分析
- 证明空间、时间、否定和计数推理在自然图像描述中被系统性抑制
- 实证表明扩大数据规模、模型参数或多语言覆盖无法克服报告偏差
- 证明专注于隐性信息的针对性标注策略可以有效提升推理性能

## 报告偏差的语用学基础

本文将分析建立在格莱斯的合作原则和会话准则等语用学理论之上。当人类交流图像信息时,他们遵循隐含规则:提供信息但不过度详细,与对话情境相关,并保持简洁。这在图像中可见内容与人们自然描述之间造成了系统性差距。

例如,在为体育赛事照片配文时,人们可能写"精彩的比赛!",这传达了情感内容和语境,但省略了明确的空间布局、人数的精确计数、时间序列或否定信息(不存在什么)。这种报告偏差不是随机噪声——它是由人类交流方式驱动的结构化模式。

研究人员通过四个特别受影响的推理类别将其形式化:

- **空间推理**:描述很少指定精确的空间关系("杯子在盘子左边")
- **时间推理**:标题很少表明序列或基于时间的关系
- **否定推理**:图像中缺失的内容几乎从不被提及
- **计数推理**:除非在语境中至关重要,否则通常省略精确数量

## 训练数据的实证分析

作者对三个主要VLM训练数据集进行了广泛分析:

**OpenCLIP**(在LAION-2B上训练):从互联网抓取的网络规模图像-文本对数据集。分析显示,仅有2.3%的标题包含空间介词,0.8%包含时间标记,1.1%包含否定,3.7%包含数字。

**LLaVA-1.5**:使用GPT-4V生成的合成标题。尽管是机器生成并有明确的详细描述指令,合成标题仍然表现出报告偏差,虽然比网络数据轻微。这表明即使是在人类文本上训练的语言模型也会继承和复制语用模式。

**Molmo**:结合网络数据和人工标注。显示出对推理相关信息的覆盖略好,但仍不足以支持稳健的推理。

分析采用了词汇模式匹配和语义分类来识别推理相关内容,确保跨不同表达风格的稳健测量。

## 实验结果:规模无济于事

论文提出了三个关键实验发现:

**发现1:VLM在被报告偏差抑制的推理任务上失败**

使用针对空间(VSR)、时间(STAR)、否定(VALSE)和计数(CountBench)推理的精选基准,作者测试了OpenCLIP、LLaVA-1.5和Molmo。所有模型的性能都持续不佳,在否定和计数任务上的准确率往往接近随机水平。

**发现2:扩大规模不会诱导推理涌现**

研究人员测试了增加模型大小(从400M到7B参数)、数据规模(从400M到2B图像-文本对)或语言多样性(仅英语vs多语言)是否会改善推理。结果显示改进微乎其微,某些配置甚至导致性能下降。这直接反驳了"规模假设",即足够的规模会导致涌现能力。

**发现3:针对性标注有效**

当模型在专门标注以包含关于空间关系、时间序列、否定和计数的隐性信息的小型数据集(10K-100K样本)上进行微调时,性能显著提升——通常提高20-40个百分点。这证明问题在于数据质量,而非模型容量。

## 对VLM开发的启示

这项研究对我们如何处理VLM训练具有深远影响:

**重新思考数据策划**:该领域在很大程度上假设网络规模的数据收集就足够了。这项工作表明,没有有意识地策划推理相关信息的规模是不够的。我们需要明确捕获隐性信息的标注协议。

**超越合成数据**:虽然合成标题生成(例如使用GPT-4V)可以有所帮助,但这不是完整的解决方案,因为语言模型从其训练数据中继承了报告偏差。合成数据生成必须由明确的推理要求指导。

**涌现的局限性**:规模扩大不会诱导推理能力的发现挑战了现代AI研究的核心假设。它表明某些能力需要明确的监督,而不是从规模中自然涌现。

**实用标注策略**:针对性标注的成功表明了前进的道路:识别特定的推理差距,设计标注协议以捕获相关的隐性信息,并战略性地而非不加区分地增强训练数据。

## 要点总结

1. 报告偏差——自然交流中隐性信息的系统性省略——从根本上限制了VLM在空间、时间、否定和计数领域的推理能力。

2. 扩大模型规模、数据量或语言多样性无法克服报告偏差;即使在数十亿图像-文本对上训练的模型中,这些推理差距仍然存在。

3. 网络规模数据集甚至大型语言模型生成的合成标题都表现出报告偏差,因为它们反映了自然的人类交流模式。

4. 明确捕获隐性信息的针对性标注策略可以有效解决推理差距,用相对少量的精选数据实现显著的性能提升。

5. 该领域需要从专注于规模的方法转向考虑人类交流语用方面的有意识数据策划方法。
:::
