---
title:
  en: "Symmetry in Language Statistics Shapes the Geometry of Model Representations"
  zh: "语言统计中的对称性塑造模型表征的几何结构"
description:
  en: "This paper proves that translation symmetries in language co-occurrence statistics mathematically determine the emergence of geometric structures like circles and manifolds in neural network embeddings."
  zh: "本文证明了语言共现统计中的平移对称性如何从数学上决定神经网络嵌入中圆形和流形等几何结构的涌现。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.lg", "cond-mat.dis-nn", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15029](https://arxiv.org/abs/2602.15029)  
**Authors**: Dhruva Karkada, Daniel J. Korchinski, Andres Nava, Matthieu Wyart, Yasaman Bahri  
**Categories**: cs.LG, cond-mat.dis-nn, cs.CL

## Abstract

Neural network representations exhibit remarkable geometric structures that remain theoretically unexplained. This paper establishes a rigorous mathematical connection between statistical symmetries in language data and the geometric organization of learned embeddings. The authors prove that translation symmetry in word co-occurrence statistics—where the probability of two words appearing together depends only on their relative relationship rather than absolute position—directly determines the emergence of simple geometric structures in high-dimensional embedding spaces. Examples include calendar months forming circles, years organizing along smooth one-dimensional manifolds, and geographic coordinates being linearly decodable. Remarkably, these structures persist even under strong perturbations to the training data and at moderate embedding dimensions, a robustness the authors explain through the presence of underlying continuous latent variables controlling co-occurrence patterns.

## Key Contributions

- Establishes a mathematical proof linking translation symmetry in co-occurrence statistics to geometric structures in embeddings
- Demonstrates that circular and manifold structures emerge naturally from symmetric statistical patterns in language
- Shows empirically that geometric structures are robust to significant perturbations in training data
- Introduces a theoretical framework explaining robustness through continuous latent variables
- Validates the framework across word embeddings (Word2Vec, GloVe), text embeddings (BERT, RoBERTa), and large language models (GPT-2, Llama)

## Theoretical Framework

The core insight revolves around translation symmetry in co-occurrence matrices. For a set of words with an underlying ordering (like months or years), the co-occurrence probability $P(w_i, w_j)$ exhibits translation symmetry when it depends only on $|i - j|$ rather than the specific values of $i$ and $j$. Mathematically:

$$P(w_i, w_j) = f(|i - j|)$$

The authors prove that when embedding models like Word2Vec or GloVe optimize objectives based on such symmetric co-occurrence statistics, the learned embeddings $\mathbf{v}_i$ naturally organize into geometric structures. For periodic symmetries (like months repeating annually), embeddings form circles. For monotonic relationships (like years or temperatures), they form smooth one-dimensional manifolds.

The key mathematical result shows that the embedding geometry is determined by the spectral properties of the co-occurrence matrix. When this matrix has translation symmetry, its eigenvectors have specific harmonic structures that force the embeddings into predictable geometric configurations. This holds even in high-dimensional spaces where embeddings have hundreds of dimensions, because the relevant structure concentrates in a low-dimensional subspace.

## Robustness and Latent Variables

A surprising finding is that geometric structures persist even when co-occurrence statistics are heavily perturbed. The authors demonstrate this by removing all sentences where certain word pairs appear together, yet the circular structure of months remains intact. This robustness cannot be explained by direct co-occurrence alone.

The paper introduces a generative model where co-occurrence statistics are controlled by continuous latent variables. For example, months co-occur not just because of their calendar proximity, but because they share underlying seasonal, cultural, and temporal contexts. These latent variables create a "soft" translation symmetry that is more robust than exact statistical patterns.

Formally, if words are associated with latent variables $\theta_i$ on a continuous space, and co-occurrence probabilities depend on $|\theta_i - \theta_j|$, then even with noisy or incomplete observations, the learned embeddings will approximate the latent space geometry. This explains why removing direct co-occurrences doesn't destroy the structure—the latent variable relationships persist through indirect statistical dependencies.

## Empirical Validation

The authors validate their theory across multiple model architectures and scales:

**Word Embeddings**: Word2Vec and GloVe trained on Wikipedia and Common Crawl show clear circular structures for months and linear structures for years, consistent with theoretical predictions.

**Text Embeddings**: Contextual models like BERT and RoBERTa, which generate context-dependent representations, still exhibit these geometric structures in their token embeddings, suggesting the phenomenon is fundamental rather than architecture-specific.

**Large Language Models**: Even in models like GPT-2 and Llama with billions of parameters, the geometric structures emerge in the residual stream representations. The authors use linear probes to decode latent variables (like month position or geographic coordinates) and find that accuracy remains high even with perturbed training data.

Dimensionality analysis reveals that these structures are robust down to surprisingly low embedding dimensions (around 50-100), though they become clearer in higher dimensions. This suggests the geometric organization is a fundamental feature of how neural networks compress statistical regularities.

## Takeaways

1. Geometric structures in neural embeddings are not accidental but mathematically determined by symmetries in the training data's statistical patterns
2. Translation symmetry in co-occurrence statistics directly predicts whether embeddings will form circles, lines, or other manifolds
3. The robustness of these structures under data perturbation indicates they capture deep statistical regularities mediated by latent variables
4. This framework applies across model architectures from simple word embeddings to large language models with billions of parameters
5. Understanding these geometric principles could inform better embedding design, interpretability methods, and data efficiency in training neural networks
:::

:::zh
**论文**: [2602.15029](https://arxiv.org/abs/2602.15029)  
**作者**: Dhruva Karkada, Daniel J. Korchinski, Andres Nava, Matthieu Wyart, Yasaman Bahri  
**分类**: cs.LG, cond-mat.dis-nn, cs.CL

## 摘要

神经网络表征展现出令人瞩目的几何结构,但其理论基础仍未得到充分解释。本文在语言数据的统计对称性与学习嵌入的几何组织之间建立了严格的数学联系。作者证明了词共现统计中的平移对称性——即两个词共同出现的概率仅取决于它们的相对关系而非绝对位置——直接决定了高维嵌入空间中简单几何结构的涌现。例如日历月份形成圆形、年份沿光滑一维流形组织、地理坐标可被线性探测解码。值得注意的是,这些结构即使在训练数据受到强烈扰动和中等嵌入维度下仍然保持,作者通过控制共现模式的底层连续潜变量来解释这种鲁棒性。

## 主要贡献

- 建立了共现统计中平移对称性与嵌入几何结构之间的数学证明
- 展示了圆形和流形结构如何从语言中的对称统计模式自然涌现
- 实证表明几何结构对训练数据的显著扰动具有鲁棒性
- 引入通过连续潜变量解释鲁棒性的理论框架
- 在词嵌入(Word2Vec、GloVe)、文本嵌入(BERT、RoBERTa)和大语言模型(GPT-2、Llama)中验证该框架

## 理论框架

核心洞察围绕共现矩阵中的平移对称性展开。对于具有内在顺序的词集(如月份或年份),当共现概率$P(w_i, w_j)$仅依赖于$|i - j|$而非$i$和$j$的具体值时,就表现出平移对称性。数学表达为:

$$P(w_i, w_j) = f(|i - j|)$$

作者证明,当Word2Vec或GloVe等嵌入模型基于这种对称共现统计优化目标时,学习到的嵌入$\mathbf{v}_i$自然组织成几何结构。对于周期性对称(如每年重复的月份),嵌入形成圆形。对于单调关系(如年份或温度),它们形成光滑的一维流形。

关键数学结果表明,嵌入几何由共现矩阵的谱性质决定。当该矩阵具有平移对称性时,其特征向量具有特定的谐波结构,迫使嵌入进入可预测的几何配置。即使在嵌入具有数百维的高维空间中这也成立,因为相关结构集中在低维子空间中。

## 鲁棒性与潜变量

一个令人惊讶的发现是,即使共现统计受到严重扰动,几何结构仍然保持。作者通过移除所有包含特定词对的句子来证明这一点,但月份的圆形结构仍然完整。这种鲁棒性无法仅通过直接共现来解释。

论文引入了一个生成模型,其中共现统计由连续潜变量控制。例如,月份共现不仅因为它们在日历上的接近性,还因为它们共享底层的季节、文化和时间背景。这些潜变量创造了比精确统计模式更鲁棒的"软"平移对称性。

形式上,如果词与连续空间上的潜变量$\theta_i$相关联,且共现概率依赖于$|\theta_i - \theta_j|$,那么即使观测有噪声或不完整,学习到的嵌入也会近似潜空间几何。这解释了为什么移除直接共现不会破坏结构——潜变量关系通过间接统计依赖性持续存在。

## 实证验证

作者在多种模型架构和规模上验证了他们的理论:

**词嵌入**:在维基百科和Common Crawl上训练的Word2Vec和GloVe显示出月份的清晰圆形结构和年份的线性结构,与理论预测一致。

**文本嵌入**:生成上下文相关表征的BERT和RoBERTa等上下文模型,在其词元嵌入中仍然表现出这些几何结构,表明该现象是基础性的而非特定于架构。

**大语言模型**:即使在具有数十亿参数的GPT-2和Llama等模型中,几何结构也在残差流表征中涌现。作者使用线性探测来解码潜变量(如月份位置或地理坐标),发现即使训练数据受到扰动,准确率仍然很高。

维度分析表明,这些结构在低至50-100的嵌入维度下具有鲁棒性,尽管在更高维度中变得更清晰。这表明几何组织是神经网络压缩统计规律性的基本特征。

## 要点总结

1. 神经嵌入中的几何结构并非偶然,而是由训练数据统计模式中的对称性在数学上决定的
2. 共现统计中的平移对称性直接预测嵌入是否会形成圆形、直线或其他流形
3. 这些结构在数据扰动下的鲁棒性表明它们捕获了由潜变量介导的深层统计规律
4. 该框架适用于从简单词嵌入到具有数十亿参数的大语言模型的各种模型架构
5. 理解这些几何原理可以为更好的嵌入设计、可解释性方法和神经网络训练中的数据效率提供指导
:::
