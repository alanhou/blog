---
title:
  en: "Diffusion-Pretrained Dense and Contextual Embeddings for Web-Scale Retrieval"
  zh: "基于扩散预训练的密集与上下文嵌入模型用于网络规模检索"
description:
  en: "pplx-embed introduces diffusion-pretrained embedding models with bidirectional attention and late chunking for superior retrieval performance, achieving state-of-the-art results on contextualized embedding benchmarks."
  zh: "pplx-embed 推出基于扩散预训练的嵌入模型,采用双向注意力和延迟分块技术实现卓越检索性能,在上下文化嵌入基准测试中达到最先进水平。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.lg", "cs.cl", "cs.ir"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.11151](https://arxiv.org/abs/2602.11151)
**Authors**: Sedigheh Eslami, Maksim Gaiduk, Markus Krimmel, Louis Milliken, Bo Wang, Denis Bykov
**Categories**: cs.LG, cs.CL, cs.IR

## Abstract

This paper presents pplx-embed, a novel family of multilingual embedding models designed for web-scale retrieval tasks. The key innovation lies in leveraging diffusion-based pretraining to create language model backbones with full bidirectional attention, which are then fine-tuned using multi-stage contrastive learning. Unlike traditional encoder-only models, the diffusion pretraining approach enables comprehensive bidirectional context capture within passages, allowing for effective mean pooling and a late chunking strategy that preserves global document context across long texts. The authors release two variants: pplx-embed-v1 for standard dense retrieval and pplx-embed-context-v1 for contextualized embeddings that incorporate document-level context into passage representations. Extensive evaluation demonstrates competitive performance on major benchmarks including MTEB, MIRACL, and BERGEN, with pplx-embed-context-v1 achieving state-of-the-art results on the ConTEB contextualized embedding benchmark.

## Key Contributions

- Introduction of diffusion-pretrained language models as embedding backbones, enabling true bidirectional attention for improved context capture
- Development of a late chunking strategy that preserves global document context when processing long documents
- Release of two model variants: pplx-embed-v1 for standard retrieval and pplx-embed-context-v1 for contextualized embeddings
- State-of-the-art performance on ConTEB benchmark for contextualized embeddings
- Validation on real-world, large-scale search scenarios with tens of millions of documents

## Technical Methodology

The pplx-embed architecture builds upon diffusion-pretrained language models, which differ fundamentally from traditional masked language models or causal decoders. Diffusion pretraining involves training models to denoise corrupted text, naturally encouraging bidirectional attention patterns that capture both forward and backward context simultaneously. This bidirectional capability is crucial for embedding quality, as it allows the model to understand each token in the full context of its surrounding passage.

The training process employs multi-stage contrastive learning, where the model learns to distinguish relevant passages from irrelevant ones through carefully constructed positive and negative pairs. The contrastive objective can be formalized as maximizing the similarity between query $q$ and relevant passage $p^+$ while minimizing similarity with negative passages $p^-$:

$$\mathcal{L} = -\log \frac{\exp(\text{sim}(q, p^+)/\tau)}{\exp(\text{sim}(q, p^+)/\tau) + \sum_{i} \exp(\text{sim}(q, p^-_i)/\tau)}$$

where $\tau$ is a temperature parameter and $\text{sim}(\cdot, \cdot)$ represents cosine similarity.

For long document processing, the late chunking strategy splits documents after embedding computation rather than before, allowing each chunk to benefit from the full document context during encoding. This approach contrasts with traditional early chunking methods that process each segment independently, losing valuable cross-segment information.

## Experimental Results and Benchmarks

pplx-embed-v1 demonstrates competitive performance across multiple established benchmarks. On MTEB (Multilingual, v2), the model achieves strong results in retrieval tasks spanning diverse languages and domains. The MTEB (Code) benchmark evaluates code search capabilities, where pplx-embed-v1 shows robust performance in matching natural language queries to code snippets. On MIRACL, a multilingual retrieval benchmark covering 18 languages, the model maintains consistent quality across linguistic boundaries.

The contextualized variant, pplx-embed-context-v1, specifically targets scenarios where passage understanding requires document-level context. On the ConTEB benchmark, which evaluates contextualized embeddings, this model achieves state-of-the-art results, demonstrating the effectiveness of incorporating global document information into passage representations. This is particularly valuable for tasks like question answering over long documents, where understanding a passage often requires knowledge of surrounding content.

Beyond public benchmarks, the authors report strong performance on internal evaluation suites designed to simulate real-world search scenarios at scale. These evaluations involve retrieval over tens of millions of documents, testing both quality and efficiency in production-like environments. The results validate that pplx-embed models can maintain high retrieval quality while meeting the latency and throughput requirements of large-scale deployment.

## Implications for Retrieval Systems

The introduction of diffusion-pretrained embeddings represents a significant architectural shift in dense retrieval. Traditional embedding models typically rely on BERT-style masked language model pretraining or GPT-style causal pretraining, both of which have limitations for bidirectional context modeling. Diffusion pretraining offers a compelling alternative that naturally encourages comprehensive bidirectional understanding.

The late chunking strategy addresses a long-standing challenge in document retrieval: how to effectively represent long documents while preserving coherence across segments. By maintaining document-level context during embedding computation, this approach enables more semantically meaningful chunk representations. This has practical implications for applications like legal document search, scientific literature retrieval, and enterprise knowledge bases where documents often exceed typical context window limits.

The dual-model release strategy—offering both standard and contextualized variants—acknowledges that different retrieval scenarios have different requirements. Standard dense retrieval benefits from passage-level embeddings that can be precomputed and indexed efficiently. Contextualized embeddings, while potentially more computationally expensive, provide superior quality when document context is crucial for understanding.

## Takeaways

1. Diffusion-based pretraining provides a novel and effective approach for creating embedding model backbones with true bidirectional attention capabilities
2. Late chunking strategies that preserve global document context during embedding computation significantly improve long document retrieval quality
3. pplx-embed-context-v1 achieves state-of-the-art results on contextualized embedding benchmarks, demonstrating the value of document-level context integration
4. The models demonstrate strong performance not only on public benchmarks but also in real-world, large-scale search scenarios with tens of millions of documents
5. The release of both standard and contextualized variants provides flexibility for different retrieval use cases and computational constraints
:::

:::zh
**论文**: [2602.11151](https://arxiv.org/abs/2602.11151)
**作者**: Sedigheh Eslami, Maksim Gaiduk, Markus Krimmel, Louis Milliken, Bo Wang, Denis Bykov
**分类**: cs.LG, cs.CL, cs.IR

## 摘要

本文提出了 pplx-embed,这是一个专为网络规模检索任务设计的多语言嵌入模型家族。其核心创新在于利用基于扩散的预训练方法创建具有完整双向注意力的语言模型主干,然后通过多阶段对比学习进行微调。与传统的纯编码器模型不同,扩散预训练方法能够在段落内捕获全面的双向上下文,从而实现有效的均值池化和延迟分块策略,在长文本中保留全局文档上下文。作者发布了两个变体:用于标准密集检索的 pplx-embed-v1 和用于上下文化嵌入的 pplx-embed-context-v1,后者将文档级上下文整合到段落表示中。广泛的评估表明,该模型在 MTEB、MIRACL 和 BERGEN 等主要基准测试中表现出色,其中 pplx-embed-context-v1 在 ConTEB 上下文化嵌入基准测试中达到了最先进的结果。

## 主要贡献

- 引入扩散预训练语言模型作为嵌入主干,实现真正的双向注意力以改善上下文捕获能力
- 开发延迟分块策略,在处理长文档时保留全局文档上下文
- 发布两个模型变体:用于标准检索的 pplx-embed-v1 和用于上下文化嵌入的 pplx-embed-context-v1
- 在 ConTEB 上下文化嵌入基准测试中达到最先进性能
- 在包含数千万文档的真实大规模搜索场景中验证模型有效性

## 技术方法论

pplx-embed 架构建立在扩散预训练语言模型之上,这与传统的掩码语言模型或因果解码器有本质区别。扩散预训练涉及训练模型对损坏的文本进行去噪,这自然地鼓励了双向注意力模式,能够同时捕获前向和后向上下文。这种双向能力对于嵌入质量至关重要,因为它允许模型在其周围段落的完整上下文中理解每个标记。

训练过程采用多阶段对比学习,模型通过精心构建的正负样本对学习区分相关段落和不相关段落。对比目标可以形式化为最大化查询 $q$ 与相关段落 $p^+$ 之间的相似度,同时最小化与负样本段落 $p^-$ 的相似度:

$$\mathcal{L} = -\log \frac{\exp(\text{sim}(q, p^+)/\tau)}{\exp(\text{sim}(q, p^+)/\tau) + \sum_{i} \exp(\text{sim}(q, p^-_i)/\tau)}$$

其中 $\tau$ 是温度参数,$\text{sim}(\cdot, \cdot)$ 表示余弦相似度。

对于长文档处理,延迟分块策略在嵌入计算之后而不是之前分割文档,使每个块在编码过程中都能受益于完整的文档上下文。这种方法与传统的早期分块方法形成对比,后者独立处理每个片段,丢失了宝贵的跨片段信息。

## 实验结果与基准测试

pplx-embed-v1 在多个已建立的基准测试中展现出竞争力。在 MTEB(多语言,v2)上,该模型在跨越不同语言和领域的检索任务中取得了优异成绩。MTEB(代码)基准测试评估代码搜索能力,pplx-embed-v1 在将自然语言查询匹配到代码片段方面表现稳健。在涵盖 18 种语言的多语言检索基准 MIRACL 上,该模型在跨语言边界保持了一致的质量。

上下文化变体 pplx-embed-context-v1 专门针对段落理解需要文档级上下文的场景。在评估上下文化嵌入的 ConTEB 基准测试中,该模型达到了最先进的结果,证明了将全局文档信息整合到段落表示中的有效性。这对于长文档问答等任务特别有价值,在这些任务中,理解一个段落通常需要了解周围内容。

除了公开基准测试外,作者还报告了在内部评估套件上的强劲表现,这些套件旨在模拟大规模的真实搜索场景。这些评估涉及在数千万文档上进行检索,测试生产类环境中的质量和效率。结果验证了 pplx-embed 模型能够在满足大规模部署的延迟和吞吐量要求的同时保持高检索质量。

## 对检索系统的影响

扩散预训练嵌入的引入代表了密集检索领域的重大架构转变。传统嵌入模型通常依赖于 BERT 风格的掩码语言模型预训练或 GPT 风格的因果预训练,这两者在双向上下文建模方面都有局限性。扩散预训练提供了一个令人信服的替代方案,自然地鼓励全面的双向理解。

延迟分块策略解决了文档检索中的一个长期挑战:如何在保持片段间连贯性的同时有效表示长文档。通过在嵌入计算期间维护文档级上下文,这种方法能够生成更具语义意义的块表示。这对法律文档搜索、科学文献检索和企业知识库等应用具有实际意义,在这些应用中,文档通常超过典型的上下文窗口限制。

双模型发布策略——提供标准和上下文化变体——承认不同的检索场景有不同的需求。标准密集检索受益于可以高效预计算和索引的段落级嵌入。上下文化嵌入虽然计算成本可能更高,但在文档上下文对理解至关重要时提供了卓越的质量。

## 要点总结

1. 基于扩散的预训练为创建具有真正双向注意力能力的嵌入模型主干提供了一种新颖而有效的方法
2. 在嵌入计算期间保留全局文档上下文的延迟分块策略显著提高了长文档检索质量
3. pplx-embed-context-v1 在上下文化嵌入基准测试中达到最先进结果,证明了文档级上下文整合的价值
4. 这些模型不仅在公开基准测试中表现出色,而且在包含数千万文档的真实大规模搜索场景中也展现了强劲性能
5. 标准和上下文化变体的发布为不同的检索用例和计算约束提供了灵活性
:::
