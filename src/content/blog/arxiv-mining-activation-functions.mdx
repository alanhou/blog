---
title:
  en: "Mining Generalizable Activation Functions via Evolutionary Search"
  zh: "通过进化搜索挖掘可泛化的激活函数"
description:
  en: "Google DeepMind uses LLM-guided evolutionary search to discover novel activation functions that outperform ReLU on out-of-distribution benchmarks"
  zh: "Google DeepMind使用LLM引导的进化搜索发现在分布外基准上优于ReLU的新型激活函数"
date: 2026-02-05
tags: ["arxiv", "ai", "activation-functions", "evolutionary-search", "deepmind", "neural-architecture", "cs.LG", "cs.AI"]
image: "/arxiv-visuals/arxiv-mining-activation-functions.png"
---

:::en
**Paper**: [2602.05688](https://arxiv.org/abs/2602.05688)
**Authors**: Alex Vitvitskyi, Michael Boratko, Matej Grcic, Razvan Pascanu, Deep Shah, Petar Veličković
**Affiliations**: Google DeepMind
**Categories**: cs.LG, cs.AI

## Abstract

This paper investigates evolutionary search as a framework for discovering activation functions. Using frontier LLMs as mutation operators (via AlphaEvolve), the search space expands to all possible Python functions within a given FLOP budget, eliminating manual constraint design. Out-of-distribution performance serves as the fitness metric to discover activation functions that encode specific inductive biases.

## Key Contributions

- **LLM-guided evolutionary search**: Uses frontier LLMs as mutation operators via AlphaEvolve
- **Expanded search space**: Searches over all possible Python functions within a FLOP budget
- **OOD fitness metric**: Targets out-of-distribution robustness rather than in-distribution performance
- **Practical discoveries**: Found GELU-Sinc-Perturbation, GELUSine, GMTU, and Turbulent Activation Function

## The Activation Function Design Problem

Activation functions are a fundamental building block of neural networks, yet the design space has been explored mostly through human intuition: ReLU, GELU, SiLU/Swish, and a handful of others. Each was designed by hand with specific properties in mind.

Can we do better by searching systematically?

## AlphaEvolve-Powered Search

The key innovation is using **frontier LLMs as mutation operators** within an evolutionary search framework (AlphaEvolve). This has two advantages:

1. **Unconstrained search space**: Instead of searching over parameterized function families, the search covers all Python functions within a FLOP budget
2. **Semantic mutations**: LLMs can propose meaningful modifications to activation functions, not just random perturbations

## OOD Fitness: The Right Objective

A critical design choice: using **out-of-distribution performance** as the fitness metric rather than in-distribution accuracy. This targets activation functions that encode useful inductive biases — functions that help the network generalize, not just memorize.

## Discovered Functions

The search yielded several novel activation functions:

- **GELU-Sinc-Perturbation**: GELU with sinc-function perturbation
- **GELUSine**: GELU-sine hybrid
- **GMTU**: Gated multiplicative trigonometric unit
- **Turbulent Activation Function**: A more complex composition

These outperform ReLU on out-of-distribution benchmarks while maintaining competitive in-distribution performance.

## Takeaways

1. **LLMs as search operators**: Using LLMs for evolutionary search over code is a powerful paradigm
2. **Small-scale discovery transfers**: Functions found on small synthetic datasets generalize to larger problems
3. **OOD as objective**: Targeting generalization during search produces more robust components
4. **Activation functions are not solved**: There's still room for improvement in this fundamental building block
:::

:::zh
**论文**: [2602.05688](https://arxiv.org/abs/2602.05688)
**作者**: Alex Vitvitskyi, Michael Boratko, Matej Grcic, Razvan Pascanu, Deep Shah, Petar Veličković
**机构**: Google DeepMind
**分类**: cs.LG, cs.AI

## 摘要

本文研究进化搜索作为发现激活函数的框架。使用前沿LLM作为变异算子（通过AlphaEvolve），搜索空间扩展到给定FLOP预算内的所有可能Python函数，消除了手动约束设计。分布外性能作为适应度指标，以发现编码特定归纳偏置的激活函数。

## 主要贡献

- **LLM引导的进化搜索**：通过AlphaEvolve使用前沿LLM作为变异算子
- **扩展搜索空间**：在FLOP预算内搜索所有可能的Python函数
- **OOD适应度指标**：针对分布外鲁棒性而非分布内性能
- **实际发现**：发现了GELU-Sinc-Perturbation、GELUSine、GMTU和Turbulent激活函数

## 激活函数设计问题

激活函数是神经网络的基本构建块，但设计空间主要通过人类直觉来探索：ReLU、GELU、SiLU/Swish以及少数其他函数。每个都是手工设计的，具有特定的属性。

我们能否通过系统搜索做得更好？

## AlphaEvolve驱动的搜索

关键创新是在进化搜索框架（AlphaEvolve）中使用**前沿LLM作为变异算子**。这有两个优势：

1. **无约束搜索空间**：不是在参数化函数族中搜索，而是覆盖FLOP预算内的所有Python函数
2. **语义变异**：LLM可以对激活函数提出有意义的修改，而非随机扰动

## OOD适应度：正确的目标

一个关键的设计选择：使用**分布外性能**作为适应度指标，而非分布内准确率。这针对编码有用归纳偏置的激活函数——帮助网络泛化而非仅仅记忆的函数。

## 发现的函数

搜索产生了几个新型激活函数：

- **GELU-Sinc-Perturbation**：带sinc函数扰动的GELU
- **GELUSine**：GELU-正弦混合
- **GMTU**：门控乘法三角单元
- **Turbulent Activation Function**：更复杂的组合

这些在分布外基准上优于ReLU，同时保持有竞争力的分布内性能。

## 要点总结

1. **LLM作为搜索算子**：使用LLM进行代码进化搜索是一种强大的范式
2. **小规模发现可迁移**：在小型合成数据集上发现的函数可泛化到更大问题
3. **OOD作为目标**：在搜索过程中针对泛化能力产生更稳健的组件
4. **激活函数未被解决**：这个基本构建块仍有改进空间
:::
