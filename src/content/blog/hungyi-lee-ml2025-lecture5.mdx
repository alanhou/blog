---
title:
  en: "Hung-yi Lee ML 2025 Lecture 5: The Power and Limits of Pretrain-Alignment"
  zh: "李宏毅机器学习2025 第五讲：预训练-对齐范式的强大与极限"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on LLM training: why alignment is just 'adding eyes to the dragon', knowledge distillation, and what alignment can't do"
  zh: "台大李宏毅教授课程笔记：LLM训练方法，为何对齐只是画龙点睛，知识蒸馏，以及对齐的局限性"
date: 2025-02-23
tags: ["ai", "llm", "pretraining", "alignment", "fine-tuning", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/Ozos6M1JtIE/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 5
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="Ozos6M1JtIE" title="李宏毅机器学习2025 第五讲：预训练-对齐的强大与极限" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## The Three Stages of LLM Training

Modern LLM training follows three stages:

1. **Pre-training**: Learn basic text completion from massive internet data
2. **Supervised Fine-Tuning (SFT)**: Learn to follow instructions with human-provided Q&A pairs
3. **RLHF**: Learn from human feedback on what responses are better

Stages 2 and 3 together are called **alignment** - aligning the model's outputs with human values and needs.

## The Illusion of Alignment's Power

Compare LLaMA-2-7B-base (no alignment) vs LLaMA-2-7B-chat (with alignment):

- **Base model**: Asked "What is machine learning?", it answers correctly... then repeats itself endlessly
- **Chat model**: Gives a well-structured, complete answer

On MT-Bench:
- LLaMA-3-70B (base): 2.7 points
- LLaMA-3-70B-instruct: 8.63 points

Looks like alignment is the MVP and pre-training is just along for the ride, right?

**Wrong.** Don't let the scoring system fool you.

## Alignment: Adding Eyes to the Dragon

The shocking truth: alignment uses remarkably little data.

- **LLaMA-2**: Only 27,540 examples for SFT
- **LIMA paper**: Only 1,000 carefully curated examples
- **Homework 5**: Only 100 examples (and it still works!)

Alignment is like "adding eyes to the dragon" (画龙点睛) - the dragon's body is pre-training, but it only flies when you add the eyes. The eyes are small but crucial.

**Quality over quantity**: LLaMA-2's team initially bought millions of examples from third parties. They were useless compared to their own 27,000 carefully crafted examples. Hence their section title: "Quality is All You Need."

## The Mystery of Good Alignment Data

What makes alignment data effective? Nobody really knows.

**The Ruozhi Ba (弱智吧) Experiment**: Researchers tested various data sources for fine-tuning. Zhihu (Chinese Q&A site) with 8,000+ examples improved accuracy from 53% to 63%. But "Ruozhi Ba" (a Chinese forum for absurd questions) with only 240 examples achieved 83%!

Sample Ruozhi Ba questions:
- "I boiled my bank card in a pressure cooker. Why is it still frozen?"
- "I'm 16 years old. Is it normal that I'm under 18?"
- "One jin of iron and one jin of cotton fall into water. Which do you save first?"

Why does this work? The paper's authors speculate it's the diversity. But there's a hidden detail: **the answers were generated by GPT-4**. So it's really knowledge distillation.

## Knowledge Distillation: Learning from the Teacher

When people didn't know how to do alignment well, they reverse-engineered ChatGPT:

1. Ask ChatGPT questions
2. Use its answers as training data
3. Fine-tune your model to match

Famous examples:
- **Alpaca**: LLaMA-7B + ChatGPT distillation, ~$100
- **Vicuna**: Similar approach
- **SkyT1**: Qwen-32B + QwQ distillation, $450
- **S1**: Qwen-2.5-32B + Gemini distillation, 1,000 examples

Note: These cost estimates don't include data generation costs (querying the teacher model) or data cleaning.

## Data Selection: Longer is Better?

**AlpacaGas**: Filter Alpaca's 50K examples by having an LLM score quality. Keep only high-scoring examples. Better results with less data.

**Long is More for Alignment**: Just select the 1,000 longest answers. That's it. This simple heuristic beat:
- Original Alpaca (50K examples)
- AlpacaGas (LLM-filtered)
- LIMA (1,000 human-curated examples)

Why? Still unclear. But it works.

## The Questions Don't Even Matter

Professor Lee's lab (Chun-Cheng Hsieh's work) discovered something wild:

1. Take random sentences from the internet
2. Cut them in half
3. Give the first half to ChatGPT, let it complete the second half
4. Use this as training data

The "questions" aren't even questions. The "answers" are just text completions. Yet:
- Mistral-7B base: 3.73 → 7.3 (beats official Mistral-7B-instruct at 6.84!)
- LLaMA-3-8B base: 5.5 → 7.0
- LLaMA-3-70B base: 2.7 → 8.0

## Why Does This Work? Alignment Changes Less Than You Think

Research shows that aligned vs unaligned models differ less than expected:

**Token-level analysis**: Compare what token the aligned model outputs vs what the base model would output:
- **Unshift**: Same top token (most common)
- **Marginal**: Correct token is 2nd or 3rd choice
- **Shift**: Correct token not in top 3 (rare)

Most tokens are unshift! The models behave almost identically. The shifted tokens are mostly:
- Greeting phrases ("Thank you for asking")
- Connectors
- **End-of-sequence tokens** (crucial!)

The base model's main problem: it doesn't know when to stop talking.

## Alignment Without Questions

**Response Tuning**: Train only on answers, no questions at all. Still works! Not quite as good as instruction tuning, but surprisingly effective.

**Rule-Based "Alignment"**: Without any fine-tuning, just add three rules:
1. Increase end-of-sequence probability (more as response gets longer)
2. Manually adjust some token probabilities
3. Penalize repeated tokens

Result: Base model goes from 2% win rate vs instruction model to 24% win rate. Just from rules!

## Self-Alignment: The Model Teaches Itself

Since alignment is easy, models can align themselves:

1. Ask the model a question
2. Generate multiple answers
3. Have the model score its own answers
4. Use RL to increase probability of good answers

This is the **Self-Rewarding Model** approach (Meta, 2024). It works because alignment doesn't require learning new knowledge - just adjusting behavior.

## What Makes Pre-training Effective?

From "Physics of Language Models Part 3.1":

**The experiment**: Create fake biographies. Pre-train a model. Then test if it can answer questions about people it saw during pre-training but not during alignment.

**Result**: 0% accuracy. The model can't answer.

**Why?** If each person appears only once, the model learns wrong associations. It thinks "Takamatsu Tou + Yuushou Girls Academy + Astronomy Club" is ONE entity that is "MyGO's vocalist." It doesn't understand that "Takamatsu Tou" alone is a person with properties.

**Solution**: Multiple descriptions of the same entity. If the model sees different phrasings of the same information, it learns that "Takamatsu Tou" is an entity with various properties.

**Key finding**: You don't need ALL entities to have multiple descriptions. Just having SOME entities with multiple versions teaches the model how to understand entities in general.

## The Scale of Pre-training Data

Current models use massive data:
- LLaMA-3: 15T tokens
- DeepSeek-V3: 14.8T tokens

**The data wall**: A 2022 paper estimated we might exhaust usable internet text by 2028-2030. LLaMA-2 used 1T tokens; LLaMA-3 used 15T - a 10x increase in one year. Internet content doesn't grow that fast.

**Good news**: HuggingFace released FineWeb with 15T cleaned tokens. Data is available; compute is the bottleneck.

## Data Quality Matters

**Textbooks Are All You Need** (Microsoft Phi): Training on textbook-style content (with exercises) beats raw code from the internet. But note: these "textbooks" were generated by GPT-3.5, so it might be knowledge distillation again.

**Rephrase the Web** (Apple): Use a model to rewrite messy web content into cleaner text. Training on rephrased content is 3x more data-efficient.

**Deduplication is crucial**: Models should see diverse data, not the same content repeatedly. After seeing the same data 4 times, performance starts degrading. After 40 times, learning stops entirely.

## The Limits of Alignment

### Alignment Can't Teach New Knowledge

Categorize training data by what the base model knows:
- **Highly known**: Base model answers correctly
- **Maybe known**: Correct with right prompting
- **Weakly known**: Sometimes correct with sampling
- **Unknown**: Never correct

Intuition says unknown data should help most. Reality:
- **Unknown data hurts the most**
- **Maybe known data helps the most**

Training on unknown data doesn't teach new knowledge - it damages existing capabilities.

### Why RL Works

RL doesn't force the model to learn impossible things. It only reinforces answers the model already generates. RL is about "activating potential," not "implanting knowledge."

### The Ashes of Pre-training

**ROT13 phenomenon**: GPT-4 can decode ROT13 (shift letters by 13) but fails at ROT8. Why? ROT13 appears frequently on the internet as a meme. Pre-training data distribution affects capabilities in unexpected ways.

### Alignment is a Mask

Using logit lens analysis on GPT-2 base vs DPO-aligned:
- Base model: Starts wanting to output profanity around layer 18
- Aligned model: Suppresses this tendency

But the profanity-related parameters still exist! Alignment just prevents them from being activated. The knowledge is still there, masked but not removed.

The famous meme: The model is a monster that learned everything from the internet. Alignment just puts a mask on it to look presentable.

## Key Takeaways

1. **Alignment is powerful but limited** - it's adding eyes to the dragon, not building the dragon
2. **Quality over quantity** - 1,000 good examples beat millions of bad ones
3. **Knowledge distillation works** - learning from stronger models is effective
4. **Alignment changes behavior, not knowledge** - it adjusts which tokens get output, not what the model knows
5. **Pre-training determines capabilities** - alignment can only surface what's already there
6. **The mask can slip** - aligned models still contain everything from pre-training
:::

:::zh
本文整理自台湾大学李宏毅教授的「生成式AI时代下的机器学习(2025)」课程。

## LLM训练的三个阶段

现代LLM训练分三个阶段：

1. **预训练**：从大量网络数据学习基本的文字接龙能力
2. **监督式微调（SFT）**：用人类提供的问答对学习遵循指令
3. **RLHF**：从人类反馈中学习什么样的回答更好

第2和第3阶段合称**对齐**——让模型的输出与人类价值观和需求对齐。

## 对齐能力的假象

比较LLaMA-2-7B-base（无对齐）和LLaMA-2-7B-chat（有对齐）：

- **Base模型**：问「什么是机器学习？」，它回答正确...然后无限重复
- **Chat模型**：给出结构良好、完整的答案

在MT-Bench上：
- LLaMA-3-70B（base）：2.7分
- LLaMA-3-70B-instruct：8.63分

看起来对齐是MVP，预训练只是躺赢狗？

**错了。** 不要被评分系统骗了。

## 对齐：画龙点睛

惊人的事实：对齐用的数据少得惊人。

- **LLaMA-2**：SFT只用了27,540笔数据
- **LIMA论文**：只用1,000笔精心挑选的数据
- **作业5**：只用100笔数据（而且还有效！）

对齐就像「画龙点睛」——龙的身体是预训练，但只有点上眼睛龙才会飞。眼睛很小但很关键。

**质量胜过数量**：LLaMA-2团队一开始从第三方买了上百万笔数据。结果没什么用，还不如他们自己做的27,000笔数据。所以他们那一节的标题是：「Quality is All You Need」。

## 好的对齐数据之谜

什么样的对齐数据有效？没人真正知道。

**弱智吧实验**：研究者测试了各种数据来源做微调。知乎用了8,000多笔数据，正确率从53%提升到63%。但「弱智吧」（一个问荒谬问题的中文论坛）只用240笔数据就达到83%！

弱智吧问题范例：
- 「我把银行卡在高压锅里煮了一碗，为什么还是冻结的状态？」
- 「我16岁，未满18岁正常吗？」
- 「一斤铁一斤棉花掉到水里，你先救谁？」

为什么有效？论文作者猜测是多样性。但有个隐藏细节：**答案是GPT-4生成的**。所以其实是知识蒸馏。

## 知识蒸馏：向老师学习

当人们不知道怎么做好对齐时，他们对ChatGPT做逆向工程：

1. 问ChatGPT问题
2. 用它的答案当训练数据
3. 微调你的模型来模仿

著名例子：
- **Alpaca**：LLaMA-7B + ChatGPT蒸馏，约100美元
- **Vicuna**：类似方法
- **SkyT1**：Qwen-32B + QwQ蒸馏，450美元
- **S1**：Qwen-2.5-32B + Gemini蒸馏，1,000笔数据

注意：这些成本估算不包括生成数据的成本（查询老师模型）或数据清理。

## 数据选择：越长越好？

**AlpacaGas**：用LLM给Alpaca的5万笔数据打分，只保留高分数据。用更少数据得到更好结果。

**Long is More for Alignment**：就选答案最长的1,000笔。就这样。这个简单的启发式方法打败了：
- 原版Alpaca（5万笔数据）
- AlpacaGas（LLM筛选）
- LIMA（1,000笔人工精选数据）

为什么？还不清楚。但就是有效。

## 问题甚至不重要

李老师实验室（谢俊成同学的研究）发现了疯狂的事：

1. 从网络上随便拿句子
2. 切成两半
3. 把前半段给ChatGPT，让它续写后半段
4. 用这个当训练数据

「问题」根本不是问题。「答案」只是文字续写。然而：
- Mistral-7B base：3.73 → 7.3（打败官方Mistral-7B-instruct的6.84！）
- LLaMA-3-8B base：5.5 → 7.0
- LLaMA-3-70B base：2.7 → 8.0

## 为什么有效？对齐改变的比你想的少

研究显示对齐前后的模型差异比预期小：

**Token层级分析**：比较对齐模型输出的token和base模型会输出的token：
- **Unshift**：最高概率的token相同（最常见）
- **Marginal**：正确token是第2或第3选择
- **Shift**：正确token不在前3名（罕见）

大多数token都是unshift！模型行为几乎相同。被shift的token主要是：
- 打招呼的话（「Thank you for asking」）
- 连接词
- **结束符号**（关键！）

Base模型的主要问题：不知道什么时候该停止说话。

## 没有问题的对齐

**Response Tuning**：只用答案训练，完全没有问题。还是有效！虽然不如instruction tuning好，但效果惊人。

**基于规则的「对齐」**：不做任何微调，只加三条规则：
1. 增加结束符号的概率（回答越长概率越高）
2. 手动调整某些token的概率
3. 惩罚重复的token

结果：Base模型对instruction模型的胜率从2%变成24%。只靠规则！

## 自我对齐：模型教自己

既然对齐很容易，模型可以自己对齐自己：

1. 问模型一个问题
2. 生成多个答案
3. 让模型给自己的答案打分
4. 用RL提高好答案的概率

这就是**Self-Rewarding Model**方法（Meta，2024）。有效是因为对齐不需要学新知识——只是调整行为。

## 什么让预训练有效？

来自「Physics of Language Models Part 3.1」：

**实验**：创造假的人物简历。预训练模型。然后测试它能否回答关于预训练时见过、但对齐时没见过的人的问题。

**结果**：0%正确率。模型答不出来。

**为什么？** 如果每个人只出现一次，模型会学到错误的关联。它以为「高松灯+羽丘女子学院+天文社」是一个实体，这个实体是「MyGO的主唱」。它不理解「高松灯」本身是一个有各种属性的人。

**解决方案**：同一个实体有多种描述。如果模型看到同一信息的不同表述，它就学会「高松灯」是一个有各种属性的实体。

**关键发现**：不需要所有实体都有多种描述。只要有部分实体有多个版本，就能教会模型如何理解实体。

## 预训练数据的规模

当前模型使用海量数据：
- LLaMA-3：15T tokens
- DeepSeek-V3：14.8T tokens

**数据墙**：2022年的论文估计，到2028-2030年可能会用尽可用的网络文本。LLaMA-2用了1T tokens；LLaMA-3用了15T——一年内增长10倍。网络内容增长没那么快。

**好消息**：HuggingFace发布了FineWeb，有15T清理过的tokens。数据有了；算力才是瓶颈。

## 数据质量很重要

**Textbooks Are All You Need**（微软Phi）：用教科书风格的内容（带习题）训练，比网络上的原始代码效果好。但注意：这些「教科书」是GPT-3.5生成的，所以可能又是知识蒸馏。

**Rephrase the Web**（苹果）：用模型把杂乱的网络内容改写成更干净的文本。用改写后的内容训练，数据效率提高3倍。

**去重很关键**：模型应该看多样化的数据，而不是重复看同样的内容。看同样数据4次后，性能开始下降。40次后，学习完全停止。

## 对齐的极限

### 对齐无法教新知识

按base模型知道什么来分类训练数据：
- **Highly known**：Base模型直接答对
- **Maybe known**：用对的方式问才答对
- **Weakly known**：有时候采样能答对
- **Unknown**：怎么都答不对

直觉说unknown数据应该最有帮助。现实：
- **Unknown数据伤害最大**
- **Maybe known数据帮助最大**

用unknown数据训练不会教新知识——反而会损害现有能力。

### 为什么RL有效

RL不会强迫模型学不可能的事。它只强化模型本来就能生成的答案。RL是「激发潜力」，不是「植入知识」。

### 预训练的灰烬

**ROT13现象**：GPT-4能解码ROT13（字母移动13位）但做不了ROT8。为什么？ROT13在网络上作为梗出现得特别多。预训练数据分布以意想不到的方式影响能力。

### 对齐是面具

用logit lens分析GPT-2 base vs DPO对齐后：
- Base模型：大约从第18层开始想输出脏话
- 对齐模型：抑制这种倾向

但跟脏话相关的参数还在！对齐只是阻止它们被激活。知识还在那里，被遮住但没有移除。

著名的梗图：模型是从网络上学到一切的怪物。对齐只是给它戴上面具，让它看起来人模人样。

## 重点整理

1. **对齐强大但有限**——是画龙点睛，不是造龙
2. **质量胜过数量**——1,000笔好数据打败百万笔烂数据
3. **知识蒸馏有效**——向更强的模型学习是有效的
4. **对齐改变行为，不改变知识**——调整哪些token被输出，而不是模型知道什么
5. **预训练决定能力**——对齐只能激发已经存在的东西
6. **面具可能滑落**——对齐后的模型仍然包含预训练的一切
:::
