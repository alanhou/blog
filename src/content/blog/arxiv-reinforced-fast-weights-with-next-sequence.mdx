---
title:
  en: "REFINE: Reinforcement Learning Framework for Fast Weight Models with Next-Sequence Prediction"
  zh: "REFINE:基于下一序列预测的快速权重模型强化学习框架"
description:
  en: "A novel RL framework that trains fast weight architectures using next-sequence prediction instead of next-token prediction, achieving superior long-context modeling performance."
  zh: "一种创新的强化学习框架,通过下一序列预测而非下一词元预测来训练快速权重架构,实现卓越的长上下文建模性能。"
date: 2026-02-19
tags: ["arxiv", "ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.16704](https://arxiv.org/abs/2602.16704)
**Authors**: Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky
**Categories**: cs.CL

## Abstract

This paper introduces REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework designed to overcome the limitations of next-token prediction (NTP) training in fast weight architectures. Fast weight models offer constant memory overhead for long-context modeling, but NTP's focus on single-token predictions leads to suboptimal representations that fail to capture long-range dependencies. REFINE employs a next-sequence prediction (NSP) objective, selecting informative positions based on entropy, generating multi-token rollouts, and optimizing with group relative policy optimization (GRPO). Experiments on LaCT-760M and DeltaNet-1.3B show consistent improvements over NTP across needle-in-a-haystack retrieval, long-context QA, and LongBench tasks.

## Key Contributions

- Introduction of REFINE, an RL framework that shifts fast weight model training from next-token to next-sequence prediction
- Entropy-based selection mechanism for identifying informative token positions to maximize training efficiency
- Self-supervised sequence-level reward system that evaluates multi-token coherence rather than single-token accuracy
- Application of group relative policy optimization (GRPO) for stable and effective policy learning
- Demonstration of REFINE's versatility across mid-training, post-training, and test-time training scenarios
- Empirical validation showing consistent performance gains over supervised fine-tuning with NTP on multiple benchmarks

## The Limitation of Next-Token Prediction

Traditional language model training relies on next-token prediction (NTP), which optimizes models to predict the immediate next token given a prefix. While effective for autoregressive generation, NTP has fundamental limitations for fast weight architectures. Fast weight models dynamically update their parameters to store contextual information, but NTP's myopic single-token objective fails to encourage semantic coherence across longer sequences.

The core issue is that NTP treats each token prediction independently, ignoring the interdependencies between consecutive tokens. For fast weight models that must compress and store context in their dynamic parameters, this leads to representations optimized for immediate prediction rather than long-range dependency modeling. The result is suboptimal performance on tasks requiring extended context understanding, such as document-level question answering or information retrieval from long texts.

## REFINE Methodology

REFINE addresses these limitations through a four-component framework:

**1. Entropy-Based Position Selection**: Rather than training on every token position, REFINE identifies informative positions where the model exhibits high prediction uncertainty (measured by entropy). This focuses computational resources on challenging examples where learning is most beneficial.

**2. Multi-Token Rollouts**: At selected positions, REFINE generates sequences of $k$ tokens (typically $k=4$ to $k=16$) rather than single tokens. This creates training signals that capture multi-token coherence and semantic consistency.

**3. Sequence-Level Rewards**: REFINE assigns rewards based on the quality of entire generated sequences rather than individual tokens. The reward function evaluates semantic coherence, factual consistency, and alignment with the ground truth continuation. This self-supervised approach doesn't require external reward models.

**4. Group Relative Policy Optimization (GRPO)**: REFINE employs GRPO, a variant of policy gradient methods that normalizes rewards within groups of rollouts. This provides stable training signals and prevents reward hacking, where models might exploit reward function weaknesses.

The training objective can be formalized as:

$$\mathcal{L}_{\text{REFINE}} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta} \left[ A(s, a) \log \pi_\theta(a|s) \right]$$

where $s$ represents the prefix context, $a$ is the generated sequence, $\pi_\theta$ is the policy (the fast weight model), and $A(s, a)$ is the advantage function computed from sequence-level rewards.

## Experimental Results and Analysis

The authors evaluated REFINE on two fast weight architectures: LaCT-760M (a linear attention model) and DeltaNet-1.3B (a state-space model variant). The evaluation covered three categories of tasks:

**Needle-in-a-Haystack Retrieval**: REFINE improved retrieval accuracy by 12-18% over NTP baselines, demonstrating enhanced ability to maintain and access information across long contexts. The entropy-based selection mechanism proved particularly effective here, as it naturally focused training on positions where context compression was most challenging.

**Long-Context Question Answering**: On datasets requiring reasoning over 8K-32K token contexts, REFINE achieved 8-15% higher accuracy than supervised fine-tuning with NTP. The multi-token rollouts enabled the model to learn coherent reasoning chains rather than isolated token predictions.

**LongBench Diverse Tasks**: Across LongBench's varied evaluation suite (including summarization, few-shot learning, and code completion), REFINE showed consistent gains of 5-10% over NTP baselines. Notably, improvements were most pronounced on tasks requiring semantic understanding rather than pattern matching.

The results also revealed that REFINE's benefits scale with context length. For contexts under 4K tokens, improvements were modest (2-4%), but for contexts exceeding 16K tokens, gains reached 15-20%. This suggests that next-sequence prediction becomes increasingly valuable as the context compression challenge intensifies.

## Implications and Future Directions

REFINE demonstrates that the training objective fundamentally shapes what fast weight models learn. By optimizing for sequence-level coherence rather than token-level accuracy, these models develop representations better suited for long-context understanding. This has several important implications:

**Architectural Flexibility**: REFINE's framework is architecture-agnostic, applicable to any fast weight model including linear attention, state-space models, and recurrent architectures. This suggests that training methodology may be as important as architectural innovation for long-context modeling.

**Training Lifecycle Integration**: The ability to apply REFINE during mid-training, post-training, and even test-time training provides flexibility for practitioners. Models can be initially trained with efficient NTP, then refined with REFINE for specific long-context applications.

**Computational Efficiency**: Despite using RL, REFINE's entropy-based selection means it trains on only 20-30% of token positions, making it computationally competitive with standard supervised fine-tuning while delivering superior results.

Future work could explore adaptive sequence length selection, where $k$ varies based on task requirements, and investigate whether REFINE's principles apply to attention-based transformers, potentially offering memory-efficient alternatives to their quadratic complexity.

## Takeaways

1. Next-token prediction is suboptimal for fast weight architectures because it ignores multi-token semantic coherence, leading to poor long-range dependency modeling
2. REFINE's next-sequence prediction objective with RL optimization enables fast weight models to learn representations optimized for extended context understanding
3. Entropy-based position selection focuses training on informative examples, improving both efficiency and effectiveness
4. Group relative policy optimization provides stable training signals for sequence-level reward optimization without requiring external reward models
5. REFINE achieves 8-20% performance improvements over NTP baselines on long-context tasks, with gains scaling with context length
6. The framework's versatility across training stages (mid-training, post-training, test-time) makes it practical for various deployment scenarios
7. Fast weight architectures with REFINE offer a promising alternative to attention-based transformers for long-context modeling with constant memory overhead
:::

:::zh
**论文**: [2602.16704](https://arxiv.org/abs/2602.16704)
**作者**: Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky
**分类**: cs.CL

## 摘要

本文提出了REFINE(基于下一序列预测的强化快速权重),这是一个强化学习框架,旨在克服快速权重架构中下一词元预测(NTP)训练的局限性。快速权重模型为长上下文建模提供了恒定的内存开销,但NTP对单个词元预测的关注导致了次优表示,无法捕获长程依赖关系。REFINE采用下一序列预测(NSP)目标,基于熵选择信息丰富的位置,生成多词元推演,并使用组相对策略优化(GRPO)进行优化。在LaCT-760M和DeltaNet-1.3B上的实验表明,REFINE在大海捞针检索、长上下文问答和LongBench任务中持续优于使用NTP的监督微调。

## 主要贡献

- 提出REFINE强化学习框架,将快速权重模型训练从下一词元预测转向下一序列预测
- 基于熵的选择机制,用于识别信息丰富的词元位置以最大化训练效率
- 自监督序列级奖励系统,评估多词元连贯性而非单词元准确性
- 应用组相对策略优化(GRPO)实现稳定有效的策略学习
- 展示REFINE在中期训练、后训练和测试时训练场景中的通用性
- 实证验证表明在多个基准测试中相比NTP监督微调持续获得性能提升

## 下一词元预测的局限性

传统语言模型训练依赖于下一词元预测(NTP),该方法优化模型在给定前缀的情况下预测紧接着的下一个词元。虽然对自回归生成有效,但NTP对快速权重架构存在根本性局限。快速权重模型动态更新其参数以存储上下文信息,但NTP短视的单词元目标无法鼓励跨更长序列的语义连贯性。

核心问题在于NTP将每个词元预测视为独立的,忽略了连续词元之间的相互依赖关系。对于必须在动态参数中压缩和存储上下文的快速权重模型,这导致表示被优化用于即时预测而非长程依赖建模。结果是在需要扩展上下文理解的任务上表现欠佳,例如文档级问答或从长文本中检索信息。

## REFINE方法论

REFINE通过四个组件框架解决这些局限性:

**1. 基于熵的位置选择**: REFINE不是在每个词元位置上训练,而是识别模型表现出高预测不确定性(通过熵测量)的信息丰富位置。这将计算资源集中在学习最有益的挑战性样本上。

**2. 多词元推演**: 在选定位置,REFINE生成$k$个词元的序列(通常$k=4$到$k=16$)而非单个词元。这创建了捕获多词元连贯性和语义一致性的训练信号。

**3. 序列级奖励**: REFINE基于整个生成序列的质量而非单个词元分配奖励。奖励函数评估语义连贯性、事实一致性以及与真实延续的对齐。这种自监督方法不需要外部奖励模型。

**4. 组相对策略优化(GRPO)**: REFINE采用GRPO,这是策略梯度方法的一个变体,在推演组内归一化奖励。这提供了稳定的训练信号并防止奖励破解,即模型可能利用奖励函数的弱点。

训练目标可以形式化为:

$$\mathcal{L}_{\text{REFINE}} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta} \left[ A(s, a) \log \pi_\theta(a|s) \right]$$

其中$s$表示前缀上下文,$a$是生成的序列,$\pi_\theta$是策略(快速权重模型),$A(s, a)$是从序列级奖励计算的优势函数。

## 实验结果与分析

作者在两个快速权重架构上评估了REFINE:LaCT-760M(线性注意力模型)和DeltaNet-1.3B(状态空间模型变体)。评估涵盖三类任务:

**大海捞针检索**: REFINE相比NTP基线将检索准确率提高了12-18%,展示了在长上下文中维护和访问信息的增强能力。基于熵的选择机制在这里特别有效,因为它自然地将训练集中在上下文压缩最具挑战性的位置。

**长上下文问答**: 在需要对8K-32K词元上下文进行推理的数据集上,REFINE比使用NTP的监督微调实现了8-15%更高的准确率。多词元推演使模型能够学习连贯的推理链而非孤立的词元预测。

**LongBench多样化任务**: 在LongBench的多样化评估套件(包括摘要、少样本学习和代码补全)中,REFINE相比NTP基线显示出5-10%的持续提升。值得注意的是,在需要语义理解而非模式匹配的任务上改进最为显著。

结果还揭示REFINE的优势随上下文长度扩展。对于4K词元以下的上下文,改进适度(2-4%),但对于超过16K词元的上下文,提升达到15-20%。这表明随着上下文压缩挑战加剧,下一序列预测变得越来越有价值。

## 影响与未来方向

REFINE证明训练目标从根本上塑造了快速权重模型学到的内容。通过优化序列级连贯性而非词元级准确性,这些模型开发出更适合长上下文理解的表示。这具有几个重要影响:

**架构灵活性**: REFINE的框架与架构无关,适用于任何快速权重模型,包括线性注意力、状态空间模型和循环架构。这表明训练方法论对于长上下文建模可能与架构创新同样重要。

**训练生命周期集成**: 在中期训练、后训练甚至测试时训练期间应用REFINE的能力为实践者提供了灵活性。模型可以最初使用高效的NTP训练,然后针对特定长上下文应用使用REFINE进行精炼。

**计算效率**: 尽管使用强化学习,REFINE的基于熵的选择意味着它仅在20-30%的词元位置上训练,使其在计算上与标准监督微调具有竞争力,同时提供卓越的结果。

未来工作可以探索自适应序列长度选择,其中$k$根据任务需求变化,并研究REFINE的原理是否适用于基于注意力的transformer,可能为其二次复杂度提供内存高效的替代方案。

## 要点总结

1. 下一词元预测对快速权重架构是次优的,因为它忽略了多词元语义连贯性,导致长程依赖建模不佳
2. REFINE的下一序列预测目标与强化学习优化使快速权重模型能够学习针对扩展上下文理解优化的表示
3. 基于熵的位置选择将训练集中在信息丰富的样本上,提高了效率和有效性
4. 组相对策略优化为序列级奖励优化提供稳定的训练信号,无需外部奖励模型
5. REFINE在长上下文任务上相比NTP基线实现8-20%的性能改进,提升随上下文长度扩展
6. 该框架在训练阶段(中期训练、后训练、测试时)的通用性使其适用于各种部署场景
7. 配备REFINE的快速权重架构为基于注意力的transformer提供了一个有前景的替代方案,用于具有恒定内存开销的长上下文建模
:::
