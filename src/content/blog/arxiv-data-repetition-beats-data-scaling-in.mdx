---
title:
  en: "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning"
  zh: "数据重复优于数据扩展:长链思维监督微调的新发现"
description:
  en: "Research showing that training language models on smaller datasets for more epochs outperforms single-epoch training on larger datasets, challenging conventional wisdom about data scaling."
  zh: "研究表明,在较小数据集上进行多轮训练优于在大规模数据集上单轮训练,挑战了关于数据扩展的传统认知。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.11149](https://arxiv.org/abs/2602.11149)
**Authors**: Dawid J. Kopiczko, Sagar Vaze, Tijmen Blankevoort, Yuki M. Asano
**Categories**: cs.CL

## Abstract

This paper challenges the conventional machine learning wisdom that more unique training samples lead to better generalization. The authors demonstrate that supervised fine-tuning (SFT) for chain-of-thought reasoning benefits significantly from data repetition. Under a fixed computational budget, training for multiple epochs on smaller datasets substantially outperforms single-epoch training on larger datasets. Using Olmo3-7B on AIME'24/25 and GPQA benchmarks, they show that 128 epochs on 400 samples outperforms 1 epoch on 51,200 samples by 12-26 percentage points, without catastrophic forgetting. The study identifies training token accuracy as a reliable signal for when repetition saturates, with improvements plateauing at full memorization.

## Key Contributions

- **Counterintuitive finding**: Multi-epoch training on small datasets outperforms single-epoch training on proportionally larger datasets under fixed update budgets
- **Empirical validation**: Demonstrated 12-26 percentage point improvements on challenging reasoning benchmarks (AIME'24/25, GPQA)
- **Practical stopping criterion**: Training token accuracy reliably signals saturation, providing a cost-effective alternative to expensive data scaling
- **No catastrophic forgetting**: Extensive repetition does not degrade performance on held-out evaluation sets
- **New research direction**: Poses the "repetition advantage" as an open problem for understanding LLM training dynamics

## Methodology and Experimental Design

The researchers conducted systematic experiments controlling for the total number of gradient updates while varying the ratio of dataset size to training epochs. They used Olmo3-7B as their base model and focused on chain-of-thought reasoning tasks requiring multi-step mathematical and scientific reasoning.

The experimental setup maintained a constant update budget: training for $E$ epochs on $N$ samples equals $E \times N$ total sample presentations. For instance, 128 epochs on 400 samples ($51,200$ total presentations) was compared against 1 epoch on 51,200 unique samples. This design isolates the effect of repetition from computational cost.

The authors evaluated on two challenging benchmarks:
- **AIME** (American Invitational Mathematics Examination): High-difficulty mathematical problems requiring sophisticated reasoning
- **GPQA** (Graduate-Level Google-Proof Q&A): Expert-level science questions designed to be difficult even for search-assisted humans

Training token accuracy—the model's accuracy on predicting the next token during training—served as the primary monitoring metric for determining when additional epochs cease to provide benefits.

## Results and Analysis

The results reveal a striking pattern: repetition consistently outperforms diversity under fixed budgets. On AIME'24, the 128-epoch configuration achieved 26 percentage points higher accuracy than the single-epoch baseline. On GPQA, the improvement was 12 percentage points. These gains are substantial given the difficulty of these benchmarks.

The relationship between training token accuracy and generalization performance follows a predictable trajectory. As models approach 100% training token accuracy (full memorization), test performance improvements plateau. This saturation point occurs consistently across different dataset sizes and model configurations, suggesting a fundamental property of reasoning SFT rather than an artifact of specific experimental choices.

Critically, the authors found no evidence of catastrophic forgetting even after 128 epochs of training. Models maintained their performance on held-out evaluation sets, indicating that memorization of training data does not come at the cost of general reasoning capability. This finding contradicts common concerns about overfitting in deep learning.

The computational efficiency implications are significant. Rather than collecting and curating tens of thousands of unique chain-of-thought examples, practitioners can achieve superior results with hundreds of high-quality examples trained for many epochs. This dramatically reduces data collection costs while improving model performance.

## Implications for LLM Training

This work has profound implications for how we approach supervised fine-tuning of reasoning models. The traditional paradigm emphasizes scaling dataset size, often at considerable expense in terms of human annotation, data curation, and quality control. The repetition advantage suggests a fundamentally different approach: invest in smaller, higher-quality datasets and train longer.

From a practical standpoint, this enables more efficient development cycles. Teams can iterate on dataset quality with smaller samples, using training token accuracy as a reliable signal for when to stop. The predictability of the saturation point provides clear guidance for resource allocation.

Theoretically, these findings raise important questions about the learning dynamics of large language models. Why does memorization coincide with improved generalization in this setting? Classical learning theory typically treats memorization as antithetical to generalization, yet here they appear aligned. The authors suggest this may relate to the structured nature of chain-of-thought reasoning, where memorizing reasoning patterns transfers to novel problems.

The absence of catastrophic forgetting despite extreme repetition also warrants investigation. Standard neural network theory predicts that repeated exposure to the same examples should lead to overfitting and degraded performance on unseen data. The robustness observed here suggests that reasoning tasks may have different learning dynamics than traditional supervised learning problems.

## Takeaways

1. Under fixed computational budgets, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets for chain-of-thought reasoning tasks
2. Training token accuracy serves as a reliable stopping criterion—improvements plateau when models achieve full memorization of training data
3. Extensive repetition (128+ epochs) does not cause catastrophic forgetting on reasoning benchmarks
4. The "repetition advantage" represents a new open problem in understanding LLM training dynamics, particularly the relationship between memorization and generalization
5. Practitioners should prioritize dataset quality over quantity for reasoning SFT, potentially reducing data collection costs while improving performance
:::

:::zh
**论文**: [2602.11149](https://arxiv.org/abs/2602.11149)
**作者**: Dawid J. Kopiczko, Sagar Vaze, Tijmen Blankevoort, Yuki M. Asano
**分类**: cs.CL

## 摘要

本文挑战了机器学习领域的传统认知,即更多独特训练样本能带来更好的泛化能力。作者证明,针对链式思维推理的监督微调(SFT)能从数据重复中显著受益。在固定计算预算下,在较小数据集上进行多轮训练的效果大幅优于在大规模数据集上进行单轮训练。使用Olmo3-7B模型在AIME'24/25和GPQA基准测试中,400个样本训练128轮的表现比51,200个样本训练1轮高出12-26个百分点,且不会出现灾难性遗忘。研究发现,训练token准确率能可靠地指示重复何时达到饱和,改进在完全记忆时趋于平稳。

## 主要贡献

- **反直觉发现**:在固定更新预算下,小数据集多轮训练优于按比例扩大的数据集单轮训练
- **实证验证**:在高难度推理基准(AIME'24/25、GPQA)上展示了12-26个百分点的性能提升
- **实用停止准则**:训练token准确率可靠地指示饱和点,提供了替代昂贵数据扩展的经济方案
- **无灾难性遗忘**:大量重复训练不会降低模型在保留评估集上的性能
- **新研究方向**:提出"重复优势"作为理解大语言模型训练动态的开放性问题

## 方法论与实验设计

研究人员进行了系统性实验,在控制梯度更新总次数的同时,改变数据集规模与训练轮数的比例。他们使用Olmo3-7B作为基础模型,专注于需要多步骤数学和科学推理的链式思维任务。

实验设置保持恒定的更新预算:在$N$个样本上训练$E$轮等于$E \times N$次样本呈现总数。例如,400个样本训练128轮($51,200$次总呈现)与51,200个独特样本训练1轮进行对比。这种设计将重复效应与计算成本隔离开来。

作者在两个高难度基准上进行评估:
- **AIME**(美国数学邀请赛):需要复杂推理的高难度数学问题
- **GPQA**(研究生级别防搜索问答):专家级科学问题,即使借助搜索也很难解答

训练token准确率——模型在训练期间预测下一个token的准确率——作为判断额外训练轮次何时不再带来收益的主要监控指标。

## 结果与分析

结果揭示了一个显著模式:在固定预算下,重复训练始终优于数据多样性。在AIME'24上,128轮配置比单轮基线高出26个百分点。在GPQA上,提升为12个百分点。考虑到这些基准的难度,这些增益相当可观。

训练token准确率与泛化性能之间的关系遵循可预测的轨迹。当模型接近100%训练token准确率(完全记忆)时,测试性能的改进趋于平稳。这个饱和点在不同数据集规模和模型配置中一致出现,表明这是推理SFT的基本属性,而非特定实验选择的产物。

关键的是,作者没有发现灾难性遗忘的证据,即使经过128轮训练。模型在保留评估集上保持了性能,表明训练数据的记忆不会以牺牲通用推理能力为代价。这一发现与深度学习中关于过拟合的常见担忧相矛盾。

计算效率的影响意义重大。从业者无需收集和整理数万个独特的链式思维样本,而是可以用数百个高质量样本训练多轮来获得更优结果。这大幅降低了数据收集成本,同时提升了模型性能。

## 对大语言模型训练的启示

这项工作对我们如何进行推理模型的监督微调具有深远影响。传统范式强调扩大数据集规模,这在人工标注、数据整理和质量控制方面往往成本高昂。重复优势提出了一种根本不同的方法:投资于更小、更高质量的数据集,并进行更长时间的训练。

从实践角度看,这使得开发周期更加高效。团队可以用较小样本迭代数据集质量,使用训练token准确率作为何时停止的可靠信号。饱和点的可预测性为资源分配提供了明确指导。

从理论上讲,这些发现引发了关于大语言模型学习动态的重要问题。为什么在这种情况下记忆与改进的泛化能力相一致?经典学习理论通常将记忆视为泛化的对立面,但在这里它们似乎是一致的。作者认为这可能与链式思维推理的结构化特性有关,记忆推理模式能够迁移到新问题上。

尽管进行了极端重复,却没有出现灾难性遗忘,这一点也值得研究。标准神经网络理论预测,重复暴露于相同样本应该导致过拟合和未见数据性能下降。这里观察到的鲁棒性表明,推理任务可能具有与传统监督学习问题不同的学习动态。

## 要点总结

1. 在固定计算预算下,针对链式思维推理任务,小数据集多轮训练优于大数据集单轮训练
2. 训练token准确率是可靠的停止准则——当模型完全记忆训练数据时,改进趋于平稳
3. 大量重复训练(128+轮)不会在推理基准上造成灾难性遗忘
4. "重复优势"代表了理解大语言模型训练动态的新开放问题,特别是记忆与泛化之间的关系
5. 从业者应优先考虑推理SFT的数据集质量而非数量,这可能在提升性能的同时降低数据收集成本
:::
