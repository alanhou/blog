---
title:
  en: "Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations"
  zh: "智能体记忆系统解剖:评估与系统局限性的分类与实证分析"
description:
  en: "A comprehensive survey examining why agentic memory systems for LLM agents underperform their theoretical promise, analyzing architectural taxonomies and empirical limitations including benchmark saturation, metric misalignment, and system-level costs."
  zh: "一项全面的综述研究,探讨大语言模型智能体记忆系统为何未能达到理论预期,分析架构分类和实证局限性,包括基准饱和、指标错位和系统级成本。"
date: 2026-02-24
tags: ["arxiv", "ai", "cs.cl", "cs.ai"]
image: "/arxiv-visuals/arxiv-anatomy-of-agentic-memory-taxonomy-and.png"
---

:::en
**Paper**: [2602.19320](https://arxiv.org/abs/2602.19320)
**Authors**: Dongming Jiang, Yi Li, Songtao Wei, Jinxin Yang, Ayushi Kishore, Alysa Zhao, Dingyi Kang, Xu Hu, Feng Chen, Qiannan Li
**Categories**: cs.CL, cs.AI

## Abstract

This survey provides a critical examination of agentic memory systems that enable LLM agents to maintain state across extended interactions. While these systems promise to overcome fixed context window limitations and support long-horizon reasoning, their empirical performance often falls short of theoretical expectations. The authors present a structured taxonomy of Memory-Augmented Generation (MAG) systems based on four core memory structures, then systematically analyze the pain points that limit current implementations: underscaled benchmarks that saturate quickly, evaluation metrics misaligned with semantic utility, significant performance variance across different backbone models, and substantial latency and throughput overhead from memory maintenance operations. By connecting architectural choices to empirical limitations, this work clarifies the gap between promise and practice in agentic memory systems.

## Key Contributions

- A concise taxonomy of MAG systems organized around four fundamental memory structures, providing a unified framework for understanding diverse architectural approaches
- Systematic identification of benchmark saturation effects, demonstrating how existing evaluation datasets fail to capture the complexity of real-world memory-dependent tasks
- Analysis of metric validity and judge sensitivity issues, revealing how current evaluation approaches may not accurately reflect semantic utility
- Empirical evidence of backbone-dependent accuracy variations, showing that memory system performance is not architecture-agnostic
- Quantification of system-level costs including latency and throughput overhead introduced by memory maintenance operations

## Memory Structure Taxonomy

The paper introduces a four-category taxonomy that organizes agentic memory systems by their underlying memory structures. This classification moves beyond surface-level architectural differences to focus on fundamental organizational principles.

The first category encompasses systems using flat memory structures, where information is stored without hierarchical organization. These systems prioritize simplicity and direct access but may struggle with scalability as memory grows. The second category includes hierarchical memory structures that organize information in tree-like or nested arrangements, enabling more efficient retrieval through structured navigation but introducing complexity in maintenance.

The third category covers graph-based memory structures that represent relationships between memory elements explicitly. These systems excel at capturing complex interdependencies but face challenges in efficient traversal and update operations. The fourth category comprises hybrid approaches that combine multiple structural paradigms to balance different performance characteristics.

This taxonomy reveals that architectural choices in memory structure directly influence the types of empirical limitations systems encounter, creating a foundation for understanding why certain designs underperform in specific scenarios.

## Empirical Limitations and Pain Points

The survey identifies four critical pain points that explain the performance gap in current agentic memory systems.

Benchmark saturation emerges as a significant issue, where existing evaluation datasets are too small or too simple to meaningfully differentiate between systems. Many benchmarks can be solved with relatively basic memory mechanisms, failing to stress-test the sophisticated capabilities that advanced systems claim to provide. This saturation effect masks real performance differences and provides false confidence in system capabilities.

Metric validity and judge sensitivity present another challenge. Current evaluation metrics often focus on surface-level correctness rather than semantic utility. When using LLM-as-judge approaches, the choice of judge model significantly impacts evaluation outcomes, introducing instability and making cross-study comparisons difficult. Metrics may reward systems that produce plausible-sounding but semantically incorrect responses.

Backbone-dependent accuracy reveals that memory system performance varies dramatically based on the underlying LLM. A memory architecture that performs well with one backbone model may fail with another, suggesting that memory mechanisms are not as modular or transferable as often assumed. This dependency complicates system design and deployment decisions.

System-level costs including latency and throughput overhead are frequently overlooked in evaluations that focus solely on accuracy. Memory maintenance operations—retrieval, update, consolidation—introduce substantial computational costs that can make systems impractical for real-world deployment despite strong accuracy metrics.

## Implications for Future Research

This analysis has several important implications for advancing agentic memory systems. First, the field needs more challenging, diverse benchmarks that can differentiate between sophisticated memory mechanisms. These benchmarks should include tasks requiring genuine long-horizon reasoning and complex state maintenance.

Second, evaluation methodologies must evolve beyond simple accuracy metrics to capture semantic utility, system efficiency, and robustness across different backbone models. Multi-dimensional evaluation frameworks that consider accuracy, latency, throughput, and memory efficiency simultaneously would provide more realistic performance pictures.

Third, architectural research should explicitly account for backbone model characteristics when designing memory systems. Rather than pursuing one-size-fits-all solutions, the field may benefit from developing memory architectures optimized for specific model families or capabilities.

Fourth, system-level optimization deserves equal attention to algorithmic innovation. Reducing memory maintenance overhead through efficient indexing, caching strategies, and incremental update mechanisms could unlock practical deployment of theoretically sound approaches.

## Takeaways

1. Current agentic memory systems underperform their theoretical promise due to systematic evaluation and implementation limitations rather than fundamental architectural flaws
2. The four-category memory structure taxonomy (flat, hierarchical, graph-based, hybrid) provides a unified framework for understanding diverse MAG system designs
3. Benchmark saturation is a critical issue—existing evaluation datasets are too simple to meaningfully differentiate between sophisticated memory mechanisms
4. Evaluation metrics often misalign with semantic utility, and LLM-as-judge approaches introduce significant sensitivity to judge model selection
5. Memory system performance exhibits strong backbone model dependency, challenging assumptions about architectural modularity and transferability
6. System-level costs including latency and throughput overhead are frequently overlooked but can render otherwise accurate systems impractical for deployment
7. Future progress requires simultaneous advances in benchmark design, evaluation methodology, backbone-aware architecture, and system-level optimization
:::

:::zh
**论文**: [2602.19320](https://arxiv.org/abs/2602.19320)
**作者**: Dongming Jiang, Yi Li, Songtao Wei, Jinxin Yang, Ayushi Kishore, Alysa Zhao, Dingyi Kang, Xu Hu, Feng Chen, Qiannan Li
**分类**: cs.CL, cs.AI

## 摘要

本综述对使大语言模型智能体能够在长时间交互中维持状态的记忆系统进行了批判性审视。尽管这些系统承诺克服固定上下文窗口的限制并支持长期推理,但其实证性能往往未能达到理论预期。作者提出了基于四种核心记忆结构的记忆增强生成(MAG)系统分类法,然后系统性地分析了限制当前实现的痛点:快速饱和的小规模基准测试、与语义效用错位的评估指标、不同骨干模型间的显著性能差异,以及记忆维护操作带来的大量延迟和吞吐量开销。通过将架构选择与实证局限性联系起来,本研究阐明了智能体记忆系统在承诺与实践之间的差距。

## 主要贡献

- 提出围绕四种基本记忆结构组织的MAG系统简明分类法,为理解多样化架构方法提供统一框架
- 系统性识别基准饱和效应,展示现有评估数据集如何无法捕捉真实世界记忆依赖任务的复杂性
- 分析指标有效性和评判者敏感性问题,揭示当前评估方法可能无法准确反映语义效用
- 提供骨干模型依赖准确性变化的实证证据,表明记忆系统性能并非架构无关
- 量化系统级成本,包括记忆维护操作引入的延迟和吞吐量开销

## 记忆结构分类法

论文引入了四类分类法,根据底层记忆结构组织智能体记忆系统。这种分类超越了表面的架构差异,聚焦于基本的组织原则。

第一类包含使用扁平记忆结构的系统,信息存储时不带层次组织。这些系统优先考虑简单性和直接访问,但随着记忆增长可能在可扩展性上遇到困难。第二类包括层次记忆结构,以树状或嵌套方式组织信息,通过结构化导航实现更高效的检索,但在维护上引入了复杂性。

第三类涵盖基于图的记忆结构,显式表示记忆元素之间的关系。这些系统擅长捕捉复杂的相互依赖关系,但在高效遍历和更新操作上面临挑战。第四类包括混合方法,结合多种结构范式以平衡不同的性能特征。

这一分类法揭示了记忆结构的架构选择直接影响系统遇到的实证局限类型,为理解某些设计在特定场景下表现不佳的原因奠定了基础。

## 实证局限性与痛点

综述识别了四个关键痛点,解释了当前智能体记忆系统的性能差距。

基准饱和成为一个重要问题,现有评估数据集过小或过于简单,无法有意义地区分不同系统。许多基准可以用相对基础的记忆机制解决,未能对高级系统声称提供的复杂能力进行压力测试。这种饱和效应掩盖了真实的性能差异,并对系统能力产生虚假信心。

指标有效性和评判者敏感性带来另一挑战。当前评估指标往往关注表面正确性而非语义效用。使用大语言模型作为评判者的方法中,评判模型的选择显著影响评估结果,引入不稳定性并使跨研究比较变得困难。指标可能奖励产生听起来合理但语义错误的响应的系统。

骨干模型依赖准确性揭示记忆系统性能基于底层大语言模型而剧烈变化。在一个骨干模型上表现良好的记忆架构可能在另一个上失败,表明记忆机制并非如通常假设的那样模块化或可迁移。这种依赖性使系统设计和部署决策复杂化。

系统级成本包括延迟和吞吐量开销,在仅关注准确性的评估中经常被忽视。记忆维护操作——检索、更新、整合——引入大量计算成本,可能使系统尽管具有强大的准确性指标,但在实际部署中不切实际。

## 对未来研究的启示

这一分析对推进智能体记忆系统具有几个重要启示。首先,该领域需要更具挑战性、更多样化的基准,能够区分复杂的记忆机制。这些基准应包括需要真正长期推理和复杂状态维护的任务。

其次,评估方法必须超越简单的准确性指标,捕捉语义效用、系统效率和跨不同骨干模型的鲁棒性。同时考虑准确性、延迟、吞吐量和记忆效率的多维评估框架将提供更现实的性能图景。

第三,架构研究在设计记忆系统时应明确考虑骨干模型特征。该领域可能受益于开发针对特定模型家族或能力优化的记忆架构,而非追求一刀切的解决方案。

第四,系统级优化应得到与算法创新同等的关注。通过高效索引、缓存策略和增量更新机制减少记忆维护开销,可以解锁理论上合理方法的实际部署。

## 要点总结

1. 当前智能体记忆系统未能达到理论预期是由于系统性的评估和实现局限,而非根本性的架构缺陷
2. 四类记忆结构分类法(扁平、层次、基于图、混合)为理解多样化MAG系统设计提供了统一框架
3. 基准饱和是一个关键问题——现有评估数据集过于简单,无法有意义地区分复杂的记忆机制
4. 评估指标往往与语义效用错位,大语言模型作为评判者的方法对评判模型选择引入显著敏感性
5. 记忆系统性能表现出强烈的骨干模型依赖性,挑战了关于架构模块化和可迁移性的假设
6. 系统级成本包括延迟和吞吐量开销经常被忽视,但可能使原本准确的系统在部署中不切实际
7. 未来进展需要在基准设计、评估方法、骨干感知架构和系统级优化方面同时取得进展
:::
