---
title:
  en: "Hung-yi Lee ML 2025 Lecture 8: When LLMs Think Too Much - Controlling Reasoning Length"
  zh: "李宏毅机器学习2025 第八讲：当大模型想太多 - 控制推理长度"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on LLM reasoning efficiency: why longer isn't always better, and how to control reasoning length"
  zh: "台大李宏毅教授课程笔记：LLM推理效率，为何更长不一定更好，以及如何控制推理长度"
date: 2025-02-23
tags: ["ai", "llm", "reasoning", "efficiency", "chain-of-thought", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/ip3XnTpcxoA/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 8
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="ip3XnTpcxoA" title="李宏毅机器学习2025 第八讲：推理长度控制" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## The Problem: LLMs Think Too Much

Modern reasoning models like DeepSeek-R1 can produce extremely long outputs—often 5,000 to 10,000 tokens of reasoning for a single problem. But is longer reasoning always better?

**Key observation**: Research shows that longer reasoning doesn't necessarily mean higher accuracy. In fact, there's often a negative correlation—longer answers can indicate the model is struggling.

## The Giraffe Analogy

Professor Lee uses the giraffe's evolution as an analogy:
- Giraffes evolved long necks to reach high leaves
- But necks became so long they're actually disadvantageous—giraffes must bend down to drink water and eat low vegetation
- During famines, short-necked giraffes actually survive better because they need less energy

**The lesson**: More isn't always better. The optimal solution uses just enough resources to solve the problem.

## Research Findings

### Longer ≠ Better

Multiple papers show:
- Reasoning length and accuracy are not positively correlated
- Sometimes they're negatively correlated
- The correlation might be spurious—harder problems naturally require longer reasoning AND have lower accuracy

### The Real Relationship

When controlling for problem difficulty:
- Easy problems: Short reasoning is sufficient
- Hard problems: Longer reasoning is needed
- Forcing uniform length hurts performance on both

## Methods to Control Reasoning Length

### Method 1: Prompt Engineering (Chain of Draft)

Instead of "think step by step," use prompts like:
- "Think step by step, but keep each step under 5 words"
- "Provide a draft outline, not detailed reasoning"

**Results**: Can reduce output length significantly while maintaining most accuracy.

### Method 2: RL with Length Penalty

Add length constraints to the RL reward function:

```
Reward = Correctness - |Target_Length - Actual_Length|
```

**Key insight**: Don't use absolute length limits. Use relative limits based on problem difficulty:
- If reasoning is shorter than average needed → OK
- If reasoning is longer than average needed → Penalty

### Method 3: Knowledge Distillation with Short Answers

1. Have a teacher model solve problems multiple times
2. Collect the shortest correct reasoning paths
3. Train student model only on these short examples

**Result**: Student learns to reason efficiently without verbose explanations.

### Method 4: Implicit Chain-of-Thought

Train models to reason internally without outputting the reasoning process:
- Model learns to "think" without writing it down
- Like learning mental math instead of showing work

**Surprising finding**: Models can maintain accuracy even without explicit reasoning output, suggesting they've internalized the reasoning process.

## Practical Implications

### For Users
- Longer model outputs don't guarantee better answers
- Consider using models that can control output length based on your compute budget

### For Developers
- Don't just optimize for accuracy—consider efficiency
- The best AI isn't one that always produces perfect answers, but one that produces good answers within resource constraints

### The Best Engineer Analogy
> "The best engineer isn't one who does everything perfectly, but one who does the best possible work within limited resources."

## Experimental Results

Testing on math benchmarks (AIME, GSM8K):
- Models trained with length control can match or exceed verbose models
- 20-40% reduction in output length with minimal accuracy loss
- Works across different model sizes and architectures

## Key Takeaways

1. **Excess is as bad as deficiency** (过犹不及): Too much reasoning can be wasteful
2. **Adaptive length is key**: Different problems need different amounts of reasoning
3. **Efficiency matters**: In real applications, compute resources are limited
4. **Multiple approaches work**: Prompt engineering, RL, and distillation can all help
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 问题：大模型想太多

现代推理模型如DeepSeek-R1可以产生极长的输出——单个问题往往需要5000到10000个token的推理。但更长的推理一定更好吗？

**关键观察**：研究表明，更长的推理并不一定意味着更高的准确率。事实上，往往存在负相关——更长的答案可能表明模型在挣扎。

## 长颈鹿的类比

李教授用长颈鹿的演化作为类比：
- 长颈鹿演化出长脖子是为了够到高处的树叶
- 但脖子变得太长反而成为劣势——长颈鹿必须弯下脖子喝水和吃低处的植物
- 在饥荒期间，短脖子的长颈鹿实际上存活率更高，因为它们需要的能量更少

**教训**：更多不一定更好。最优解只使用刚好足够的资源来解决问题。

## 研究发现

### 更长 ≠ 更好

多篇论文表明：
- 推理长度和准确率不是正相关的
- 有时甚至是负相关的
- 这种相关性可能是虚假的——更难的问题自然需要更长的推理，同时准确率也更低

### 真实的关系

当控制问题难度时：
- 简单问题：短推理就足够了
- 困难问题：需要更长的推理
- 强制统一长度会损害两种情况的表现

## 控制推理长度的方法

### 方法一：提示工程（Chain of Draft）

不用"一步一步思考"，而是使用这样的提示：
- "一步一步思考，但每步不超过5个字"
- "提供草稿大纲，而非详细推理"

**结果**：可以显著减少输出长度，同时保持大部分准确率。

### 方法二：带长度惩罚的强化学习

在RL奖励函数中加入长度约束：

```
奖励 = 正确性 - |目标长度 - 实际长度|
```

**关键洞察**：不要使用绝对长度限制。使用基于问题难度的相对限制：
- 如果推理比平均所需更短 → 可以
- 如果推理比平均所需更长 → 惩罚

### 方法三：用短答案进行知识蒸馏

1. 让教师模型多次解决问题
2. 收集最短的正确推理路径
3. 只用这些短示例训练学生模型

**结果**：学生学会高效推理，而不需要冗长的解释。

### 方法四：隐式思维链

训练模型在内部推理而不输出推理过程：
- 模型学会"思考"而不写出来
- 就像学习心算而不是展示计算过程

**惊人发现**：即使没有显式推理输出，模型也能保持准确率，这表明它们已经内化了推理过程。

## 实践启示

### 对用户
- 更长的模型输出不保证更好的答案
- 考虑使用可以根据计算预算控制输出长度的模型

### 对开发者
- 不要只优化准确率——也要考虑效率
- 最好的AI不是总能产生完美答案的，而是能在资源限制内产生好答案的

### 最佳工程师类比
> "最好的工程师不是把事情做到完美的人，而是在有限资源下把事情做到最好的人。"

## 实验结果

在数学基准测试（AIME、GSM8K）上：
- 经过长度控制训练的模型可以匹配或超越冗长模型
- 输出长度减少20-40%，准确率损失最小
- 在不同模型大小和架构上都有效

## 关键要点

1. **过犹不及**：太多推理可能是浪费
2. **自适应长度是关键**：不同问题需要不同量的推理
3. **效率很重要**：在实际应用中，计算资源是有限的
4. **多种方法都有效**：提示工程、RL和蒸馏都可以帮助
:::
