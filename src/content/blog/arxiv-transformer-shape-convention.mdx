---
title:
  en: "Revisiting the Shape Convention of Transformer Language Models"
  zh: "重新审视Transformer语言模型的形状约定"
description:
  en: "MediaTek Research re-examines tensor shape conventions in transformers, revealing that alternative configurations can improve efficiency and performance"
  zh: "联发科研究院重新审视Transformer中的张量形状约定，揭示替代配置可以提高效率和性能"
date: 2026-02-05
tags: ["arxiv", "ai", "transformer", "architecture", "efficiency", "cs.CL"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.06471](https://arxiv.org/abs/2602.06471)
**Authors**: MediaTek Research
**Categories**: cs.CL

## Abstract

This paper re-examines the tensor shape conventions used in transformer language model architectures. The standard transformer uses specific dimension ratios between model width, FFN hidden size, and attention head dimensions that have been largely inherited from the original "Attention Is All You Need" paper without systematic justification. This work investigates whether alternative shape configurations can yield better efficiency-performance tradeoffs.

## Key Contributions

- **Systematic analysis**: Rigorous examination of how dimension ratios affect transformer performance
- **Alternative conventions**: Identifies configurations that deviate from standard practice but yield improvements
- **Practical implications**: Guidance for architecture design decisions in new model development

## The Shape Convention Problem

The original Transformer paper established dimension ratios that have been widely adopted: model dimension d_model, FFN hidden size 4×d_model, and attention head dimension d_model/num_heads. These ratios have been inherited across GPT, LLaMA, Qwen, and most modern architectures with minimal questioning.

But are these ratios optimal? Or are they historical artifacts that persist through convention?

## Key Findings

The paper systematically varies dimension ratios and evaluates the impact on:

- **Model quality**: Perplexity and downstream task performance
- **Training efficiency**: Convergence speed and compute utilization
- **Inference efficiency**: Throughput and latency characteristics

The results suggest that alternative configurations can yield meaningful improvements, particularly when accounting for modern hardware characteristics that differ from the GPU landscape when transformers were first designed.

## Practical Implications

- Architecture designers should treat dimension ratios as hyperparameters to optimize, not fixed constants
- The optimal ratios may depend on the target hardware and deployment scenario
- Small changes in shape conventions can compound across many layers

## Takeaways

1. **Question conventions**: Inherited design choices deserve periodic re-evaluation
2. **Hardware co-design**: Optimal model shapes depend on the target compute platform
3. **Low-hanging fruit**: Shape optimization is a simple lever that's often overlooked
:::

:::zh
**论文**: [2602.06471](https://arxiv.org/abs/2602.06471)
**作者**: MediaTek Research（联发科研究院）
**分类**: cs.CL

## 摘要

本文重新审视了Transformer语言模型架构中使用的张量形状约定。标准Transformer在模型宽度、FFN隐藏层大小和注意力头维度之间使用特定的维度比率，这些比率在很大程度上继承自原始的"Attention Is All You Need"论文，缺乏系统性的论证。本工作研究替代形状配置是否能产生更好的效率-性能权衡。

## 主要贡献

- **系统性分析**：严格检验维度比率如何影响Transformer性能
- **替代约定**：识别偏离标准实践但产生改进的配置
- **实际意义**：为新模型开发中的架构设计决策提供指导

## 形状约定问题

原始Transformer论文建立了被广泛采用的维度比率：模型维度d_model、FFN隐藏层大小4×d_model、注意力头维度d_model/num_heads。这些比率在GPT、LLaMA、Qwen和大多数现代架构中被继承，几乎没有被质疑。

但这些比率是最优的吗？还是通过约定延续的历史遗留？

## 关键发现

论文系统地改变维度比率并评估对以下方面的影响：

- **模型质量**：困惑度和下游任务性能
- **训练效率**：收敛速度和计算利用率
- **推理效率**：吞吐量和延迟特性

结果表明，替代配置可以产生有意义的改进，特别是考虑到现代硬件特性与Transformer首次设计时的GPU环境不同。

## 实际意义

- 架构设计者应将维度比率视为需要优化的超参数，而非固定常数
- 最优比率可能取决于目标硬件和部署场景
- 形状约定的小变化可以在多层中累积

## 要点总结

1. **质疑约定**：继承的设计选择值得定期重新评估
2. **硬件协同设计**：最优模型形状取决于目标计算平台
3. **低垂果实**：形状优化是一个经常被忽视的简单杠杆
:::
