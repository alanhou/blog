---
title:
  en: "RWML: Reinforcement World Model Learning for LLM-based Agents"
  zh: "RWML：基于LLM智能体的强化世界模型学习"
description:
  en: "A self-supervised approach enabling LLMs to develop action-conditioned world models using sim-to-real gap rewards for semantic consistency"
  zh: "一种自监督方法，使LLM能够利用模拟-现实差距奖励开发动作条件世界模型，实现语义一致性"
date: 2026-02-05
tags: ["arxiv", "ai", "agents", "world-models", "reinforcement-learning", "self-supervised", "cs.CL"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.05842](https://arxiv.org/abs/2602.05842)
**Authors**: Xiao Yu, Baolin Peng, Ruize Xu, Yelong Shen, Pengcheng He, Suman Nath, Nikhil Singh, Jiangfeng Gao, Zhou Yu
**Categories**: cs.CL

## Abstract

RWML is a self-supervised approach enabling language models to develop action-conditioned world models for textual environments. The method uses sim-to-real gap rewards to align predicted states with actual environment outcomes. Unlike token-level prediction, this technique emphasizes semantic consistency rather than exact wording, reducing model collapse risks.

## Key Contributions

- **Self-supervised world model learning**: No need for expert demonstrations or environment-specific reward engineering
- **Sim-to-real gap rewards**: Uses pre-trained embedding space for semantic alignment between predicted and actual states
- **Robust training signal**: More stable than "LLM-as-a-judge" approaches

## The Agent World Model Problem

LLM-based agents operating in interactive environments need to predict the consequences of their actions. Current approaches face several challenges:

- **Token-level prediction**: Requiring exact text reproduction is brittle and prone to model collapse
- **LLM-as-a-judge**: Using another LLM to evaluate predictions is expensive and inconsistent
- **Expert demonstrations**: Requiring human-labeled trajectories limits scalability

## RWML: Self-Supervised World Models

RWML takes a different approach by training the LLM to predict **semantic outcomes** rather than exact text:

### Sim-to-Real Gap Rewards

Instead of comparing predicted and actual states at the token level, RWML computes similarity in a pre-trained embedding space. This means the model is rewarded for capturing the **meaning** of state transitions, not their exact wording.

### Self-Supervised Training

The agent generates predictions about what will happen after taking an action, then receives reward based on how semantically close its prediction was to the actual outcome. No expert labels needed.

## Results

On ALFWorld and τ² Bench benchmarks:

- Outperformed direct reward RL by **6.9 and 5.7 points** respectively
- Achieved performance comparable to expert-trained models
- More robust training dynamics than token-level prediction approaches

## Why This Matters

1. **Scalable agent training**: Self-supervised approach eliminates the need for expert demonstrations
2. **Semantic robustness**: Embedding-space rewards are more stable than token-level or LLM-judge approaches
3. **World model as auxiliary task**: Learning to predict outcomes improves the agent's planning ability
4. **Generalizable framework**: Applicable to any text-based interactive environment
:::

:::zh
**论文**: [2602.05842](https://arxiv.org/abs/2602.05842)
**作者**: Xiao Yu, Baolin Peng, Ruize Xu, Yelong Shen, Pengcheng He, Suman Nath, Nikhil Singh, Jiangfeng Gao, Zhou Yu
**分类**: cs.CL

## 摘要

RWML是一种自监督方法，使语言模型能够为文本环境开发动作条件世界模型。该方法使用模拟-现实差距奖励来对齐预测状态与实际环境结果。与token级预测不同，该技术强调语义一致性而非精确措辞，降低了模型崩溃风险。

## 主要贡献

- **自监督世界模型学习**：无需专家演示或环境特定的奖励工程
- **模拟-现实差距奖励**：使用预训练嵌入空间进行预测状态与实际状态之间的语义对齐
- **稳健的训练信号**：比"LLM作为评判者"方法更稳定

## 智能体世界模型问题

在交互环境中运行的基于LLM的智能体需要预测其行动的后果。当前方法面临几个挑战：

- **Token级预测**：要求精确文本复现是脆弱的，容易导致模型崩溃
- **LLM作为评判者**：使用另一个LLM评估预测既昂贵又不一致
- **专家演示**：需要人工标注的轨迹限制了可扩展性

## RWML：自监督世界模型

RWML采用不同的方法，训练LLM预测**语义结果**而非精确文本：

### 模拟-现实差距奖励

RWML不在token级别比较预测和实际状态，而是在预训练嵌入空间中计算相似度。这意味着模型因捕获状态转换的**含义**而获得奖励，而非其精确措辞。

### 自监督训练

智能体生成关于采取行动后会发生什么的预测，然后根据其预测与实际结果的语义接近程度获得奖励。无需专家标签。

## 实验结果

在ALFWorld和τ² Bench基准测试上：

- 分别比直接奖励RL**高出6.9和5.7分**
- 达到与专家训练模型相当的性能
- 比token级预测方法具有更稳健的训练动态

## 重要意义

1. **可扩展的智能体训练**：自监督方法消除了对专家演示的需求
2. **语义鲁棒性**：嵌入空间奖励比token级或LLM评判方法更稳定
3. **世界模型作为辅助任务**：学习预测结果改善了智能体的规划能力
4. **可泛化框架**：适用于任何基于文本的交互环境
:::
