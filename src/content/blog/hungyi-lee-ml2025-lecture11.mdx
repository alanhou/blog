---
title:
  en: "Hung-yi Lee ML 2025 Lecture 11: Model Merging - Equipping Your Foundation Model with Task Vectors"
  zh: "李宏毅机器学习2025 第十一讲：模型合并 - 为基础模型装备任务向量"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on Model Merging: combining models without training, Task Vectors, and the future of modular AI"
  zh: "台大李宏毅教授课程笔记：模型合并，无需训练的模型组合，任务向量，以及模块化AI的未来"
date: 2025-02-23
tags: ["ai", "llm", "model-merging", "task-vectors", "dare", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/jFUwoCkdqAo/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 11
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="jFUwoCkdqAo" title="李宏毅机器学习2025 第十一讲：模型合并" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is Model Merging?

Model Merging combines multiple fine-tuned models into one **without any additional training**. It's like equipping your character in an RPG with different weapons and armor!

### The Key Insight: Task Vectors

When you fine-tune a foundation model on a task:
- Original model parameters: θ
- Fine-tuned parameters: θ_A
- **Task Vector**: τ_A = θ_A - θ

This task vector represents the "skill" learned during fine-tuning.

## The Magic of Task Vectors

### Addition: Combining Skills
If you have:
- τ_A: Chinese language ability
- τ_B: Safety alignment

You can create a model with BOTH abilities:
```
θ_new = θ + τ_A + τ_B
```

No training required!

### Subtraction: Removing Skills
Want to remove a capability? Subtract the task vector:
```
θ_new = θ - τ_B  // Remove skill B
```

### Analogy: A = B implies C = D
If you know the relationship between tasks:
- Task A → Task B (e.g., English base → English chat)
- You can apply the same transformation to Task C:
```
τ_D = τ_C + (τ_B - τ_A)
```

## Real-World Examples

### Example 1: Chinese LLaMA with Alignment
- LLaMA-2-base: English foundation model
- LLaMA-2-Chat: English with safety alignment
- Chinese LLaMA: Chinese capability but no alignment

**Solution**:
```
τ_alignment = LLaMA-2-Chat - LLaMA-2-base
Chinese_aligned = Chinese_LLaMA + τ_alignment
```

Result: A Chinese model with safety alignment!

### Example 2: Creating a "Saint" Model
1. Fine-tune a model on profanity data (learns to curse)
2. Calculate the "profanity vector"
3. **Subtract** it from a normal model
4. Result: A model that refuses to use profanity!

### Example 3: Vision + Code Review
- Model A: Can see images (vision capability)
- Model B: Can review code (but text-only)

Merge them → A model that can review code from screenshots!

## Why Does This Work?

### The Hypothesis
Different tasks modify different parameters in the neural network. If tasks don't overlap much in which parameters they change, merging works well.

### When Merging Fails
- Tasks modify the same parameters → Interference
- Too many tasks merged → Degradation
- Incompatible foundation models → Failure

## Advanced Techniques

### DARE (Drop And REscale)
Problem: Different fine-tuned models change many overlapping parameters.

Solution:
1. Randomly drop some parameter changes
2. Rescale remaining changes
3. Merge the sparse task vectors

This reduces interference between tasks!

### TIES (Trim, Elect Sign, Merge)
Another technique to reduce conflicts:
1. Trim small changes (likely noise)
2. Resolve sign conflicts
3. Merge remaining changes

## The Task Vector Store Vision

Imagine a future where:
1. Foundation models are shared publicly
2. People create and share Task Vectors for specific skills
3. You can "shop" for capabilities and equip your model
4. No training needed - just add vectors!

Like an RPG equipment store for AI!

## Applications Beyond Text

### Speech Recognition
Problem: No real audio data for specialized domains (legal, medical meetings)

Solution:
1. Use TTS to synthesize domain-specific audio
2. Fine-tune on synthetic data
3. Calculate "Synthetic2Real" task vector
4. Apply to real domain → Works surprisingly well!

## Key Factors for Success

1. **Same Foundation Model**: Models must start from the same base
2. **Same Architecture**: Network structure must match
3. **Non-overlapping Changes**: Tasks should modify different parameters
4. **Larger Models Work Better**: More parameters = less overlap

## Limitations

- Not guaranteed to work
- Can find many failure cases
- Generalization is limited
- Still an active research area

## Key Takeaways

1. **Task Vectors are powerful**: Represent learned skills as parameter differences

2. **No training needed**: Just arithmetic on parameters

3. **Modular AI is possible**: Mix and match capabilities

4. **DARE/TIES help**: Reduce interference between merged models

5. **Future potential**: Task Vector marketplaces could democratize AI customization
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 什么是模型合并？

模型合并将多个微调后的模型组合成一个，**无需任何额外训练**。就像在RPG游戏中为你的角色装备不同的武器和盔甲！

### 关键洞察：任务向量

当你在某个任务上微调基础模型时：
- 原始模型参数：θ
- 微调后参数：θ_A
- **任务向量**：τ_A = θ_A - θ

这个任务向量代表了微调过程中学到的"技能"。

## 任务向量的魔力

### 加法：组合技能
如果你有：
- τ_A：中文能力
- τ_B：安全对齐

你可以创建一个同时具有两种能力的模型：
```
θ_new = θ + τ_A + τ_B
```

不需要训练！

### 减法：移除技能
想要移除某种能力？减去任务向量：
```
θ_new = θ - τ_B  // 移除技能B
```

### 类比：A之于B等于C之于D
如果你知道任务之间的关系：
- 任务A → 任务B（例如，英文基础 → 英文对话）
- 你可以将同样的转换应用到任务C：
```
τ_D = τ_C + (τ_B - τ_A)
```

## 真实世界的例子

### 例子1：带对齐的中文LLaMA
- LLaMA-2-base：英文基础模型
- LLaMA-2-Chat：带安全对齐的英文
- 中文LLaMA：有中文能力但没有对齐

**解决方案**：
```
τ_alignment = LLaMA-2-Chat - LLaMA-2-base
Chinese_aligned = Chinese_LLaMA + τ_alignment
```

结果：一个带安全对齐的中文模型！

### 例子2：创建"圣人"模型
1. 在脏话数据上微调模型（学会说脏话）
2. 计算"脏话向量"
3. 从正常模型中**减去**它
4. 结果：一个拒绝使用脏话的模型！

### 例子3：视觉 + 代码审查
- 模型A：可以看图（视觉能力）
- 模型B：可以审查代码（但只能处理文字）

合并它们 → 一个可以从截图审查代码的模型！

## 为什么这有效？

### 假设
不同的任务修改神经网络中不同的参数。如果任务在修改的参数上重叠不多，合并就能很好地工作。

### 合并失败的情况
- 任务修改相同的参数 → 干扰
- 合并太多任务 → 退化
- 不兼容的基础模型 → 失败

## 高级技术

### DARE（丢弃并重新缩放）
问题：不同的微调模型改变了许多重叠的参数。

解决方案：
1. 随机丢弃一些参数变化
2. 重新缩放剩余的变化
3. 合并稀疏的任务向量

这减少了任务之间的干扰！

### TIES（修剪、选择符号、合并）
另一种减少冲突的技术：
1. 修剪小的变化（可能是噪声）
2. 解决符号冲突
3. 合并剩余的变化

## 任务向量商店愿景

想象一个未来：
1. 基础模型公开共享
2. 人们为特定技能创建和分享任务向量
3. 你可以"购买"能力并装备你的模型
4. 不需要训练——只需添加向量！

就像AI的RPG装备商店！

## 文本之外的应用

### 语音识别
问题：没有专业领域（法律、医疗会议）的真实音频数据

解决方案：
1. 使用TTS合成领域特定的音频
2. 在合成数据上微调
3. 计算"合成到真实"任务向量
4. 应用到真实领域 → 效果出奇地好！

## 成功的关键因素

1. **相同的基础模型**：模型必须从相同的基础开始
2. **相同的架构**：网络结构必须匹配
3. **不重叠的变化**：任务应该修改不同的参数
4. **更大的模型效果更好**：更多参数 = 更少重叠

## 局限性

- 不保证有效
- 可以找到很多失败案例
- 泛化性有限
- 仍是活跃的研究领域

## 关键要点

1. **任务向量很强大**：将学到的技能表示为参数差异

2. **不需要训练**：只是参数的算术运算

3. **模块化AI是可能的**：混合搭配各种能力

4. **DARE/TIES有帮助**：减少合并模型之间的干扰

5. **未来潜力**：任务向量市场可以使AI定制民主化
:::
