---
title:
  en: "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?"
  zh: "扩散语言模型为何难以实现真正的并行(非自回归)解码?"
description:
  en: "This paper identifies why diffusion language models default to autoregressive-like behavior and proposes NAP, a data-centric approach that enables genuinely parallel token generation through independent reasoning trajectories."
  zh: "本文揭示了扩散语言模型倾向于自回归行为的原因,并提出NAP方法,通过独立推理轨迹实现真正的并行token生成。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.cl", "cs.ai"]
image: "/arxiv-visuals/why-diffusion-language-models-struggle-with/HeroScene.png"
---

![Concept animation](/arxiv-visuals/why-diffusion-language-models-struggle-with/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/why-diffusion-language-models-struggle-with/HeroScene.png)



:::en
**Paper**: [2602.23225](https://arxiv.org/abs/2602.23225)
**Authors**: Pengxiang Li, Dilxat Muhtar, Lu Yin, Tianlong Chen, Shiwei Liu
**Categories**: cs.CL, cs.AI

## Abstract

Diffusion Language Models (DLMs) promise parallel token generation, yet in practice they often converge to autoregressive (AR)-like left-to-right decoding patterns. This paper diagnoses the root cause: a fundamental mismatch between DLM training objectives and the sequential structure of standard training data, including pretraining corpora and chain-of-thought (CoT) supervision. The authors propose NAP (Non-Autoregressive Parallel DLMs), a data-centric solution that curates training examples as multiple independent reasoning trajectories and employs parallel-forced decoding to encourage multi-token parallel updates. Experiments on math reasoning benchmarks demonstrate that NAP achieves superior performance under parallel decoding compared to DLMs trained on standard sequential CoT data, with performance gains increasing as parallelism scales.

## Key Contributions

- Identifies the data-objective mismatch as the primary driver of AR-like behavior in DLMs, where sequential training data conflicts with parallel generation objectives
- Proposes NAP, a data-centric approach that restructures supervision into multiple independent reasoning trajectories rather than single sequential chains
- Introduces parallel-forced decoding strategy that actively encourages simultaneous multi-token updates during training
- Demonstrates empirically that NAP outperforms standard DLM training on math reasoning tasks, with gains scaling positively with increased parallelism
- Provides open-source implementation to facilitate further research in non-autoregressive generation

## The Core Problem: Data-Objective Mismatch

The paper reveals a critical insight: while DLMs are theoretically capable of parallel generation, their training on inherently sequential data creates a fundamental tension. Standard pretraining corpora and long CoT supervision exhibit strong left-to-right dependencies, where each token heavily depends on preceding context. When DLMs are trained on such data, they learn these sequential patterns and naturally gravitate toward AR-like decoding dynamics during inference, even though the diffusion framework permits parallel updates.

This mismatch manifests in practical fast DLMs that, despite their architectural capacity for parallelism, decode tokens in a predominantly sequential manner. The sequential bottleneck negates the primary advantage of non-AR models: exploiting parallel hardware to reduce latency and improve scaling with output length. The authors argue that addressing this requires rethinking not just model architecture or training objectives, but fundamentally how supervision data is structured.

## NAP Methodology: Restructuring Supervision

NAP tackles the data-objective mismatch through two complementary strategies:

**Independent Reasoning Trajectories**: Instead of providing single long CoT sequences, NAP curates training examples as multiple independent reasoning paths. Each trajectory can be processed in parallel without strong sequential dependencies. For math reasoning tasks, this means decomposing problems into sub-problems that can be solved simultaneously rather than in strict order.

**Parallel-Forced Decoding**: During training, NAP employs a decoding strategy that actively encourages the model to update multiple tokens simultaneously. This contrasts with standard DLM training where the model may learn to update tokens sequentially even when parallel updates are possible. The parallel-forcing mechanism creates training dynamics that better align with the desired inference behavior.

The combination creates a virtuous cycle: data structure permits parallelism, and training strategy reinforces parallel generation patterns. This alignment between supervision structure and generation objective is the key innovation distinguishing NAP from prior DLM approaches.

## Experimental Results and Analysis

NAP was evaluated on math reasoning benchmarks, comparing against DLMs trained on standard long CoT data. The results reveal several important patterns:

**Scaling with Parallelism**: NAP consistently outperforms baseline DLMs under parallel decoding conditions, with the performance gap widening as the degree of parallelism increases. This validates the core hypothesis that data-centric restructuring enables better exploitation of parallel generation capacity.

**Genuine Non-AR Behavior**: Analysis of decoding dynamics shows that NAP models exhibit more genuinely parallel token updates compared to baselines, which tend toward sequential left-to-right patterns. This behavioral difference directly translates to the performance improvements observed.

**Task Performance**: Beyond decoding dynamics, NAP achieves stronger task-level performance on math reasoning benchmarks. This demonstrates that parallel generation is not merely a computational efficiency gain but can also improve solution quality when properly implemented.

The results suggest that the data-centric approach is not just theoretically motivated but practically effective, offering a principled path toward realizing the full potential of non-AR generation in DLMs.

## Takeaways

1. The primary obstacle to parallel generation in DLMs is not architectural but data-related: sequential training data conflicts with parallel generation objectives
2. NAP demonstrates that restructuring supervision as independent reasoning trajectories enables genuinely non-AR parallel decoding
3. Parallel-forced decoding during training is crucial for encouraging models to learn parallel update patterns rather than defaulting to sequential behavior
4. Performance gains from NAP scale positively with increased parallelism, validating the approach's effectiveness at exploiting parallel hardware
5. Data-centric solutions offer a principled and practical direction for advancing non-autoregressive generation in language models
:::

:::zh
**论文**: [2602.23225](https://arxiv.org/abs/2602.23225)
**作者**: Pengxiang Li, Dilxat Muhtar, Lu Yin, Tianlong Chen, Shiwei Liu
**分类**: cs.CL, cs.AI

## 摘要

扩散语言模型(DLMs)理论上支持并行token生成,但实际应用中往往退化为类似自回归(AR)的从左到右解码模式。本文诊断出根本原因:DLM训练目标与标准训练数据的顺序结构之间存在根本性不匹配,包括预训练语料库和思维链(CoT)监督数据。作者提出NAP(非自回归并行DLMs),这是一种以数据为中心的解决方案,将训练样本构建为多个独立推理轨迹,并采用并行强制解码策略来鼓励多token并行更新。在数学推理基准测试中,NAP在并行解码条件下的性能优于使用标准顺序CoT数据训练的DLMs,且性能提升随并行度增加而扩大。

## 主要贡献

- 识别出数据-目标不匹配是DLMs产生类AR行为的主要驱动因素,顺序训练数据与并行生成目标相冲突
- 提出NAP方法,这是一种以数据为中心的方案,将监督信号重构为多个独立推理轨迹而非单一顺序链
- 引入并行强制解码策略,在训练期间主动鼓励同时进行多token更新
- 通过实验证明NAP在数学推理任务上优于标准DLM训练,且收益随并行度提升而增长
- 提供开源实现以促进非自回归生成领域的进一步研究

## 核心问题:数据-目标不匹配

本文揭示了一个关键洞察:尽管DLMs理论上具备并行生成能力,但在本质上顺序的数据上训练会产生根本性矛盾。标准预训练语料库和长CoT监督数据表现出强烈的从左到右依赖性,每个token严重依赖于前文上下文。当DLMs在这类数据上训练时,它们学习到这些顺序模式,在推理时自然倾向于类AR解码动态,即使扩散框架允许并行更新。

这种不匹配在实际的快速DLMs中表现明显:尽管架构上具备并行能力,但解码token时主要采用顺序方式。顺序瓶颈抵消了非AR模型的主要优势:利用并行硬件降低延迟并改善输出长度的扩展性。作者认为,解决这个问题不仅需要重新思考模型架构或训练目标,更根本的是要重构监督数据的组织方式。

## NAP方法论:重构监督信号

NAP通过两个互补策略解决数据-目标不匹配问题:

**独立推理轨迹**:NAP不提供单一的长CoT序列,而是将训练样本构建为多个独立的推理路径。每条轨迹可以并行处理,无需强顺序依赖。对于数学推理任务,这意味着将问题分解为可以同时求解的子问题,而非严格顺序处理。

**并行强制解码**:训练期间,NAP采用主动鼓励模型同时更新多个token的解码策略。这与标准DLM训练形成对比,后者即使在可以并行更新时,模型也可能学习顺序更新token。并行强制机制创建的训练动态更好地对齐期望的推理行为。

这种组合创造了良性循环:数据结构允许并行,训练策略强化并行生成模式。监督结构与生成目标之间的这种对齐是NAP区别于先前DLM方法的关键创新。

## 实验结果与分析

NAP在数学推理基准上进行评估,与使用标准长CoT数据训练的DLMs进行对比。结果揭示了几个重要模式:

**随并行度扩展**:在并行解码条件下,NAP始终优于基线DLMs,且性能差距随并行度增加而扩大。这验证了核心假设:以数据为中心的重构能够更好地利用并行生成能力。

**真正的非AR行为**:解码动态分析显示,NAP模型表现出更真实的并行token更新,而基线模型倾向于顺序的从左到右模式。这种行为差异直接转化为观察到的性能提升。

**任务性能**:除了解码动态,NAP在数学推理基准的任务级性能上也更强。这表明并行生成不仅是计算效率的提升,在正确实现时还能改善解决方案质量。

结果表明,以数据为中心的方法不仅有理论动机,而且实际有效,为实现DLMs中非AR生成的全部潜力提供了原则性路径。

## 要点总结

1. DLMs实现并行生成的主要障碍不在架构而在数据:顺序训练数据与并行生成目标相冲突
2. NAP证明将监督信号重构为独立推理轨迹能够实现真正的非AR并行解码
3. 训练期间的并行强制解码对于鼓励模型学习并行更新模式而非默认顺序行为至关重要
4. NAP的性能提升随并行度增加而正向扩展,验证了该方法在利用并行硬件方面的有效性
5. 以数据为中心的解决方案为推进语言模型中的非自回归生成提供了原则性和实用性方向
:::
