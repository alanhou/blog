---
title:
  en: "WHOLE: Holistic Hand-Object Reconstruction from Egocentric Videos"
  zh: "WHOLE: 从第一人称视频中整体重建手部与物体运动"
description:
  en: "A generative approach that jointly reconstructs hand and object motion in world space from egocentric videos, achieving state-of-the-art performance by learning interaction priors."
  zh: "一种生成式方法,通过学习交互先验从第一人称视频中联合重建手部和物体在世界空间中的运动,达到最先进性能。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cv"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

![Concept animation](/arxiv-visuals/whole-world-grounded-hand-object-lifted/ConceptScene.gif)



:::en
**Paper**: [2602.22209](https://arxiv.org/abs/2602.22209)
**Authors**: Yufei Ye, Jiaman Li, Ryan Rong, C. Karen Liu
**Categories**: cs.CV

## Abstract

Egocentric manipulation videos present unique challenges for 3D reconstruction due to severe occlusions during hand-object interactions and objects frequently moving in and out of the camera's field of view. WHOLE addresses these challenges by introducing a holistic approach that jointly reconstructs hand and object motion in world space from egocentric videos. Unlike existing methods that process hands and objects independently—leading to inconsistent spatial relationships—WHOLE learns a generative prior over hand-object motion to reason about their interactions jointly. At inference time, this pretrained prior generates trajectories that align with video observations, substantially outperforming separate reconstruction approaches. The method achieves state-of-the-art results on hand motion estimation, 6D object pose estimation, and hand-object interaction reconstruction.

## Key Contributions

- A joint generative framework that learns priors over hand-object motion to capture interaction patterns and spatial relationships
- World-space reconstruction that maintains consistency even when objects exit and re-enter the camera view
- A test-time optimization strategy that guides the generative prior to conform to video observations
- State-of-the-art performance on multiple benchmarks for hand tracking, object pose estimation, and interaction reconstruction

## Technical Approach

The core innovation of WHOLE lies in its generative modeling of hand-object interactions. Rather than treating hand and object reconstruction as separate problems, the method learns a joint distribution over their motions in world space. This is achieved through a diffusion-based generative model trained on hand-object interaction data.

The architecture consists of two main components: a generative prior network and an observation guidance module. The prior network learns the manifold of plausible hand-object configurations and their temporal dynamics. During training, it captures patterns such as contact constraints, physical plausibility, and typical manipulation trajectories.

At test time, the method performs guided generation where the pretrained prior is conditioned on visual observations from the egocentric video. The guidance mechanism ensures that generated trajectories remain consistent with detected hand keypoints and object features while respecting the learned interaction constraints. This approach naturally handles occlusions and out-of-view scenarios by leveraging the learned prior to fill in missing information.

The world-space formulation is particularly important for egocentric videos where camera motion is significant. By reconstructing in a global coordinate frame rather than camera space, WHOLE maintains consistent object trajectories even as the viewpoint changes dramatically.

## Results and Performance

WHOLE demonstrates substantial improvements over existing methods across multiple evaluation metrics. On hand motion estimation, the method reduces tracking error by leveraging object context—when hands are occluded by objects, the learned interaction prior provides strong cues about likely hand configurations.

For 6D object pose estimation, WHOLE achieves state-of-the-art accuracy by jointly reasoning about hand-object contact. This is particularly effective during manipulation phases where traditional object-only methods struggle due to occlusions. The joint modeling also enables the method to maintain accurate pose estimates when objects temporarily leave the field of view, using hand motion to infer object location.

The most significant gains appear in interaction reconstruction metrics that evaluate the consistency of hand-object spatial relationships. Methods that process hands and objects independently often produce physically implausible configurations with interpenetration or floating hands. WHOLE's joint approach naturally avoids these artifacts by learning valid interaction patterns.

Qualitative results show that the method produces smooth, physically plausible trajectories even in challenging scenarios with rapid motion, severe occlusions, and frequent viewpoint changes. The world-space reconstruction enables coherent visualization of manipulation sequences from novel viewpoints.

## Implications and Future Directions

WHOLE represents a significant step toward understanding human manipulation from egocentric video. The joint modeling approach has implications beyond reconstruction—the learned interaction priors could inform robotic manipulation, enable better action recognition, or support AR/VR applications that require accurate hand-object tracking.

The generative framework is particularly promising for handling the inherent ambiguity in egocentric videos. When visual evidence is limited due to occlusion or motion blur, the learned prior provides reasonable completions based on typical interaction patterns. This is more robust than purely discriminative approaches that may fail catastrophically when inputs are degraded.

Future work could extend this approach to handle multiple objects, deformable objects, or two-handed manipulation. The generative modeling framework is flexible enough to incorporate these extensions. Additionally, the method currently requires object templates—relaxing this requirement through category-level modeling would increase practical applicability.

The success of learning interaction priors also suggests opportunities for incorporating physics-based constraints more explicitly, potentially through differentiable simulation or hybrid learned-physics models.

## Takeaways

1. Joint modeling of hand-object interactions through generative priors substantially outperforms independent reconstruction approaches
2. World-space formulation is crucial for maintaining consistency in egocentric videos with significant camera motion
3. Learned interaction priors enable robust reconstruction even during occlusions and out-of-view scenarios
4. The method achieves state-of-the-art performance on hand tracking, object pose estimation, and interaction reconstruction
5. Generative modeling provides a principled way to handle ambiguity and missing information in challenging egocentric scenarios
:::

:::zh
**论文**: [2602.22209](https://arxiv.org/abs/2602.22209)
**作者**: Yufei Ye, Jiaman Li, Ryan Rong, C. Karen Liu
**分类**: cs.CV

## 摘要

第一人称操作视频由于手部与物体交互时的严重遮挡以及物体频繁进出相机视野而带来独特挑战。WHOLE通过引入整体方法来应对这些挑战,从第一人称视频中联合重建手部和物体在世界空间中的运动。与现有方法独立处理手部和物体导致空间关系不一致不同,WHOLE学习手部-物体运动的生成先验来联合推理它们的交互。在推理时,这个预训练先验生成与视频观测一致的轨迹,大幅超越分离重建方法。该方法在手部运动估计、6D物体姿态估计和手部-物体交互重建上达到最先进结果。

## 主要贡献

- 联合生成框架,学习手部-物体运动先验以捕获交互模式和空间关系
- 世界空间重建,即使物体离开并重新进入相机视野也能保持一致性
- 测试时优化策略,引导生成先验符合视频观测
- 在手部跟踪、物体姿态估计和交互重建的多个基准上达到最先进性能

## 技术方法

WHOLE的核心创新在于对手部-物体交互的生成建模。该方法不是将手部和物体重建视为独立问题,而是学习它们在世界空间中运动的联合分布。这通过在手部-物体交互数据上训练的基于扩散的生成模型实现。

架构包含两个主要组件:生成先验网络和观测引导模块。先验网络学习合理手部-物体配置的流形及其时间动态。在训练期间,它捕获接触约束、物理合理性和典型操作轨迹等模式。

在测试时,该方法执行引导生成,其中预训练先验以第一人称视频的视觉观测为条件。引导机制确保生成的轨迹与检测到的手部关键点和物体特征保持一致,同时遵守学习到的交互约束。这种方法通过利用学习到的先验填充缺失信息,自然地处理遮挡和视野外场景。

世界空间表述对于相机运动显著的第一人称视频特别重要。通过在全局坐标系而非相机空间中重建,WHOLE即使在视点剧烈变化时也能保持一致的物体轨迹。

## 结果与性能

WHOLE在多个评估指标上展示了相对现有方法的显著改进。在手部运动估计上,该方法通过利用物体上下文减少跟踪误差——当手部被物体遮挡时,学习到的交互先验提供关于可能手部配置的强线索。

对于6D物体姿态估计,WHOLE通过联合推理手部-物体接触达到最先进精度。这在操作阶段特别有效,传统的仅物体方法由于遮挡而困难。联合建模还使该方法能够在物体暂时离开视野时保持准确的姿态估计,利用手部运动推断物体位置。

最显著的提升出现在评估手部-物体空间关系一致性的交互重建指标上。独立处理手部和物体的方法通常产生物理上不合理的配置,存在穿透或悬浮手部。WHOLE的联合方法通过学习有效的交互模式自然避免这些伪影。

定性结果显示,即使在快速运动、严重遮挡和频繁视点变化的挑战场景中,该方法也能产生平滑、物理合理的轨迹。世界空间重建使得从新视点连贯可视化操作序列成为可能。

## 影响与未来方向

WHOLE代表了从第一人称视频理解人类操作的重要进步。联合建模方法的影响超越重建——学习到的交互先验可以指导机器人操作、实现更好的动作识别,或支持需要精确手部-物体跟踪的AR/VR应用。

生成框架对于处理第一人称视频中固有的歧义性特别有前景。当由于遮挡或运动模糊导致视觉证据有限时,学习到的先验基于典型交互模式提供合理补全。这比纯判别方法更鲁棒,后者在输入退化时可能灾难性失败。

未来工作可以扩展这种方法以处理多个物体、可变形物体或双手操作。生成建模框架足够灵活以纳入这些扩展。此外,该方法目前需要物体模板——通过类别级建模放松这一要求将增加实际适用性。

学习交互先验的成功也表明有机会更明确地纳入基于物理的约束,可能通过可微分仿真或混合学习-物理模型。

## 要点总结

1. 通过生成先验联合建模手部-物体交互大幅超越独立重建方法
2. 世界空间表述对于在相机运动显著的第一人称视频中保持一致性至关重要
3. 学习到的交互先验使得即使在遮挡和视野外场景中也能实现鲁棒重建
4. 该方法在手部跟踪、物体姿态估计和交互重建上达到最先进性能
5. 生成建模提供了处理挑战性第一人称场景中歧义性和缺失信息的原则性方法
:::
