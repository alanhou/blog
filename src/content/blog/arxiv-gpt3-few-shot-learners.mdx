---
title: 'GPT-3: Language Models are Few-Shot Learners'
description: 'GPT-3：175B 参数模型通过文本提示实现少样本推理，在翻译、问答等任务上接近微调模型'
pubDate: 2020-05-28
heroImage: 'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&auto=format&fit=crop&q=60'
heroImageAlt: 'Large language model concept'
tags: ['AI', 'NLP', 'GPT-3', 'Few-Shot Learning', 'OpenAI', 'arXiv']
category: 'ai'
---

import Callout from '../../components/Callout.astro'

<Callout type="info" title="论文信息 | Paper Info">
**标题**: Language Models are Few-Shot Learners
**作者**: Tom Brown, Benjamin Mann, Nick Ryder, et al. (OpenAI)
**发表**: NeurIPS 2020
**链接**: [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)
</Callout>

## 核心贡献 | Core Contributions

GPT-3 证明了**规模化**的力量：一个足够大的语言模型可以通过简单的文本提示（prompt）完成各种任务，无需针对特定任务进行微调。

GPT-3 demonstrated the power of **scale**: a sufficiently large language model can complete various tasks through simple text prompts without task-specific fine-tuning.

## 模型规模 | Model Scale

| 模型 | 参数量 | 层数 | 隐藏维度 | 注意力头 |
|------|--------|------|---------|---------|
| GPT-3 Small | 125M | 12 | 768 | 12 |
| GPT-3 Medium | 350M | 24 | 1024 | 16 |
| GPT-3 Large | 760M | 24 | 1536 | 16 |
| GPT-3 XL | 1.3B | 24 | 2048 | 24 |
| GPT-3 6.7B | 6.7B | 32 | 4096 | 32 |
| **GPT-3 175B** | **175B** | **96** | **12288** | **96** |

## 关键创新 | Key Innovations

### 上下文学习 | In-Context Learning

GPT-3 引入了三种学习范式：

**Zero-shot**: 只给任务描述
```
Translate English to French:
cheese =>
```

**One-shot**: 给一个示例
```
Translate English to French:
sea otter => loutre de mer
cheese =>
```

**Few-shot**: 给几个示例
```
Translate English to French:
sea otter => loutre de mer
peppermint => menthe poivrée
cheese =>
```

### 涌现能力 | Emergent Abilities

随着模型规模增大，GPT-3 展现出一些小模型没有的能力：

- **算术运算**: 两位数加减法准确率达 100%
- **单词重排**: 将打乱的单词重新排列成句子
- **新词定义**: 根据上下文理解新造词的含义

As model scale increases, GPT-3 exhibits emergent abilities not present in smaller models.

## 实验结果 | Experimental Results

Few-shot GPT-3 在多个任务上接近或超过微调的 BERT：

| 任务 | BERT (微调) | GPT-3 (Few-shot) |
|------|------------|------------------|
| SuperGLUE | 71.5 | 71.8 |
| TriviaQA | 68.0 | 71.2 |
| 翻译 (En→Fr) | 40.2 BLEU | 32.6 BLEU |

## 局限性 | Limitations

论文诚实地讨论了 GPT-3 的局限：

- **文本生成**: 长文本可能失去连贯性
- **常识推理**: 某些物理常识问题表现不佳
- **双向任务**: 填空类任务不如 BERT
- **样本效率**: 需要大量计算资源

## 深远影响 | Lasting Impact

GPT-3 开启了**大语言模型 (LLM)** 时代：

- 证明了 Scaling Laws 的有效性
- 催生了 Prompt Engineering 领域
- 推动了 ChatGPT、Claude 等对话 AI 的发展
- 引发了关于 AI 安全和对齐的广泛讨论

<Callout type="tip" title="历史意义 | Historical Significance">
GPT-3 是 AI 发展的里程碑，标志着从"针对任务训练模型"到"用提示引导通用模型"的范式转变。
</Callout>
