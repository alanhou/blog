---
title:
  en: "Pretraining with Token-Level Adaptive Latent Chain-of-Thought"
  zh: "使用Token级自适应潜在思维链进行预训练"
description:
  en: "Enhancing language models with variable-length internal reasoning sequences that naturally allocate more computation to challenging tokens"
  zh: "通过可变长度内部推理序列增强语言模型，自然地为困难token分配更多计算"
date: 2026-02-10
tags: ["arxiv", "ai", "chain-of-thought", "latent-reasoning", "pretraining", "adaptive-compute", "efficiency", "cs.CL", "cs.LG"]
image: "/arxiv-visuals/arxiv-adaptive-latent-chain-of-thought.png"
---

:::en
**Paper**: [2602.08220](https://arxiv.org/abs/2602.08220)
**Authors**: Boyi Zeng, Yiqin Hao, He Li, Shixiang Song, Feichen Song, Zitong Wang, Siyuan Huang, Yi Xu, ZiWei He, Xinbing Wang, Zhouhan Lin
**Categories**: cs.CL, cs.LG

## Abstract

This work enhances language models by increasing the computational work performed per token without adding parameters. The model generates variable-length internal reasoning sequences before predicting each token. Crucially, the model naturally learns to allocate more reasoning steps to challenging tokens and fewer to simpler ones — a behavior that emerges during standard pretraining without explicit supervision. An adaptive token-wise stopping mechanism reduces computational demands. Experiments on Llama models show consistent improvements even with fewer training FLOPs than prior recurrent baselines.

## Key Contributions

- **Token-level adaptive latent reasoning**: Variable-length internal computation before each token prediction
- **Emergent difficulty-aware allocation**: The model learns to spend more compute on harder tokens without explicit signals
- **Adaptive stopping**: Token-wise early termination reduces unnecessary computation
- **FLOP-efficient improvements**: Better performance than recurrent baselines with fewer training FLOPs
- **Compatible with standard pretraining**: No special training procedures or auxiliary losses required

## Background: Compute Per Token

Standard transformers allocate exactly the same computation to every token — a single forward pass through all layers. This is clearly suboptimal: predicting "the" after "in" requires far less reasoning than predicting a complex technical term or the next step in a mathematical derivation.

Several approaches have attempted to address this:
- **Chain-of-thought prompting**: Explicit reasoning tokens, but increases sequence length and requires supervision
- **Recurrent depth**: Repeating transformer blocks, but typically with fixed iteration counts
- **Adaptive computation**: Early exit or variable depth, but usually at the sequence level

This work introduces **token-level** adaptive latent reasoning that operates in the model's hidden space.

## Method

### Latent Reasoning Sequences

Before predicting each output token $x_t$, the model generates a sequence of $k_t$ latent reasoning steps in hidden space:

$$z_t^{(1)}, z_t^{(2)}, \ldots, z_t^{(k_t)} = f_{\theta}(h_t, z_t^{(1)}, \ldots, z_t^{(k_t-1)})$$

where $h_t$ is the hidden state from the main transformer and $f_{\theta}$ is a shared reasoning module. Each step refines the representation before the final token prediction.

### Adaptive Token-Wise Stopping

The number of reasoning steps $k_varies per token. A halting mechanism learns when sufficient reasoning has been performed:

$$p_{\text{halt}}^{(j)} = \sigma(w_{\text{halt}}^T z_t^{(j)})$$

When the cumulative halting probability exceeds a threshold, reasoning stops for that token. This naturally allocates:
- **Few steps** for predictable tokens (function words, common phrases)
- **Many steps** for challenging tokens (rare words, logical conclusions, mathematical operations)

### Integration with Pretraining

The latent reasoning module is integrated directly into the pretraining objective. The standard next-token prediction loss is computed after the adaptive reasoning:

$$\mathcal{L} = -\sum_t \log P(x_t | x_{<t}, z_t^{(1:k_t)})$$

A small regularization term encourages computational efficiency by penalizing excessive reasoning steps:

$$\mathcal{L}_{\text{reg}} = \lambda \sum_t k_t$$

## Emergent Difficulty-Aware Allocation

One of the most striking findings is that the model **spontaneously** learns to allocate more reasoning steps to tokens that are objectively more difficult to predict. Without any explicit difficulty labels or curriculum:

- **High-entropy tokens** (multiple plausible continuations) receive more reasoning steps
- **Low-entropy tokens** (highly predictable) receive fewer steps
- The allocation pattern correlates with human intuitions about linguistic complexity

This suggests that adaptive computation is a natural inductive bias that emerges when the architecture permits it.

## Results

- **Consistent improvements** on Llama models across multiple benchmarks
- **Fewer training FLOPs** than prior recurrent baselines while achieving better performance
- **Graceful degradation**: Limiting maximum reasoning steps reduces compute with minimal quality loss
- **Scaling behavior**: Benefits increase with model size, suggesting the approach is complementary to parameter scaling

## Significance

1. **Efficient scaling**: Increases effective model capacity without adding parameters
2. **Natural adaptive compute**: Demonstrates that difficulty-aware computation can emerge from standard training
3. **Practical deployment**: Adaptive stopping keeps inference costs manageable
4. **Theoretical implications**: Suggests that fixed-depth transformers are fundamentally limited in their per-token reasoning capacity
5. **Pretraining integration**: Unlike post-hoc methods, this approach benefits from being present during the entire pretraining process
:::

:::zh
**论文**: [2602.08220](https://arxiv.org/abs/2602.08220)
**作者**: Boyi Zeng, Yiqin Hao, He Li, Shixiang Song, Feichen Song, Zitong Wang, Siyuan Huang, Yi Xu, ZiWei He, Xinbing Wang, Zhouhan Lin
**分类**: cs.CL, cs.LG

## 摘要

本工作通过增加每个token的计算工作量来增强语言模型，而不增加参数。模型在预测每个token之前生成可变长度的内部推理序列。关键的是，模型自然地学会为困难token分配更多推理步骤，为简单token分配更少步骤——这种行为在标准预训练期间自然涌现，无需显式监督。自适应token级停止机制减少了计算需求。在Llama模型上的实验表明，即使使用比先前循环基线更少的训练FLOP，也能获得一致的改进。

## 主要贡献

- **Token级自适应潜在推理**：在每个token预测之前进行可变长度的内部计算
- **涌现的难度感知分配**：模型在没有显式信号的情况下学会在更难的token上花费更多计算
- **自适应停止**：Token级提前终止减少不必要的计算
- **FLOP高效改进**：以更少的训练FLOP获得比循环基线更好的性能
- **与标准预训练兼容**：不需要特殊的训练程序或辅助损失

## 背景：每Token计算量

标准Transformer为每个token分配完全相同的计算量——通过所有层的单次前向传播。这显然是次优的：在"in"之后预测"the"所需的推理远少于预测复杂技术术语或数学推导的下一步。

已有多种方法尝试解决这个问题：
- **思维链提示**：显式推理token，但增加序列长度且需要监督
- **循环深度**：重复Transformer块，但通常使用固定迭代次数
- **自适应计算**：提前退出或可变深度，但通常在序列级别

本工作引入了在模型隐藏空间中运行的**token级**自适应潜在推理。

## 方法

### 潜在推理序列

在预测每个输出token $x_t$之前，模型在隐藏空间中生成$k_t$个潜在推理步骤的序列：

$$z_t^{(1)}, z_t^{(2)}, \ldots, z_t^{(k_t)} = f_{\theta}(h_t, z_t^{(1)}, \ldots, z_t^{(k_t-1)})$$

其中$h_t$是主Transformer的隐藏状态，$f_{\theta}$是共享的推理模块。每一步在最终token预测之前细化表示。

### 自适应Token级停止

推理步骤数$k_t$因token而异。停止机制学习何时已完成足够的推理：

$$p_{\text{halt}}^{(j)} = \sigma(w_{\text{halt}}^T z_t^{(j)})$$

当累积停止概率超过阈值时，该token的推理停止。这自然地分配：
- **少量步骤**给可预测的token（功能词、常见短语）
- **多量步骤**给困难的token（罕见词、逻辑结论、数学运算）

### 与预训练的集成

潜在推理模块直接集成到预训练目标中。标准的下一token预测损失在自适应推理之后计算：

$$\mathcal{L} = -\sum_t \log P(x_t | x_{<t}, z_t^{(1:k_t)})$$

一个小的正则化项通过惩罚过多的推理步骤来鼓励计算效率：

$$\mathcal{L}_{\text{reg}} = \lambda \sum_t k_t$$

## 涌现的难度感知分配

最引人注目的发现之一是，模型**自发地**学会为客观上更难预测的token分配更多推理步骤。无需任何显式的难度标签或课程：

- **高熵token**（多个合理的续写）获得更多推理步骤
- **低熵token**（高度可预测）获得更少步骤
- 分配模式与人类对语言复杂性的直觉相关

这表明自适应计算是一种自然的归纳偏置，当架构允许时会自然涌现。

## 实验结果

- 在Llama模型上跨多个基准测试获得**一致改循环基线使用**更少的训练FLOP**同时获得更好的性能
- **优雅退化**：限制最大推理步骤可以在最小质量损失下减少计算
- **缩放行为**：收益随模型规模增加而增加，表明该方法与参数缩放互补

## 重要意义

1. **高效缩放**：在不增加参数的情况下增加有效模型容量
2. **自然自适应计算**：证明难度感知计算可以从标准训练中涌现
3. **实际部署**：自适应停止使推理成本保持可控
4. **理论意义**：表明固定深度Transformer在每token推理能力上存在根本限制
5. **预训练集成**：与事后方法不同，这种方法受益于在整个预训练过程中的存在
:::
