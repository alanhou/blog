---
title:
  en: "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets"
  zh: "翻译中的恢复:基准测试和数据集自动化翻译的高效流程"
description:
  en: "A fully automated framework leveraging test-time compute scaling strategies to produce high-quality translations of AI benchmarks, preserving semantic integrity and task structure across eight European languages."
  zh: "一个利用测试时计算扩展策略的全自动框架,可生成高质量的AI基准测试翻译,在八种欧洲语言中保持语义完整性和任务结构。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cl", "cs.ai", "cs.lg"]
image: "/arxiv-visuals/recovered-in-translation-efficient-pipeline-for/HeroScene.png"
---

![Concept animation](/arxiv-visuals/recovered-in-translation-efficient-pipeline-for/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/recovered-in-translation-efficient-pipeline-for/HeroScene.png)



:::en
**Paper**: [2602.22207](https://arxiv.org/abs/2602.22207)
**Authors**: Hanna Yukhymenko, Anton Alexandrov, Martin Vechev
**Categories**: cs.CL, cs.AI, cs.LG

## Abstract

This paper addresses a critical bottleneck in multilingual LLM evaluation: the poor quality of translated benchmarks. Current translated resources suffer from semantic drift and context loss, leading to unreliable performance metrics across languages. The authors present an automated framework that applies test-time compute scaling strategies—specifically Universal Self-Improvement (USI) and a novel multi-round ranking method called T-RANK—to generate high-quality benchmark translations. The framework was validated by translating popular benchmarks into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Both reference-based metrics and LLM-as-a-judge evaluations demonstrate that these translations significantly outperform existing resources, enabling more accurate multilingual model assessment.

## Key Contributions

- Introduction of a fully automated translation pipeline that preserves benchmark integrity and linguistic nuances during localization
- Adaptation of test-time compute scaling strategies (USI and T-RANK) for translation quality improvement
- T-RANK: a novel multi-round ranking method that iteratively refines translation candidates
- Comprehensive translation of popular benchmarks into eight underrepresented European languages
- Empirical validation showing superior quality compared to existing translated resources using both automated metrics and LLM-based evaluation
- Open release of both the translation framework and improved benchmark datasets

## Methodology: Test-Time Compute Scaling for Translation

The core innovation lies in applying test-time compute scaling to the translation task. Traditional translation pipelines rely on single-pass generation, which often fails to capture nuanced semantic relationships and task-specific requirements in benchmarks.

The framework employs two key strategies:

**Universal Self-Improvement (USI)**: This approach generates multiple translation candidates and uses the model's own judgment to select the best option. By leveraging the model's understanding of both source and target languages, USI can identify translations that better preserve semantic content and task structure.

**T-RANK (Translation Ranking)**: The authors' proposed multi-round ranking method extends USI by iteratively refining the candidate pool. In each round, the system:
1. Generates $n$ translation candidates
2. Ranks them based on semantic fidelity, fluency, and task preservation
3. Uses top-ranked candidates to inform subsequent generation rounds
4. Converges on translations that optimize multiple quality dimensions simultaneously

This iterative refinement process allows the framework to escape local optima that single-pass translation often encounters, particularly for complex benchmark items with specialized terminology or intricate logical structures.

## Results and Evaluation

The authors conducted extensive evaluation across two dimensions:

**Reference-Based Metrics**: When gold-standard human translations were available, the framework's outputs showed higher BLEU, COMET, and chrF scores compared to existing translated benchmarks. The improvements were particularly pronounced for languages with limited training data (Estonian, Lithuanian) and for benchmarks requiring precise logical reasoning.

**LLM-as-a-Judge Evaluation**: Using GPT-4 and other frontier models as evaluators, the translated benchmarks demonstrated better preservation of:
- Original task difficulty and discriminative power
- Semantic equivalence across language pairs
- Cultural and linguistic appropriateness for target languages
- Consistency in multiple-choice options and answer keys

Downstream model evaluation using the improved benchmarks revealed more consistent performance patterns across languages, suggesting that the translations better capture the true multilingual capabilities of evaluated models rather than introducing translation-induced artifacts.

## Implications for Multilingual AI Development

This work has significant implications for the broader multilingual AI ecosystem:

**Democratization of Evaluation**: By providing high-quality benchmarks for underrepresented European languages, the framework enables more equitable assessment of LLM capabilities beyond English and a few high-resource languages.

**Scalability**: The fully automated nature of the pipeline means it can be applied to new languages and benchmarks as they emerge, without requiring extensive human translation efforts.

**Reproducibility**: The open release of both framework and datasets establishes a reproducible standard for benchmark translation, addressing the current fragmentation in multilingual evaluation resources.

**Quality-Compute Tradeoff**: The test-time compute scaling approach demonstrates that investing additional computation during translation can yield substantial quality improvements, offering a practical alternative to expensive human translation for many use cases.

The framework also highlights a broader principle: techniques developed for improving model outputs (like self-consistency and iterative refinement) can be effectively repurposed for data curation and benchmark development.

## Takeaways

1. Test-time compute scaling strategies like USI and T-RANK can dramatically improve automated translation quality for specialized content like AI benchmarks
2. The proposed framework successfully translates benchmarks into eight European languages while preserving task structure and semantic integrity
3. Automated translations from this pipeline outperform existing resources on both reference-based metrics and LLM-judge evaluations
4. High-quality multilingual benchmarks enable more accurate and equitable assessment of LLM capabilities across languages
5. The open-source release of framework and datasets provides a reproducible foundation for future multilingual AI evaluation efforts
:::

:::zh
**论文**: [2602.22207](https://arxiv.org/abs/2602.22207)
**作者**: Hanna Yukhymenko, Anton Alexandrov, Martin Vechev
**分类**: cs.CL, cs.AI, cs.LG

## 摘要

本文解决了多语言大语言模型评估中的一个关键瓶颈:翻译基准测试的质量不佳。当前的翻译资源存在语义漂移和上下文丢失问题,导致跨语言性能指标不可靠。作者提出了一个自动化框架,应用测试时计算扩展策略——特别是通用自我改进(USI)和一种名为T-RANK的新型多轮排序方法——来生成高质量的基准测试翻译。该框架通过将流行基准测试翻译成八种东欧和南欧语言(乌克兰语、保加利亚语、斯洛伐克语、罗马尼亚语、立陶宛语、爱沙尼亚语、土耳其语、希腊语)进行了验证。基于参考的指标和LLM评判的评估都表明,这些翻译显著优于现有资源,能够实现更准确的多语言模型评估。

## 主要贡献

- 引入了一个全自动翻译流程,在本地化过程中保持基准测试的完整性和语言细微差别
- 将测试时计算扩展策略(USI和T-RANK)应用于翻译质量改进
- T-RANK:一种新颖的多轮排序方法,可迭代优化翻译候选
- 将流行基准测试全面翻译成八种代表性不足的欧洲语言
- 使用自动化指标和基于LLM的评估进行实证验证,显示出优于现有翻译资源的质量
- 开源发布翻译框架和改进的基准测试数据集

## 方法论:用于翻译的测试时计算扩展

核心创新在于将测试时计算扩展应用于翻译任务。传统翻译流程依赖单次生成,往往无法捕捉基准测试中细微的语义关系和任务特定要求。

该框架采用两个关键策略:

**通用自我改进(USI)**:这种方法生成多个翻译候选,并使用模型自身的判断来选择最佳选项。通过利用模型对源语言和目标语言的理解,USI可以识别出更好地保留语义内容和任务结构的翻译。

**T-RANK(翻译排序)**:作者提出的多轮排序方法通过迭代优化候选池来扩展USI。在每一轮中,系统:
1. 生成$n$个翻译候选
2. 根据语义保真度、流畅性和任务保留性对其进行排序
3. 使用排名靠前的候选来指导后续生成轮次
4. 收敛到同时优化多个质量维度的翻译

这种迭代优化过程使框架能够摆脱单次翻译经常遇到的局部最优,特别是对于具有专业术语或复杂逻辑结构的复杂基准测试项目。

## 结果与评估

作者从两个维度进行了广泛评估:

**基于参考的指标**:当有黄金标准人工翻译可用时,框架的输出在BLEU、COMET和chrF分数上均高于现有翻译基准测试。对于训练数据有限的语言(爱沙尼亚语、立陶宛语)以及需要精确逻辑推理的基准测试,改进尤为显著。

**LLM评判评估**:使用GPT-4和其他前沿模型作为评估者,翻译的基准测试在以下方面表现出更好的保留:
- 原始任务难度和区分能力
- 跨语言对的语义等价性
- 目标语言的文化和语言适当性
- 多项选择选项和答案键的一致性

使用改进基准测试进行的下游模型评估揭示了跨语言更一致的性能模式,表明翻译更好地捕捉了被评估模型的真实多语言能力,而不是引入翻译诱导的伪影。

## 对多语言AI发展的影响

这项工作对更广泛的多语言AI生态系统具有重要意义:

**评估民主化**:通过为代表性不足的欧洲语言提供高质量基准测试,该框架能够更公平地评估LLM在英语和少数高资源语言之外的能力。

**可扩展性**:流程的全自动特性意味着它可以应用于新出现的语言和基准测试,而无需大量人工翻译工作。

**可重现性**:框架和数据集的开源发布为基准测试翻译建立了可重现的标准,解决了当前多语言评估资源的碎片化问题。

**质量-计算权衡**:测试时计算扩展方法表明,在翻译过程中投入额外计算可以产生显著的质量改进,为许多用例提供了昂贵人工翻译的实用替代方案。

该框架还突出了一个更广泛的原则:为改进模型输出而开发的技术(如自洽性和迭代优化)可以有效地重新用于数据整理和基准测试开发。

## 要点总结

1. 像USI和T-RANK这样的测试时计算扩展策略可以显著提高AI基准测试等专业内容的自动化翻译质量
2. 所提出的框架成功地将基准测试翻译成八种欧洲语言,同时保持任务结构和语义完整性
3. 该流程的自动化翻译在基于参考的指标和LLM评判评估上均优于现有资源
4. 高质量的多语言基准测试能够更准确、更公平地评估LLM跨语言能力
5. 框架和数据集的开源发布为未来的多语言AI评估工作提供了可重现的基础
:::
