---
title:
  en: "S1: Simple Test-Time Scaling"
  zh: "S1：简单的测试时扩展"
description:
  en: "A deep dive into S1, showing how budget forcing and just 1,000 training examples can match OpenAI o1-preview performance through test-time compute scaling."
  zh: "深入解析S1，展示如何通过预算强制和仅1,000个训练样本，通过测试时计算扩展达到OpenAI o1-preview的性能。"
date: 2025-02-06
image: https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800
tags: ["AI", "LLM", "Reasoning", "Test-Time Scaling", "Deep Learning"]
authors: ["Niklas Muennighoff", "Zitong Yang", "Weijia Shi", "Xiang Lisa Li", "Li Fei-Fei", "Hannaneh Hajishirzi", "Luke Zettlemoyer"]
paper: "https://arxiv.org/abs/2501.19393"
---

import BilingualContent from '../../components/BilingualContent.astro';

<BilingualContent>

:::en
## Overview

**S1: Simple Test-Time Scaling** demonstrates that test-time compute scaling — allowing a model to "think longer" during inference — can be achieved with remarkably simple techniques. The authors show that by fine-tuning a Qwen2.5-32B model on only 1,000 carefully curated examples and applying a technique called "budget forcing," they can match or exceed OpenAI o1-preview on competition math benchmarks.

This challenges the assumption that test-time scaling requires complex reinforcement learning pipelines like those used in DeepSeek-R1.

## Key Contributions

### 1. Curated Training Data

The authors distilled 59,000 reasoning traces from Google's Gemini Flash Thinking model, then carefully filtered them down to just 1,000 examples based on three criteria:

- **Quality**: Only traces with correct final answers
- **Difficulty**: Prioritizing harder problems that require more reasoning
- **Diversity**: Covering a broad range of domains (math, science, coding, puzzles)

This small but high-quality dataset proved sufficient to teach the model extended reasoning behavior.

### 2. Budget Forcing

Budget forcing is the core technique that controls how much computation the model uses at test time. It works through two mechanisms:

- **Forced termination**: Appending an end-of-thinking token to stop reasoning when a maximum token budget is reached
- **Forced continuation**: Suppressing end-of-thinking tokens and injecting "Wait" to force the model to keep reasoning when a minimum budget hasn't been met

This simple approach creates a controllable knob for test-time compute without any architectural changes.

### 3. Sequential vs Parallel Scaling

The paper compares two approaches to test-time scaling:

- **Sequential scaling**: Let the model think longer on a single attempt (budget forcing)
- **Parallel scaling**: Generate multiple shorter attempts and pick the best (majority voting)

Key finding: Sequential scaling is more compute-efficient for difficult problems, while parallel scaling works better for easier ones. The crossover point depends on problem difficulty.

## Results

- **s1-32B** matches OpenAI o1-preview on AIME24 (American Invitational Mathematics Examination) and MATH500 benchmarks
- Budget forcing provides up to **27% improvement** over greedy decoding on competition math
- The model achieves this with only 1,000 training examples and 26 minutes of fine-tuning on 16 H100 GPUs

## Architecture

The approach is built on three components:

1. **Base model**: Qwen2.5-32B-Instruct (already capable of reasoning)
2. **Training data**: 1,000 curated reasoning traces with explicit thinking tokens
3. **Inference control**: Budget forcing to modulate test-time compute

No changes to the model architecture are needed — it's purely a training data and inference strategy innovation.

## Significance

S1 demonstrates that:

1. **Data quality trumps quantity** — 1,000 examples can match systems trained on orders of magnitude more data
2. **Simple techniques work** — No complex RL pipelines needed for test-time scaling
3. **Test-time compute is controllable** — Budget forcing provides a simple, effective knob
4. **Open research is viable** — The entire approach is reproducible with publicly available models and modest compute
:::

:::zh
## 概述

**S1：简单的测试时扩展** 证明了测试时计算扩展——让模型在推理过程中"思考更长时间"——可以通过非常简单的技术实现。作者展示了通过在仅1,000个精心策划的样本上微调Qwen2.5-32B模型，并应用一种称为"预算强制"的技术，可以在竞赛数学基准测试中匹配或超越OpenAI o1-preview。

这挑战了测试时扩展需要像DeepSeek-R1中使用的复杂强化学习管道的假设。

## 主要贡献

### 1. 精选训练数据

作者从Google的Gemini Flash Thinking模型中蒸馏了59,000条推理轨迹，然后根据三个标准精心筛选至仅1,000个样本：

- **质量**：仅保留最终答案正确的轨迹
- **难度**：优先选择需要更多推理的较难问题
- **多样性**：覆盖广泛的领域（数学、科学、编程、谜题）

这个小而高质量的数据集足以教会模型扩展推理行为。

### 2. 预算强制

预算强制是控制模型在测试时使用多少计算量的核心技术。它通过两种机制工作：

- **强制终止**：当达到最大token预算时，附加思考结束token来停止推理
- **强制继续**：当未达到最小预算时，抑制思考结束token并注入"Wait"来强制模型继续推理

这种简单的方法创建了一个可控的测试时计算旋钮，无需任何架构更改。

### 3. 顺序扩展 vs 并行扩展

论文比较了两种测试时扩展方法：

- **顺序扩展**：让模型在单次尝试中思考更长时间（预算强制）
- **并行扩展**：生成多个较短的尝试并选择最佳结果（多数投票）

关键发现：对于困难问题，顺序扩展的计算效率更高，而对于较简单的问题，并行扩展效果更好。交叉点取决于问题难度。

## 实验结果

- **s1-32B** 在AIME24（美国数学邀请赛）和MATH500基准测试中匹配OpenAI o1-preview
- 预算强制相比贪婪解码提供高达 **27%的改进**
- 模型仅使用1,000个训练样本和在16个H100 GPU上26分钟的微调即实现了这一成果

## 架构

该方法建立在三个组件之上：

1. **基础模型**：Qwen2.5-32B-Instruct（已具备推理能力）
2. **训练数据**：1,000条带有显式思考token的精选推理轨迹
3. **推理控制**：预算强制来调节测试时计算量

不需要对模型架构进行任何更改——这纯粹是训练数据和推理策略的创新。

## 重要意义

S1证明了：

1. **数据质量胜过数量** — 1,000个样本可以匹配在数量级更多数据上训练的系统
2. **简单技术有效** — 测试时扩展不需要复杂的RL管道
3. **测试时计算可控** — 预算强制提供了简单有效的控制旋钮
4. **开放研究可行** — 整个方法可以使用公开可用的模型和适度的计算资源复现
:::

</BilingualContent>
