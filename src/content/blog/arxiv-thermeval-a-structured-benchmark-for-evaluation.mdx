---
title:
  en: "ThermEval: A Structured Benchmark for Evaluating Vision-Language Models on Thermal Imagery"
  zh: "ThermEval: 热成像视觉语言模型结构化评估基准"
description:
  en: "A comprehensive benchmark revealing that current vision-language models fail at thermal image understanding, introducing 55,000 QA pairs with dense temperature annotations to drive progress in thermal vision-language modeling."
  zh: "一个全面的基准测试揭示了当前视觉语言模型在热成像理解上的失败,引入了55,000个问答对和密集温度标注,以推动热成像视觉语言建模的进展。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.cv", "cs.ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-thermeval-a-structured-benchmark-for-evaluation.png"
---

:::en
**Paper**: [2602.14989](https://arxiv.org/abs/2602.14989)
**Authors**: Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra
**Categories**: cs.CV, cs.AI, cs.LG

## Abstract

Vision-language models (VLMs) have demonstrated impressive capabilities on RGB imagery, but their performance collapses when applied to thermal images. This paper introduces ThermEval, a structured benchmark designed to evaluate VLMs on thermal imagery understanding. The benchmark comprises approximately 55,000 thermal visual question answering pairs that assess foundational primitives required for thermal vision-language comprehension. The authors evaluate 25 state-of-the-art VLMs and find systematic failures in temperature-grounded reasoning, sensitivity to colormap transformations, and over-reliance on language priors. These findings highlight a critical gap in current multimodal AI systems and establish ThermEval as an essential benchmark for advancing thermal vision-language modeling.

## Key Contributions

- **ThermEval-B Benchmark**: A structured evaluation framework with ~55,000 thermal VQA pairs covering temperature reasoning, spatial understanding, and semantic comprehension tasks
- **ThermEval-D Dataset**: The first dataset providing dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments
- **Comprehensive VLM Evaluation**: Systematic assessment of 25 open-source and closed-source vision-language models, revealing fundamental limitations in thermal understanding
- **Analysis of Failure Modes**: Identification of three critical failure patterns: temperature-grounded reasoning failures, colormap transformation sensitivity, and defaulting to language priors

## The Thermal Vision Gap

Thermal imaging encodes fundamentally different information than RGB imagery. While RGB cameras capture reflected visible light representing color and texture, thermal cameras measure emitted infrared radiation corresponding to physical temperature. This distinction creates unique challenges for vision-language models trained predominantly on RGB data.

Thermal sensing is critical in scenarios where visible light is insufficient or unavailable: nighttime surveillance, search and rescue operations in low-visibility conditions, autonomous vehicle perception in adverse weather, medical screening for fever detection, and industrial inspection for heat anomalies. Despite these applications, existing VLM benchmarks focus exclusively on RGB imagery, leaving thermal understanding largely unexplored.

The authors argue that thermal comprehension requires distinct perceptual and reasoning capabilities. Models must interpret temperature distributions, understand thermal properties of materials, reason about heat transfer, and ground language descriptions in physical temperature values rather than visual appearance. Current RGB-centric evaluation frameworks cannot assess these capabilities.

## Benchmark Design and Methodology

ThermEval-B is structured around three core evaluation dimensions:

**Temperature Reasoning**: Tasks requiring quantitative temperature understanding, including absolute temperature estimation, relative temperature comparison, and temperature range identification. These tasks assess whether models can extract and reason about numerical temperature values from thermal imagery.

**Spatial Understanding**: Evaluation of spatial reasoning capabilities specific to thermal images, including hotspot localization, thermal gradient detection, and spatial temperature distribution analysis. These tasks test whether models can identify and describe temperature patterns across image regions.

**Semantic Comprehension**: Assessment of semantic understanding in thermal contexts, including object recognition in thermal imagery, activity inference from thermal signatures, and scene understanding based on temperature distributions.

The benchmark integrates existing public thermal datasets with ThermEval-D, a newly collected dataset providing unprecedented annotation density. ThermEval-D includes per-pixel temperature measurements calibrated to physical units (Celsius/Fahrenheit), semantic segmentation masks for body parts and objects, and diverse capture conditions spanning indoor/outdoor environments, varying ambient temperatures, and multiple subjects.

## Experimental Results and Findings

The evaluation of 25 VLMs reveals systematic failures across all model categories:

**Temperature-Grounded Reasoning Failures**: Models consistently struggle with quantitative temperature tasks. When asked to estimate absolute temperatures or compare temperature values, models either refuse to answer, provide wildly inaccurate estimates, or default to generic responses. Even when temperature information is explicitly encoded in colormaps, models fail to extract numerical values accurately.

**Colormap Transformation Sensitivity**: Performance degrades significantly when thermal images are presented with different colormap schemes (e.g., grayscale vs. jet vs. iron colormaps). This sensitivity indicates that models rely on superficial color patterns rather than understanding the underlying temperature information. The same thermal scene presented with different colormaps yields inconsistent and often contradictory responses.

**Language Prior Dominance**: Models frequently default to language priors or generate fixed responses regardless of image content. For example, when asked about temperature in various thermal images, models often respond with stereotypical phrases like "the person appears warm" without grounding in actual temperature data. This behavior suggests models are pattern-matching language rather than performing genuine visual reasoning.

**Limited Improvement from Interventions**: Both prompting strategies (few-shot examples, chain-of-thought reasoning, explicit temperature instructions) and supervised fine-tuning yield only marginal improvements. Even after fine-tuning on thermal data, models show minimal gains in temperature reasoning accuracy, suggesting fundamental architectural limitations rather than simple data distribution issues.

Quantitatively, the best-performing models achieve less than 40% accuracy on temperature reasoning tasks, compared to over 80% accuracy on analogous RGB tasks. The performance gap is even more pronounced for tasks requiring precise temperature estimation, where accuracy drops below 20%.

## Implications for Multimodal AI

These findings have significant implications for the development of multimodal AI systems:

**Domain Generalization Limitations**: The failure of VLMs on thermal imagery demonstrates that current models do not learn generalizable visual reasoning capabilities. Instead, they appear to learn RGB-specific pattern recognition that does not transfer to other imaging modalities encoding different physical properties.

**Need for Physics-Grounded Understanding**: Effective thermal understanding requires models to reason about physical properties (temperature, heat transfer, thermal conductivity) rather than purely visual features. This suggests a need for architectures that can incorporate physical knowledge and constraints.

**Evaluation Beyond RGB**: The thermal domain reveals evaluation blind spots in current multimodal AI research. Benchmarks focused exclusively on RGB imagery may overestimate model capabilities and miss critical failure modes that emerge in other sensing modalities.

**Application-Critical Gaps**: For safety-critical applications like autonomous driving, medical screening, and search and rescue, the inability to process thermal imagery represents a significant limitation. These domains cannot rely on RGB sensing alone, making thermal understanding essential for real-world deployment.

## Takeaways

1. Current vision-language models fail systematically at thermal image understanding, achieving less than 40% accuracy on temperature reasoning tasks despite strong RGB performance
2. ThermEval-B provides a structured benchmark with 55,000 thermal VQA pairs, establishing the first comprehensive evaluation framework for thermal vision-language understanding
3. Models exhibit three critical failure modes: inability to perform temperature-grounded reasoning, sensitivity to colormap transformations, and over-reliance on language priors
4. Neither advanced prompting strategies nor supervised fine-tuning significantly improve thermal understanding, suggesting fundamental architectural limitations
5. The thermal domain reveals that VLMs learn RGB-specific pattern recognition rather than generalizable visual reasoning, highlighting the need for physics-grounded multimodal architectures
:::

:::zh
**论文**: [2602.14989](https://arxiv.org/abs/2602.14989)
**作者**: Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra
**分类**: cs.CV, cs.AI, cs.LG

## 摘要

视觉语言模型(VLM)在RGB图像上表现出色,但应用于热成像时性能急剧下降。本文介绍了ThermEval,一个专门用于评估VLM热成像理解能力的结构化基准。该基准包含约55,000个热成像视觉问答对,用于评估热成像视觉语言理解所需的基础能力。作者评估了25个最先进的VLM,发现它们在基于温度的推理、色彩映射转换敏感性以及过度依赖语言先验方面存在系统性失败。这些发现揭示了当前多模态AI系统的关键缺陷,并将ThermEval确立为推进热成像视觉语言建模的重要基准。

## 主要贡献

- **ThermEval-B基准**: 包含约55,000个热成像视觉问答对的结构化评估框架,涵盖温度推理、空间理解和语义理解任务
- **ThermEval-D数据集**: 首个提供密集逐像素温度图和语义身体部位标注的数据集,涵盖多样化的室内外环境
- **全面的VLM评估**: 对25个开源和闭源视觉语言模型进行系统评估,揭示热成像理解的根本局限性
- **失败模式分析**: 识别三种关键失败模式:基于温度的推理失败、色彩映射转换敏感性以及默认使用语言先验

## 热成像视觉的鸿沟

热成像编码的信息与RGB图像根本不同。RGB相机捕获反射的可见光,表示颜色和纹理,而热成像相机测量发射的红外辐射,对应物理温度。这种区别为主要在RGB数据上训练的视觉语言模型带来了独特挑战。

热成像在可见光不足或不可用的场景中至关重要:夜间监控、低能见度条件下的搜救行动、恶劣天气下的自动驾驶感知、发烧检测的医学筛查以及热异常的工业检测。尽管有这些应用,现有的VLM基准完全专注于RGB图像,使热成像理解在很大程度上未被探索。

作者认为热成像理解需要独特的感知和推理能力。模型必须解释温度分布、理解材料的热特性、推理热传递,并将语言描述建立在物理温度值而非视觉外观上。当前以RGB为中心的评估框架无法评估这些能力。

## 基准设计与方法论

ThermEval-B围绕三个核心评估维度构建:

**温度推理**: 需要定量温度理解的任务,包括绝对温度估计、相对温度比较和温度范围识别。这些任务评估模型是否能从热成像中提取和推理数值温度值。

**空间理解**: 评估热成像特有的空间推理能力,包括热点定位、热梯度检测和空间温度分布分析。这些任务测试模型是否能识别和描述图像区域的温度模式。

**语义理解**: 评估热成像语境中的语义理解,包括热成像中的物体识别、从热特征推断活动以及基于温度分布的场景理解。

该基准整合了现有的公共热成像数据集和ThermEval-D,后者是新收集的数据集,提供了前所未有的标注密度。ThermEval-D包括校准到物理单位(摄氏度/华氏度)的逐像素温度测量、身体部位和物体的语义分割掩码,以及涵盖室内外环境、不同环境温度和多个主体的多样化捕获条件。

## 实验结果与发现

对25个VLM的评估揭示了所有模型类别的系统性失败:

**基于温度的推理失败**: 模型在定量温度任务上持续表现不佳。当被要求估计绝对温度或比较温度值时,模型要么拒绝回答,要么提供极不准确的估计,或者默认使用通用回答。即使温度信息明确编码在色彩映射中,模型也无法准确提取数值。

**色彩映射转换敏感性**: 当热成像以不同的色彩映射方案呈现时(例如灰度vs.喷射vs.铁色彩映射),性能显著下降。这种敏感性表明模型依赖表面颜色模式而非理解底层温度信息。同一热成像场景以不同色彩映射呈现会产生不一致甚至矛盾的响应。

**语言先验主导**: 模型经常默认使用语言先验或生成固定响应,而不考虑图像内容。例如,当询问各种热成像中的温度时,模型经常回答"这个人看起来很温暖"等刻板短语,而没有基于实际温度数据。这种行为表明模型在进行语言模式匹配而非真正的视觉推理。

**干预措施改进有限**: 提示策略(少样本示例、思维链推理、明确的温度指令)和监督微调都只产生边际改进。即使在热成像数据上微调后,模型在温度推理准确性上的提升也很小,这表明存在根本的架构局限性而非简单的数据分布问题。

定量来看,表现最好的模型在温度推理任务上的准确率不到40%,而在类似的RGB任务上准确率超过80%。对于需要精确温度估计的任务,性能差距更加明显,准确率降至20%以下。

## 对多模态AI的影响

这些发现对多模态AI系统的发展具有重要意义:

**领域泛化局限性**: VLM在热成像上的失败表明当前模型没有学习到可泛化的视觉推理能力。相反,它们似乎学习了RGB特定的模式识别,无法迁移到编码不同物理属性的其他成像模态。

**需要基于物理的理解**: 有效的热成像理解需要模型推理物理属性(温度、热传递、热导率)而非纯粹的视觉特征。这表明需要能够整合物理知识和约束的架构。

**超越RGB的评估**: 热成像领域揭示了当前多模态AI研究中的评估盲点。仅专注于RGB图像的基准可能高估模型能力,并遗漏在其他感知模态中出现的关键失败模式。

**应用关键缺陷**: 对于自动驾驶、医学筛查和搜救等安全关键应用,无法处理热成像代表了重大局限性。这些领域不能仅依赖RGB感知,使热成像理解对实际部署至关重要。

## 要点总结

1. 当前视觉语言模型在热成像理解上系统性失败,温度推理任务准确率不到40%,尽管RGB性能强劲
2. ThermEval-B提供了包含55,000个热成像视觉问答对的结构化基准,建立了首个热成像视觉语言理解的综合评估框架
3. 模型表现出三种关键失败模式:无法进行基于温度的推理、对色彩映射转换敏感以及过度依赖语言先验
4. 高级提示策略和监督微调都无法显著改善热成像理解,表明存在根本的架构局限性
5. 热成像领域揭示VLM学习的是RGB特定的模式识别而非可泛化的视觉推理,突显了对基于物理的多模态架构的需求
:::
