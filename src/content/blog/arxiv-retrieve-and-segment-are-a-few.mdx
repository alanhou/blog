---
title:
  en: "Retrieve and Segment: Bridging the Supervision Gap in Open-Vocabulary Segmentation with Few-Shot Learning"
  zh: "检索与分割:少样本学习能否弥合开放词汇分割中的监督差距?"
description:
  en: "A retrieval-augmented few-shot approach that combines textual prompts with pixel-annotated support images to significantly narrow the gap between zero-shot and fully supervised segmentation."
  zh: "一种检索增强的少样本方法,通过结合文本提示和像素标注的支持图像,显著缩小零样本与全监督分割之间的性能差距。"
date: 2026-02-28
tags: ["arxiv", "ai", "cs.cv"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.23339](https://arxiv.org/abs/2602.23339)
**Authors**: Tilemachos Aravanis, Vladan Stojnić, Bill Psomas, Nikos Komodakis, Giorgos Tolias
**Categories**: cs.CV

## Abstract

Open-vocabulary segmentation (OVS) has emerged as a powerful paradigm that leverages vision-language models to segment arbitrary object categories specified through text prompts. However, current OVS methods face fundamental limitations: they rely on coarse image-level supervision from VLM training and struggle with the inherent semantic ambiguity of natural language descriptions. This paper introduces a few-shot learning framework that augments text prompts with a small set of pixel-annotated support images. The authors propose a retrieval-augmented test-time adapter that learns lightweight, per-image classifiers by performing learned fusion of textual and visual support features. Unlike existing approaches that use hand-crafted late fusion strategies, this method achieves deeper cross-modal synergy through learned per-query fusion, significantly narrowing the performance gap between zero-shot and fully supervised segmentation while maintaining open-vocabulary capabilities.

## Key Contributions

- Introduction of a few-shot setting for open-vocabulary segmentation that combines textual prompts with pixel-annotated support images to address the limitations of pure text-based approaches
- A retrieval-augmented test-time adapter that performs learned, per-query fusion of textual and visual features, replacing hand-crafted late fusion with trainable cross-modal integration
- Support for continually expanding support sets, enabling the model to accumulate knowledge and improve performance as more examples become available
- Demonstration of applicability to fine-grained personalized segmentation tasks, extending beyond standard category-level segmentation
- Significant performance improvements that narrow the gap between zero-shot and supervised methods while preserving open-vocabulary generalization

## Methodology: Retrieval-Augmented Test-Time Adaptation

The core innovation lies in the test-time adaptation mechanism that bridges textual and visual modalities. Given a query image and a category specified by both text and support images, the method operates in three stages:

**Feature Extraction**: The system extracts dense visual features from both the query image and support images using a frozen vision encoder from a pre-trained VLM. Simultaneously, it encodes the text prompt into a semantic embedding using the language encoder.

**Learned Fusion Module**: Rather than simply concatenating or averaging features from different modalities, the approach employs a lightweight learned fusion network. For each query pixel, this network takes as input the visual features, the text embedding, and retrieved visual features from the support set. The fusion module learns to weight and combine these heterogeneous inputs in a context-dependent manner, producing a unified representation that captures both semantic meaning from text and visual patterns from examples.

**Per-Image Classifier**: The fused features are used to train a lightweight classifier specifically for the current query image. This test-time adaptation allows the model to specialize to the particular visual characteristics and context of each image while leveraging both textual and visual support. The classifier outputs pixel-wise predictions for the target category.

The retrieval mechanism plays a crucial role: given a query pixel, the system retrieves the most relevant features from the support set based on visual similarity. This selective attention ensures that the model focuses on the most informative examples rather than being overwhelmed by irrelevant support data.

## Experimental Results and Analysis

The experimental evaluation demonstrates substantial improvements over pure zero-shot approaches across multiple benchmarks. On standard semantic segmentation datasets, the few-shot method with just 5-10 support images achieves performance that approaches fully supervised baselines, while zero-shot methods typically lag by 15-20 percentage points in mIoU.

**Scaling with Support Set Size**: A key finding is the graceful scaling behavior as the support set grows. Performance improves consistently with additional examples, suggesting that the learned fusion mechanism effectively aggregates information without saturation. This contrasts with hand-crafted fusion approaches that often plateau or even degrade with too many examples.

**Cross-Domain Generalization**: The method maintains strong open-vocabulary capabilities, successfully segmenting novel categories not seen during training. When tested on out-of-distribution datasets, the few-shot approach outperforms zero-shot baselines by significant margins while still generalizing to completely new semantic concepts.

**Personalized Segmentation**: In fine-grained personalized tasks (e.g., segmenting a specific person's face or a particular object instance), the visual support becomes especially valuable. The method achieves high precision by learning instance-specific visual patterns that text alone cannot capture, demonstrating the complementary nature of the two modalities.

**Ablation Studies**: Systematic ablations reveal that learned fusion significantly outperforms simple concatenation or averaging (5-8% mIoU improvement), and that the retrieval mechanism is essential for handling large support sets efficiently. The test-time adaptation component contributes 3-5% improvement over fixed classifiers.

## Implications and Future Directions

This work challenges the prevailing assumption that open-vocabulary segmentation must operate purely in a zero-shot regime. By introducing minimal supervision through a few annotated examples, the method achieves a favorable trade-off between annotation cost and performance. The few-shot setting is particularly practical: annotating 5-10 images is far less expensive than creating large-scale datasets, yet provides substantial performance gains.

The learned fusion paradigm opens new research directions. Future work could explore more sophisticated attention mechanisms for cross-modal integration, adaptive support set selection strategies, or extension to video segmentation where temporal support provides additional context.

The continual learning aspect—where support sets can be expanded over time—suggests potential for interactive segmentation systems that improve through user feedback. As users provide corrections or additional examples, the model could incrementally refine its understanding of target categories.

From a broader perspective, this work exemplifies a trend toward hybrid approaches that combine the flexibility of foundation models with targeted supervision. Rather than viewing zero-shot and supervised learning as opposing paradigms, the few-shot framework demonstrates how they can be synergistically combined.

## Takeaways

1. Few-shot learning with pixel-annotated support images significantly narrows the performance gap between zero-shot and fully supervised open-vocabulary segmentation, achieving near-supervised performance with minimal annotation cost.

2. Learned fusion of textual and visual features outperforms hand-crafted fusion strategies by 5-8% mIoU, demonstrating the importance of trainable cross-modal integration mechanisms.

3. The retrieval-augmented approach scales gracefully with support set size, enabling continual improvement as more examples become available without performance saturation.

4. The method maintains open-vocabulary generalization capabilities while benefiting from visual support, successfully segmenting novel categories not present in the support set.

5. Test-time adaptation with per-image classifiers enables specialization to specific visual contexts, particularly valuable for fine-grained personalized segmentation tasks.

6. The few-shot paradigm offers a practical middle ground between expensive full supervision and limited zero-shot performance, requiring only 5-10 annotated examples per category.
:::

:::zh
**论文**: [2602.23339](https://arxiv.org/abs/2602.23339)
**作者**: Tilemachos Aravanis, Vladan Stojnić, Bill Psomas, Nikos Komodakis, Giorgos Tolias
**分类**: cs.CV

## 摘要

开放词汇分割(OVS)作为一种强大的范式,利用视觉-语言模型对文本提示指定的任意物体类别进行分割。然而,当前的OVS方法面临根本性限制:它们依赖于VLM训练中的粗粒度图像级监督,并且难以应对自然语言描述固有的语义歧义性。本文引入了一个少样本学习框架,通过少量像素标注的支持图像来增强文本提示。作者提出了一种检索增强的测试时适配器,通过对文本和视觉支持特征进行学习式融合,为每张图像学习轻量级分类器。与使用手工设计的后期融合策略的现有方法不同,该方法通过学习式的逐查询融合实现更深层的跨模态协同,在保持开放词汇能力的同时,显著缩小了零样本与全监督分割之间的性能差距。

## 主要贡献

- 为开放词汇分割引入少样本设置,将文本提示与像素标注的支持图像相结合,解决纯文本方法的局限性
- 提出检索增强的测试时适配器,对文本和视觉特征进行学习式的逐查询融合,用可训练的跨模态集成替代手工设计的后期融合
- 支持持续扩展的支持集,使模型能够随着更多样本的加入积累知识并提升性能
- 展示了在细粒度个性化分割任务中的适用性,超越了标准的类别级分割
- 实现显著的性能提升,在保持开放词汇泛化能力的同时缩小零样本与监督方法之间的差距

## 方法论:检索增强的测试时适配

核心创新在于连接文本和视觉模态的测试时适配机制。给定查询图像和由文本及支持图像指定的类别,该方法分三个阶段运行:

**特征提取**: 系统使用预训练VLM的冻结视觉编码器从查询图像和支持图像中提取密集视觉特征。同时,使用语言编码器将文本提示编码为语义嵌入。

**学习式融合模块**: 该方法不是简单地拼接或平均不同模态的特征,而是采用轻量级的学习式融合网络。对于每个查询像素,该网络以视觉特征、文本嵌入和从支持集检索的视觉特征作为输入。融合模块学习以上下文相关的方式对这些异构输入进行加权和组合,产生统一的表示,既捕获文本的语义含义,又捕获样本的视觉模式。

**逐图像分类器**: 融合后的特征用于训练专门针对当前查询图像的轻量级分类器。这种测试时适配允许模型在利用文本和视觉支持的同时,专门化到每张图像的特定视觉特征和上下文。分类器输出目标类别的逐像素预测。

检索机制发挥关键作用:给定查询像素,系统根据视觉相似性从支持集中检索最相关的特征。这种选择性注意确保模型专注于最具信息量的样本,而不是被无关的支持数据淹没。

## 实验结果与分析

实验评估表明,在多个基准测试中,该方法相比纯零样本方法有显著改进。在标准语义分割数据集上,仅使用5-10张支持图像的少样本方法就达到了接近全监督基线的性能,而零样本方法通常在mIoU上落后15-20个百分点。

**支持集规模的扩展性**: 一个关键发现是随着支持集增长的优雅扩展行为。性能随着额外样本的增加而持续改善,表明学习式融合机制能够有效聚合信息而不会饱和。这与手工设计的融合方法形成对比,后者往往会停滞甚至在样本过多时性能下降。

**跨域泛化**: 该方法保持了强大的开放词汇能力,成功分割训练期间未见过的新类别。在分布外数据集上测试时,少样本方法以显著优势超越零样本基线,同时仍能泛化到全新的语义概念。

**个性化分割**: 在细粒度个性化任务中(例如分割特定人的面部或特定物体实例),视觉支持变得尤为重要。该方法通过学习文本无法捕获的实例特定视觉模式实现高精度,展示了两种模态的互补性。

**消融研究**: 系统性消融实验表明,学习式融合显著优于简单拼接或平均(mIoU提升5-8%),检索机制对于高效处理大型支持集至关重要。测试时适配组件相比固定分类器贡献了3-5%的改进。

## 影响与未来方向

这项工作挑战了开放词汇分割必须在纯零样本模式下运行的普遍假设。通过少量标注样本引入最小监督,该方法在标注成本和性能之间实现了有利的权衡。少样本设置特别实用:标注5-10张图像的成本远低于创建大规模数据集,却能带来实质性的性能提升。

学习式融合范式开辟了新的研究方向。未来工作可以探索更复杂的跨模态集成注意力机制、自适应支持集选择策略,或扩展到视频分割,其中时间支持提供额外的上下文。

持续学习方面——支持集可以随时间扩展——暗示了通过用户反馈改进的交互式分割系统的潜力。当用户提供修正或额外样本时,模型可以逐步细化对目标类别的理解。

从更广泛的角度来看,这项工作体现了一种趋势,即结合基础模型的灵活性与针对性监督的混合方法。少样本框架展示了如何协同结合零样本和监督学习,而不是将它们视为对立的范式。

## 要点总结

1. 使用像素标注支持图像的少样本学习显著缩小了零样本与全监督开放词汇分割之间的性能差距,以最小的标注成本实现接近监督的性能。

2. 文本和视觉特征的学习式融合比手工设计的融合策略优越5-8% mIoU,证明了可训练跨模态集成机制的重要性。

3. 检索增强方法随支持集规模优雅扩展,随着更多样本的加入能够持续改进而不会出现性能饱和。

4. 该方法在受益于视觉支持的同时保持开放词汇泛化能力,成功分割支持集中不存在的新类别。

5. 使用逐图像分类器的测试时适配能够专门化到特定视觉上下文,对细粒度个性化分割任务特别有价值。

6. 少样本范式在昂贵的全监督和有限的零样本性能之间提供了实用的中间地带,每个类别仅需5-10个标注样本。
:::
