---
title:
  en: "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision"
  zh: "对话式图像分割:通过可扩展监督实现抽象概念的像素级定位"
description:
  en: "A novel approach to image segmentation that handles intent-driven queries like safety and affordances, introducing the ConverSeg benchmark and an AI-powered data engine for scalable training."
  zh: "一种新颖的图像分割方法,能够处理安全性和功能可供性等意图驱动的查询,引入了ConverSeg基准数据集和AI驱动的可扩展训练数据引擎。"
date: 2026-02-16
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/arxiv-conversational-image-segmentation-grounding-abstract-concepts.png"
---

:::en
**Paper**: [2602.13195](https://arxiv.org/abs/2602.13195)
**Authors**: Aadarsh Sahoo, Georgia Gkioxari
**Categories**: cs.CV

## Abstract

This paper introduces Conversational Image Segmentation (CIS), a paradigm that extends traditional referring image segmentation beyond simple categorical and spatial queries to handle complex, intent-driven concepts. While existing methods excel at queries like "the left-most apple," they fail at functional reasoning such as "where can I safely store the knife?" The authors present ConverSeg, a comprehensive benchmark covering entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. They also introduce ConverSeg-Net, a model that combines segmentation priors with language understanding, and an AI-powered data engine that generates training data without human annotation. Experiments demonstrate that current models struggle with CIS tasks, while ConverSeg-Net achieves substantial improvements on ConverSeg while maintaining competitive performance on existing benchmarks.

## Key Contributions

- Introduction of Conversational Image Segmentation (CIS) as a new task that grounds abstract, intent-driven concepts into pixel-accurate masks
- ConverSeg benchmark spanning seven diverse categories: entities, spatial relations, intent, affordances, functions, safety, and physical reasoning
- ConverSeg-Net architecture that fuses strong segmentation priors with language understanding capabilities
- AI-powered data engine that generates prompt-mask pairs at scale without requiring human supervision
- Comprehensive evaluation showing significant performance gaps in existing models and substantial gains with the proposed approach

## Methodology and Architecture

ConverSeg-Net builds upon the foundation of modern vision-language models by integrating specialized components for handling abstract reasoning. The architecture leverages pre-trained segmentation models as a strong prior while incorporating language encoders that can process complex, multi-faceted queries. The fusion mechanism allows the model to attend to both visual features and linguistic semantics simultaneously.

The AI-powered data engine represents a significant innovation in scalable supervision. Rather than relying on expensive human annotation for abstract concepts like safety or affordances, the system uses large language models and vision models in tandem to generate diverse prompt-mask pairs. This approach enables the creation of training data that covers the full spectrum of conversational queries, from simple object references to complex functional reasoning.

The data generation pipeline involves multiple stages: prompt synthesis using language models to create diverse and natural queries, candidate region proposal using segmentation models, and validation through consistency checks. This multi-stage process ensures high-quality training data while maintaining scalability.

## Experimental Results and Analysis

The evaluation on ConverSeg reveals critical limitations in existing language-guided segmentation models. Traditional referring expression segmentation models, which perform well on categorical queries, show dramatic performance drops when faced with intent-driven or safety-related queries. For instance, models trained on RefCOCO-style datasets struggle to understand queries like "where should I place hot items?" or "which surface is suitable for cutting?"

ConverSeg-Net demonstrates substantial improvements across all seven categories in the benchmark. The model shows particularly strong gains in affordance understanding (identifying where actions can be performed), safety reasoning (determining safe placement or interaction zones), and physical reasoning (understanding material properties and constraints). These improvements stem from both the architectural innovations and the diverse training data generated by the AI-powered engine.

Importantly, the model maintains strong performance on existing benchmarks like RefCOCO, RefCOCO+, and RefCOCOg, indicating that the approach doesn't sacrifice performance on traditional tasks while gaining capabilities for abstract reasoning. This suggests that the training methodology and architecture generalize well across different types of segmentation queries.

## Implications and Future Directions

This work opens new avenues for human-computer interaction in visual understanding tasks. By enabling models to understand intent and functional reasoning, CIS has immediate applications in robotics, where robots need to understand not just what objects are but how they can be used safely and effectively. Assistive technologies can benefit from models that understand accessibility and safety considerations in real-world environments.

The scalable supervision approach through AI-powered data generation addresses a critical bottleneck in training vision-language models for complex reasoning tasks. This methodology could be extended to other domains where abstract concepts need to be grounded in visual data, such as video understanding, 3D scene analysis, or medical imaging.

Future work could explore multi-turn conversational interactions where the model refines its understanding through dialogue, incorporate temporal reasoning for video segmentation, and extend the framework to handle even more abstract concepts like aesthetic preferences or cultural context. The integration of physical simulation could further enhance the model's understanding of material properties and interaction dynamics.

## Takeaways

1. Conversational Image Segmentation extends beyond categorical queries to handle intent, affordances, safety, and physical reasoning in pixel-accurate segmentation tasks.

2. Current language-guided segmentation models show significant performance gaps when faced with abstract, intent-driven queries, highlighting the need for specialized approaches.

3. The AI-powered data engine enables scalable generation of diverse prompt-mask pairs without human supervision, addressing the annotation bottleneck for complex reasoning tasks.

4. ConverSeg-Net achieves substantial improvements on abstract reasoning queries while maintaining competitive performance on traditional referring expression segmentation benchmarks.

5. The ConverSeg benchmark provides a comprehensive evaluation framework spanning seven categories of conversational queries, enabling systematic assessment of models' reasoning capabilities.
:::

:::zh
**论文**: [2602.13195](https://arxiv.org/abs/2602.13195)
**作者**: Aadarsh Sahoo, Georgia Gkioxari
**分类**: cs.CV

## 摘要

本文提出了对话式图像分割(CIS)这一新范式,将传统的指代图像分割从简单的类别和空间查询扩展到处理复杂的意图驱动概念。现有方法在处理"最左边的苹果"等查询时表现出色,但在功能推理方面却力不从心,例如"我可以在哪里安全地存放刀具?"作者提出了ConverSeg基准数据集,涵盖实体、空间关系、意图、功能可供性、功能、安全性和物理推理等方面。同时引入了ConverSeg-Net模型,该模型融合了强大的分割先验和语言理解能力,以及一个无需人工标注即可生成提示-掩码对的AI驱动数据引擎。实验表明,现有模型在CIS任务上表现不足,而在数据引擎上训练的ConverSeg-Net在ConverSeg上取得了显著提升,同时在现有语言引导分割基准上保持了强劲性能。

## 主要贡献

- 提出对话式图像分割(CIS)新任务,将抽象的意图驱动概念转化为像素级精确掩码
- 构建ConverSeg基准数据集,涵盖七个不同类别:实体、空间关系、意图、功能可供性、功能、安全性和物理推理
- 设计ConverSeg-Net架构,融合强大的分割先验与语言理解能力
- 开发AI驱动的数据引擎,无需人工监督即可大规模生成提示-掩码对
- 全面评估显示现有模型存在显著性能差距,而提出的方法取得了实质性提升

## 方法论与架构设计

ConverSeg-Net在现代视觉-语言模型的基础上构建,通过集成专门用于处理抽象推理的组件来增强能力。该架构利用预训练的分割模型作为强先验,同时结合能够处理复杂多面查询的语言编码器。融合机制使模型能够同时关注视觉特征和语言语义。

AI驱动的数据引擎代表了可扩展监督方面的重大创新。该系统不依赖于对安全性或功能可供性等抽象概念进行昂贵的人工标注,而是协同使用大型语言模型和视觉模型来生成多样化的提示-掩码对。这种方法能够创建涵盖对话查询全谱的训练数据,从简单的物体引用到复杂的功能推理。

数据生成流程包含多个阶段:使用语言模型进行提示合成以创建多样化和自然的查询,使用分割模型进行候选区域提议,以及通过一致性检查进行验证。这个多阶段过程在保持可扩展性的同时确保了高质量的训练数据。

## 实验结果与分析

在ConverSeg上的评估揭示了现有语言引导分割模型的关键局限性。在类别查询上表现良好的传统指代表达分割模型,在面对意图驱动或安全相关查询时性能急剧下降。例如,在RefCOCO风格数据集上训练的模型难以理解"我应该在哪里放置热物品?"或"哪个表面适合切割?"等查询。

ConverSeg-Net在基准数据集的所有七个类别上都展现出显著改进。该模型在功能可供性理解(识别可以执行动作的位置)、安全推理(确定安全的放置或交互区域)和物理推理(理解材料属性和约束)方面表现出特别强的提升。这些改进源于架构创新和AI驱动引擎生成的多样化训练数据。

重要的是,该模型在RefCOCO、RefCOCO+和RefCOCOg等现有基准上保持了强劲性能,表明该方法在获得抽象推理能力的同时不会牺牲传统任务的性能。这表明训练方法和架构在不同类型的分割查询中具有良好的泛化能力。

## 影响与未来方向

这项工作为视觉理解任务中的人机交互开辟了新途径。通过使模型能够理解意图和功能推理,CIS在机器人技术中具有直接应用价值,机器人不仅需要理解物体是什么,还需要理解如何安全有效地使用它们。辅助技术可以从理解现实环境中可访问性和安全性考虑的模型中受益。

通过AI驱动数据生成实现的可扩展监督方法解决了训练复杂推理任务视觉-语言模型的关键瓶颈。这种方法可以扩展到其他需要将抽象概念植根于视觉数据的领域,如视频理解、3D场景分析或医学影像。

未来工作可以探索多轮对话交互,使模型通过对话细化理解;结合时序推理进行视频分割;扩展框架以处理更抽象的概念,如美学偏好或文化背景。集成物理模拟可以进一步增强模型对材料属性和交互动力学的理解。

## 要点总结

1. 对话式图像分割超越了类别查询,能够在像素级精确分割任务中处理意图、功能可供性、安全性和物理推理。

2. 现有语言引导分割模型在面对抽象的意图驱动查询时表现出显著性能差距,凸显了专门方法的必要性。

3. AI驱动的数据引擎能够在无需人工监督的情况下大规模生成多样化的提示-掩码对,解决了复杂推理任务的标注瓶颈。

4. ConverSeg-Net在抽象推理查询上取得了实质性改进,同时在传统指代表达分割基准上保持了竞争力。

5. ConverSeg基准提供了涵盖七类对话查询的综合评估框架,能够系统评估模型的推理能力。
:::
