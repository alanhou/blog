---
title:
  en: "The Key to State Reduction in Linear Attention: A Rank-based Perspective"
  zh: "线性注意力状态压缩的关键：基于秩的视角"
description:
  en: "Theoretical analysis and practical framework for reducing state size in linear attention models through structured pruning"
  zh: "通过结构化剪枝减少线性注意力模型状态大小的理论分析和实用框架"
date: 2026-02-04
tags: ["arxiv", "ai", "linear-attention", "pruning", "efficiency", "transformers", "cs.LG"]
image: "/arxiv-visuals/arxiv-linear-attention-state-reduction.png"
---

:::en
**Paper**: [2602.04852](https://arxiv.org/abs/2602.04852)
**Authors**: Philipp Nazari, T. Konstantin Rusch
**Affiliations**: Max Planck Institute for Intelligent Systems, ETH Zürich, ELLIS Institute Tübingen, Liquid AI
**Code**: https://github.com/camail-official/LinearAttentionPruning
**Categories**: cs.LG

## Abstract

Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. We provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise.

## Key Contributions

- **Theoretical insights**: Analysis of how rank utilization affects retrieval error in linear attention
- **State reduction framework**: Novel post-training approach to reduce state size with minimal performance degradation
- **Structured pruning method**: Based on rank-revealing QR decomposition
- **Empirical validation**: 50% of query/key channels can be removed with only marginal perplexity increase

## Background: Linear Attention

### Softmax Attention

$$O = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

- Quadratic time and memory complexity $O(T^2)$
- Problematic for long sequences

### Linear Attention
Removes softmax and exploits associativity:

$$QK^T V = Q(K^T V)$$

Sequential form with hidden state:

$$S_t = S_{t-1} + v_t k_t^T$$
$$o_t = S_t q_t$$

Where $S_t$ is a matrix of size $(d_v, d_k)$, the **associative memory**.

Benefits:
- Linear time complexity
- Constant memory inference
- Chunkwise parallel training

## The Low-Rank Problem

Empirical observation: Trained linear attention models exhibit **low-rank structure** in their hidden states.

### Why Does This Matter?

The rank of the associative memory is bounded:

$$\text{rank}(S_t) \leq \min(\text{rank}(K_t), \text{rank}(V_t)) \leq t$$

But the **effective rank** (stable rank) is what matters:

$$\text{sr}(A) = \frac{\|A\|_F^2}{\|A\|_2^2}$$

This measures how evenly energy is distributed across singular values.

### Rank Utilization

$$u(S) = \frac{\text{sr}(S)}{\min(d_v, d_k)}$$

- **Low utilization ($u \ll 1$)**: Rank collapse, energy concentrated in few components, redundant information
- **High utilization ($u \approx 1$)**: Isotropic memory, energy evenly distributed

### Impact on Retrieval Error

For a noisy query $\tilde{q} = q^* + n$:
- Low rank utilization **amplifies query noise**
- High rank utilization provides more robust retrieval

Key insight: The conditioning of keys $\kappa(K_t)$ is tightly connected to the effective rank of the hidden state.

## State Reduction Framework

Since models underutilize their capacity, we can **prune** the state post-training.

### Key Observation

Linear attention models are **invariant under orthogonal transformations** applied jointly to queries and keys:

$$(QT)(KT)^T = QK^T$$

This allows selecting subsets of columns that contribute most to performance.

### Structured vs Unstructured Pruning

| Type | Effect | State Size |
|------|--------|------------|
| Unstructured | Sparse weights, same dimensions | Unchanged |
| Structured | Eliminates basis vectors | Reduced to $d'_k < d_k$ |

Structured pruning **strictly decreases FLOPs** for the recurrence.

### Pruning Methods

1. **Magnitude-based**: Remove columns with smallest norms
2. **Gradient-saliency**: Remove columns with smallest gradient contributions
3. **Rank-revealing QR** (proposed): Select columns that maximize rank utilization of remaining memory

## Results

Across models of varying sizes and downstream tasks:

- **50% of query/key channels can be removed** with only marginal perplexity increase
- Works even **before recovery fine-tuning**
- Complementary to other compression methods (SliceGPT, SpinQuant, QuaRot)

### Comparison

| Method | Operates on | Complementary |
|--------|-------------|---------------|
| SliceGPT | Embedding space | Yes |
| SpinQuant | Embedding space | Yes |
| This work | State space | Yes |

## Takeaways

1. **Linear attention underutilizes capacity**: Trained models exhibit low-rank hidden states
2. **Low rank hurts retrieval**: Amplifies query noise and reduces robustness
3. **Post-training pruning works**: 50% reduction with minimal performance loss
4. **Rank-revealing QR is effective**: Principled selection of important dimensions
5. **Faster and more memory-efficient**: Reduced state size directly improves inference
:::

:::zh
**论文**: [2602.04852](https://arxiv.org/abs/2602.04852)
**作者**: Philipp Nazari, T. Konstantin Rusch
**机构**: 马克斯·普朗克智能系统研究所, 苏黎世联邦理工学院, ELLIS图宾根研究所, Liquid AI
**代码**: https://github.com/camail-official/LinearAttentionPruning
**分类**: cs.LG

## 摘要

线性注意力提供了一种计算高效且表达能力强的softmax注意力替代方案。然而，最近的实证结果表明，训练后的线性注意力模型的状态通常呈现低秩结构，这表明这些模型在实践中未能充分利用其容量。我们提供了关于秩在线性注意力中作用的理论分析，揭示了低有效秩会通过放大查询噪声来影响检索误差。

## 主要贡献

- **理论洞察**：分析秩利用率如何影响线性注意力中的检索误差
- **状态压缩框架**：新颖的训练后方法，以最小的性能下降减少状态大小
- **结构化剪枝方法**：基于秩揭示QR分解
- **实证验证**：可以移除50%的查询/键通道，仅有边际困惑度增加

## 背景：线性注意力

### Softmax注意力

$$O = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

- 二次时间和内存复杂度 $O(T^2)$
- 对长序列有问题

### 线性注意力
移除softmax并利用结合律：

$$QK^T V = Q(K^T V)$$

带隐藏状态的顺序形式：

$$S_t = S_{t-1} + v_t k_t^T$$
$$o_t = S_t q_t$$

其中 $S_t$ 是大小为 $(d_v, d_k)$ 的矩阵，即**关联记忆**。

优点：
- 线性时间复杂度
- 恒定内存推理
- 分块并行训练

## 低秩问题

实证观察：训练后的线性注意力模型在其隐藏状态中呈现**低秩结构**。

### 为什么这很重要？

关联记忆的秩有界：

$$\text{rank}(S_t) \leq \min(\text{rank}(K_t), \text{rank}(V_t)) \leq t$$

但**有效秩**（稳定秩）才是关键：

$$\text{sr}(A) = \frac{\|A\|_F^2}{\|A\|_2^2}$$

这衡量能量在奇异值上分布的均匀程度。

### 秩利用率

$$u(S) = \frac{\text{sr}(S)}{\min(d_v, d_k)}$$

- **低利用率 ($u \ll 1$)**：秩崩溃，能量集中在少数分量，信息冗余
- **高利用率 ($u \approx 1$)**：各向同性记忆，能量均匀分布

### 对检索误差的影响

对于噪声查询 $\tilde{q} = q^* + n$：
- 低秩利用率**放大查询噪声**
- 高秩利用率提供更稳健的检索

关键洞察：键的条件数 $\kappa(K_t)$ 与隐藏状态的有效秩紧密相关。

## 状态压缩框架

由于模型未充分利用其容量，我们可以在训练后**剪枝**状态。

### 关键观察

线性注意力模型在对查询和键联合应用正交变换时是**不变的**：

$$(QT)(KT)^T = QK^T$$

这允许选择对性能贡献最大的列子集。

### 结构化与非结构化剪枝

| 类型 | 效果 | 状态大小 |
|------|------|----------|
| 非结构化 | 稀疏权重，相同维度 | 不变 |
| 结构化 | 消除基向量 | 减少到 $d'_k < d_k$ |

结构化剪枝**严格减少**递归的FLOPs。

### 剪枝方法

1. **基于幅度**：移除范数最小的列
2. **梯度显著性**：移除梯度贡献最小的列
3. **秩揭示QR**（提出的）：选择最大化剩余记忆秩利用率的列

## 实验结果

在不同大小的模型和下游任务中：

- **可以移除50%的查询/键通道**，仅有边际困惑度增加
- 甚至在**恢复微调之前**就有效
- 与其他压缩方法互补（SliceGPT, SpinQuant, QuaRot）

### 比较

| 方法 | 操作空间 | 互补性 |
|------|----------|--------|
| SliceGPT | 嵌入空间 | 是 |
| SpinQuant | 嵌入空间 | 是 |
| 本工作 | 状态空间 | 是 |

## 要点总结

1. **线性注意力未充分利用容量**：训练后的模型呈现低秩隐藏状态
2. **低秩损害检索**：放大查询噪声并降低稳健性
3. **训练后剪枝有效**：50%的压缩仅有最小性能损失
4. **秩揭示QR有效**：有原则地选择重要维度
5. **更快更省内存**：减少的状态大小直接改善推理
:::
