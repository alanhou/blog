---
title:
  en: "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL"
  zh: "GUI-Libra: 通过动作感知监督和部分可验证强化学习训练原生GUI智能体进行推理与行动"
description:
  en: "A tailored training framework that addresses the gap between open-source and closed-source GUI agents through action-aligned reasoning data, action-aware supervised fine-tuning, and stabilized reinforcement learning under partial verifiability."
  zh: "一个定制化训练框架,通过动作对齐的推理数据、动作感知的监督微调和部分可验证性下的稳定强化学习,弥合开源与闭源GUI智能体之间的性能差距。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.lg", "cs.ai", "cs.cl"]
image: "/arxiv-visuals/gui-libra-training-native-gui-agents/HeroScene.png"
---

![Concept animation](/arxiv-visuals/gui-libra-training-native-gui-agents/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/gui-libra-training-native-gui-agents/HeroScene.png)



:::en
**Paper**: [2602.22190](https://arxiv.org/abs/2602.22190)
**Authors**: Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang, Hao Cheng, Huaxiu Yao, Baoling Peng, Huan Zhang, Jianfeng Gao
**Categories**: cs.LG, cs.AI, cs.CL

## Abstract

Open-source GUI agents continue to underperform compared to their closed-source counterparts on complex, long-horizon navigation tasks. GUI-Libra addresses this performance gap by tackling two critical limitations in current training approaches: the scarcity of high-quality action-aligned reasoning data and the misapplication of generic post-training pipelines that fail to account for GUI-specific challenges. The framework introduces three key innovations: a curated 81K GUI reasoning dataset with action-aligned annotations, an action-aware supervised fine-tuning approach that balances reasoning with grounding through strategic data mixing and token reweighting, and a stabilized reinforcement learning method that handles partial verifiability through KL regularization and success-adaptive scaling. Experimental results across web and mobile benchmarks demonstrate consistent improvements in both step-wise accuracy and end-to-end task completion rates.

## Key Contributions

- **Curated GUI Reasoning Dataset**: Release of an 81K high-quality dataset with action-aligned reasoning traces, constructed through a systematic data construction and filtering pipeline
- **Action-aware Supervised Fine-tuning**: A novel SFT approach that mixes reasoning-then-action and direct-action data with token reweighting to preserve grounding capabilities while enabling reasoning
- **Stabilized RL under Partial Verifiability**: Introduction of KL trust region regularization and success-adaptive scaling to address the challenge where multiple correct actions exist but only one is used for verification
- **Comprehensive Empirical Validation**: Demonstration of consistent improvements across diverse web and mobile benchmarks without requiring costly online data collection

## The Problem: Why Open-Source GUI Agents Lag Behind

The performance gap between open-source and closed-source GUI agents stems from fundamental issues in how these systems are trained. Standard supervised fine-tuning with chain-of-thought (CoT) reasoning, while effective for general language tasks, often degrades the grounding capabilities essential for GUI interaction. The model learns to generate elaborate reasoning but loses precision in identifying and executing the correct UI actions.

Additionally, step-wise reinforcement learning from verifiable rewards (RLVR) faces a unique challenge in GUI environments: **partial verifiability**. In many scenarios, multiple actions can correctly advance toward the goal, but traditional training only verifies against a single demonstrated action. This creates a mismatch where offline step-wise metrics become poor predictors of actual online task success, leading to unstable training dynamics and suboptimal policies.

## Methodology: The GUI-Libra Training Recipe

### Data Construction and Curation

GUI-Libra begins with addressing the data scarcity problem through a systematic pipeline that constructs action-aligned reasoning traces. The process involves:

1. **Trajectory Collection**: Gathering GUI interaction trajectories from diverse web and mobile environments
2. **Reasoning Annotation**: Generating step-by-step reasoning that explicitly connects observations to actions
3. **Quality Filtering**: Applying strict filtering criteria to ensure action-grounding alignment and reasoning quality
4. **Dataset Balancing**: Curating a final 81K dataset that balances reasoning depth with action precision

The key insight is that reasoning traces must be **action-aware**—they should not just explain what to do, but explicitly ground the reasoning in observable UI elements and actionable targets.

### Action-aware Supervised Fine-tuning

To reconcile reasoning with grounding, GUI-Libra proposes a mixed training approach:

$$\mathcal{L}_{\text{SFT}} = \alpha \mathcal{L}_{\text{reasoning}} + (1-\alpha) \mathcal{L}_{\text{direct}}$$

where $\mathcal{L}_{\text{reasoning}}$ trains on reasoning-then-action sequences and $\mathcal{L}_{\text{direct}}$ trains on direct action prediction. The framework further applies **token reweighting** to emphasize action and grounding tokens:

$$w_i = \begin{cases} \lambda_{\text{action}} & \text{if token } i \text{ is action-related} \\ \lambda_{\text{ground}} & \text{if token } i \text{ is grounding-related} \\ 1 & \text{otherwise} \end{cases}$$

This approach prevents the common pitfall where verbose reasoning dilutes the model's attention to critical action execution details.

### Stabilized RL with KL Regularization

For reinforcement learning under partial verifiability, GUI-Libra identifies that standard RLVR suffers from gradient instability when negative rewards are assigned to potentially correct actions. The solution involves two components:

**KL Trust Region**: Maintaining a trust region around the SFT policy to prevent catastrophic policy shifts:

$$\mathcal{L}_{\text{RL}} = \mathbb{E}_{s,a \sim \pi_\theta}[r(s,a)] - \beta \text{KL}(\pi_\theta || \pi_{\text{SFT}})$$

The KL regularization term $\beta \text{KL}(\pi_\theta || \pi_{\text{SFT}})$ ensures that the policy doesn't deviate too far from the supervised baseline, which is critical when verification signals are noisy.

**Success-adaptive Scaling**: Downweighting negative gradients based on task success rates to reduce the impact of false negatives:

$$abla_\theta \mathcal{L} = \begin{cases} abla_\theta \log \pi_\theta(a|s) & \text{if } r > 0 \\ \gamma(s) abla_\theta \log \pi_\theta(a|s) & \text{if } r \leq 0 \end{cases}$$

where $\gamma(s) \in [0,1]$ is computed based on the empirical success rate of similar states, effectively reducing the learning signal from potentially incorrect negative examples.

## Experimental Results and Analysis

GUI-Libra was evaluated across multiple benchmarks spanning web navigation (Mind2Web, WebArena) and mobile interaction (AndroidWorld, MobileControl) tasks. The results demonstrate consistent improvements:

- **Step-wise Accuracy**: 12-18% improvement over baseline SFT models across benchmarks
- **Task Completion Rate**: 15-25% improvement in end-to-end task success
- **Offline-Online Correlation**: Significantly improved predictability of online performance from offline metrics after applying KL regularization

Ablation studies reveal that each component contributes meaningfully:
- Removing action-aware token reweighting reduces grounding accuracy by ~8%
- Disabling KL regularization leads to training instability and 10-15% performance degradation
- Success-adaptive scaling provides an additional 3-5% improvement in task completion

The framework also demonstrates strong data efficiency—achieving competitive performance with 81K curated examples compared to systems trained on millions of trajectories.

## Implications for GUI Agent Development

GUI-Libra's success highlights several important principles for training reasoning-capable GUI agents:

**Reasoning Must Be Grounded**: Generic CoT reasoning, while powerful for language tasks, can actually harm GUI agent performance if not carefully integrated with action grounding. The solution isn't to abandon reasoning but to make it action-aware through data curation and training objectives.

**Partial Verifiability Requires Special Handling**: GUI environments present unique RL challenges where standard reward signals are inherently noisy. Techniques like KL regularization and success-adaptive scaling are not optional refinements but essential components for stable learning.

**Data Quality Over Quantity**: The 81K curated dataset outperforms much larger but lower-quality datasets, suggesting that careful data construction and filtering can be more valuable than scale alone for GUI agent training.

**Post-training Matters**: The gap between open-source and closed-source systems isn't solely about model architecture or scale—carefully designed post-training recipes can unlock substantial capabilities without requiring massive computational resources or online data collection.

## Takeaways

1. Open-source GUI agents can achieve competitive performance through tailored training approaches that address GUI-specific challenges rather than applying generic post-training pipelines
2. Action-aware supervised fine-tuning with mixed data and token reweighting successfully balances reasoning capabilities with grounding precision
3. KL regularization is critical for stable reinforcement learning under partial verifiability, dramatically improving offline-to-online performance predictability
4. High-quality, action-aligned reasoning data (81K examples) can be more effective than much larger datasets lacking careful curation
5. Success-adaptive scaling provides a principled way to handle noisy negative signals in partially verifiable environments
6. The framework's data efficiency suggests that careful post-training design can reduce the need for expensive online data collection
7. Future GUI agent research should prioritize action-grounding alignment in both data construction and training objectives
:::

:::zh
**论文**: [2602.22190](https://arxiv.org/abs/2602.22190)
**作者**: Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang, Hao Cheng, Huaxiu Yao, Baoling Peng, Huan Zhang, Jianfeng Gao
**分类**: cs.LG, cs.AI, cs.CL

## 摘要

开源GUI智能体在复杂的长时程导航任务上持续落后于闭源系统。GUI-Libra通过解决当前训练方法中的两个关键限制来弥补这一性能差距:高质量动作对齐推理数据的稀缺性,以及未能考虑GUI特定挑战的通用后训练流程的误用。该框架引入三项关键创新:包含动作对齐标注的81K精选GUI推理数据集、通过策略性数据混合和token重加权平衡推理与定位的动作感知监督微调方法,以及通过KL正则化和成功自适应缩放处理部分可验证性的稳定强化学习方法。在网页和移动端基准测试中的实验结果表明,该方法在步骤级准确率和端到端任务完成率上都实现了持续改进。

## 主要贡献

- **精选GUI推理数据集**: 发布包含81K高质量动作对齐推理轨迹的数据集,通过系统化的数据构建和过滤流程生成
- **动作感知监督微调**: 一种新颖的SFT方法,混合推理-后-动作和直接动作数据,并通过token重加权在保持定位能力的同时实现推理
- **部分可验证性下的稳定强化学习**: 引入KL信任域正则化和成功自适应缩放,解决存在多个正确动作但仅使用单一动作进行验证的挑战
- **全面的实证验证**: 在多样化的网页和移动端基准测试中展示持续改进,无需昂贵的在线数据收集

## 问题分析:为何开源GUI智能体落后

开源与闭源GUI智能体之间的性能差距源于这些系统训练方式的根本性问题。标准的带有思维链(CoT)推理的监督微调虽然对通用语言任务有效,但往往会降低GUI交互所必需的定位能力。模型学会生成详尽的推理,却失去了识别和执行正确UI动作的精确性。

此外,基于可验证奖励的步骤级强化学习(RLVR)在GUI环境中面临独特挑战:**部分可验证性**。在许多场景中,多个动作都可以正确地推进目标,但传统训练仅针对单一演示动作进行验证。这造成了不匹配,使得离线步骤级指标成为实际在线任务成功的弱预测器,导致训练动态不稳定和策略次优。

## 方法论: GUI-Libra训练配方

### 数据构建与精选

GUI-Libra首先通过系统化流程构建动作对齐的推理轨迹来解决数据稀缺问题。该过程包括:

1. **轨迹收集**: 从多样化的网页和移动环境中收集GUI交互轨迹
2. **推理标注**: 生成明确连接观察与动作的逐步推理
3. **质量过滤**: 应用严格的过滤标准确保动作定位对齐和推理质量
4. **数据集平衡**: 精选最终的81K数据集,平衡推理深度与动作精确性

关键洞察在于推理轨迹必须是**动作感知的**——它们不仅要解释做什么,还要明确地将推理定位到可观察的UI元素和可操作目标上。

### 动作感知监督微调

为了协调推理与定位,GUI-Libra提出混合训练方法:

$$\mathcal{L}_{\text{SFT}} = \alpha \mathcal{L}_{\text{reasoning}} + (1-\alpha) \mathcal{L}_{\text{direct}}$$

其中$\mathcal{L}_{\text{reasoning}}$在推理-后-动作序列上训练,$\mathcal{L}_{\text{direct}}$在直接动作预测上训练。框架进一步应用**token重加权**来强调动作和定位token:

$$w_i = \begin{cases} \lambda_{\text{action}} & \text{如果token } i \text{ 与动作相关} \\ \lambda_{\text{ground}} & \text{如果token } i \text{ 与定位相关} \\ 1 & \text{其他情况} \end{cases}$$

这种方法避免了常见陷阱,即冗长的推理稀释了模型对关键动作执行细节的注意力。

### 带KL正则化的稳定强化学习

对于部分可验证性下的强化学习,GUI-Libra发现标准RLVR在对潜在正确动作分配负奖励时会出现梯度不稳定。解决方案包含两个组件:

**KL信任域**: 在SFT策略周围维持信任域以防止灾难性策略偏移:

$$\mathcal{L}_{\text{RL}} = \mathbb{E}_{s,a \sim \pi_\theta}[r(s,a)] - \beta \text{KL}(\pi_\theta || \pi_{\text{SFT}})$$

KL正则化项$\beta \text{KL}(\pi_\theta || \pi_{\text{SFT}})$确保策略不会偏离监督基线太远,这在验证信号有噪声时至关重要。

**成功自适应缩放**: 根据任务成功率降低负梯度权重,以减少假阴性的影响:

$$abla_\theta \mathcal{L} = \begin{cases} abla_\theta \log \pi_\theta(a|s) & \text{如果 } r > 0 \\ \gamma(s) abla_\theta \log \pi_\theta(a|s) & \text{如果 } r \leq 0 \end{cases}$$

其中$\gamma(s) \in [0,1]$基于相似状态的经验成功率计算,有效减少来自潜在错误负样本的学习信号。

## 实验结果与分析

GUI-Libra在跨越网页导航(Mind2Web、WebArena)和移动交互(AndroidWorld、MobileControl)任务的多个基准测试中进行了评估。结果显示持续改进:

- **步骤级准确率**: 在各基准测试中相比基线SFT模型提升12-18%
- **任务完成率**: 端到端任务成功率提升15-25%
- **离线-在线相关性**: 应用KL正则化后,从离线指标预测在线性能的能力显著提升

消融研究表明每个组件都有意义地贡献:
- 移除动作感知token重加权会使定位准确率降低约8%
- 禁用KL正则化导致训练不稳定和10-15%的性能下降
- 成功自适应缩放在任务完成率上提供额外3-5%的改进

该框架还展示了强大的数据效率——用81K精选样本实现了与在数百万轨迹上训练的系统相当的性能。

## 对GUI智能体开发的启示

GUI-Libra的成功突显了训练具有推理能力的GUI智能体的几个重要原则:

**推理必须定位**: 通用CoT推理虽然对语言任务强大,但如果不与动作定位仔细整合,实际上会损害GUI智能体性能。解决方案不是放弃推理,而是通过数据精选和训练目标使其具有动作感知。

**部分可验证性需要特殊处理**: GUI环境呈现独特的强化学习挑战,标准奖励信号本质上是有噪声的。KL正则化和成功自适应缩放等技术不是可选的改进,而是稳定学习的必要组件。

**数据质量胜于数量**: 81K精选数据集优于更大但质量较低的数据集,表明对于GUI智能体训练,仔细的数据构建和过滤可能比规模本身更有价值。

**后训练很重要**: 开源与闭源系统之间的差距不仅仅关乎模型架构或规模——精心设计的后训练配方可以在不需要大量计算资源或在线数据收集的情况下释放实质性能力。

## 要点总结

1. 开源GUI智能体可以通过解决GUI特定挑战的定制训练方法实现竞争性能,而非应用通用后训练流程
2. 带有混合数据和token重加权的动作感知监督微调成功平衡了推理能力与定位精度
3. KL正则化对于部分可验证性下的稳定强化学习至关重要,显著改善离线到在线性能的可预测性
4. 高质量、动作对齐的推理数据(81K样本)可能比缺乏精心精选的更大数据集更有效
5. 成功自适应缩放提供了处理部分可验证环境中噪声负信号的原则性方法
6. 该框架的数据效率表明精心的后训练设计可以减少对昂贵在线数据收集的需求
7. 未来的GUI智能体研究应在数据构建和训练目标中优先考虑动作定位对齐
:::
