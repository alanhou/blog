---
title:
  en: "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill"
  zh: "QUOKA：面向查询的KV选择实现高效LLM预填充"
description:
  en: "Training-free, hardware-agnostic sparse attention achieving 3x TTFT reduction and 5x attention speedup using 88% fewer KV pairs"
  zh: "无需训练、硬件无关的稀疏注意力方法，使用减少88%的KV对实现3倍TTFT降低和5倍注意力加速"
date: 2026-02-10
tags: ["arxiv", "ai", "attention", "kv-cache", "efficiency", "inference", "sparse-attention", "cs.LG"]
image: "/arxiv-visuals/arxiv-quoka-query-kv-selection.png"
---

:::en
**Paper**: [2602.08722](https://arxiv.org/abs/2602.08722)
**Authors**: Dalton Jones, Junyoung Park, Matthew Morse, Mingu Lee, Chris Lott, Harper Langston
**Categories**: cs.LG

## Abstract

QUOKA introduces a training-free, hardware-agnostic sparse attention mechanism for accelerating transformer inference under chunked prefill. The method is grounded in a key observation: queries with low cosine similarity to the mean query vector interact more strongly with a broader set of keys and contribute most significantly to attention logits. By retaining a small set of representative queries and subselecting the most aligned keys, QUOKA achieves a 3x reduction in time-to-first-token (TTFT), 5x attention speedup on Nvidia GPUs, 7x on Intel Xeon CPUs, and near-baseline accuracy while using 88% fewer KV pairs.

## Key Contributions

- **Query-oriented KV selection**: A novel perspective that selects KV pairs based on query characteristics rather than key/value statistics alone
- **Cosine similarity insight**: Discovery that queries deviating from the mean query are the most informationally rich and interact with more keys
- **Training-free deployment**: No fine-tuning or retraining required, works as a drop-in replacement
- **Hardware-agnostic speedups**: Demonstrated gains on both GPU (Nvidia) and CPU (Intel Xeon) platforms
- **Extreme KV reduction**: Near-baseline accuracy with only 12% of original KV pairs

## Background: The Prefill Bottleneck

In autoregressive LLM inference, the **prefill phase** processes the entire input prompt to populate the KV cache before generating the first output token. For long prompts, this phase dominates latency and is bounded by the quadratic cost of self-attention:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Chunked prefill splits the input into manageable chunks, but each chunk still computes full attention over all accumulated KV pairs. As context grows, this becomes the primary bottleneck.

## The QUOKA Method

### Step 1: Query Analysis

QUOKA first computes the mean query vector $\bar{q}$ across all queries in a chunk:

$$\bar{q} = \frac{1}{n}\sum_{i=1}^{n} q_i$$

Then it computes the cosine similarity of each query to this mean:

$$s_i = \frac{q_i \cdot \bar{q}}{\|q_i\| \|\bar{q}\|}$$

### Step 2: Representative Query Selection

Queries with **low** cosine similarity to the mean (i.e., outlier queries) are selected as representatives. These queries are the most "demanding" in terms of attention — they attend to a wider range of keys and contribute disproportionately to the final attention output.

### Step 3: Key Subselection

For each representative query, QUOKA identifies the most aligned keys using dot-product scoring. Only these high-alignment KV pairs are retained for the full attention computation. The remaining queries (those close to the mean) can be adequately served by this reduced KV set.

### Why Outlier Queries Matter

The core insight is counterintuitive: queries that are most similar to each other (close to the mean) tend to produce diffuse, low-entropy attention distributions. Outlier queries produce sharper, more focused attention patterns that capture the critical information flow. By ensuring these outlier queries have access to their most relevant keys, QUOKA preserves the essential attention computation while discarding redundant KV interactions.

## Results

| Metric | Value |
|--------|-------|
| TTFT Reduction | 3x |
| Attention Speedup (Nvidia GPU) | 5x |
| Attention Speedup (Intel Xeon CPU) | 7x |
| KV Pairs Used | 12% (88% reduction) |
| Accuracy Degradation | Near-baseline |

- Consistent improvements across multiple model sizes and architectures
- Benefits compound with longer context lengths
- Compatible with existing KV cache compression techniques

## Significance

1. **Practical inference acceleration**: Directly addresses the prefill bottleneck that limits real-world LLM deployment
2. **No training cost**:ike learned sparse attention methods, QUOKA requires zero additional training
3. **Cross-platform**: Rare to see a method that accelerates both GPU and CPU inference
4. **Composable**: Can be combined with other efficiency techniques (quantization, pruning, etc.)
5. **Theoretical insight**: The query-centric view of attention sparsity opens new research directions for understanding which computations in attention are truly necessary
:::

:::zh
**论文**: [2602.08722](https://arxiv.org/abs/2602.08722)
**作者**: Dalton Jones, Junyoung Park, Matthew Morse, Mingu Lee, Chris Lott, Harper Langston
**分类**: cs.LG

## 摘要

QUOKA引入了一种无需训练、硬件无关的稀疏注意力机制，用于加速分块预填充下的Transformer推关键观察：与平均查询向量余弦相似度较低的查询与更多的键产生更强的交互，并对注意力logits贡献最大。通过保留一小组代表性查询，然后子选择最对齐的键，QUOKA实现了3倍的首token生成时间（TTFT）降低、Nvidia GPU上5倍的注意力加速、Intel Xeon CPU上7倍的加速，以及在使用减少88%的KV对的情况下接近基线的准确率。

## 主要贡献

- **面向查询的KV选择**：一种基于查询特征而非仅基于键/值统计来选择KV对的新视角
- **余弦相似度洞察**：发现偏离平均查询的查询信息量最丰富，与更多键交互
- **无需训练部署**：不需要微调或重新训练，可作为即插即用替代
- **硬件无关加速**：在GPU（Nvidia）和CPU（Intel Xeon）平台上均展示了性能提升
- **极端KV缩减**：仅使用12%的原始KV对即可达到接近基线的准确率

## 背景：预填充瓶颈

在自回归LLM推理中，**预填充阶段**处理整个输入提示以填充KV缓存，然后才能生成第一个输出token。对于长提示，这个阶段主导延迟，受限于自注意力的二次方成本：

$$\text{Attention}(Q, K, V) = \text{softmax}\left}{\sqrt{d_k}}\right)V$$

分块预填充将输入分成可管理的块，但每个块仍然需要对所有累积的KV对计算完整注意力。随着上下文增长，这成为主要瓶颈。

## QUOKA方法

### 步骤1：查询分析

QUOKA首先计算一个块中所有查询的平均查询向量$\bar{q}$：

$$\bar{q} = \frac{1}{n}\sum_{i=1}^{n} q_i$$

然后计算每个查询与该平均值的余弦相似度：

$$s_i = \frac{q_i \cdot \bar{q}}{\|q_i\| \|\bar{q}\|}$$

### 步骤2：代表性查询选择

选择与平均值余弦相似度**低**的查询（即离群查询）作为代表。这些查询在注意力方面最为"苛刻"——它们关注更广泛的键范围，并对最终注意力输出产生不成比例的贡献。

### 步骤3：键子选择

对于每个代表性查询，QUOKA使用点积评分识别最对齐的键。只有这些高对齐度的KV对被保留用于完整的注意力计算。其余查询（接近平均值的查询）可以由这个缩减的KV集合充分服务。

### 为什么离群查询重要

核心洞察是反直觉的：彼此最相似的查询（接近平均值）倾向于产生分散的、低熵的注意力分布。离群查询产生更尖锐、更集中的注意力模式，捕获关键的信息流。通过确保这些离群查询能够访问其最相关的键，QUOKA保留了本质的注意力计算，同时丢弃了冗余的KV交互。

## 实验结果

| 指标 | 数值 |
|------|------|
| TTFT降低 | 3倍 |
| 注意力加速（Nvidia GPU） | 5倍 |
| 注意力加速（Intel Xeon CPU） | 7倍 |
| 使用的KV对 | 12%（减少88%） |
| 准确率下降 | 接近基线 |

- 在多种模型规模和架构上表现一致
- 随着上下文长度增加，收益更加显著
- 与现有KV缓存压缩技术兼容

## 重要意义

1. **实用推理加速**：直接解决限制实际LLM部署的预填充瓶颈
2. **零训练成本**：与学习型稀疏注意力方法不同，QUOKA不需要额外训练
3. **跨平台**：同时加速GPU和CPU推理的方法非常罕见
4. **可组合**：可与其他效率技术（量化、剪枝等）结合使用
5. **理论洞察**：以查询为中心的注意力稀疏性视角为理解注意力中哪些计算真正必要开辟了新的研究方向
:::
