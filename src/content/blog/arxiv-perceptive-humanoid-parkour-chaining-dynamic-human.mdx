---
title:
  en: "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching"
  zh: "感知型人形机器人跑酷:通过动作匹配链接动态人类技能"
description:
  en: "A modular framework enabling humanoid robots to perform autonomous, vision-based parkour by composing human motion skills through motion matching and reinforcement learning."
  zh: "一个模块化框架,使人形机器人能够通过动作匹配和强化学习组合人类动作技能,实现自主的基于视觉的跑酷运动。"
date: 2026-02-18
tags: ["arxiv", "ai", "cs.ro", "cs.ai", "cs.lg", "eess.sy"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15827](https://arxiv.org/abs/2602.15827)
**Authors**: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza
**Categories**: cs.RO, cs.AI, cs.LG, eess.SY

## Abstract

This paper introduces Perceptive Humanoid Parkour (PHP), a framework that enables humanoid robots to autonomously navigate complex obstacle courses using vision-based parkour skills. While recent progress in humanoid locomotion has achieved stable walking, replicating the agility and fluidity of dynamic human movements like parkour remains challenging. PHP addresses this by combining motion matching for skill composition, reinforcement learning for motion tracking, and depth-based perception for autonomous decision-making. The system successfully demonstrates highly dynamic maneuvers on a Unitree G1 robot, including climbing obstacles up to 1.25m tall (96% of robot height) and executing long-horizon multi-obstacle traversal with real-time adaptation.

## Key Contributions

- A motion matching framework that composes retargeted atomic human skills into long-horizon kinematic trajectories through nearest-neighbor search in feature space, preserving the naturalness of human motion
- A hierarchical learning pipeline combining motion-tracking RL expert policies with policy distillation (DAgger + RL) into a single depth-based multi-skill student policy
- Autonomous, perception-driven decision-making that enables context-aware skill selection (stepping over, climbing, vaulting, rolling) using only onboard depth sensing and 2D velocity commands
- Extensive real-world validation on Unitree G1 demonstrating dynamic parkour capabilities and closed-loop adaptation to obstacle perturbations

## Technical Methodology

The PHP framework operates through a three-stage pipeline that bridges human motion capture data to robot execution.

**Stage 1: Motion Matching for Skill Composition**

The system begins with a database of retargeted human motion clips representing atomic parkour skills. Motion matching is formulated as a nearest-neighbor search problem in a carefully designed feature space. At each timestep $t$, the algorithm searches for the motion frame that best matches the current state and desired future trajectory:

$$\text{cost}(i) = w_p \|p_i - p_{\text{current}}\|^2 + w_v \|v_i - v_{\text{current}}\|^2 + w_{\text{traj}} \sum_{j} \|p_{i+j} - p_{\text{desired},j}\|^2$$

where $p$ represents positions, $v$ velocities, and the trajectory term ensures smooth transitions toward desired future states. This approach enables flexible composition of complex skill chains while maintaining the elegance of human motion. The feature space includes joint positions, velocities, contact states, and future trajectory predictions, weighted to prioritize smooth transitions and goal-directed behavior.

**Stage 2: Motion-Tracking RL Expert Policies**

For each composed kinematic trajectory, the system trains specialized RL policies to track the reference motion on the physical robot. The reward function combines tracking accuracy, stability, and energy efficiency:

$$r_t = w_{\text{pose}} r_{\text{pose}} + w_{\text{vel}} r_{\text{vel}} + w_{\text{contact}} r_{\text{contact}} - w_{\text{energy}} \|\tau_t\|^2$$

These expert policies learn robust tracking behaviors that handle the sim-to-real gap and physical constraints of the robot hardware. The training uses domain randomization over physical parameters, ground friction, and actuator dynamics to ensure robustness.

**Stage 3: Perception-Based Policy Distillation**

The final stage distills multiple expert policies into a single student policy that operates directly from depth observations. This uses a combination of Dataset Aggregation (DAgger) and RL:

- DAgger phase: The student policy is trained to imitate expert actions while the expert corrects mistakes, building a dataset of state-action pairs that covers the student's actual state distribution
- RL fine-tuning: The student policy is further refined with RL to handle perception noise and improve closed-loop performance

The student policy takes as input a depth image from an onboard camera, proprioceptive state information, and a discrete 2D velocity command. It outputs joint position targets and autonomously selects which skill to execute based on perceived obstacle geometry.

## Experimental Results and Analysis

The authors conducted extensive real-world experiments on a Unitree G1 humanoid robot across various obstacle configurations.

**Single Obstacle Performance**

PHP successfully executed four distinct parkour skills:
- Step-over: For low obstacles (0.3-0.5m), maintaining continuous forward motion
- Climb: For tall obstacles (0.8-1.25m), using dynamic pulling motions
- Vault: For medium-height obstacles (0.5-0.8m), placing hands on top and swinging legs over
- Roll-off: For descending from elevated platforms

The system achieved a 96% success rate on climbing obstacles up to 1.25m (96% of the robot's 1.3m height), demonstrating remarkable dynamic capability. The motion matching approach preserved the natural fluidity of human parkour movements, with smooth transitions between stance and aerial phases.

**Multi-Obstacle Traversal**

In long-horizon experiments, PHP navigated courses with 3-5 consecutive obstacles of varying heights and types. The depth-based perception system enabled autonomous skill selection without pre-programmed sequences. Key findings:

- The robot successfully adapted to real-time obstacle perturbations, including obstacles moved during approach
- Average traversal time for 3-obstacle courses was 8-12 seconds, comparable to cautious human performance
- The system maintained balance recovery capabilities, with successful recovery from 89% of minor perturbations

**Ablation Studies**

The authors validated design choices through systematic ablations:
- Removing motion matching and using fixed skill sequences reduced success rate by 34% and eliminated smooth transitions
- Training separate policies instead of distillation increased inference time by 5x and required manual skill switching
- Using RGB instead of depth reduced robustness to lighting variations, dropping success rate by 18%

## Implications and Future Directions

PHP represents a significant advance in humanoid robotics by demonstrating that complex, dynamic human skills can be transferred to robots through careful integration of motion capture, learning, and perception. Several implications emerge:

**For Humanoid Robotics**: The motion matching approach provides a scalable path to expanding robot skill repertoires by leveraging human motion databases. This could accelerate development of general-purpose humanoid systems capable of human-like agility.

**For Embodied AI**: The hierarchical architecture—composing skills at the kinematic level, then learning robust tracking, then distilling to perception-based policies—offers a template for other complex manipulation and locomotion tasks.

**For Real-World Deployment**: The closed-loop adaptation and perception-driven decision-making demonstrate readiness for unstructured environments, though challenges remain in handling completely novel obstacle types and recovering from major disturbances.

Future work could explore:
- Expanding the motion database to include more diverse parkour skills and transitions
- Online adaptation and learning from experience to improve performance over time
- Integration with higher-level planning for navigation in complex multi-room environments
- Transfer to other humanoid platforms with different morphologies and actuation capabilities

## Takeaways

1. Motion matching enables natural composition of human skills while preserving motion quality, formulated as nearest-neighbor search in a feature space combining pose, velocity, and trajectory features
2. Hierarchical learning (expert policies → distillation) successfully bridges the gap between kinematic planning and perception-based control
3. Depth-based perception is sufficient for autonomous parkour decision-making, enabling context-aware skill selection without manual programming
4. Real-world validation on Unitree G1 demonstrates climbing up to 96% of robot height and successful multi-obstacle traversal with closed-loop adaptation
5. The modular framework is generalizable and could accelerate development of agile humanoid capabilities across different platforms and tasks
:::

:::zh
**论文**: [2602.15827](https://arxiv.org/abs/2602.15827)
**作者**: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza
**分类**: cs.RO, cs.AI, cs.LG, eess.SY

## 摘要

本文介绍了感知型人形机器人跑酷(PHP)框架,使人形机器人能够使用基于视觉的跑酷技能自主导航复杂障碍课程。虽然人形机器人运动的最新进展已实现稳定行走,但复制跑酷等动态人类运动的敏捷性和流畅性仍然具有挑战性。PHP通过结合用于技能组合的动作匹配、用于动作跟踪的强化学习以及用于自主决策的基于深度的感知来解决这一问题。该系统在Unitree G1机器人上成功展示了高度动态的动作,包括攀爬高达1.25米的障碍物(机器人身高的96%)以及执行具有实时适应能力的长时程多障碍穿越。

## 主要贡献

- 一个动作匹配框架,通过特征空间中的近邻搜索将重定向的原子人类技能组合成长时程运动学轨迹,保持人类动作的自然性
- 一个分层学习管道,将动作跟踪强化学习专家策略与策略蒸馏(DAgger + RL)结合,形成单一的基于深度的多技能学生策略
- 自主的感知驱动决策,仅使用车载深度感知和2D速度命令即可实现上下文感知的技能选择(跨越、攀爬、跳跃、翻滚)
- 在Unitree G1上进行的广泛真实世界验证,展示了动态跑酷能力和对障碍物扰动的闭环适应

## 技术方法

PHP框架通过三阶段管道运行,将人类动作捕捉数据桥接到机器人执行。

**阶段1:用于技能组合的动作匹配**

系统从代表原子跑酷技能的重定向人类动作片段数据库开始。动作匹配被表述为精心设计的特征空间中的近邻搜索问题。在每个时间步$t$,算法搜索最匹配当前状态和期望未来轨迹的动作帧:

$$\text{cost}(i) = w_p \|p_i - p_{\text{current}}\|^2 + w_v \|v_i - v_{\text{current}}\|^2 + w_{\text{traj}} \sum_{j} \|p_{i+j} - p_{\text{desired},j}\|^2$$

其中$p$表示位置,$v$表示速度,轨迹项确保向期望未来状态的平滑过渡。这种方法能够灵活组合复杂的技能链,同时保持人类动作的优雅性。特征空间包括关节位置、速度、接触状态和未来轨迹预测,加权以优先考虑平滑过渡和目标导向行为。

**阶段2:动作跟踪强化学习专家策略**

对于每个组合的运动学轨迹,系统训练专门的强化学习策略来跟踪物理机器人上的参考动作。奖励函数结合了跟踪精度、稳定性和能量效率:

$$r_t = w_{\text{pose}} r_{\text{pose}} + w_{\text{vel}} r_{\text{vel}} + w_{\text{contact}} r_{\text{contact}} - w_{\text{energy}} \|\tau_t\|^2$$

这些专家策略学习鲁棒的跟踪行为,处理仿真到现实的差距和机器人硬件的物理约束。训练使用物理参数、地面摩擦和执行器动力学的域随机化来确保鲁棒性。

**阶段3:基于感知的策略蒸馏**

最后阶段将多个专家策略蒸馏成直接从深度观测操作的单一学生策略。这使用数据集聚合(DAgger)和强化学习的组合:

- DAgger阶段:学生策略被训练模仿专家动作,同时专家纠正错误,构建覆盖学生实际状态分布的状态-动作对数据集
- 强化学习微调:学生策略通过强化学习进一步优化,以处理感知噪声并改善闭环性能

学生策略将车载相机的深度图像、本体感受状态信息和离散2D速度命令作为输入。它输出关节位置目标,并根据感知到的障碍物几何形状自主选择执行哪个技能。

## 实验结果与分析

作者在Unitree G1人形机器人上进行了跨各种障碍物配置的广泛真实世界实验。

**单障碍物性能**

PHP成功执行了四种不同的跑酷技能:
- 跨越:对于低障碍物(0.3-0.5米),保持连续向前运动
- 攀爬:对于高障碍物(0.8-1.25米),使用动态拉动动作
- 跳跃:对于中等高度障碍物(0.5-0.8米),将手放在顶部并摆动腿部越过
- 翻滚:用于从高台下降

系统在攀爬高达1.25米(机器人1.3米身高的96%)的障碍物上实现了96%的成功率,展示了卓越的动态能力。动作匹配方法保持了人类跑酷动作的自然流畅性,在站立和空中阶段之间平滑过渡。

**多障碍物穿越**

在长时程实验中,PHP导航了包含3-5个不同高度和类型的连续障碍物的课程。基于深度的感知系统实现了无需预编程序列的自主技能选择。关键发现:

- 机器人成功适应实时障碍物扰动,包括在接近过程中移动的障碍物
- 3障碍物课程的平均穿越时间为8-12秒,与谨慎的人类表现相当
- 系统保持平衡恢复能力,从89%的轻微扰动中成功恢复

**消融研究**

作者通过系统消融验证了设计选择:
- 移除动作匹配并使用固定技能序列使成功率降低34%,并消除了平滑过渡
- 训练单独策略而不是蒸馏使推理时间增加5倍,并需要手动技能切换
- 使用RGB而不是深度降低了对光照变化的鲁棒性,成功率下降18%

## 影响与未来方向

PHP通过展示复杂的动态人类技能可以通过动作捕捉、学习和感知的精心集成转移到机器人,代表了人形机器人技术的重大进步。出现了几个影响:

**对人形机器人技术**:动作匹配方法通过利用人类动作数据库提供了扩展机器人技能库的可扩展路径。这可以加速开发能够实现类人敏捷性的通用人形系统。

**对具身人工智能**:分层架构——在运动学层面组合技能,然后学习鲁棒跟踪,然后蒸馏到基于感知的策略——为其他复杂操作和运动任务提供了模板。

**对真实世界部署**:闭环适应和感知驱动决策展示了对非结构化环境的准备,尽管在处理完全新颖的障碍物类型和从重大干扰中恢复方面仍存在挑战。

未来工作可以探索:
- 扩展动作数据库以包含更多样化的跑酷技能和过渡
- 在线适应和从经验中学习以随时间改善性能
- 与更高级别的规划集成,用于复杂多房间环境中的导航
- 转移到具有不同形态和驱动能力的其他人形平台

## 要点总结

1. 动作匹配能够自然组合人类技能同时保持动作质量,表述为结合姿态、速度和轨迹特征的特征空间中的近邻搜索
2. 分层学习(专家策略→蒸馏)成功弥合了运动学规划和基于感知的控制之间的差距
3. 基于深度的感知足以实现自主跑酷决策,无需手动编程即可实现上下文感知的技能选择
4. 在Unitree G1上的真实世界验证展示了攀爬高达机器人身高96%的障碍物以及具有闭环适应的成功多障碍物穿越
5. 模块化框架具有通用性,可以加速跨不同平台和任务的敏捷人形能力的开发
:::
