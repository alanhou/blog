---
title:
  en: "New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR"
  zh: "新技能还是更锐利的原语？RLVR中推理涌现的概率视角"
description:
  en: "Investigates whether RLVR gives LMs genuine new abilities or activates dormant ones, proposing a probabilistic framework centered on atomic step sharpening"
  zh: "研究RLVR是否赋予语言模型真正的新能力还是激活休眠能力，提出以原子步骤锐化为核心的概率框架"
date: 2026-02-10
tags: ["arxiv", "ai", "reasoning", "reinforcement-learning", "rlvr", "emergence", "cs.CL", "cs.LG"]
image: "/arxiv-visuals/arxiv-reasoning-emergence-rlvr.png"
---

:::en
**Paper**: [2602.08281](https://arxiv.org/abs/2602.08281)
**Authors**: Zhilin Wang, Yafu Li, Shunkai Zhang, Zhi Wang, Haoran Zhang, Xiaoye Qu, Yu Cheng
**Categories**: cs.CL, cs.LG

## Abstract

This paper investigates whether Reinforcement Learning with Verifiable Rewards (RLVR) gives language models genuine new reasoning abilities or merely activates dormant ones. The authors propose a probabilistic framework with a central hypothesis: the emergence of complex reasoning is driven by the sharpening of atomic step probabilities. Using the Algebrarium framework, models are trained on single-step operations and tested on unseen multi-step tasks. Key findings include: RLVR encourages exploration of previously inaccessible reasoning paths; composite task performance correlates strongly with the joint probability of atomic steps (Pearson correlation 0.69-0.96); and RLVR can sacrifice specific skills to maximize overall reward.

## Key Contributions

- **Probabilistic framework**: A formal model connecting atomic step probabilities to composite reasoning performance
- **Sharpening hypothesis**: RLVR improves reasoning by increasing the probability of individual atomic steps
- **Algebrarium evaluation**: A controlled framework isolating single-step vs. multi-step reasoning
- **Trade-off discovmay sacrifice performance on specific skills to optimize global reward

## The Central Question

When a model trained with RLVR solves a complex multi-step reasoning problem it could not solve before, what actually changed?

Two competing hypotheses:

1. **New skills**: RLVR teaches the model fundamentally new reasoning capabilities
2. **Sharper primitives**: RLVR increases the reliability of existing atomic reasoning steps, making their composition more likely to succeed

This paper provides strong evidence for hypothesis 2.

## Probabilistic Framework

### Atomic Steps and Composite Tasks

Define a composite reasoning task $T$ as aquence of $n$ atomic steps:

$$T = s_1 \circ s_2 \circ \cdots \circ s_n$$

Each atomic step $s_i$ has a probability $p_i$ of being executed correctly by the model. The probability of successfully completing the composite task is approximately:

$$P(T) \approx \prod_{i=1}^{n} p_i$$

This multiplicative relationship means that even small improvements in atomic step probabilities can yield large improvements in composite task success:

If each $p_i$ increases from 0.7 to 0.9, a 5-step task goes from $0.7^5 = 0.168$ to $0.9^5 = 0.590$ -- a 3.5x improvement.

### The Sharpening Effect

RLVR "sharpens" the model's policy by increasing atomic step probabilities:

$$p_i^{\text{after RLVR}} > p_i^{\text{before RLVR}}$$

This sharpening is sufficient to explain the emergence of complex reasoning without invoking new capabilities. The model always "knew" how to perform each step; RLVR simply made each step more reliable.

## The Algebrarium Framework

### Design

Algebrarium provides a controlled environment for testing the sharpening hypothesis:

- **Atomic operations**: Single-step algebraic operations (addition, multiplication, modular arithmetic, etc.)
- **Composite tasks**: Multi-step problems requiring chains of atomic operations
- **Training**: Models are trained with RLVR on single-step operations only
- **Testing**: Models are evaluated on unseen multi-step compositions

This design ensures that any improvement on composite tasks must come from improved atomic step execution, not from learning new compositional strategies.

### Training Protocol

1. Pre-train a language model on standard data
2. Apply RLVR training using only single-step algebraic problems
3. Evaluate on multi-step problems never seen during RLVR training

## Key Findings

### Finding 1: RLVR Encourages Exploration

After RLVR training, models explore reasoning paths that were previously inaccessible:

- The model's policy becomes less peaked on common but suboptimal paths
- Previously low-probability correct paths become accessible
- This exploration is not random but guided by the reward signal

### Finding 2: Strong Correlation with Joint Probability

The composite task performance correlates strongly with the joint probability of atomic steps:

$$\text{Pearson}(P(T), \prod_i p_i) \in [0.69, 0.96]$$

This strong correlation across different task types and model sizes provides compelling evidence for the multiplicative probability model. The composite performance is well-predicted by simply multiplying the individual step probabilities.

### Finding 3: Skill Trade-offs

RLVR can sacrifice performance on specific atomic skills to maximize overall reward:

- If improving skill $A$ by $\delta$ costs skill $B$ by $\epsilon$, RLVR will make this trade if the net effect on expected reward is positive
- This explains why RLVR-trained models sometimes perform worse on specific subtasks despite overall improvement
- The optimization is global, not local to individual skills

### Finding 4: No Evidence of New Capabilities

Across all experiments, the authors find no evidence that RLVR creates genuinely new reasoning capabilities:

- All atomic operations the model performs after RLVR were already present (at lower probability) before
- The improvement is quantitative (higher probability) not qualitative (new operations)
- Multi-step success is fully explained by atomic step probability improvements

## Analysis

### Why Sharpening Is Sufficient

Consider a 5-step reasoning chain where each step has probability 0.8 before RLVR:

$$P(\text{success}) = 0.8^5 = 0.328$$

After RLVR sharpens each step to 0.95:

$$P(\text{success}) = 0.95^5 = 0.774$$

The model goes from failing most of the time to succeeding most of the time, yet no new capability was added -- each step was already possible.

### Implications for Scaling

The multiplicative model suggests that:

- **Longer reasoning chains** are exponentially harder (probability drops exponentially with chain length)
- **Diminishing returns**: Once atomic steps are near-perfect, further RLVR training yields little improvement
- **Bottleneck identification**: The weakest atomic step limits composite performance most

### Connection to Exploration in RL

The exploration finding connects to classical RL theory:

- RLVR's reward signal guides the model toward correct reasoning paths
- Paths that were theoretically possible but practically never sampled become accessible
- This is exploration in reasoning space, not just action space

## Significance

This paper provides a principled answer to a fundamental question about RLVR and reasoning emergence. The probabilistic framework elegantly explains why RLVR appears to create new reasoning abilities when it is actually sharpening existing ones. The strong empirical correlation between atomic step probabilities and composite performance (Pearson 0.69-0.96) provides compelling quantitative support. This understanding has practical implications: to improve complex reasoning, focus on identifying and strengthening the weakest atomic steps rather than training on complex tasks directly.
:::

:::zh
**论文**: [2602.08281](https://arxiv.org/abs/2602.08281)
**作者**: Zhilin Wang, Yafu Li, Shunkai Zhang, Zhi Wang, Haoran Zhang, Xiaoye Qu, Yu Cheng
**分类**: cs.CL, cs.LG

## 摘要

本文研究带有可验证奖励的强化学习（RLVR）是否赋予语言模型真正的新推理能力还是仅仅激活休眠的能力。作者提出一个概率框架，核心假设是：复杂推理的涌现由原子步骤概率的锐化驱动。使用Algebrarium框架，模型在单步操作上训练并在未见过的多步任务上测试。关键发现包括：RLVR鼓励探索以前不可达的推理路径；复合任务性能与原子步骤的联合概率强相关（Pearson相关系数0.69-0.96）；RLVR可能牺牲特定技能以最大化整体奖励。

## 主要贡献

- **概率框架**：连接原子步骤概率与复合推理性能的形式化模型
- **锐化假设**：RLVR通过增加单个原子步骤的概率来改善推理
- **Algebrarium评估**：隔离单步与多步推理的受控框架
- **权衡发现**：RLVR可能牺牲特定技能的性能以优化全局奖励

## 核心问题

当用RLVR训练的模型解决了以前无法解决的复杂多步推理问题时，到底发生了什么变化？

两个竞争假设：

1. **新技能**：RLVR教会模型根本性的新推理能力
2. **更锐利的原语**：RLVR增加现有原子推理步骤的可靠性，使其组合更可能成功

本文为假设2提供了强有力的证据。

## 概率框架

### 原子步骤与复合任务

将复合推理任务 $T$ 定义为 $n$ 个原子步骤的序列：

$$T = s_1 \circ s_2 \circ \cdots \circ s_n$$

每个原子步骤 $s_i$ 有概率 $p_i$ 被模型正确执行。成功完成复合任务的概率近似为：

$$P(T) \approx \prod_{i=1}^{n} p_i$$

这种乘法关系意味着原子步骤概率的微小改进可以产生复合任务成功率的大幅提升：

如果每个 $p_i$ 从0.7增加到0.9，5步任务从 $0.7^5 = 0.168$ 变为 $0.9^5 = 0.590$ —— 3.5倍的改进。

### 锐化效应

RLVR通过增加原子步骤概率来"锐化"模型的策略：

$$p_i^{\text{RLVR后}} > p_i^{\text{RLVR前}}$$

这种锐化足以的涌现，无需引入新能力。模型一直"知道"如何执行每个步骤；RLVR只是使每个步骤更可靠。

## Algebrarium框架

### 设计

Algebrarium提供了测试锐化假设的受控环境：

- **原子操作**：单步代数运算（加法、乘法、模运算等）
- **复合任务**：需要原子操作链的多步问题
- **训练**：模型仅在单步操作上用RLVR训练
- **测试**：模型在未见过的多步组合上评估

这种设计确保复合任务上的任何改进必须来自改进的原子步骤执行，而非学习新的组合策略。

### 训练协议

1. 在标准数据上预训练语言模型
2. 仅使用单步代数问题应用RLVR训练
3. 在RLVR训练期间从未见过的多步问题上评估

## 关键发现

### 发现1：RLVR鼓励探索

RLVR训练后，模型探索以前不可达的推理路径：

- 模型的策略在常见但次优路径上变得不那么集中
- 以前低概率的正确路径变得可达
- 这种探索不是随机的，而是由奖励信号引导的

### 发现2：与联合概率的强相关

复合任务性能与原子步骤的联合概率强相关：

$$\text{Pearson}(P(T), \prod_i p_i) \in [0.69, 0.96]$$

这种跨不同任务类型和模型规模的强相关为乘法概率模型提供了令人信服的证据。复合性能可以通过简单地乘以各个步骤概率来很好地预测。

### 发现3：技能权衡

RLVR可能牺牲特定原子技能的性能以最大化整体奖励：

- 如果改善技能 $A$ $\delta$ 以技能 $B$ 损失 $\epsilon$ 为代价，当对期望奖励的净效应为正时RLVR会做出这种权衡
- 这解释了为什么RLVR训练的模型有时在特定子任务上表现更差，尽管整体有所改善
- 优化是全局的，不是局部于单个技能

### 发现4：没有新能力的证据

在所有实验中，作者没有发现RLVR创造真正新推理能力的证据：

- RLVR后模型执行的所有原子操作在之前就已存在（以较低概率）
- 改进是定量的（更高概率）而非定性的（新操作）
- 多步成功完全由原子步骤概率改进解释

## 分析

### 为什么锐化就足够了

考虑一个5步推理链，RLVR前每步概率为0.8：

$$P(\text{成功}) = 0.8^5 = 0.328$$

RLVR将每步锐化到0.95后：

$$P(\text{成功}) = 0.95^5 = 0.774$$

模型从大多数时候失败变为大多数时候成功，但没有添加新能力——每个步骤本来就是可能的。

### 对扩展的启示

乘法模型表明：

- **更长的推理链**指数级更难（概率随链长度指数下降）
- **收益递减**：一旦原子步骤接近完美，进一步RLVR训练收益甚微
- **瓶颈识别**：最弱的原子步骤最大程度地限制复合性能

### 与RL中探索的联系

探索发现与经典RL理论相连：

- RLVR的奖励信号引导模型走向正确的推理路径
- 理论上可能但实际上从未被采样的路径变得可达
- 这是推理空间中的探索，不仅仅是动作空间

## 意义

本文为关于RLVR和推理涌现的根本问题提供了有原则的答案。概率框架优雅地解释了为什么RLVR看起来创造了新的推理能力，而实际上是在锐化现有能力。原子步骤概率与复合性能之间的强实证相关（Pearson 0.69-0.96）提供了令人信服的定量支持。这种理解具有实际意义：要改善复杂推理，应专注于识别和加强最弱的原子步骤，而不是直接在复杂任务上训练。
:::
