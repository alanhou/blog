---
title:
  en: "Quantization-Robust LLM Unlearning via Low-Rank Adaptation"
  zh: "基于低秩适配的量化鲁棒大语言模型遗忘"
description:
  en: "This paper addresses the challenge of preserving machine unlearning effectiveness in quantized LLMs by using LoRA to concentrate updates in trainable adapters, achieving up to 7.93 point improvements in 4-bit utility while reducing privacy leakage."
  zh: "本文通过使用LoRA将更新集中在可训练适配器中,解决了量化大语言模型中保持机器遗忘有效性的挑战,在4位量化下实现了高达7.93点的效用提升,同时降低了隐私泄露。"
date: 2026-02-16
tags: ["arxiv", "ai", "cs.lg", "cs.cl"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.13151](https://arxiv.org/abs/2602.13151)
**Authors**: João Vitor Boer Abitante, Joana Meneguzzo Pasquali, Luan Fonseca Garcia, Ewerton de Oliveira, Thomas da Silva Paula, Rodrigo C. Barros, Lucas S. Kupssinskü
**Categories**: cs.LG, cs.CL

## Abstract

Large language models deployed in production environments typically undergo post-training quantization (PTQ) to reduce memory footprint and inference costs. However, this paper reveals a critical problem: aggressive low-bit quantization can completely erase the effects of machine unlearning, causing models to revert to their pre-unlearning state. The authors demonstrate that standard full-parameter fine-tuning for unlearning produces parameter changes too subtle to survive 4-bit quantization. Their solution leverages Low-Rank Adaptation (LoRA) to concentrate unlearning updates into trainable adapter modules while freezing the base model, ensuring these updates remain effective post-quantization. Experiments on Llama-2-7B using the MUSE benchmark show substantial improvements: up to 7.93 points in 4-bit utility and significant reductions in privacy leakage while maintaining strong forgetting guarantees.

## Key Contributions

- **Identification of quantization vulnerability**: Demonstrates that standard unlearning methods fail under aggressive PTQ, with parameter changes being too small to survive 4-bit quantization
- **LoRA-based solution**: Proposes using Low-Rank Adaptation to concentrate unlearning updates in trainable adapters, making them robust to quantization
- **Comprehensive evaluation**: Tests multiple unlearning algorithms (NPO, GA) with retention methods (GDR, KLR) on MUSE dataset (BOOKS and NEWS domains)
- **Quantifiable improvements**: Shows up to 7.93 point utility gains and substantial privacy leakage reduction (e.g., PrivLeak from -25.68 to -5.86) under 4-bit quantization

## The Quantization-Unlearning Conflict

Machine unlearning aims to remove specific knowledge from trained models, addressing privacy concerns and regulatory requirements like GDPR's "right to be forgotten." However, practical LLM deployment demands quantization for efficiency. This creates a fundamental tension: unlearning typically modifies a small subset of parameters with subtle changes, while aggressive quantization rounds weights to a limited set of discrete values.

The paper demonstrates this conflict empirically. When standard full-parameter unlearning is applied followed by 4-bit PTQ, the quantization process effectively "rounds away" the unlearning updates. The model's behavior reverts to its pre-unlearning state, defeating the purpose of the unlearning procedure. This happens because the magnitude of parameter changes induced by unlearning is often smaller than the quantization bin width in 4-bit representations.

The core insight is that quantization acts as a lossy compression that disproportionately affects small, distributed parameter changes—exactly the type produced by standard unlearning methods.

## Methodology: LoRA for Quantization-Robust Unlearning

The proposed solution exploits the structure of Low-Rank Adaptation. Instead of fine-tuning all model parameters, LoRA freezes the pre-trained weight matrix $W_0$ and introduces trainable low-rank decomposition matrices $A$ and $B$ such that the effective weight becomes:

$$W = W_0 + BA$$

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.

For unlearning, this architecture offers a crucial advantage: the unlearning updates are concentrated entirely in the adapter matrices $A$ and $B$, which remain in full precision even when the base model $W_0$ is quantized. The quantization process becomes:

$$W_{quantized} = Q(W_0) + BA$$

where $Q(\cdot)$ represents the quantization function. Since $BA$ is not quantized, the unlearning signal is preserved regardless of how aggressively $W_0$ is compressed.

The authors evaluate this approach with multiple unlearning algorithms:
- **Negative Preference Optimization (NPO)**: Optimizes the model to assign lower probability to forget samples
- **Gradient Ascent (GA)**: Directly maximizes loss on forget data

Combined with retention strategies:
- **Gradient Difference Retention (GDR)**: Balances forgetting and retention gradients
- **KL Retention (KLR)**: Maintains distribution similarity on retain data

## Experimental Results and Analysis

Testing on Llama-2-7B with the MUSE benchmark reveals striking improvements. On the BOOKS domain with NPO+GDR, LoRA increases 4-bit utility from 50.17 to 58.10 (7.93 point gain). For NEWS domain with GA+GDR, utility improves from 40.06 to 44.82 (4.76 point increase).

Privacy metrics show even more dramatic improvements. The PrivLeak metric measures how much the model's predictions on forget data deviate from a reference model (ideal value: 0). For GA+KLR on BOOKS, standard unlearning yields PrivLeak of -25.68 under 4-bit quantization (indicating severe privacy leakage), while LoRA achieves -5.86—much closer to the ideal.

Critically, these utility and privacy gains come without sacrificing forgetting effectiveness. Metrics like VerMem (verbatim memorization) and KnowMem (knowledge memorization) remain near zero, confirming that the targeted information is successfully removed.

The results demonstrate that LoRA doesn't just preserve unlearning under quantization—it often outperforms full-parameter methods even in full precision scenarios, likely due to the regularization effect of the low-rank constraint.

## Implications for Production Deployment

This work has immediate practical implications for organizations deploying LLMs under privacy constraints. Many real-world scenarios require both efficient inference (necessitating quantization) and the ability to remove user data on request (requiring unlearning). Previously, these requirements were in tension.

The LoRA-based approach enables a viable workflow:
1. Train base model and quantize for deployment
2. When unlearning is needed, apply LoRA adapters
3. Keep base model quantized while adapters remain full precision
4. Deploy with minimal memory overhead (adapters are small)

The memory cost is minimal since LoRA adapters typically add less than 1% of parameters. For a 7B parameter model with rank $r=8$, the adapter overhead is negligible compared to the 4× memory savings from 4-bit quantization.

This also opens possibilities for modular unlearning: different adapters could handle different unlearning requests, allowing fine-grained control over what knowledge is removed without retraining the entire model.

## Takeaways

1. Standard unlearning methods fail under aggressive quantization because parameter changes are too small to survive discretization
2. LoRA concentrates unlearning updates in full-precision adapters, making them robust to base model quantization
3. On Llama-2-7B with MUSE, LoRA improves 4-bit utility by up to 7.93 points while dramatically reducing privacy leakage
4. The approach enables practical deployment scenarios where both efficiency (quantization) and privacy (unlearning) are required
5. LoRA's low-rank constraint may provide beneficial regularization, sometimes outperforming full-parameter methods even without quantization
:::

:::zh
**论文**: [2602.13151](https://arxiv.org/abs/2602.13151)
**作者**: João Vitor Boer Abitante, Joana Meneguzzo Pasquali, Luan Fonseca Garcia, Ewerton de Oliveira, Thomas da Silva Paula, Rodrigo C. Barros, Lucas S. Kupssinskü
**分类**: cs.LG, cs.CL

## 摘要

部署在生产环境中的大语言模型通常需要经过训练后量化(PTQ)以降低内存占用和推理成本。然而,本文揭示了一个关键问题:激进的低比特量化会完全抹除机器遗忘的效果,导致模型恢复到遗忘前的状态。作者证明,用于遗忘的标准全参数微调产生的参数变化过于微小,无法在4位量化中存活。他们的解决方案利用低秩适配(LoRA)将遗忘更新集中到可训练的适配器模块中,同时冻结基础模型,确保这些更新在量化后仍然有效。在使用MUSE基准测试Llama-2-7B的实验中显示了显著改进:4位量化下效用提升高达7.93点,隐私泄露大幅降低,同时保持强大的遗忘保证。

## 主要贡献

- **识别量化脆弱性**:证明标准遗忘方法在激进PTQ下失效,参数变化过小无法在4位量化中存活
- **基于LoRA的解决方案**:提出使用低秩适配将遗忘更新集中在可训练适配器中,使其对量化具有鲁棒性
- **全面评估**:在MUSE数据集(BOOKS和NEWS领域)上测试多种遗忘算法(NPO、GA)与保留方法(GDR、KLR)的组合
- **可量化的改进**:在4位量化下显示高达7.93点的效用增益和显著的隐私泄露减少(例如,PrivLeak从-25.68改善到-5.86)

## 量化与遗忘的冲突

机器遗忘旨在从训练模型中移除特定知识,以应对隐私问题和GDPR"被遗忘权"等监管要求。然而,实际的大语言模型部署需要量化以提高效率。这产生了根本性的矛盾:遗忘通常以微小变化修改一小部分参数,而激进的量化将权重舍入到有限的离散值集合。

论文通过实验证明了这一冲突。当应用标准全参数遗忘后进行4位PTQ时,量化过程有效地"舍入掉"了遗忘更新。模型行为恢复到遗忘前的状态,使遗忘过程失去意义。这是因为遗忘引起的参数变化幅度通常小于4位表示中的量化区间宽度。

核心洞察是:量化作为有损压缩,不成比例地影响小幅度、分布式的参数变化——恰恰是标准遗忘方法产生的类型。

## 方法论:用于量化鲁棒遗忘的LoRA

提出的解决方案利用了低秩适配的结构。LoRA不是微调所有模型参数,而是冻结预训练权重矩阵$W_0$,并引入可训练的低秩分解矩阵$A$和$B$,使有效权重变为:

$$W = W_0 + BA$$

其中$B \in \mathbb{R}^{d \times r}$和$A \in \mathbb{R}^{r \times k}$,秩$r \ll \min(d,k)$。

对于遗忘,这种架构提供了关键优势:遗忘更新完全集中在适配器矩阵$A$和$B$中,即使基础模型$W_0$被量化,它们仍保持全精度。量化过程变为:

$$W_{quantized} = Q(W_0) + BA$$

其中$Q(\cdot)$表示量化函数。由于$BA$未被量化,无论$W_0$被压缩得多么激进,遗忘信号都得以保留。

作者使用多种遗忘算法评估此方法:
- **负偏好优化(NPO)**:优化模型以对遗忘样本分配更低概率
- **梯度上升(GA)**:直接最大化遗忘数据上的损失

结合保留策略:
- **梯度差异保留(GDR)**:平衡遗忘和保留梯度
- **KL保留(KLR)**:在保留数据上维持分布相似性

## 实验结果与分析

在使用MUSE基准测试Llama-2-7B的实验中显示了显著改进。在BOOKS领域使用NPO+GDR时,LoRA将4位效用从50.17提升到58.10(增益7.93点)。对于NEWS领域使用GA+GDR,效用从40.06提升到44.82(增加4.76点)。

隐私指标显示了更显著的改进。PrivLeak指标衡量模型在遗忘数据上的预测与参考模型的偏离程度(理想值:0)。对于BOOKS上的GA+KLR,标准遗忘在4位量化下产生-25.68的PrivLeak(表明严重的隐私泄露),而LoRA达到-5.86——更接近理想值。

关键是,这些效用和隐私增益并未牺牲遗忘有效性。VerMem(逐字记忆)和KnowMem(知识记忆)等指标保持在接近零的水平,确认目标信息被成功移除。

结果表明,LoRA不仅在量化下保留了遗忘效果——它通常甚至在全精度场景中也优于全参数方法,这可能是由于低秩约束的正则化效应。

## 对生产部署的影响

这项工作对在隐私约束下部署大语言模型的组织具有直接的实际意义。许多现实场景既需要高效推理(需要量化)又需要应请求移除用户数据的能力(需要遗忘)。以前,这些需求是相互矛盾的。

基于LoRA的方法实现了可行的工作流程:
1. 训练基础模型并量化以供部署
2. 需要遗忘时,应用LoRA适配器
3. 保持基础模型量化,适配器保持全精度
4. 以最小内存开销部署(适配器很小)

内存成本很小,因为LoRA适配器通常增加不到1%的参数。对于秩$r=8$的70亿参数模型,适配器开销相比4位量化节省的4倍内存可以忽略不计。

这也为模块化遗忘开辟了可能性:不同的适配器可以处理不同的遗忘请求,允许对移除哪些知识进行细粒度控制,而无需重新训练整个模型。

## 要点总结

1. 标准遗忘方法在激进量化下失效,因为参数变化太小无法在离散化中存活
2. LoRA将遗忘更新集中在全精度适配器中,使其对基础模型量化具有鲁棒性
3. 在使用MUSE的Llama-2-7B上,LoRA将4位效用提升高达7.93点,同时显著降低隐私泄露
4. 该方法支持同时需要效率(量化)和隐私(遗忘)的实际部署场景
5. LoRA的低秩约束可能提供有益的正则化,有时即使在没有量化的情况下也优于全参数方法
:::
