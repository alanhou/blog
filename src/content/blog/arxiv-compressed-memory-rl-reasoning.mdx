---
title:
  en: "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning"
  zh: "通过端到端强化学习实现压缩记忆上的动态长上下文推理"
description:
  en: "Joint RL training of memory compressor and reasoner enables context extrapolation from 7K to 1.75M tokens with 2x memory reduction and 6x speedup"
  zh: "记忆压缩器和推理器的联合强化学习训练实现从7K到1.75M token的上下文外推，内存减少2倍，速度提升6倍"
date: 2026-02-10
tags: ["arxiv", "ai", "long-context", "memory", "reinforcement-learning", "efficiency", "reasoning", "cs.CL", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.08382](https://arxiv.org/abs/2602.08382)
**Authors**: Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang
**Categories**: cs.CL, cs.LG

## Abstract

This paper addresses the computational challenges of reasoning over extended contexts in large language models. The proposed framework segments inputs into chunks, converts each chunk to compressed memory representations via a learned compressor, and employs a gating mechanism to selectively retrieve relevant memory blocks. A reasoning module processes these iteratively with an evolving working memory. Both the compressor and reasoner are jointly trained via end-to-end reinforcement learning. The system achieves competitive performance on multi-hop reasoning tasks, demonstrates context extrapolation from 7K to 1.75M tokens, provides a 2x reduction in peak GPU memory, and delivers 6x inference speedup over MemAgent.

## Key Contributions

- **Learned memory compression**: A trainable compressor that distills long chunks into compact memory representations
- **Gated selective retrieval**: A gating mechanism that retrieves only the most relevant memory blocks for each reasoning step
- **Iterative working memory**: A reasoning module that maintains and updates working memory across multiple processing steps
- **End-to-end RL training**: Joint optimization of compressor and reasoner through reinforcement learning
- **Extreme context extrapolation**: Generalizes from 7K training context to 1.75M tokens at inference

## Background: Long Context Challenges

Processing long contexts in transformers faces two fundamental bottlenecks:

1. **Quadratic attention cost**: Self-attention scales as $O(n^2)$ with sequence length $n$
2. **Linear memory cost**: KV cache grows linearly, consuming GPU memory

Existing approaches include:
- **Sparse attention**: Reduces computation but may miss critical long-range dependencies
- **Retrieval-augmented generation (RAG)**: External retrieval but disconnected from the model's reasoning
- **Memory-augmented models**: Fixed compression schemes that don't adapt to the reasoning task

## Architecture

### Memory Compressor

The input sequence is divided into chunks $\{c_1, c_2, \ldots, c_N\}$. Each chunk is processed by the compressor to produce a compact memory representation:

$$m_i = \text{Compress}_\phi(c_i) \in \mathbb{R}^{k \times d}$$

where $k \ll |c_i|$ is the compressed memory size and $d$ is the hidden dimension. The compressor learns to retain the most task-relevant informach chunk.

### Gated Memory Retrieval

Not all memory blocks are relevant to every reasoning step. A gating mechanism computes relevance scores:

$$g_i = \sigma(W_g [q; m_i])$$

where $q$ is the current reasoning query and $m_i$ is the $i$-th memory block. Only memory blocks with $g_i > \tau$ are retrieved, dramatically reducing the effective context size.

### Iterative Reasoning Module

The reasoner maintains a working memory $w$ that evolves across reasoning steps:

$$w^{(t+1)} = \text{Reason}_\psi(w^{(t)}, \{m_i : g_i^{(t)} > \tau\})$$

At each step, the reasoner:
1. Queries the compressed memories via the gating mechanism
2. Integrates retrieved information into working memory
3. Decides whether to continue reasoning or produce an output

### End-to-End RL Training

The entire pipeline — compressor, gate, and reasoner — is trained jointly using reinforcement learning. The reward signal comes from task performance (e.g., correctness on multi-hop QA):

$$\mathcal{J}(\phi, \psi) = \mathbb{E}\left[R(\text{answer}, \text{ground\_truth})\right]$$

RL training is essential because the discrete gating decisions and iterative reasoning steps are not differentiable through standard backpropagation.

## Context Extrapolation

A remarkable property of this framework is its ability to extrapolate fond training context lengths. Trained on sequences of only 7K tokens, the model successfully processes contexts up to 1.75M tokens — a 250x extrapolation factor. This is possible because:

- The compressor operates on fixed-size chunks regardless of total length
- The gating mechanism scales sublinearly with the number of memory blocks
- The iterative reasoner can perform as many steps as needed

## Results

| Metric | Value |
|--------|-------|
| Context Extrapolation | 7K → 1.75M tokens (250x) |
| Peak GPU Memory | 2x reduction |
| Inference Speed | 6x faster than MemAgent |
| Multi-hop Reasoning | Compe with full-context baselines |

## Significance

1. **Practical long-context processing**: Enables reasoning over million-token contexts on standard hardware
2. **RL for memory management**: Demonstrates that RL can effectively train memory compression and retrieval jointly
3. **Extreme generalization**: 250x context extrapolation suggests the learned compression captures fundamental information structure
4. **Efficiency gains**: 2x memory and 6x speed improvements make long-context reasoning practical for deployment
5. **Modular design**: The compressor, gate, and reasoner can potentially be improved independently
:::

:::zh
**论文**: [2602.08382](https://arxiv.org/abs/2602.08382)
**作者**: Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang
**分类**: cs.CL, cs.LG

## 摘要

本文解决了大语言模型在扩展上下文上进行推理的计算挑战。所提出的框架将输入分割成块，通过学习的压缩器将每个块转换为压缩记忆表示，并采用门控机制选择性地检索相关记忆块。推理模块通过不断演化的工作记忆迭代处理这些信息。压缩器和推理器通过端到端强化学习联合训练。该系统在多跳推理任务上达到有竞争力的性能，展示了从7K到1.75M token的上下文外推能力，峰值GPU内存减少2倍，推理速度比MemAgent快6倍。

## 主要贡献

- **学习的记忆压缩**：可训练的压缩器将长块蒸馏为紧凑的记忆表示
- **门控选择性检索**：门控机制在每个推理步骤中仅检索最相关的记忆块
- **迭代工作记忆**：推理模块在多个处理步骤中维护和更新工作记忆
- **端到端RL训练**：通过强化学习联合优化压缩器和推理器
- **极端上下文外推**：从7K训练上下文泛化到推理时的1.75M token

## 背景：长上下文挑战

在Transformer中处理长上下文面临两个根本瓶颈：

1. **二次方注意力成本**：长度$n$按$O(n^2)$缩放
2. **线性内存成本**：KV缓存线性增长，消耗GPU内存

现有方法包括：
- **稀疏注意力**：减少计算但可能遗漏关键的长距离依赖
- **检索增强生成（RAG）**：外部检索但与模型推理断开
- **记忆增强模型**：固定压缩方案，不适应推理任务

## 架构

### 记忆压缩器

输入序列被分成块$\{c_1, c_2, \ldots, c_N\}$。每个块由压缩器处理以产生紧凑的记忆表示：

$$m_i = \text{Compress}_\phi(c_i) \in \mathbb{R}^{k \times d}$$

其中$k \ll |c_i|$是压缩记忆大小，$d$是隐藏维度。压缩器学习从每个块中保留最与任务相关的信息。

### 门控记忆检索

并非所有记忆块都与每个推理步骤相关。门控机制计算相关性分数：

$$g_i = \sigma(W_g [q; m_i])$$

其中$q$是当前推理查询，$m_i$是第$i$个记忆块。只有$g_i > \tau$的记忆块被检索，大幅减少有效上下文大小。

### 迭代推理模块

推理器维护一个跨推理步骤演化的工作记忆$w$：

$$w^{(t+1)} = \text{Reason}_\psi(w^{(t)}, \{m_i : g_i^{(t)} > \tau\})$$

在每一步中，推理器：
1. 通过门控机制查询压缩记忆
2. 将检索到的信息整合到工作记忆中
3. 决定是继续推理还是产生输出

### 端到端RL训练

整个流水线——压缩器、门控和推理器——使用强化学习联合训练。奖励信号来自任务性能（如多跳问答的正确性）：

$$\mathcal{J}(\phi, \psi) = \mathbb{E}\left[R(\text{answer}, \text{ground\_truth})\right]$$

RL训练是必要的，因为离散的门控决策和迭代推理步骤无法通过标准反向传播进行微分。

## 上下文外推

该框架的一个显著特性是能够远超训练上下文长度进行外推。仅在7K token的序列上训练，模型成功处理了高达1.75M token的上下文——250倍的外推因子。这之所以可能是因为：

- 压缩器在固定大小的块上操作，与总长度无关
- 门控机制随记忆块数量亚线性缩放
- 迭代推理器可以执行所需的任意多步骤

## 实验结果

| 指标 | 数值 |
|------|------|
| 上下文外推 | 7K → 1.75M token（250倍） |
| 峰值GPU内存 | 减少2倍 |
| 推理速度 | 比MemAgent快6倍 |
| 多跳推理 | 与全上下文基线有竞争力 |

## 重要意义

1. **实用长上下文处理**：在标准硬件上实现百万token上下文的推理
2. **RL用于记忆管理**：证明RL可以有效地联合训练记忆压缩和检索
3. **极端泛化**：250倍上下文外推表明学习到的压缩捕获了基本的信息结构
4. **效率提升**：2倍内存和6倍速度改进使长上下文推理在部署中变得实用
5. **模块化设计**：压缩器、门控和推理器可以独立改进
:::
