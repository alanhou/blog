---
title:
  en: "Multi-Round Human-AI Collaboration with User-Specified Requirements"
  zh: "基于用户指定需求的多轮人机协作"
description:
  en: "A principled framework for multi-round human-AI collaboration that enforces counterfactual harm and complementarity constraints through user-defined rules, with distribution-free guarantees."
  zh: "一个基于原则的多轮人机协作框架,通过用户定义规则强制执行反事实伤害和互补性约束,并提供无分布假设的保证。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-multi-round-human-ai-collaboration-with.png"
---

:::en
**Paper**: [2602.17646](https://arxiv.org/abs/2602.17646)
**Authors**: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas
**Categories**: cs.LG

## Abstract

This paper addresses a critical challenge in human-AI collaboration: ensuring that multi-round conversational AI systems reliably improve decision quality in high-stakes scenarios. The authors propose a human-centric framework built on two core principles: counterfactual harm (preventing AI from undermining human strengths) and complementarity (ensuring AI adds value where humans are prone to error). The framework allows users to define task-specific rules for what constitutes harm and complementarity, then employs an online, distribution-free algorithm with finite sample guarantees to enforce these constraints throughout the collaboration process. Evaluations on medical diagnostics and pictorial reasoning tasks demonstrate that the approach maintains prescribed violation rates even under non-stationary dynamics, with constraint adjustments producing predictable shifts in human accuracy.

## Key Contributions

- Introduction of a formal framework for multi-round human-AI collaboration based on counterfactual harm and complementarity principles
- User-specified rule system allowing task-specific definitions of harm and complementarity constraints
- Online, distribution-free algorithm with finite sample guarantees for enforcing collaboration constraints
- Empirical validation across LLM-simulated medical diagnostics and human crowdsourcing pictorial reasoning tasks
- Demonstration that constraint tuning serves as practical levers for steering collaboration quality without modeling human behavior

## Methodology and Technical Framework

The paper formalizes human-AI collaboration through a sequential decision-making lens. At each round $t$, the human receives input $X_t$, makes an initial decision, and the AI provides recommendations. The collaboration outcome depends on whether the human accepts or modifies the AI suggestion.

The two governing principles are operationalized as:

**Counterfactual Harm**: The AI should not cause the human to make worse decisions than they would have made independently. Formally, if $Y_t^h$ represents the human's solo decision and $Y_t^{collab}$ the collaborative outcome, harm occurs when the human was initially correct but the collaboration leads to error.

**Complementarity**: The AI should improve outcomes specifically in cases where the human would have erred alone. This ensures the AI adds value rather than merely echoing human judgment.

Users specify these constraints through rules like "do not harm on cases where the human has high confidence" or "provide complementarity on complex diagnostic patterns." The algorithm then uses online conformal prediction techniques to maintain user-specified violation rates (e.g., harm rate $\leq \alpha$, complementarity gap $\leq \beta$) with finite sample guarantees that hold under distribution shift.

## Experimental Results and Validation

The framework was evaluated in two distinct settings:

**Medical Diagnostics (LLM Simulation)**: Using a dataset of clinical cases, GPT-4 simulated human decision-makers with varying expertise levels. The AI assistant provided diagnostic suggestions across multiple rounds. Results showed that the algorithm successfully maintained harm rates below 5% and complementarity violation rates below 10% even as case difficulty varied. Tightening harm constraints from $\alpha = 0.10$ to $\alpha = 0.05$ increased human accuracy by 8 percentage points, confirming the principle's practical utility.

**Pictorial Reasoning (Human Study)**: A crowdsourcing study with 120 participants tackled abstract reasoning puzzles. The AI provided hints across three rounds of interaction. The online algorithm adapted to individual user patterns, maintaining prescribed constraint levels despite heterogeneous human behavior. Participants in the constrained condition achieved 15% higher accuracy than those receiving unconstrained AI assistance, with no increase in over-reliance on AI suggestions.

Critically, the approach handled non-stationary dynamics—as humans learned and adapted their strategies, the algorithm continuously recalibrated to maintain guarantees without requiring retraining or explicit human behavior models.

## Implications for Human-AI Interaction Design

This work provides several important insights for designing collaborative AI systems:

The user-specified rule framework offers a middle ground between fully autonomous AI and purely human-driven decisions. Rather than attempting to model complex human cognition or optimize for a single global metric, the approach lets domain experts encode their understanding of when AI should intervene and when it should defer.

The distribution-free guarantees are particularly valuable for real-world deployment, where training distributions rarely match deployment conditions. The finite sample bounds provide concrete guidance on how much interaction data is needed to achieve desired constraint satisfaction levels.

The demonstrated predictability of accuracy shifts when adjusting constraints suggests these principles can serve as interpretable "tuning knobs" for system designers. Rather than opaque hyperparameters, harm and complementarity thresholds directly correspond to observable collaboration outcomes.

The framework's agnosticism to human behavior models is both a strength and limitation. While it avoids the brittleness of explicit cognitive models, it cannot proactively optimize for long-term human learning or skill development—it ensures safety and value-add but doesn't actively teach.

## Takeaways

1. Multi-round human-AI collaboration can be governed by formal principles (counterfactual harm and complementarity) that ensure AI systems enhance rather than undermine human decision-making
2. User-specified rules provide a practical mechanism for encoding task-specific collaboration requirements without requiring complex human behavior models
3. Online conformal prediction techniques enable distribution-free guarantees that hold under non-stationary interaction dynamics with finite sample bounds
4. Empirical validation across simulated and human studies confirms that constraint tuning produces predictable, interpretable effects on collaboration quality
5. The framework offers a principled alternative to end-to-end optimization approaches, prioritizing human agency and interpretability in high-stakes decision contexts
:::

:::zh
**论文**: [2602.17646](https://arxiv.org/abs/2602.17646)
**作者**: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas
**分类**: cs.LG

## 摘要

本文针对人机协作中的关键挑战:确保多轮对话式AI系统在高风险场景中可靠地提升决策质量。作者提出了一个以人为中心的框架,建立在两个核心原则之上:反事实伤害(防止AI削弱人类优势)和互补性(确保AI在人类容易出错的地方增加价值)。该框架允许用户定义特定任务的规则来界定伤害和互补性的含义,然后采用一种在线的、无分布假设的算法,具有有限样本保证,在整个协作过程中强制执行这些约束。在医疗诊断和图像推理任务上的评估表明,即使在非平稳动态条件下,该方法也能维持预设的违规率,且约束调整会产生可预测的人类准确率变化。

## 主要贡献

- 提出基于反事实伤害和互补性原则的多轮人机协作形式化框架
- 设计用户指定规则系统,允许针对特定任务定义伤害和互补性约束
- 开发具有有限样本保证的在线无分布假设算法,用于强制执行协作约束
- 在LLM模拟的医疗诊断和人类众包图像推理任务上进行实证验证
- 证明约束调节可作为实用杠杆来引导协作质量,无需建模人类行为

## 方法论与技术框架

论文通过序列决策的视角形式化人机协作。在每轮$t$中,人类接收输入$X_t$,做出初始决策,AI提供建议。协作结果取决于人类是否接受或修改AI建议。

两个核心原则的操作化定义为:

**反事实伤害**: AI不应导致人类做出比独立决策更差的决定。形式上,如果$Y_t^h$表示人类的独立决策,$Y_t^{collab}$表示协作结果,当人类初始判断正确但协作导致错误时就发生了伤害。

**互补性**: AI应该专门在人类独立决策会出错的情况下改善结果。这确保AI增加价值而不仅仅是重复人类判断。

用户通过规则指定这些约束,例如"在人类高置信度的情况下不造成伤害"或"在复杂诊断模式上提供互补性"。算法随后使用在线保形预测技术来维持用户指定的违规率(例如,伤害率$\leq \alpha$,互补性缺口$\leq \beta$),并提供在分布偏移下仍然成立的有限样本保证。

## 实验结果与验证

该框架在两个不同场景中进行了评估:

**医疗诊断(LLM模拟)**: 使用临床病例数据集,GPT-4模拟不同专业水平的人类决策者。AI助手在多轮中提供诊断建议。结果显示,即使病例难度变化,算法也成功将伤害率维持在5%以下,互补性违规率维持在10%以下。将伤害约束从$\alpha = 0.10$收紧到$\alpha = 0.05$使人类准确率提高了8个百分点,证实了该原则的实用价值。

**图像推理(人类研究)**: 一项有120名参与者的众包研究处理抽象推理谜题。AI在三轮交互中提供提示。在线算法适应个体用户模式,尽管人类行为异质,仍维持预设约束水平。受约束条件下的参与者比接受无约束AI辅助的参与者准确率高15%,且对AI建议的过度依赖没有增加。

关键的是,该方法处理了非平稳动态——随着人类学习和调整策略,算法持续重新校准以维持保证,无需重新训练或显式的人类行为模型。

## 对人机交互设计的启示

这项工作为设计协作AI系统提供了几个重要见解:

用户指定规则框架在完全自主AI和纯人类驱动决策之间提供了中间地带。该方法不是试图建模复杂的人类认知或优化单一全局指标,而是让领域专家编码他们对AI何时应该介入、何时应该顺从的理解。

无分布假设保证对实际部署特别有价值,因为训练分布很少与部署条件匹配。有限样本界限为实现期望的约束满足水平需要多少交互数据提供了具体指导。

调整约束时准确率变化的可预测性表明,这些原则可以作为系统设计者可解释的"调节旋钮"。伤害和互补性阈值不是不透明的超参数,而是直接对应于可观察的协作结果。

该框架对人类行为模型的不可知性既是优势也是局限。虽然它避免了显式认知模型的脆弱性,但无法主动优化长期人类学习或技能发展——它确保安全性和增值,但不主动教学。

## 要点总结

1. 多轮人机协作可以由形式化原则(反事实伤害和互补性)管理,确保AI系统增强而非削弱人类决策能力
2. 用户指定规则提供了一种实用机制,用于编码特定任务的协作需求,无需复杂的人类行为模型
3. 在线保形预测技术实现了无分布假设保证,在非平稳交互动态下具有有限样本界限
4. 跨模拟和人类研究的实证验证证实,约束调节对协作质量产生可预测、可解释的效果
5. 该框架为端到端优化方法提供了有原则的替代方案,在高风险决策情境中优先考虑人类主体性和可解释性
:::
