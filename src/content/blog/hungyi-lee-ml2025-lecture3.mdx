---
title:
  en: "Hung-yi Lee ML 2025 Lecture 3: The Neuroscience of AI - Inside Language Model Mechanisms"
  zh: "李宏毅机器学习2025 第三讲：AI的脑科学 - 语言模型内部运作机制剖析"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on analyzing LLM internals: from single neurons to functional vectors and logit lens"
  zh: "台大李宏毅教授课程笔记：从单一神经元到功能向量与logit lens的LLM内部分析"
date: 2025-02-23
tags: ["ai", "llm", "interpretability", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/Xnil63UDW2o/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="Xnil63UDW2o" title="李宏毅机器学习2025 第三讲：AI的脑科学" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## The Neuroscience of AI

This lecture explores how transformers actually work internally - not how to train them, but what happens inside an already-trained model. Think of it as neuroscience for AI.

**Important caveat**: Most interpretability research is done on smaller, older models (like GPT-2) because analyzing massive models requires enormous compute. Results may not fully generalize to frontier models.

## What Does a Single Neuron Do?

In a transformer's feed-forward layer, each output dimension is computed by taking a weighted sum of inputs, passing through an activation function (ReLU). Each such computation is called a "neuron."

**How to analyze a neuron's function:**
1. **Observe**: When does this neuron activate? What outputs follow?
2. **Intervene**: Remove the neuron - does the behavior disappear?
3. **Modulate**: Does stronger activation produce stronger effects?

**The Trump Neuron**: In 2021, OpenAI researchers found a neuron in CLIP (an image model) that activates specifically for Trump-related content - his face, cartoons of him, even text containing his name. It's highly selective: activates for Trump and his associates, but not for Obama or other politicians.

**The Grandmother Neuron Problem**: In neuroscience, the "grandmother neuron" hypothesis (one neuron = one memory) was proposed as a strawman to contrast with the more accepted view that memories involve many neurons working together. Similarly in AI, single neurons rarely have clean, interpretable functions.

Most neurons are **polysemantic** - they respond to multiple unrelated concepts. One neuron might activate for physics terms, medical terminology, AND counterfeit-related words. When asked to explain what a random neuron does, even GPT-4.5 gives vague, unhelpful answers.

## From Single Neurons to Functional Vectors

If single neurons don't have clear functions, maybe **combinations** of neurons do. The hypothesis: specific patterns of neuron activations (vectors) correspond to specific functions.

**Finding functional vectors:**
1. Collect many inputs that trigger a behavior (e.g., refusal)
2. Extract the representation at a specific layer for each
3. Average them to get "refusal + other stuff"
4. Do the same for inputs that don't trigger the behavior
5. Subtract to isolate the functional vector

**The Refusal Vector**: Researchers found a vector that, when added to representations, makes models refuse even harmless requests ("Yoga is dangerous, I can't help"). When subtracted, models comply with harmful requests they'd normally refuse.

**Other discovered vectors:**
- **Sycophancy vector**: Add it, and the model agrees with everything ("Your idea is the greatest in human history!")
- **Truthfulness vector**: Add it, and the model gives literal answers instead of playing along with idioms
- **In-context learning vector**: Extracted from demonstrations, it can make models follow patterns without seeing examples

These vectors can even be combined arithmetically: (first-word + capital) - (first-word + copy) + (last-word + copy) = (last-word + capital).

## Sparse Autoencoders: Better Feature Discovery

The problem with raw neurons: they're polysemantic. Solution: train a **sparse autoencoder** to decompose representations into interpretable features.

A sparse autoencoder learns to:
1. Expand the representation into a much higher-dimensional space
2. Force most dimensions to be zero (sparsity)
3. Reconstruct the original representation

The resulting features are often **monosemantic** - each corresponds to a single concept. Anthropic found features in Claude for:
- The Golden Gate Bridge
- Sycophantic praise
- Code errors
- Specific languages

**Steering with features**: Amplify the "Golden Gate Bridge" feature, and Claude becomes obsessed with it, mentioning it in every response. Amplify the sycophancy feature, and it calls your made-up proverb "the greatest sentence since humanity began."

## Logit Lens: Reading the Model's Mind

Transformers have **residual connections** - each layer adds information to a "residual stream" that flows from input to output. The final layer converts this stream to token probabilities via "unembedding."

**Key insight**: You can apply unembedding to intermediate layers too! This reveals what the model is "thinking" at each stage.

**Example - Capital cities**: Ask "What is the capital of Poland?" and trace the residual stream:
- Early layers: Confused, no clear token
- Layer 15: Suddenly "Poland" appears
- Layer 19: Switches to "Warsaw"

The model first identifies the subject, then retrieves the answer.

**Language of thought**: When LLaMA-2 translates French→Chinese, logit lens reveals it goes French→English→Chinese internally. English is its "language of thought."

## PatchScope: Richer Interpretations

Logit lens only outputs single tokens. **PatchScope** extracts richer descriptions by:
1. Creating a template: "Leonardo: American actor. TSMC: Taiwanese company. X:"
2. Replacing X's representation with the one you want to analyze
3. The model completes with a description

This reveals how understanding evolves across layers. For "Diana, Princess of Wales":
- Layers 1-2: "A country in the United Kingdom" (just sees "Wales")
- Layer 4: "A title for royal women"
- Layer 5: "Wife of the Prince of Wales"
- Layer 6: Full information about Diana

## Practical Application: Fixing Multi-Hop Reasoning

For questions like "The spouse of the performer of Imagine is ___":
- E1 (explicit): Imagine (the album)
- E2 (implicit): John Lennon (performer)
- E3 (answer): Yoko Ono (spouse)

Analysis shows E2 must be resolved by ~layer 15 for E3 to be computed by ~layer 25. When E2 resolves too late, the model fails.

**Solution (Backpatching)**: Take later-layer representations and inject them into earlier layers, giving the model "more time" to reason. This fixes 40-60% of previously wrong answers - similar in spirit to chain-of-thought reasoning ("when depth isn't enough, use length").

## Key Takeaways

1. Single neurons rarely have clean, interpretable functions
2. Functional vectors (neuron combinations) can control specific behaviors
3. Sparse autoencoders find monosemantic features for precise steering
4. Logit lens and PatchScope reveal the model's "thoughts" at each layer
5. Understanding internals enables targeted interventions and fixes
:::

:::zh
本文整理自台湾大学李宏毅教授的「生成式AI时代下的机器学习(2025)」课程。

## AI的脑科学

这堂课探索transformer内部实际如何运作——不是如何训练，而是已训练好的模型内部发生什么事。可以把它想成AI的脑神经科学。

**重要提醒**：多数可解释性研究是在较小、较旧的模型（如GPT-2）上进行的，因为分析大型模型需要庞大算力。结果可能无法完全推广到最新模型。

## 单一神经元在做什么？

在transformer的feed-forward层中，每个输出维度是通过对输入做加权总和，再通过激活函数（ReLU）计算出来的。每个这样的计算称为一个「神经元」。

**如何分析神经元的功能：**
1. **观察**：这个神经元何时启动？接下来有什么输出？
2. **介入**：移除神经元——行为是否消失？
3. **调节**：更强的启动是否产生更强的效果？

**川普神经元**：2021年，OpenAI研究人员在CLIP（图像模型）中发现一个神经元，专门对川普相关内容启动——他的脸、他的卡通、甚至包含他名字的文字。它有高度选择性：对川普和他的同伙启动，但对奥巴马或其他政治人物不会。

**祖母神经元问题**：在脑神经科学中，「祖母神经元」假说（一个神经元=一个记忆）是作为稻草人理论提出的，用来对照更被接受的观点：记忆涉及许多神经元共同运作。AI中也类似，单一神经元很少有清晰、可解释的功能。

多数神经元是**多义的**——它们对多个不相关的概念有反应。一个神经元可能对物理术语、医学术语、还有仿冒相关词汇都会启动。当被要求解释一个随机神经元在做什么时，即使GPT-4.5也给出模糊、没帮助的答案。

## 从单一神经元到功能向量

如果单一神经元没有清晰功能，也许神经元的**组合**有。假设：特定的神经元启动模式（向量）对应特定功能。

**找出功能向量：**
1. 收集许多触发某行为的输入（如拒绝）
2. 提取每个在特定层的representation
3. 平均得到「拒绝+其他东西」
4. 对不触发该行为的输入做同样的事
5. 相减以分离出功能向量

**拒绝向量**：研究人员发现一个向量，加到representation上会让模型拒绝甚至无害的请求（「瑜伽很危险，我不能帮忙」）。减掉时，模型会配合它通常会拒绝的有害请求。

**其他发现的向量：**
- **谄媚向量**：加上它，模型同意一切（「你的想法是人类有史以来最伟大的！」）
- **说真话向量**：加上它，模型给字面答案而不是配合谚语
- **上下文学习向量**：从示范中提取，可以让模型在没看到例子的情况下遵循模式

这些向量甚至可以做算术组合：（第一个字+首都）-（第一个字+复制）+（最后一个字+复制）=（最后一个字+首都）。

## 稀疏自编码器：更好的特征发现

原始神经元的问题：它们是多义的。解决方案：训练**稀疏自编码器**将representation分解成可解释的特征。

稀疏自编码器学习：
1. 将representation扩展到更高维空间
2. 强制大多数维度为零（稀疏性）
3. 重建原始representation

得到的特征通常是**单义的**——每个对应单一概念。Anthropic在Claude中发现了以下特征：
- 金门大桥
- 谄媚赞美
- 程序错误
- 特定语言

**用特征操控**：放大「金门大桥」特征，Claude就会对它着迷，每个回应都提到它。放大谄媚特征，它会称你编造的谚语是「人类有史以来最伟大的句子」。

## Logit Lens：读取模型的心智

Transformer有**残差连接**——每一层将信息加到从输入流向输出的「残差流」中。最后一层通过「unembedding」将这个流转换成token概率。

**关键洞见**：你也可以对中间层应用unembedding！这揭示了模型在每个阶段「想」什么。

**范例——首都城市**：问「波兰的首都是什么？」并追踪残差流：
- 早期层：困惑，没有清晰的token
- 第15层：突然出现「Poland」
- 第19层：切换到「Warsaw」

模型先识别主题，然后检索答案。

**思考的语言**：当LLaMA-2翻译法文→中文时，logit lens揭示它内部走的是法文→英文→中文。英文是它的「思考语言」。

## PatchScope：更丰富的解释

Logit lens只输出单一token。**PatchScope**通过以下方式提取更丰富的描述：
1. 创建模板：「李奥纳多：美国演员。台积电：台湾公司。X：」
2. 用你想分析的representation替换X的representation
3. 模型完成描述

这揭示了理解如何跨层演变。对于「戴安娜，威尔斯王妃」：
- 第1-2层：「英国的一个国家」（只看到「Wales」）
- 第4层：「给皇室女性的头衔」
- 第5层：「威尔斯王子的妻子」
- 第6层：关于戴安娜的完整信息

## 实际应用：修复多跳推理

对于像「Imagine的表演者的配偶是___」这样的问题：
- E1（明确）：Imagine（专辑）
- E2（隐含）：约翰·列侬（表演者）
- E3（答案）：小野洋子（配偶）

分析显示E2必须在约第15层解析出来，E3才能在约第25层计算出来。当E2解析太晚时，模型会失败。

**解决方案（Backpatching）**：取后面层的representation注入到前面层，给模型更多「时间」推理。这修复了40-60%原本错误的答案——精神上类似思维链推理（「深度不够，长度来凑」）。

## 重点整理

1. 单一神经元很少有清晰、可解释的功能
2. 功能向量（神经元组合）可以控制特定行为
3. 稀疏自编码器找到单义特征以进行精确操控
4. Logit lens和PatchScope揭示模型在每层的「想法」
5. 理解内部机制能实现针对性的介入和修复
:::
