---
title:
  en: "Training-Free Cross-Architecture Merging for Graph Neural Networks"
  zh: "图神经网络的免训练跨架构融合"
description:
  en: "H-GRAMA enables merging of heterogeneous GNN architectures without retraining by lifting the merging process from parameter space to operator space, achieving 1.2x-1.9x inference speedups."
  zh: "H-GRAMA通过将融合过程从参数空间提升到算子空间,实现异构GNN架构的免训练融合,推理速度提升1.2倍至1.9倍。"
date: 2026-02-24
tags: ["arxiv", "ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.19332](https://arxiv.org/abs/2602.19332)
**Authors**: Rishabh Bhattacharya, Vikaskumar Kalsariya, Naresh Manwani
**Categories**: cs.LG

## Abstract

This paper introduces H-GRAMA (Heterogeneous Graph Routing and Message Alignment), a training-free framework that enables merging of Graph Neural Networks with different architectures. Unlike existing model merging techniques that require identical architectures, H-GRAMA operates in operator space rather than parameter space, allowing heterogeneous GNN models like GCN and GAT to be combined without retraining. The framework formalizes Universal Message Passing Mixture (UMPM), which expresses diverse GNN layers through a unified functional representation. Experimental results demonstrate that H-GRAMA maintains specialist model accuracy while achieving 1.2x to 1.9x inference speedups compared to ensemble methods.

## Key Contributions

- Introduction of H-GRAMA, the first training-free framework for cross-architecture GNN merging
- Formalization of Universal Message Passing Mixture (UMPM) as a shared operator family that unifies heterogeneous GNN architectures
- Demonstration that operator-space merging overcomes the topology-dependent limitations of parameter-space approaches
- Empirical validation showing preserved specialist accuracy with significant inference speedups over traditional ensembles

## The Challenge of GNN Model Merging

Traditional model merging techniques operate in parameter space, averaging or interpolating weights between models with identical architectures. This approach fails for GNNs due to their fundamental reliance on message passing mechanisms that are inherently topology-dependent. Different GNN architectures implement message passing in structurally incompatible ways—GCN uses symmetric normalized aggregation, GAT employs attention-weighted mechanisms, and GraphSAGE applies sampling-based neighborhood aggregation.

The core issue is that direct parameter averaging between, say, a GCN and GAT layer produces meaningless hybrid parameters that don't correspond to any coherent message passing operation. The attention coefficients in GAT have no semantic correspondence to the fixed aggregation weights in GCN, making naive parameter-space merging fundamentally unreliable for cross-architecture scenarios.

## Universal Message Passing Mixture (UMPM)

H-GRAMA's solution is to lift the merging operation from parameter space to operator space through UMPM. The key insight is that despite architectural differences, all GNN layers can be expressed as compositions of fundamental operations: neighborhood aggregation, feature transformation, and attention/weighting mechanisms.

UMPM defines a universal operator family where each GNN layer $l$ is represented as:

$$h_v^{(l)} = \sigma\left(\sum_{u \in \mathcal{N}(v)} \alpha_{uv} W^{(l)} h_u^{(l-1)}\right)$$

where $\alpha_{uv}$ captures architecture-specific weighting (uniform for GCN, learned attention for GAT), $W^{(l)}$ represents learnable transformations, and $\sigma$ is the activation function. By decomposing each architecture into this common functional language, H-GRAMA can route and align message passing operations across heterogeneous models.

## H-GRAMA Architecture and Routing Mechanism

The framework consists of three core components:

**Operator Decomposition**: Each specialist GNN model is decomposed into its constituent UMPM operators, preserving the learned parameters but expressing them in the universal representation.

**Dynamic Routing**: H-GRAMA employs a routing mechanism that selects which specialist operator to apply at each layer based on input characteristics. The routing is deterministic and training-free, using graph structural properties (degree distribution, clustering coefficient) to make routing decisions.

**Message Alignment**: To ensure compatibility between layers from different architectures, H-GRAMA applies alignment transformations that normalize message scales and dimensions, preventing the accumulation of misalignment errors across layers.

The merged model operates by dynamically selecting operators from the specialist pool at inference time, creating an adaptive computation path through the heterogeneous operator space.

## Experimental Results and Performance Analysis

H-GRAMA was evaluated on standard graph benchmarks including Cora, Citeseer, and PubMed for node classification tasks. The experiments merged various architecture pairs: GCN-GAT, GCN-GraphSAGE, and GAT-GraphSAGE.

Key findings include:

**Accuracy Preservation**: In compatible depth settings (when specialist models have the same number of layers), H-GRAMA retained 95-98% of specialist model accuracy, significantly outperforming naive parameter averaging which degraded to near-random performance.

**Inference Speedup**: Compared to ensemble methods that require running all specialist models, H-GRAMA achieved 1.2x speedup for 2-model merging and up to 1.9x for 3-model scenarios, as only one operator path is executed per layer.

**Depth Sensitivity**: Performance degraded when merging models with mismatched depths, highlighting that architectural compatibility extends beyond just the operator types to include structural properties like network depth.

**Routing Efficiency**: The training-free routing mechanism successfully identified appropriate operators for different graph regions, with high-degree nodes benefiting from attention-based operators (GAT) and low-degree nodes from simpler aggregation (GCN).

## Takeaways

1. H-GRAMA demonstrates that cross-architecture model merging is feasible for GNNs when approached through operator space rather than parameter space
2. The Universal Message Passing Mixture provides a principled framework for expressing heterogeneous GNN architectures in a common functional language
3. Training-free merging can achieve significant inference speedups (1.2x-1.9x) over ensembles while maintaining high accuracy
4. Architectural compatibility for merging extends beyond operator types to include structural properties like network depth
5. Dynamic routing based on graph structural properties enables adaptive computation that leverages specialist strengths for different input characteristics
:::

:::zh
**论文**: [2602.19332](https://arxiv.org/abs/2602.19332)
**作者**: Rishabh Bhattacharya, Vikaskumar Kalsariya, Naresh Manwani
**分类**: cs.LG

## 摘要

本文提出了H-GRAMA(异构图路由与消息对齐),这是一个免训练框架,能够融合不同架构的图神经网络。与现有的要求相同架构的模型融合技术不同,H-GRAMA在算子空间而非参数空间中操作,允许GCN和GAT等异构GNN模型在无需重新训练的情况下进行组合。该框架形式化了通用消息传递混合(UMPM),通过统一的函数表示来表达不同的GNN层。实验结果表明,H-GRAMA在保持专家模型精度的同时,相比集成方法实现了1.2倍至1.9倍的推理加速。

## 主要贡献

- 提出H-GRAMA,首个用于跨架构GNN融合的免训练框架
- 形式化通用消息传递混合(UMPM)作为统一异构GNN架构的共享算子族
- 证明算子空间融合克服了参数空间方法的拓扑依赖性限制
- 实证验证显示在保持专家精度的同时相比传统集成方法显著提升推理速度

## GNN模型融合的挑战

传统模型融合技术在参数空间中操作,对具有相同架构的模型进行权重平均或插值。这种方法对GNN失效,因为GNN从根本上依赖于消息传递机制,而这些机制本质上是拓扑依赖的。不同的GNN架构以结构上不兼容的方式实现消息传递——GCN使用对称归一化聚合,GAT采用注意力加权机制,GraphSAGE应用基于采样的邻域聚合。

核心问题在于,直接对GCN和GAT层之间进行参数平均会产生无意义的混合参数,这些参数不对应任何连贯的消息传递操作。GAT中的注意力系数与GCN中的固定聚合权重没有语义对应关系,使得朴素的参数空间融合对于跨架构场景从根本上不可靠。

## 通用消息传递混合(UMPM)

H-GRAMA的解决方案是通过UMPM将融合操作从参数空间提升到算子空间。关键洞察是,尽管架构不同,所有GNN层都可以表示为基本操作的组合:邻域聚合、特征变换和注意力/加权机制。

UMPM定义了一个通用算子族,其中每个GNN层$l$表示为:

$$h_v^{(l)} = \sigma\left(\sum_{u \in \mathcal{N}(v)} \alpha_{uv} W^{(l)} h_u^{(l-1)}\right)$$

其中$\alpha_{uv}$捕获架构特定的加权(GCN为均匀,GAT为学习的注意力),$W^{(l)}$表示可学习变换,$\sigma$是激活函数。通过将每个架构分解为这种通用函数语言,H-GRAMA可以在异构模型之间路由和对齐消息传递操作。

## H-GRAMA架构与路由机制

该框架由三个核心组件构成:

**算子分解**:每个专家GNN模型被分解为其组成的UMPM算子,保留学习到的参数但用通用表示来表达它们。

**动态路由**:H-GRAMA采用路由机制,根据输入特征在每一层选择应用哪个专家算子。路由是确定性且免训练的,使用图结构属性(度分布、聚类系数)来做出路由决策。

**消息对齐**:为确保来自不同架构的层之间的兼容性,H-GRAMA应用对齐变换来归一化消息尺度和维度,防止跨层累积错误对齐。

融合模型通过在推理时从专家池中动态选择算子来操作,在异构算子空间中创建自适应计算路径。

## 实验结果与性能分析

H-GRAMA在标准图基准上进行了评估,包括用于节点分类任务的Cora、Citeseer和PubMed。实验融合了各种架构对:GCN-GAT、GCN-GraphSAGE和GAT-GraphSAGE。

主要发现包括:

**精度保持**:在兼容深度设置下(当专家模型具有相同层数时),H-GRAMA保留了95-98%的专家模型精度,显著优于降级到接近随机性能的朴素参数平均。

**推理加速**:与需要运行所有专家模型的集成方法相比,H-GRAMA在2模型融合中实现了1.2倍加速,在3模型场景中达到1.9倍,因为每层只执行一个算子路径。

**深度敏感性**:融合深度不匹配的模型时性能下降,突显架构兼容性不仅限于算子类型,还包括网络深度等结构属性。

**路由效率**:免训练路由机制成功为不同图区域识别了适当的算子,高度节点受益于基于注意力的算子(GAT),低度节点受益于更简单的聚合(GCN)。

## 要点总结

1. H-GRAMA证明了当通过算子空间而非参数空间处理时,GNN的跨架构模型融合是可行的
2. 通用消息传递混合为在通用函数语言中表达异构GNN架构提供了原则性框架
3. 免训练融合可以在保持高精度的同时实现相对集成方法的显著推理加速(1.2倍-1.9倍)
4. 融合的架构兼容性不仅限于算子类型,还包括网络深度等结构属性
5. 基于图结构属性的动态路由实现了自适应计算,针对不同输入特征利用专家优势
:::
