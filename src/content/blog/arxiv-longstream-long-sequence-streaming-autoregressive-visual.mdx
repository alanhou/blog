---
title:
  en: "LongStream: Enabling Kilometer-Scale 3D Reconstruction with Gauge-Decoupled Streaming"
  zh: "LongStream: 基于规范解耦流式处理的公里级3D重建"
description:
  en: "A novel autoregressive visual geometry model that achieves stable metric-scale 3D reconstruction across thousands of frames by decoupling pose estimation from scale learning and solving Transformer cache degradation."
  zh: "一种新型自回归视觉几何模型,通过解耦位姿估计与尺度学习并解决Transformer缓存退化问题,实现跨越数千帧的稳定度量尺度3D重建。"
date: 2026-02-16
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/arxiv-longstream-long-sequence-streaming-autoregressive-visual.png"
---

:::en
**Paper**: [2602.13172](https://arxiv.org/abs/2602.13172)
**Authors**: Chong Cheng, Xianda Chen, Tao Xie, Wei Yin, Weiqiang Ren, Qian Zhang, Xiaoyuang Guo, Hao Wang
**Categories**: cs.CV

## Abstract

Long-sequence 3D reconstruction has been a persistent challenge in computer vision, with existing autoregressive models struggling to maintain accuracy over extended sequences. LongStream addresses this by introducing a gauge-decoupled streaming architecture that fundamentally rethinks how visual geometry models handle long sequences. The key insight is abandoning the traditional first-frame anchor paradigm in favor of keyframe-relative pose prediction, transforming the problem from long-range extrapolation to consistent local estimation. Combined with orthogonal scale learning and cache-consistent training strategies, LongStream achieves state-of-the-art performance on kilometer-scale sequences at 18 FPS with stable metric-scale reconstruction.

## Key Contributions

- Keyframe-relative pose prediction that eliminates first-frame anchoring, converting long-range extrapolation into constant-difficulty local tasks
- Orthogonal scale learning framework that fully disentangles geometric estimation from scale prediction to suppress drift accumulation
- Cache-consistent training methodology with periodic refresh mechanisms to address attention-sink reliance and KV-cache contamination
- Demonstrated stable metric-scale reconstruction over thousands of frames at real-time performance (18 FPS)

## Technical Methodology

The architecture of LongStream revolves around three core innovations that work synergistically to enable ultra-long sequence processing.

The first innovation addresses the fundamental limitation of first-frame anchoring. Traditional autoregressive models predict poses relative to the initial frame, which creates an increasingly difficult extrapolation problem as sequences grow longer. This leads to attention decay where the model struggles to maintain focus on distant reference frames, and scale drift where small errors compound over time. LongStream reformulates this by predicting poses relative to recent keyframes instead. Mathematically, instead of estimating $\mathbf{T}_{i \to 0}$ (transformation from frame $i$ to frame 0), the model predicts $\mathbf{T}_{i \to k}$ where $k$ is a recent keyframe. This keeps the prediction task within a bounded difficulty range regardless of sequence length.

The second innovation introduces orthogonal scale learning. In traditional approaches, scale and geometry are entangled in the estimation process, causing scale drift to corrupt geometric predictions. LongStream decouples these by learning scale in a separate orthogonal subspace. The model maintains two parallel estimation streams: one for relative geometry (rotation and translation direction) and another for absolute scale. This disentanglement is crucial because geometric relationships are more stable across frames than absolute scale, which is prone to accumulation errors.

The third innovation tackles Transformer-specific challenges in streaming scenarios. Long sequences create two problems: attention-sink effects where the model over-relies on initial tokens, and KV-cache contamination where outdated information degrades prediction quality. LongStream introduces cache-consistent training where the model is trained with the same cache management strategy used during inference. Additionally, periodic cache refresh discards stale KV pairs at regular intervals, preventing long-term contamination. This reduces the train-inference gap that typically causes performance degradation in production deployments.

## Experimental Results

LongStream demonstrates substantial improvements over existing methods across multiple benchmarks. On long-sequence datasets, the model maintains consistent accuracy over thousands of frames where baseline methods experience catastrophic drift. The metric-scale reconstruction remains stable even on kilometer-scale trajectories, with absolute trajectory error (ATE) remaining bounded rather than growing linearly with sequence length.

Performance metrics show LongStream achieves 18 FPS processing speed, making it viable for real-time applications. This is particularly impressive given the model processes full sequences rather than isolated frame pairs. The throughput is sufficient for autonomous navigation, AR/VR applications, and real-time mapping scenarios.

Ablation studies validate each component's contribution. Removing keyframe-relative prediction causes rapid accuracy degradation beyond 500 frames. Disabling orthogonal scale learning leads to scale drift that corrupts reconstruction quality. Without cache-consistent training and periodic refresh, attention degradation becomes severe after 1000 frames, with prediction quality dropping significantly.

The model also shows strong generalization to diverse environments. Testing across indoor scenes, urban environments, and natural landscapes demonstrates that the architectural innovations are not dataset-specific but address fundamental challenges in long-sequence processing.

## Implications and Future Directions

LongStream represents a significant step toward practical long-sequence 3D reconstruction. The gauge-decoupled approach offers a principled solution to problems that have plagued autoregressive visual geometry models. By reformulating the task structure rather than simply scaling up model capacity, the work demonstrates that architectural innovation can overcome fundamental limitations.

The real-time performance opens new application possibilities. Autonomous vehicles could maintain consistent maps over extended drives without periodic reinitialization. AR devices could track environments across long sessions without drift. Robotics applications could perform continuous SLAM over extended operation periods.

Several directions emerge for future work. Extending the approach to handle dynamic scenes would broaden applicability. Incorporating semantic understanding could improve robustness in challenging conditions. Investigating how the gauge-decoupled framework applies to other sequential prediction tasks beyond visual geometry could reveal broader principles for long-sequence modeling.

The cache management strategies also suggest general solutions for deploying Transformers in streaming scenarios. The train-inference gap problem affects many production systems, and cache-consistent training with periodic refresh could benefit other domains requiring long-context processing.

## Takeaways

1. First-frame anchoring is a fundamental bottleneck for long-sequence autoregressive models; keyframe-relative prediction transforms unbounded extrapolation into consistent local estimation
2. Disentangling scale from geometry through orthogonal learning prevents drift accumulation that corrupts reconstruction quality over long sequences
3. Cache-consistent training with periodic refresh addresses Transformer-specific degradation in streaming scenarios, reducing the train-inference gap
4. Real-time performance (18 FPS) on kilometer-scale sequences demonstrates that architectural innovation can achieve both accuracy and efficiency
5. The gauge-decoupled framework offers a principled approach to long-sequence modeling that may generalize beyond visual geometry tasks
:::

:::zh
**论文**: [2602.13172](https://arxiv.org/abs/2602.13172)
**作者**: Chong Cheng, Xianda Chen, Tao Xie, Wei Yin, Weiqiang Ren, Qian Zhang, Xiaoyuang Guo, Hao Wang
**分类**: cs.CV

## 摘要

长序列3D重建一直是计算机视觉领域的持续挑战,现有自回归模型在处理扩展序列时难以保持精度。LongStream通过引入规范解耦的流式架构从根本上重新思考视觉几何模型如何处理长序列。核心洞察是放弃传统的首帧锚定范式,转而采用关键帧相对位姿预测,将问题从长程外推转化为一致的局部估计。结合正交尺度学习和缓存一致训练策略,LongStream在公里级序列上以18 FPS实现了最先进的性能和稳定的度量尺度重建。

## 主要贡献

- 关键帧相对位姿预测,消除首帧锚定,将长程外推转换为恒定难度的局部任务
- 正交尺度学习框架,完全解耦几何估计与尺度预测以抑制漂移累积
- 缓存一致训练方法配合周期性刷新机制,解决注意力汇聚依赖和KV缓存污染问题
- 在数千帧上实现稳定的度量尺度重建,达到实时性能(18 FPS)

## 技术方法

LongStream的架构围绕三个协同工作的核心创新,使超长序列处理成为可能。

第一个创新解决了首帧锚定的根本局限。传统自回归模型预测相对于初始帧的位姿,随着序列增长会产生越来越困难的外推问题。这导致注意力衰减,模型难以保持对远距离参考帧的关注,以及尺度漂移,小误差随时间复合累积。LongStream通过预测相对于近期关键帧的位姿来重新表述这个问题。数学上,模型不是估计$\mathbf{T}_{i \to 0}$(从帧$i$到帧0的变换),而是预测$\mathbf{T}_{i \to k}$,其中$k$是近期关键帧。这使预测任务的难度保持在有界范围内,与序列长度无关。

第二个创新引入正交尺度学习。在传统方法中,尺度和几何在估计过程中纠缠在一起,导致尺度漂移破坏几何预测。LongStream通过在独立的正交子空间中学习尺度来解耦这些因素。模型维护两个并行的估计流:一个用于相对几何(旋转和平移方向),另一个用于绝对尺度。这种解耦至关重要,因为几何关系在帧间比绝对尺度更稳定,后者容易产生累积误差。

第三个创新解决流式场景中Transformer特有的挑战。长序列产生两个问题:注意力汇聚效应(模型过度依赖初始token)和KV缓存污染(过时信息降低预测质量)。LongStream引入缓存一致训练,模型使用与推理时相同的缓存管理策略进行训练。此外,周期性缓存刷新定期丢弃陈旧的KV对,防止长期污染。这减少了通常导致生产部署性能下降的训练-推理差距。

## 实验结果

LongStream在多个基准测试中展示了相对现有方法的显著改进。在长序列数据集上,模型在数千帧上保持一致的精度,而基线方法经历灾难性漂移。度量尺度重建即使在公里级轨迹上也保持稳定,绝对轨迹误差(ATE)保持有界而非随序列长度线性增长。

性能指标显示LongStream达到18 FPS的处理速度,使其适用于实时应用。考虑到模型处理完整序列而非孤立帧对,这尤其令人印象深刻。吞吐量足以支持自主导航、AR/VR应用和实时建图场景。

消融研究验证了每个组件的贡献。移除关键帧相对预测会导致超过500帧后精度快速下降。禁用正交尺度学习导致尺度漂移破坏重建质量。没有缓存一致训练和周期性刷新,1000帧后注意力退化变得严重,预测质量显著下降。

模型还展示了对不同环境的强泛化能力。跨室内场景、城市环境和自然景观的测试表明,架构创新不是特定于数据集的,而是解决了长序列处理中的根本挑战。

## 影响与未来方向

LongStream代表了向实用长序列3D重建迈出的重要一步。规范解耦方法为困扰自回归视觉几何模型的问题提供了原则性解决方案。通过重新表述任务结构而非简单扩大模型容量,这项工作证明架构创新可以克服根本性限制。

实时性能开启了新的应用可能性。自动驾驶车辆可以在长距离行驶中维护一致的地图而无需周期性重新初始化。AR设备可以在长时间会话中跟踪环境而不产生漂移。机器人应用可以在扩展操作期间执行连续SLAM。

未来工作的几个方向浮现。将方法扩展到处理动态场景将拓宽适用性。结合语义理解可以提高在挑战性条件下的鲁棒性。研究规范解耦框架如何应用于视觉几何之外的其他序列预测任务,可能揭示长序列建模的更广泛原则。

缓存管理策略也为在流式场景中部署Transformer提供了通用解决方案。训练-推理差距问题影响许多生产系统,缓存一致训练配合周期性刷新可能使其他需要长上下文处理的领域受益。

## 要点总结

1. 首帧锚定是长序列自回归模型的根本瓶颈;关键帧相对预测将无界外推转化为一致的局部估计
2. 通过正交学习解耦尺度与几何,防止在长序列上破坏重建质量的漂移累积
3. 缓存一致训练配合周期性刷新解决流式场景中Transformer特有的退化问题,减少训练-推理差距
4. 在公里级序列上的实时性能(18 FPS)证明架构创新可以同时实现精度和效率
5. 规范解耦框架为长序列建模提供了原则性方法,可能泛化到视觉几何任务之外
:::
