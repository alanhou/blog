---
title:
  en: "OSI-FL: One-Shot Incremental Federated Learning with Catastrophic Forgetting Resilience"
  zh: "OSI-FL: 具有灾难性遗忘弹性的单轮增量联邦学习"
description:
  en: "A novel federated learning framework that achieves one-shot communication while handling incremental tasks and preventing catastrophic forgetting through vision-language embeddings and selective sample retention."
  zh: "一种新型联邦学习框架,通过视觉-语言嵌入和选择性样本保留,实现单轮通信的同时处理增量任务并防止灾难性遗忘。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.lg", "cs.dc"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.17625](https://arxiv.org/abs/2602.17625)
**Authors**: Obaidullah Zaland, Zulfiqar Ahmad Khan, Monowar Bhuyan
**Categories**: cs.LG, cs.DC

## Abstract

This paper introduces One-Shot Incremental Federated Learning (OSI-FL), a groundbreaking framework that simultaneously tackles two critical challenges in federated learning: communication efficiency and catastrophic forgetting in incremental learning scenarios. Traditional federated learning requires multiple communication rounds and struggles with dynamic data streams, while OSI-FL achieves effective model training with just a single communication round. The framework leverages frozen vision-language models (VLMs) to extract category-specific embeddings from client data, which are then transmitted to a central server. A pre-trained diffusion model synthesizes training data from these embeddings, enabling server-side model training without accessing raw client data. To address catastrophic forgetting as new tasks arrive incrementally, OSI-FL incorporates Selective Sample Retention (SSR), which intelligently preserves the most informative samples based on loss metrics. Experimental validation across three benchmark datasets demonstrates superior performance over both traditional multi-round federated learning and existing one-shot approaches in class-incremental and domain-incremental settings.

## Key Contributions

- First federated learning framework to simultaneously address one-shot communication and catastrophic forgetting in incremental learning scenarios
- Novel architecture combining frozen VLMs for embedding extraction with diffusion models for privacy-preserving data synthesis
- Selective Sample Retention (SSR) mechanism that identifies and retains top-$p$ most informative samples per category-task pair based on loss metrics
- Comprehensive evaluation demonstrating superiority over baselines in both class-incremental and domain-incremental learning across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets

## Technical Methodology

The OSI-FL framework operates through a carefully orchestrated pipeline. Each client maintains a frozen vision-language model that processes local data to generate category-specific embeddings. These compact representations capture semantic information about data distributions without exposing raw samples, preserving privacy. In a single communication round, clients transmit these embeddings to the central server.

The server employs a pre-trained diffusion model conditioned on the received embeddings to synthesize artificial training samples. This generative approach reconstructs data distributions similar to those at clients while maintaining privacy guarantees. The synthesized dataset enables the server to train a global model without direct access to client data.

As new tasks arrive incrementally, the framework faces the dual challenge of incorporating new knowledge while retaining previously learned information. The Selective Sample Retention mechanism addresses this by computing loss values for all samples and retaining the top-$p$ most informative ones per category-task combination. These retained samples are strategically incorporated into subsequent training iterations, creating a memory buffer that bounds forgetting. The selection criterion based on sample loss ensures that the most challenging and representative examples are preserved, maximizing the efficiency of the limited memory budget.

## Experimental Results and Analysis

The experimental evaluation spans three benchmark datasets with varying complexity levels. On CIFAR-10 and CIFAR-100, OSI-FL demonstrates consistent improvements over traditional federated learning approaches that require multiple communication rounds. The framework shows particular strength in class-incremental scenarios where new categories are introduced sequentially.

In domain-incremental settings, where the task structure remains constant but data distributions shift, OSI-FL maintains robust performance. The Selective Sample Retention mechanism proves crucial in these scenarios, with ablation studies showing significant performance degradation when SSR is removed. The retention of informative samples creates an effective rehearsal mechanism that stabilizes learning across tasks.

Comparison with existing one-shot federated learning methods reveals OSI-FL's advantages. While other one-shot approaches achieve communication efficiency, they typically suffer from severe catastrophic forgetting. OSI-FL's integration of VLM-based embeddings with diffusion-based synthesis and strategic sample retention creates a synergistic effect that outperforms component techniques in isolation.

The scalability analysis demonstrates that OSI-FL maintains efficiency even as the number of clients and tasks increases. The single communication round constraint ensures predictable bandwidth requirements, while the SSR mechanism's computational overhead remains manageable through efficient loss-based ranking.

## Implications and Future Directions

OSI-FL represents a significant advancement for privacy-sensitive applications requiring continuous learning from distributed data sources. The framework's ability to operate with minimal communication makes it particularly suitable for edge computing scenarios, IoT networks, and cross-organizational collaborations where bandwidth is limited or expensive.

The combination of vision-language models and diffusion-based synthesis opens new avenues for federated learning research. Future work could explore adaptive retention strategies that dynamically adjust the number of preserved samples based on task difficulty or available memory. Integration with other continual learning techniques, such as elastic weight consolidation or progressive neural networks, could further enhance forgetting resistance.

The framework's reliance on pre-trained models (VLMs and diffusion models) raises questions about generalization to domains where such models are unavailable or perform poorly. Investigating domain-specific adaptations or developing lightweight alternatives could broaden applicability. Additionally, theoretical analysis of the privacy guarantees provided by embedding-based communication compared to raw data transmission would strengthen the framework's foundations.

## Takeaways

1. OSI-FL achieves effective federated learning with just one communication round while preventing catastrophic forgetting through intelligent sample retention
2. Vision-language model embeddings combined with diffusion-based synthesis enable privacy-preserving data reconstruction at the server
3. Selective Sample Retention based on loss metrics provides an efficient memory mechanism that bounds forgetting across incremental tasks
4. The framework outperforms both traditional multi-round federated learning and existing one-shot approaches across multiple benchmark datasets
5. OSI-FL's architecture is particularly well-suited for bandwidth-constrained environments requiring continuous learning from distributed, privacy-sensitive data sources
:::

:::zh
**论文**: [2602.17625](https://arxiv.org/abs/2602.17625)
**作者**: Obaidullah Zaland, Zulfiqar Ahmad Khan, Monowar Bhuyan
**分类**: cs.LG, cs.DC

## 摘要

本文提出了单轮增量联邦学习(OSI-FL),这是一个突破性的框架,同时解决了联邦学习中的两个关键挑战:通信效率和增量学习场景中的灾难性遗忘。传统联邦学习需要多轮通信且难以处理动态数据流,而OSI-FL仅需单轮通信即可实现有效的模型训练。该框架利用冻结的视觉-语言模型(VLM)从客户端数据中提取类别特定的嵌入表示,然后将其传输到中央服务器。预训练的扩散模型根据这些嵌入合成训练数据,使服务器端能够在不访问原始客户端数据的情况下进行模型训练。为了解决新任务增量到达时的灾难性遗忘问题,OSI-FL引入了选择性样本保留(SSR)机制,该机制基于损失指标智能地保留最具信息量的样本。在三个基准数据集上的实验验证表明,该方法在类增量和域增量设置下均优于传统的多轮联邦学习和现有的单轮方法。

## 主要贡献

- 首个同时解决单轮通信和增量学习场景中灾难性遗忘问题的联邦学习框架
- 创新架构,结合冻结VLM进行嵌入提取与扩散模型进行隐私保护的数据合成
- 选择性样本保留(SSR)机制,基于损失指标识别并保留每个类别-任务对中前$p$个最具信息量的样本
- 在CIFAR-10、CIFAR-100和Tiny-ImageNet数据集上进行全面评估,证明在类增量和域增量学习中优于基线方法

## 技术方法论

OSI-FL框架通过精心设计的流程运作。每个客户端维护一个冻结的视觉-语言模型,该模型处理本地数据以生成类别特定的嵌入表示。这些紧凑的表示捕获了数据分布的语义信息,而不暴露原始样本,从而保护隐私。在单轮通信中,客户端将这些嵌入传输到中央服务器。

服务器采用预训练的扩散模型,以接收到的嵌入为条件合成人工训练样本。这种生成方法重建了与客户端相似的数据分布,同时保持隐私保证。合成的数据集使服务器能够在不直接访问客户端数据的情况下训练全局模型。

当新任务增量到达时,框架面临着整合新知识同时保留先前学习信息的双重挑战。选择性样本保留机制通过计算所有样本的损失值并保留每个类别-任务组合中前$p$个最具信息量的样本来解决这一问题。这些保留的样本被策略性地纳入后续训练迭代中,创建了一个限制遗忘的记忆缓冲区。基于样本损失的选择标准确保保留最具挑战性和代表性的样例,最大化有限内存预算的效率。

## 实验结果与分析

实验评估涵盖三个不同复杂度级别的基准数据集。在CIFAR-10和CIFAR-100上,OSI-FL相比需要多轮通信的传统联邦学习方法展现出持续的改进。该框架在类增量场景中表现尤为突出,在这些场景中新类别被顺序引入。

在域增量设置中,任务结构保持不变但数据分布发生变化,OSI-FL保持了稳健的性能。选择性样本保留机制在这些场景中被证明至关重要,消融研究显示当移除SSR时性能显著下降。信息样本的保留创建了一个有效的重演机制,稳定了跨任务的学习。

与现有单轮联邦学习方法的比较揭示了OSI-FL的优势。虽然其他单轮方法实现了通信效率,但它们通常遭受严重的灾难性遗忘。OSI-FL将基于VLM的嵌入与基于扩散的合成以及策略性样本保留相结合,创造了一种协同效应,其性能优于单独使用的组件技术。

可扩展性分析表明,即使客户端和任务数量增加,OSI-FL仍保持高效。单轮通信约束确保了可预测的带宽需求,而SSR机制的计算开销通过高效的基于损失的排序保持在可管理范围内。

## 影响与未来方向

OSI-FL代表了需要从分布式数据源持续学习的隐私敏感应用的重大进展。该框架以最小通信运行的能力使其特别适合边缘计算场景、物联网网络以及带宽受限或昂贵的跨组织协作。

视觉-语言模型与基于扩散的合成的结合为联邦学习研究开辟了新途径。未来工作可以探索自适应保留策略,根据任务难度或可用内存动态调整保留样本的数量。与其他持续学习技术(如弹性权重巩固或渐进式神经网络)的集成可以进一步增强抗遗忘能力。

该框架对预训练模型(VLM和扩散模型)的依赖引发了关于在这些模型不可用或性能不佳的领域中泛化能力的问题。研究特定领域的适应或开发轻量级替代方案可以扩大适用性。此外,对基于嵌入的通信相比原始数据传输所提供的隐私保证进行理论分析将加强框架的基础。

## 要点总结

1. OSI-FL仅需一轮通信即可实现有效的联邦学习,同时通过智能样本保留防止灾难性遗忘
2. 视觉-语言模型嵌入结合基于扩散的合成,在服务器端实现隐私保护的数据重建
3. 基于损失指标的选择性样本保留提供了一种高效的记忆机制,限制了跨增量任务的遗忘
4. 该框架在多个基准数据集上优于传统的多轮联邦学习和现有的单轮方法
5. OSI-FL的架构特别适合需要从分布式隐私敏感数据源持续学习的带宽受限环境
:::
