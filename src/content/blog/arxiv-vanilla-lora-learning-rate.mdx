---
title:
  en: "Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-Tuning"
  zh: "学习率很重要：原版LoRA可能就够了"
description:
  en: "Systematic re-evaluation showing LoRA variants achieve similar peak performance (within 1-2%) when learning rates are properly tuned"
  zh: "系统性重新评估表明，当学习率正确调优时，LoRA变体达到相似的峰值性能（差距在1-2%以内）"
date: 2026-02-04
tags: ["arxiv", "ai", "lora", "fine-tuning", "hyperparameter", "learning-rate", "cs.LG", "cs.AI", "cs.CL"]
image: "/arxiv-visuals/arxiv-vanilla-lora-learning-rate.png"
---

:::en
**Paper**: [2602.04998](https://arxiv.org/abs/2602.04998)
**Authors**: Yu-Ang Lee, Ching-Yun Ko, Pin-Yu Chen, Mi-Yen Yeh
**Categories**: cs.LG, cs.AI, cs.CL

## Abstract

A systematic re-evaluation of LoRA variants through comprehensive hyperparameter optimization reveals that once learning rates are properly tuned, all methods achieve similar peak performance (within 1-2%) across mathematical and code generation tasks at various model scales. Reported improvements in LoRA variants may reflect inadequate hyperparameter tuning rather than genuine methodological advantages.

## Key Contributions

- **Debunking LoRA variants**: Most reported improvements disappear with proper learning rate tuning
- **Second-order analysis**: Links performance differences to variations in the largest Hessian eigenvalue
- **Practical guidance**: Proper hyperparameter search matters more than adapter architecture

## The Uncomfortable Truth

The LoRA ecosystem has produced many variants — DoRA, rsLoRA, AdaLoRA, PiSSA, and others — each claiming improvements over vanilla LoRA. This paper challenges those claims by showing that the improvements largely disappear when you properly tune the learning rate.

The key insight: **different LoRA variants have different optimal learning rate ranges**. When papers compare methods using the same learning rate, they're not comparing the methods fairly — they're comparing how well each method happens to work at that particular learning rate.

## Methodology

The authors conducted comprehensive hyperparameter sweeps across:

- Multiple model scales
- Mathematical reasoning and code generation tasks
- All major LoRA variants

### Second-Order Analysis

The paper connects the learning rate sensitivity to the **largest Hessian eigenvalue** — a measure of the loss landscape's curvature. Different LoRA variants create different loss landscapes, requiring different learning rates to navigate effectively.

## Practical Implications

1. **Don't chase variants**: Invest time in learning rate tuning instead of switching LoRA architectures
2. **Fair comparisons require sweeps**: Any LoRA comparison without proper hyperparameter optimization is suspect
3. **Vanilla LoRA is a strong baseline**: With proper tuning, it matches or comes within 1-2% of all variants

## Takeaways

1. **Hyperparameters > architecture**: For LoRA fine-tuning, learning rate selection matters more than adapter design
2. **Occam's razor applies**: The simplest approach (vanilla LoRA) works when properly configured
3. **Beware of confounded comparisons**: Many reported improvements in the literature may be artifacts of suboptimal hyperparameter tuning
:::

:::zh
**论文**: [2602.04998](https://arxiv.org/abs/2602.04998)
**作者**: Yu-Ang Lee, Ching-Yun Ko, Pin-Yu Chen, Mi-Yen Yeh
**分类**: cs.LG, cs.AI, cs.CL

## 摘要

通过全面的超参数优化对LoRA变体进行系统性重新评估，发现一旦学习率正确调优，所有方法在不同模型规模的数学和代码生成任务上达到相似的峰值性能（差距在1-2%以内）。LoRA变体报告的改进可能反映的是超参数调优不足，而非真正的方法论优势。

## 主要贡献

- **揭穿LoRA变体**：大多数报告的改进在正确的学习率调优后消失
- **二阶分析**：将性能差异与最大Hessian特征值的变化联系起来
- **实用指导**：正确的超参数搜索比适配器架构更重要

## 令人不安的真相

LoRA生态系统产生了许多变体——DoRA、rsLoRA、AdaLoRA、PiSSA等——每个都声称比原版LoRA有所改进。本文通过展示当正确调优学习率时这些改进基本消失来挑战这些声明。

关键洞察：**不同的LoRA变体有不同的最优学习率范围**。当论文使用相同的学习率比较方法时，它们并没有公平地比较方法——而是在比较每种方法在该特定学习率下碰巧的表现。

## 方法论

作者进行了全面的超参数搜索，涵盖：

- 多种模型规模
- 数学推理和代码生成任务
- 所有主要LoRA变体

### 二阶分析

论文将学习率敏感性与**最大Hessian特征值**联系起来——这是损失景观曲率的度量。不同的LoRA变体创建不同的损失景观，需要不同的学习率来有效导航。

## 实际意义

1. **不要追逐变体**：将时间投入学习率调优，而非切换LoRA架构
2. **公平比较需要搜索**：任何没有正确超参数优化的LoRA比较都值得怀疑
3. **原版LoRA是强基线**：正确调优后，它匹配或接近所有变体（差距在1-2%以内）

## 要点总结

1. **超参数 > 架构**：对于LoRA微调，学习率选择比适配器设计更重要
2. **奥卡姆剃刀适用**：最简单的方法（原版LoRA）在正确配置时有效
3. **警惕混淆比较**：文献中许多报告的改进可能是次优超参数调优的伪影
:::
