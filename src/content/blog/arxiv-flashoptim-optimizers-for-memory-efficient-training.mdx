---
title:
  en: "FlashOptim: Cutting Neural Network Training Memory by Over 50%"
  zh: "FlashOptim: 将神经网络训练内存减少超过50%"
description:
  en: "FlashOptim introduces novel quantization techniques that reduce optimizer memory from 16 to 7 bytes per parameter while maintaining model quality across vision and language tasks."
  zh: "FlashOptim引入创新量化技术,将优化器内存从每参数16字节降至7字节,同时在视觉和语言任务中保持模型质量。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.23349](https://arxiv.org/abs/2602.23349)
**Authors**: Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock
**Categories**: cs.LG, cs.AI

## Abstract

Training large neural networks demands substantial accelerator memory, with standard mixed-precision training requiring approximately 16 bytes per parameter when using optimizers like AdamW. This memory footprint encompasses the parameter itself, its gradient, and optimizer state variables (momentum and variance estimates). For researchers with limited GPU resources, training even moderately-sized models like 7B parameter networks becomes impractical without access to 100GB+ of memory.

FlashOptim addresses this challenge through a suite of optimizations that reduce per-parameter memory consumption by over 50% without sacrificing model quality or breaking API compatibility. The approach combines improved master weight splitting with novel companding functions for 8-bit optimizer state quantization, achieving a reduction from 16 bytes to 7 bytes per parameter for AdamW (or 5 bytes with gradient release). Comprehensive experiments across SGD, AdamW, and Lion optimizers demonstrate no measurable quality degradation on standard vision and language benchmarks, including Llama-3.1-8B finetuning.

## Key Contributions

- **Improved master weight splitting** with tight quantization error bounds that enable more aggressive compression while maintaining numerical stability
- **Novel companding functions** specifically designed for 8-bit optimizer state quantization, dramatically reducing quantization error compared to naive approaches
- **Unified framework** reducing AdamW memory from 16 bytes to 7 bytes per parameter (or 5 bytes with gradient release) while preserving full training dynamics
- **Checkpoint size reduction** by more than 50%, significantly lowering storage and transfer costs for model distribution
- **Empirical validation** across multiple optimizers (SGD, AdamW, Lion) and diverse benchmarks showing zero quality degradation

## Technical Methodology

### Memory Breakdown in Standard Training

Traditional mixed-precision training with AdamW allocates memory as follows:
- **FP32 master weights**: 4 bytes per parameter
- **FP32 first moment (momentum)**: 4 bytes per parameter  
- **FP32 second moment (variance)**: 4 bytes per parameter
- **FP16/BF16 gradients**: 2 bytes per parameter
- **FP16/BF16 working weights**: 2 bytes per parameter

This totals 16 bytes per parameter, making a 7B parameter model require approximately 112GB just for optimizer states and parameters.

### Master Weight Splitting with Error Bounds

FlashOptim's first innovation improves upon existing master weight splitting techniques. The key insight is deriving a tight theoretical bound on quantization error when splitting FP32 master weights into lower-precision representations. 

The method maintains high-precision updates by carefully managing the error introduced during quantization. By exploiting the mathematical properties of floating-point arithmetic and the structure of gradient updates, FlashOptim ensures that the accumulated quantization error remains bounded throughout training. This allows more aggressive quantization than previous approaches while guaranteeing convergence properties equivalent to full-precision training.

### Companding Functions for Optimizer States

The second major contribution addresses the challenge of quantizing optimizer state variables (momentum and variance estimates in AdamW). Naive uniform quantization of these states introduces significant error because their distributions are highly non-uniform.

FlashOptim introduces specialized companding functions—nonlinear transformations that compress the dynamic range before quantization and expand it after dequantization. These functions are designed based on the empirical distributions of optimizer states observed across diverse training scenarios. The companding approach achieves several objectives:

1. **Dynamic range compression**: Maps the wide range of optimizer state values into a more uniform distribution suitable for 8-bit quantization
2. **Error minimization**: Allocates quantization levels proportionally to the density of values in the original distribution
3. **Computational efficiency**: Uses simple mathematical operations that add negligible overhead during training

The combination of improved master weight splitting and companding reduces optimizer state memory from 12 bytes (3×FP32) to approximately 3 bytes (3×INT8 with metadata), while gradients remain at 2 bytes (FP16).

### Gradient Release Optimization

FlashOptim further reduces memory by implementing gradient release, where gradients are deallocated immediately after the optimizer step rather than being retained. This technique, combined with the other optimizations, achieves the minimal 5 bytes per parameter configuration:
- 2 bytes: FP16 working weights
- 1 byte: quantized momentum
- 1 byte: quantized variance  
- 1 byte: quantized master weight residual

## Experimental Results

### Benchmark Coverage

The authors evaluate FlashOptim across a comprehensive suite of tasks:

**Vision tasks**: Image classification on ImageNet, object detection, semantic segmentation
**Language tasks**: Language modeling, text classification, question answering
**Large model finetuning**: Llama-3.1-8B on instruction-following datasets

### Performance Metrics

Across all benchmarks, FlashOptim demonstrates:
- **Zero measurable quality degradation**: Metrics (accuracy, perplexity, F1 scores) match full-precision baselines within statistical noise
- **Consistent convergence**: Training curves align with baseline optimizers throughout training
- **Optimizer generality**: Results hold for SGD, AdamW, and Lion optimizers

### Memory Savings Analysis

For a 7B parameter model:
- **Baseline AdamW**: ~112GB (16 bytes/param)
- **FlashOptim AdamW**: ~49GB (7 bytes/param) — 56% reduction
- **FlashOptim + gradient release**: ~35GB (5 bytes/param) — 69% reduction

These savings enable training on consumer-grade hardware that would otherwise be impossible, democratizing access to large model training.

### Checkpoint Size Reduction

Model checkpoints, which typically store optimizer states for resumable training, see dramatic size reductions:
- A 7B parameter checkpoint shrinks from ~112GB to ~49GB
- This reduction accelerates checkpoint saving/loading and reduces storage costs
- Distributed training scenarios benefit from faster checkpoint synchronization

## Implications and Future Directions

### Democratization of AI Research

FlashOptim's memory efficiency has significant implications for the AI research community. By reducing memory requirements by over 50%, the techniques enable:
- Training larger models on existing hardware
- Conducting research with consumer-grade GPUs
- Reducing cloud computing costs for academic and small-scale industrial research
- Enabling experimentation with model architectures previously out of reach

### Theoretical Insights

The work provides valuable theoretical contributions beyond the immediate practical benefits:
- Tight error bounds for quantized optimization provide guarantees about convergence behavior
- The companding function design methodology can be applied to other numerical optimization contexts
- The analysis of optimizer state distributions informs future work on adaptive precision training

### Integration with Existing Frameworks

FlashOptim's emphasis on API compatibility means it can be integrated into existing training pipelines with minimal code changes. This practical consideration accelerates adoption and allows researchers to immediately benefit from the memory savings without rewriting training infrastructure.

### Potential Extensions

Several promising directions for future work emerge:
- **Adaptive precision**: Dynamically adjusting quantization levels based on training phase
- **Hardware co-design**: Specialized accelerator support for companding operations
- **Extended optimizer coverage**: Applying techniques to newer optimizers like Sophia or Shampoo
- **Extreme quantization**: Exploring 4-bit or lower representations with more sophisticated error correction

## Takeaways

1. FlashOptim reduces AdamW memory from 16 to 7 bytes per parameter (56% reduction) or 5 bytes with gradient release (69% reduction) without quality loss
2. The approach combines improved master weight splitting with novel companding functions for 8-bit optimizer state quantization
3. Comprehensive experiments across vision and language tasks, including Llama-3.1-8B finetuning, show zero measurable degradation in model quality
4. Checkpoint sizes are reduced by over 50%, significantly lowering storage and transfer costs
5. The techniques work across multiple optimizers (SGD, AdamW, Lion) and maintain API compatibility with existing training frameworks
6. Memory savings enable training 7B parameter models on hardware with less than 50GB of memory, democratizing access to large-scale model training
7. Theoretical error bounds provide convergence guarantees equivalent to full-precision training
:::

:::zh
**论文**: [2602.23349](https://arxiv.org/abs/2602.23349)
**作者**: Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock
**分类**: cs.LG, cs.AI

## 摘要

训练大型神经网络需要大量加速器内存,使用AdamW等优化器的标准混合精度训练每个参数需要约16字节。这些内存占用包括参数本身、梯度以及优化器状态变量(动量和方差估计)。对于GPU资源有限的研究人员来说,即使训练70亿参数规模的模型也变得不切实际,除非拥有100GB以上的内存。

FlashOptim通过一套优化技术解决了这一挑战,在不牺牲模型质量或破坏API兼容性的情况下,将每参数内存消耗减少超过50%。该方法结合了改进的主权重分割和专为8位优化器状态量化设计的新型压扩函数,将AdamW的内存从每参数16字节降至7字节(使用梯度释放时为5字节)。在SGD、AdamW和Lion优化器上的全面实验表明,在包括Llama-3.1-8B微调在内的标准视觉和语言基准测试中,模型质量没有可测量的下降。

## 主要贡献

- **改进的主权重分割**,具有严格的量化误差界限,能够在保持数值稳定性的同时实现更激进的压缩
- **新型压扩函数**,专门为8位优化器状态量化设计,与朴素方法相比大幅降低量化误差
- **统一框架**,将AdamW内存从每参数16字节降至7字节(使用梯度释放时为5字节),同时保持完整的训练动态
- **检查点大小减少**超过50%,显著降低模型分发的存储和传输成本
- **实证验证**,在多个优化器(SGD、AdamW、Lion)和多样化基准测试中显示零质量下降

## 技术方法论

### 标准训练中的内存分解

使用AdamW的传统混合精度训练内存分配如下:
- **FP32主权重**: 每参数4字节
- **FP32一阶矩(动量)**: 每参数4字节
- **FP32二阶矩(方差)**: 每参数4字节
- **FP16/BF16梯度**: 每参数2字节
- **FP16/BF16工作权重**: 每参数2字节

总计每参数16字节,使得70亿参数模型仅优化器状态和参数就需要约112GB内存。

### 带误差界限的主权重分割

FlashOptim的第一个创新改进了现有的主权重分割技术。关键洞察是推导出将FP32主权重分割为低精度表示时量化误差的严格理论界限。

该方法通过仔细管理量化过程中引入的误差来维持高精度更新。通过利用浮点运算的数学性质和梯度更新的结构,FlashOptim确保累积的量化误差在整个训练过程中保持有界。这允许比以前的方法更激进的量化,同时保证与全精度训练等效的收敛性质。

### 优化器状态的压扩函数

第二个主要贡献解决了量化优化器状态变量(AdamW中的动量和方差估计)的挑战。对这些状态进行朴素的均匀量化会引入显著误差,因为它们的分布高度非均匀。

FlashOptim引入了专门的压扩函数——在量化前压缩动态范围并在反量化后扩展的非线性变换。这些函数基于在不同训练场景中观察到的优化器状态的经验分布设计。压扩方法实现了几个目标:

1. **动态范围压缩**: 将优化器状态值的宽范围映射到适合8位量化的更均匀分布
2. **误差最小化**: 根据原始分布中值的密度按比例分配量化级别
3. **计算效率**: 使用简单的数学运算,在训练期间增加的开销可忽略不计

改进的主权重分割和压扩的组合将优化器状态内存从12字节(3×FP32)减少到约3字节(3×INT8加元数据),而梯度保持在2字节(FP16)。

### 梯度释放优化

FlashOptim通过实现梯度释放进一步减少内存,即在优化器步骤后立即释放梯度而不是保留它们。这种技术与其他优化相结合,实现了每参数5字节的最小配置:
- 2字节: FP16工作权重
- 1字节: 量化动量
- 1字节: 量化方差
- 1字节: 量化主权重残差

## 实验结果

### 基准测试覆盖范围

作者在一套全面的任务中评估FlashOptim:

**视觉任务**: ImageNet上的图像分类、目标检测、语义分割
**语言任务**: 语言建模、文本分类、问答
**大模型微调**: 在指令遵循数据集上微调Llama-3.1-8B

### 性能指标

在所有基准测试中,FlashOptim展示了:
- **零可测量质量下降**: 指标(准确率、困惑度、F1分数)在统计噪声范围内与全精度基线匹配
- **一致的收敛性**: 训练曲线在整个训练过程中与基线优化器对齐
- **优化器通用性**: 结果适用于SGD、AdamW和Lion优化器

### 内存节省分析

对于70亿参数模型:
- **基线AdamW**: 约112GB(每参数16字节)
- **FlashOptim AdamW**: 约49GB(每参数7字节)——减少56%
- **FlashOptim + 梯度释放**: 约35GB(每参数5字节)——减少69%

这些节省使得在消费级硬件上进行原本不可能的训练成为可能,使大模型训练的访问民主化。

### 检查点大小减少

通常存储优化器状态以实现可恢复训练的模型检查点,其大小显著减少:
- 70亿参数检查点从约112GB缩小到约49GB
- 这种减少加速了检查点的保存/加载并降低了存储成本
- 分布式训练场景受益于更快的检查点同步

## 影响与未来方向

### AI研究的民主化

FlashOptim的内存效率对AI研究社区具有重要意义。通过将内存需求减少超过50%,这些技术使得:
- 在现有硬件上训练更大的模型
- 使用消费级GPU进行研究
- 降低学术和小规模工业研究的云计算成本
- 能够实验以前无法触及的模型架构

### 理论见解

该工作提供了超越直接实际效益的宝贵理论贡献:
- 量化优化的严格误差界限提供了关于收敛行为的保证
- 压扩函数设计方法可应用于其他数值优化环境
- 对优化器状态分布的分析为未来的自适应精度训练工作提供信息

### 与现有框架的集成

FlashOptim对API兼容性的重视意味着它可以以最小的代码更改集成到现有的训练管道中。这一实际考虑加速了采用,并允许研究人员立即从内存节省中受益,而无需重写训练基础设施。

### 潜在扩展

几个有前景的未来工作方向浮现:
- **自适应精度**: 根据训练阶段动态调整量化级别
- **硬件协同设计**: 为压扩操作提供专门的加速器支持
- **扩展优化器覆盖**: 将技术应用于Sophia或Shampoo等更新的优化器
- **极端量化**: 探索具有更复杂纠错的4位或更低表示

## 要点总结

1. FlashOptim将AdamW内存从每参数16字节减少到7字节(减少56%)或使用梯度释放时5字节(减少69%),且无质量损失
2. 该方法结合了改进的主权重分割和用于8位优化器状态量化的新型压扩函数
3. 在包括Llama-3.1-8B微调在内的视觉和语言任务上的全面实验显示模型质量零可测量下降
4. 检查点大小减少超过50%,显著降低存储和传输成本
5. 这些技术适用于多个优化器(SGD、AdamW、Lion)并保持与现有训练框架的API兼容性
6. 内存节省使得在内存少于50GB的硬件上训练70亿参数模型成为可能,使大规模模型训练的访问民主化
7. 理论误差界限提供与全精度训练等效的收敛保证
:::
