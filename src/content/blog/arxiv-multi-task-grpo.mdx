---
title:
  en: "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks"
  zh: "多任务GRPO：跨任务的可靠LLM推理"
description:
  en: "MT-GRPO addresses task imbalance in multi-task RL fine-tuning with dynamic weight adaptation, achieving 16-28% worst-task improvement over standard GRPO"
  zh: "MT-GRPO通过动态权重适应解决多任务RL微调中的任务不平衡问题，最差任务性能比标准GRPO提升16-28%"
date: 2026-02-05
tags: ["arxiv", "ai", "grpo", "reinforcement-learning", "multi-task", "reasoning", "cs.CL", "cs.AI", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.05547](https://arxiv.org/abs/2602.05547)
**Authors**: Shyam Sundhar Ramesh, Xiaotong Ji, Matthieu Zimmer, Sangwoong Yoon, Zhiyong Wang, Haitham Bou Ammar, Aurelien Lucchi, Ilija Bogunovic
**Categories**: cs.CL, cs.AI, cs.LG

## Abstract

A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. MT-GRPO incorporates dynamic task weight adaptation targeting worst-case performance and a ratio-preserving sampler ensuring policy gradients align with adapted weights.

## Key Contributions

- **Dynamic task weight adaptation**: Automatically adjusts task weights to target worst-case performance
- **Ratio-preserving sampler**: Ensures policy gradients align with the adapted weights
- **Significant improvements**: 16-28% absolute improvement on worst-task performance over standard GRPO

## The Multi-Task Problem

When applying GRPO across multiple reasoning tasks simultaneously, a naive approach leads to **task imbalance**: some tasks dominate the optimization while others stagnate or regress. This happens because:

- Tasks have different difficulty levels and reward scales
- Easier tasks generate stronger gradient signals
- The model preferentially improves on tasks where it already performs well

## MT-GRPO: Two Key Innovations

### 1. Dynamic Task Weight Adaptation

Instead of fixed task weights, MT-GRPO dynamically adjusts weights to focus on the worst-performing task. This minimax-style objective ensures no task is left behind.

### 2. Ratio-Preserving Sampler

A critical implementation detail: the sampler ensures that the actual ratio of training examples matches the dynamically adapted weights. Without this, the policy gradient estimates become biased, undermining the weight adaptation.

## Results

- **16-28% absolute improvement** on worst-task performance over standard GRPO
- **6% improvement** over DAPO (another multi-task baseline)
- **50% fewer training steps** to reach 50% worst-task accuracy in 3-task settings
- Competitive average accuracy maintained — improving worst-case doesn't sacrifice overall performance

## Why This Matters

1. **Real-world relevance**: Production LLMs need to reason across many task types, not just one
2. **Fairness in capability**: Ensures the model doesn't sacrifice some reasoning abilities for others
3. **Training efficiency**: Faster convergence through better gradient utilization
4. **Practical GRPO extension**: Drop-in replacement for standard multi-task GRPO training
:::

:::zh
**论文**: [2602.05547](https://arxiv.org/abs/2602.05547)
**作者**: Shyam Sundhar Ramesh, Xiaotong Ji, Matthieu Zimmer, Sangwoong Yoon, Zhiyong Wang, Haitham Bou Ammar, Aurelien Lucchi, Ilija Bogunovic
**分类**: cs.CL, cs.AI, cs.LG

## 摘要

GRPO的直接多任务适应通常导致不平衡的结果，某些任务主导优化而其他任务停滞不前。MT-GRPO引入了针对最差情况性能的动态任务权重适应和确保策略梯度与适应权重对齐的比率保持采样器。

## 主要贡献

- **动态任务权重适应**：自动调整任务权重以针对最差情况性能
- **比率保持采样器**：确保策略梯度与适应后的权重对齐
- **显著改进**：最差任务性能比标准GRPO绝对提升16-28%

## 多任务问题

当同时在多个推理任务上应用GRPO时，朴素方法会导致**任务不平衡**：某些任务主导优化，而其他任务停滞或退化。原因包括：

- 任务具有不同的难度级别和奖励尺度
- 较简单的任务产生更强的梯度信号
- 模型优先改进已经表现良好的任务

## MT-GRPO：两个关键创新

### 1. 动态任务权重适应

MT-GRPO不使用固定任务权重，而是动态调整权重以关注表现最差的任务。这种极小极大风格的目标确保没有任务被落下。

### 2. 比率保持采样器

一个关键的实现细节：采样器确保训练样本的实际比率与动态适应的权重匹配。没有这一点，策略梯度估计会产生偏差，破坏权重适应的效果。

## 实验结果

- 最差任务性能比标准GRPO**绝对提升16-28%**
- 比DAPO（另一个多任务基线）**提升6%**
- 在3任务设置中达到50%最差任务准确率所需的**训练步数减少50%**
- 保持有竞争力的平均准确率——改善最差情况不会牺牲整体性能

## 重要意义

1. **现实世界相关性**：生产环境的LLM需要跨多种任务类型推理，而非仅一种
2. **能力公平性**：确保模型不会为了某些推理能力而牺牲其他能力
3. **训练效率**：通过更好的梯度利用实现更快收敛
4. **实用GRPO扩展**：标准多任务GRPO训练的即插即用替代方案
:::
