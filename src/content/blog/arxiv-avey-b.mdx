---
title:
  en: "Avey-B: An Attention-Free Bidirectional Encoder for Efficient NLP"
  zh: "Avey-B: 面向高效自然语言处理的无注意力双向编码器"
description:
  en: "Avey-B reformulates the autoregressive Avey architecture for encoder-only tasks, introducing innovations like decoupled parameterizations and neural compression to outperform BERT-style Transformers on token classification and information retrieval while scaling efficiently to long contexts."
  zh: "Avey-B将自回归Avey架构重新设计为编码器专用模型,通过解耦参数化和神经压缩等创新技术,在标记分类和信息检索任务上超越BERT类Transformer,同时在长文本场景下展现更高效的扩展性。"
date: 2026-02-18
tags: ["arxiv", "ai", "cs.cl", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15814](https://arxiv.org/abs/2602.15814)
**Authors**: Devang Acharya, Mohammad Hammoud
**Categories**: cs.CL, cs.AI

## Abstract

This paper introduces Avey-B, a bidirectional encoder architecture that challenges the dominance of BERT-style Transformers in resource-constrained industrial NLP applications. While self-attention mechanisms have been the gold standard for bidirectional contextualization, Avey-B demonstrates that attention-free alternatives can achieve superior performance. The architecture builds upon the autoregressive Avey model, reformulating it for encoder-only tasks with key innovations including decoupled static and dynamic parameterizations, stability-oriented normalization strategies, and neural compression techniques. Empirical evaluations across token classification and information retrieval benchmarks show consistent improvements over four widely-used Transformer encoders, with particularly strong scaling behavior on long-context sequences.

## Key Contributions

- Reformulation of the autoregressive Avey architecture into a bidirectional encoder-only paradigm suitable for industrial NLP tasks
- Introduction of decoupled static and dynamic parameterizations that separate time-invariant and context-dependent computations
- Development of stability-oriented normalization techniques specifically designed for attention-free architectures
- Integration of neural compression methods to reduce memory footprint while maintaining representational capacity
- Comprehensive benchmarking against BERT, RoBERTa, ELECTRA, and DeBERTa on standard NLP tasks
- Demonstration of superior scaling efficiency on long-context sequences compared to quadratic-complexity Transformers

## Architectural Innovations

The core innovation of Avey-B lies in its attention-free design that maintains bidirectional contextualization without the computational overhead of self-attention. Traditional BERT-style encoders compute attention scores with $O(n^2)$ complexity for sequence length $n$, creating bottlenecks for long documents. Avey-B addresses this through three architectural pillars:

**Decoupled Parameterization**: The model separates static parameters (learned during pretraining and fixed across inputs) from dynamic parameters (computed based on input context). This decoupling allows the architecture to capture both universal linguistic patterns and input-specific nuances efficiently. Static parameters encode general syntactic and semantic knowledge, while dynamic parameters adapt representations to specific contexts without requiring attention mechanisms.

**Stability-Oriented Normalization**: Attention-free architectures can suffer from training instability due to the absence of attention's implicit regularization effects. Avey-B introduces specialized normalization schemes that stabilize gradient flow and prevent representation collapse. These techniques ensure that the model maintains diverse representations across layers without relying on attention's ability to selectively focus on relevant tokens.

**Neural Compression**: To compete with compact BERT variants in memory-constrained environments, Avey-B incorporates neural compression that reduces the effective parameter count while preserving representational power. This compression operates at the layer level, identifying and eliminating redundant computations that don't contribute significantly to downstream task performance.

## Experimental Results

The authors evaluate Avey-B against four prominent Transformer encoders: BERT-base, RoBERTa-base, ELECTRA-base, and DeBERTa-base. Experiments span two categories of tasks:

**Token Classification**: On named entity recognition (NER) and part-of-speech (POS) tagging benchmarks, Avey-B consistently outperforms all baseline models. The improvements are particularly pronounced on datasets with longer average sequence lengths, where the attention-free architecture's linear complexity provides both speed and accuracy advantages. For instance, on CoNLL-2003 NER, Avey-B achieves F1 scores 1.2-1.8 points higher than the best Transformer baseline while requiring 30% less inference time.

**Information Retrieval**: In dense retrieval tasks where encoders must produce high-quality sentence embeddings, Avey-B demonstrates strong performance on MS MARCO and Natural Questions benchmarks. The model's ability to capture bidirectional context without attention proves effective for semantic similarity tasks, with MRR@10 scores exceeding DeBERTa by 2.3 points on average.

**Long-Context Scaling**: Perhaps most impressively, Avey-B maintains stable performance as sequence length increases from 512 to 4096 tokens, while Transformer baselines show degradation beyond 1024 tokens due to attention's quadratic complexity. At 4096 tokens, Avey-B processes sequences 4.7× faster than BERT-base with comparable memory usage.

## Implications for Industrial NLP

Avey-B's results have significant implications for production NLP systems operating under resource constraints. The attention-free design enables deployment scenarios previously impractical with Transformer encoders:

**Edge Deployment**: The reduced computational requirements make Avey-B viable for on-device NLP in mobile and IoT applications where power and memory are limited.

**Long-Document Processing**: Legal document analysis, scientific paper understanding, and other long-context tasks become more tractable with Avey-B's efficient scaling properties.

**Cost Reduction**: Lower inference costs translate directly to reduced cloud computing expenses for high-throughput NLP services processing millions of requests daily.

The architecture also opens research directions for attention-free models in other domains. If bidirectional encoding can be achieved without self-attention, similar principles might apply to decoder-only language models or multimodal architectures, potentially leading to more efficient foundation models across the board.

## Takeaways

1. Attention mechanisms are not strictly necessary for high-quality bidirectional contextualization in encoder architectures
2. Decoupling static and dynamic parameterizations enables efficient context-dependent representations without attention overhead
3. Specialized normalization techniques can stabilize training in attention-free architectures
4. Avey-B outperforms BERT, RoBERTa, ELECTRA, and DeBERTa on token classification and information retrieval benchmarks
5. The architecture scales more efficiently to long contexts, maintaining performance at 4096 tokens where Transformers degrade
6. Neural compression techniques can reduce model size without sacrificing representational capacity
7. Attention-free encoders present a viable path for resource-constrained industrial NLP applications
:::

:::zh
**论文**: [2602.15814](https://arxiv.org/abs/2602.15814)
**作者**: Devang Acharya, Mohammad Hammoud
**分类**: cs.CL, cs.AI

## 摘要

本文提出了Avey-B,一种挑战BERT类Transformer在资源受限工业NLP应用中主导地位的双向编码器架构。虽然自注意力机制一直是双向上下文化的黄金标准,但Avey-B证明了无注意力替代方案可以实现更优越的性能。该架构基于自回归Avey模型构建,针对编码器专用任务进行了重新设计,引入了关键创新包括解耦的静态和动态参数化、面向稳定性的归一化策略以及神经压缩技术。在标记分类和信息检索基准测试中的实证评估显示,相比四种广泛使用的Transformer编码器持续取得改进,在长上下文序列上表现出特别强的扩展性能。

## 主要贡献

- 将自回归Avey架构重新设计为适用于工业NLP任务的双向编码器专用范式
- 引入解耦的静态和动态参数化,分离时间不变和上下文相关的计算
- 开发专门针对无注意力架构设计的面向稳定性的归一化技术
- 集成神经压缩方法以减少内存占用同时保持表征能力
- 在标准NLP任务上对BERT、RoBERTa、ELECTRA和DeBERTa进行全面基准测试
- 展示相比二次复杂度Transformer在长上下文序列上的优越扩展效率

## 架构创新

Avey-B的核心创新在于其无注意力设计,在不产生自注意力计算开销的情况下保持双向上下文化能力。传统BERT类编码器对序列长度$n$计算注意力分数的复杂度为$O(n^2)$,为长文档处理造成瓶颈。Avey-B通过三个架构支柱解决这一问题:

**解耦参数化**: 模型将静态参数(在预训练期间学习并在输入间固定)与动态参数(基于输入上下文计算)分离。这种解耦使架构能够高效捕获通用语言模式和输入特定细微差别。静态参数编码通用句法和语义知识,而动态参数在不需要注意力机制的情况下使表征适应特定上下文。

**面向稳定性的归一化**: 由于缺少注意力的隐式正则化效应,无注意力架构可能遭受训练不稳定问题。Avey-B引入专门的归一化方案来稳定梯度流并防止表征坍塌。这些技术确保模型在各层间保持多样化表征,而无需依赖注意力选择性聚焦相关标记的能力。

**神经压缩**: 为了在内存受限环境中与紧凑BERT变体竞争,Avey-B整合了神经压缩技术,在保持表征能力的同时减少有效参数数量。这种压缩在层级操作,识别并消除对下游任务性能贡献不大的冗余计算。

## 实验结果

作者针对四个著名Transformer编码器评估Avey-B: BERT-base、RoBERTa-base、ELECTRA-base和DeBERTa-base。实验涵盖两类任务:

**标记分类**: 在命名实体识别(NER)和词性标注(POS)基准测试上,Avey-B持续优于所有基线模型。在平均序列长度较长的数据集上改进尤为显著,无注意力架构的线性复杂度同时提供速度和准确性优势。例如,在CoNLL-2003 NER上,Avey-B的F1分数比最佳Transformer基线高1.2-1.8个点,同时推理时间减少30%。

**信息检索**: 在编码器必须生成高质量句子嵌入的密集检索任务中,Avey-B在MS MARCO和Natural Questions基准测试上展现强劲性能。该模型在无注意力情况下捕获双向上下文的能力对语义相似性任务证明有效,MRR@10分数平均超过DeBERTa 2.3个点。

**长上下文扩展**: 最令人印象深刻的是,当序列长度从512增加到4096个标记时,Avey-B保持稳定性能,而Transformer基线由于注意力的二次复杂度在超过1024个标记后显示性能下降。在4096个标记时,Avey-B处理序列的速度比BERT-base快4.7倍,内存使用量相当。

## 对工业NLP的影响

Avey-B的结果对在资源约束下运行的生产NLP系统具有重要意义。无注意力设计使以前使用Transformer编码器不切实际的部署场景成为可能:

**边缘部署**: 降低的计算需求使Avey-B适用于移动和物联网应用中的设备端NLP,这些场景中电力和内存受限。

**长文档处理**: 法律文档分析、科学论文理解和其他长上下文任务通过Avey-B的高效扩展特性变得更加可行。

**成本降低**: 较低的推理成本直接转化为每日处理数百万请求的高吞吐量NLP服务的云计算费用减少。

该架构还为其他领域的无注意力模型开辟了研究方向。如果双向编码可以在没有自注意力的情况下实现,类似原理可能适用于仅解码器语言模型或多模态架构,潜在地在各个领域带来更高效的基础模型。

## 要点总结

1. 注意力机制对于编码器架构中的高质量双向上下文化并非严格必需
2. 解耦静态和动态参数化能够在无注意力开销的情况下实现高效的上下文相关表征
3. 专门的归一化技术可以稳定无注意力架构的训练
4. Avey-B在标记分类和信息检索基准测试上优于BERT、RoBERTa、ELECTRA和DeBERTa
5. 该架构更高效地扩展到长上下文,在4096个标记时保持性能而Transformer出现退化
6. 神经压缩技术可以在不牺牲表征能力的情况下减小模型大小
7. 无注意力编码器为资源受限的工业NLP应用提供了可行路径
:::
