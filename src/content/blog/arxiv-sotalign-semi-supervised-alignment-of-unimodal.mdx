---
title:
  en: "SOTAlign: Semi-Supervised Alignment of Vision and Language Models via Optimal Transport"
  zh: "SOTAlign: 通过最优传输实现视觉与语言模型的半监督对齐"
description:
  en: "A novel semi-supervised framework that aligns pretrained vision and language models using minimal paired data and optimal transport, significantly outperforming existing methods."
  zh: "一种新颖的半监督框架,使用少量配对数据和最优传输技术对齐预训练的视觉与语言模型,显著超越现有方法。"
date: 2026-02-27
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

![Concept animation](/arxiv-visuals/sotalign-semi-supervised-alignment-of-unimodal/ConceptScene.gif)



:::en
**Paper**: [2602.23353](https://arxiv.org/abs/2602.23353)
**Authors**: Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata
**Categories**: cs.LG, cs.AI

## Abstract

This paper introduces SOTAlign, a semi-supervised framework for aligning pretrained vision and language models with minimal supervision. Building on the Platonic Representation Hypothesis—which suggests neural networks trained on different modalities converge toward shared representations—the authors propose a two-stage approach that uses small amounts of paired image-text data alongside large quantities of unpaired samples. The method first establishes a coarse alignment using a linear teacher on limited paired data, then refines this alignment through an optimal-transport-based divergence that preserves relational structure in the unpaired data. SOTAlign demonstrates substantial improvements over supervised and semi-supervised baselines across multiple datasets and encoder combinations.

## Key Contributions

- Introduction of a semi-supervised alignment setting that dramatically reduces the need for paired image-text data while maintaining or improving alignment quality
- A two-stage framework combining linear teacher initialization with optimal transport refinement for robust cross-modal alignment
- Novel application of optimal transport divergence to transfer relational structure from unpaired data without overconstraining the embedding space
- Comprehensive experimental validation showing significant performance gains over existing supervised and semi-supervised methods across diverse encoder pairs and datasets

## Methodology: Two-Stage Alignment Framework

SOTAlign operates through two distinct phases designed to maximize the utility of both paired and unpaired data.

**Stage 1: Linear Teacher Initialization**

The first stage establishes a coarse alignment using the limited paired samples available. Given pretrained vision encoder $f_v$ and language encoder $f_l$, the method learns a lightweight linear projection layer that maps between the two embedding spaces. This linear teacher provides an initial geometric correspondence that serves as the foundation for subsequent refinement. The simplicity of the linear mapping ensures that the method can learn meaningful structure even from very small paired datasets (e.g., 1-10% of typical training data).

**Stage 2: Optimal Transport Refinement**

The second stage leverages unpaired images and text to refine the alignment. Rather than treating unpaired samples as independent points, SOTAlign uses optimal transport to capture and transfer the relational structure within each modality. The key insight is that while individual samples may not have cross-modal correspondences, the geometric relationships between samples within a modality (e.g., semantic similarity between images) should be preserved in the aligned space.

The optimal transport formulation minimizes a divergence measure:

$$\mathcal{L}_{OT} = W_2(\mu_v, \mu_l)$$

where $W_2$ denotes the 2-Wasserstein distance between the distributions of vision embeddings $\mu_v$ and language embeddings $\mu_l$ after projection. This approach allows the model to learn from the rich structural information in unpaired data without requiring explicit correspondences, effectively transferring intra-modal relationships to the joint embedding space.

## Experimental Results and Analysis

The authors conduct extensive experiments across multiple benchmark datasets and encoder combinations, demonstrating SOTAlign's effectiveness in low-supervision regimes.

**Performance with Limited Paired Data**

When trained with only 1-10% of paired samples, SOTAlign consistently outperforms fully supervised baselines trained on 100% of paired data. This remarkable result suggests that the method successfully exploits the abundant unpaired data to compensate for limited supervision. The performance gap widens as the amount of paired data decreases, highlighting the framework's robustness in extreme low-resource scenarios.

**Comparison with Semi-Supervised Baselines**

SOTAlign significantly outperforms existing semi-supervised alignment methods that rely on pseudo-labeling or consistency regularization. Unlike these approaches, which often struggle to effectively utilize unpaired data from both modalities simultaneously, SOTAlign's optimal transport formulation naturally handles asymmetric unpaired data and preserves the distinct geometric properties of each modality.

**Cross-Dataset and Cross-Encoder Generalization**

The learned alignments demonstrate strong generalization across different datasets and encoder architectures. When aligned encoders are evaluated on downstream tasks like zero-shot classification and cross-modal retrieval, SOTAlign maintains competitive performance even when the test distribution differs from training. This robustness suggests that the method learns fundamental cross-modal correspondences rather than dataset-specific artifacts.

## Implications and Future Directions

SOTAlign addresses a critical bottleneck in multimodal learning: the requirement for massive paired datasets. By demonstrating that effective alignment can be achieved with minimal supervision, this work opens several promising research directions.

**Practical Impact**: The reduced data requirements make cross-modal alignment accessible for domains where paired data is expensive or difficult to obtain, such as medical imaging with clinical reports, scientific figures with captions, or specialized technical domains.

**Theoretical Insights**: The success of optimal transport in this context provides empirical support for the Platonic Representation Hypothesis, suggesting that pretrained models do indeed converge toward compatible geometric structures that can be aligned with minimal supervision.

**Scalability**: The two-stage design allows for efficient training, as the computationally intensive optimal transport calculations operate on batches of unpaired data rather than requiring exhaustive pairwise comparisons. This makes the approach practical for large-scale applications.

Future work could explore extending SOTAlign to more than two modalities, investigating different optimal transport formulations, or combining the approach with self-supervised pretraining objectives to further reduce supervision requirements.

## Takeaways

1. Meaningful vision-language alignment can be achieved with dramatically less paired data than previously thought necessary—as little as 1-10% of typical training sets
2. Optimal transport provides an effective mechanism for transferring relational structure from unpaired data, preserving intra-modal geometry while learning cross-modal correspondences
3. The two-stage framework (linear teacher + OT refinement) balances computational efficiency with alignment quality, making semi-supervised alignment practical at scale
4. SOTAlign's strong generalization across datasets and encoder architectures suggests it learns fundamental cross-modal relationships rather than superficial correlations
5. This work validates the Platonic Representation Hypothesis in practice, demonstrating that pretrained models from different modalities possess compatible geometric structures that can be aligned with minimal supervision
:::

:::zh
**论文**: [2602.23353](https://arxiv.org/abs/2602.23353)
**作者**: Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata
**分类**: cs.LG, cs.AI

## 摘要

本文提出了SOTAlign,一个用于对齐预训练视觉和语言模型的半监督框架,仅需最少的监督数据。基于柏拉图表征假设——即在不同模态上训练的神经网络会收敛到共享的世界表征——作者提出了一种两阶段方法,使用少量配对的图像-文本数据和大量未配对样本。该方法首先在有限的配对数据上使用线性教师建立粗略对齐,然后通过基于最优传输的散度在未配对数据上精炼对齐,在保持关系结构的同时不过度约束目标空间。SOTAlign在多个数据集和编码器组合上展示了相比监督和半监督基线的显著改进。

## 主要贡献

- 引入半监督对齐设置,大幅减少对配对图像-文本数据的需求,同时保持或提升对齐质量
- 提出两阶段框架,结合线性教师初始化和最优传输精炼,实现鲁棒的跨模态对齐
- 创新性地应用最优传输散度从未配对数据中转移关系结构,而不过度约束嵌入空间
- 全面的实验验证表明,在不同编码器对和数据集上,该方法显著优于现有的监督和半监督方法

## 方法论:两阶段对齐框架

SOTAlign通过两个不同阶段运作,旨在最大化利用配对和未配对数据。

**阶段1:线性教师初始化**

第一阶段使用有限的配对样本建立粗略对齐。给定预训练的视觉编码器$f_v$和语言编码器$f_l$,该方法学习一个轻量级线性投影层,在两个嵌入空间之间建立映射。这个线性教师提供了初始的几何对应关系,作为后续精炼的基础。线性映射的简单性确保了即使在非常小的配对数据集(例如典型训练数据的1-10%)上也能学习到有意义的结构。

**阶段2:最优传输精炼**

第二阶段利用未配对的图像和文本来精炼对齐。SOTAlign不是将未配对样本视为独立点,而是使用最优传输来捕获和转移每个模态内的关系结构。关键洞察是,虽然单个样本可能没有跨模态对应关系,但模态内样本之间的几何关系(例如图像之间的语义相似性)应该在对齐空间中得到保留。

最优传输公式最小化散度度量:

$$\mathcal{L}_{OT} = W_2(\mu_v, \mu_l)$$

其中$W_2$表示投影后视觉嵌入分布$\mu_v$和语言嵌入分布$\mu_l$之间的2-Wasserstein距离。这种方法允许模型从未配对数据中丰富的结构信息中学习,而不需要显式的对应关系,有效地将模态内关系转移到联合嵌入空间。

## 实验结果与分析

作者在多个基准数据集和编码器组合上进行了广泛实验,展示了SOTAlign在低监督场景下的有效性。

**有限配对数据下的性能**

当仅使用1-10%的配对样本训练时,SOTAlign始终优于使用100%配对数据训练的完全监督基线。这一显著结果表明,该方法成功利用了丰富的未配对数据来补偿有限的监督。随着配对数据量的减少,性能差距扩大,突显了该框架在极端低资源场景下的鲁棒性。

**与半监督基线的比较**

SOTAlign显著优于依赖伪标签或一致性正则化的现有半监督对齐方法。与这些往往难以有效同时利用两种模态未配对数据的方法不同,SOTAlign的最优传输公式自然地处理非对称的未配对数据,并保留每个模态的独特几何属性。

**跨数据集和跨编码器泛化**

学习到的对齐在不同数据集和编码器架构上展示了强大的泛化能力。当对齐的编码器在零样本分类和跨模态检索等下游任务上评估时,即使测试分布与训练不同,SOTAlign也保持了竞争力的性能。这种鲁棒性表明该方法学习的是基本的跨模态对应关系,而非数据集特定的伪影。

## 影响与未来方向

SOTAlign解决了多模态学习中的一个关键瓶颈:对大规模配对数据集的需求。通过证明可以用最少的监督实现有效对齐,这项工作开启了几个有前景的研究方向。

**实际影响**:降低的数据需求使得跨模态对齐在配对数据昂贵或难以获取的领域变得可行,例如带有临床报告的医学影像、带有标题的科学图表或专业技术领域。

**理论洞察**:最优传输在此背景下的成功为柏拉图表征假设提供了经验支持,表明预训练模型确实收敛到可以用最少监督对齐的兼容几何结构。

**可扩展性**:两阶段设计允许高效训练,因为计算密集的最优传输计算在未配对数据批次上操作,而不需要穷举的成对比较。这使得该方法在大规模应用中具有实用性。

未来工作可以探索将SOTAlign扩展到两个以上的模态,研究不同的最优传输公式,或将该方法与自监督预训练目标结合以进一步减少监督需求。

## 要点总结

1. 有意义的视觉-语言对齐可以用比以前认为必要的少得多的配对数据实现——仅需典型训练集的1-10%
2. 最优传输提供了从未配对数据转移关系结构的有效机制,在学习跨模态对应关系的同时保留模态内几何结构
3. 两阶段框架(线性教师+最优传输精炼)平衡了计算效率和对齐质量,使半监督对齐在规模上变得实用
4. SOTAlign在数据集和编码器架构上的强泛化能力表明它学习的是基本的跨模态关系而非表面相关性
5. 这项工作在实践中验证了柏拉图表征假设,证明来自不同模态的预训练模型具有可以用最少监督对齐的兼容几何结构
:::
