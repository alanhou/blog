---
title:
  en: "Shared LoRA Subspaces for Almost Strict Continual Learning"
  zh: "共享LoRA子空间实现近乎严格的持续学习"
description:
  en: "A novel approach using shared low-rank subspaces across tasks to enable continual learning with minimal forgetting in large language models"
  zh: "一种使用跨任务共享低秩子空间的新方法，在大语言模型中实现最小遗忘的持续学习"
date: 2026-02-06
tags: ["arxiv", "ai", "lora", "continual-learning", "llm", "fine-tuning", "catastrophic-forgetting", "cs.LG"]
image: "/arxiv-visuals/arxiv-shared-lora-subspaces.png"
---

:::en
**Paper**: [2602.06043](https://arxiv.org/abs/2602.06043)
**Categories**: cs.LG, cs.CL

## Abstract

Continual learning in large language models faces the fundamental challenge of catastrophic forgetting - when learning new tasks degrades performance on previously learned ones. This paper introduces Shared LoRA Subspaces, a method that leverages the low-rank structure of LoRA adapters to enable almost strict continual learning, where knowledge from previous tasks is preserved while efficiently learning new ones.

## The Problem: Catastrophic Forgetting

When fine-tuning LLMs sequentially on multiple tasks:
- Learning task $T_2$ overwrites weights learned for task $T_1$
- Performance on earlier tasks degrades significantly
- Naive solutions require storing separate models per task

### Traditional Approaches

| Method | Storage | Forgetting | Efficiency |
|--------|---------|------------|------------|
| Full fine-tuning | High | Severe | Low |
| Separate LoRA per task | Linear growth | None | Moderate |
| Regularization methods | Low | Moderate | High |

## Key Contributions

- **Shared Subspace Discovery**: Identifies common low-rank subspaces across tasks that can be reused
- **Almost Strict Continual Learning**: Achieves near-zero forgetting while maintaining plasticity
- **Parameter Efficiency**: Sublinear growth in parameters as tasks increase
- **Theoretical Foundation**: Proves conditions under which shared subspaces preserve task performance

## Methodology

### LoRA Recap

Standard LoRA decomposes weight updates as:

$$\Delta W = BA$$

Where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d, k)$.

### Shared Subspace Formulation

Instead of independent LoRA adapters per task, decompose as:

$$\Delta W_t = B_{\text{shared}} A_t + B_t A_t^{\text{task}}$$

Where:
- $B_{\text{shared}}$ captures common structure across tasks
- $A_t$ projects task-specific information into shared space
- $B_t A_t^{\text{task}}$ handles task-specific residuals

### Subspace Identification

The shared subspace is identified by analyzing the principal components of LoRA updates across tasks:

$$B_{\text{shared}} = \text{SVD}_r\left(\sum_{i=1}^{t-1} B_i A_i\right)$$

This captures the dominant directions of adaptation that generalize across tasks.

### Orthogonal Projection for Forgetting Prevention

To prevent forgetting, new task updates are projected orthogonally to the shared subspace:

$$\Delta W_t^{\perp} = (I - B_{\text{shared}} B_{\text{shared}}^T) \Delta W_t$$

This ensures new learning does not interfere with previously learned representations.

## Almost Strict Continual Learning

The method achieves "almost strict" continual learning:

$$\mathcal{L}_{T_i}(\theta + \Delta\theta_t) \leq \mathcal{L}_{T_i}(\theta + \Delta\theta_i) + \epsilon$$

Where $\epsilon$ is bounded and decreases as the shared subspace better captures task structure.

### Theoretical Guarantee

Under mild assumptions on task similarity:

$$\|\Delta W_{\text{shared}}\|_F \leq \sqrt{\sum_{i=1}^t \|\Delta W_i\|_F^2 / t}$$

The shared representation grows sublinearly with the number of tasks.

## Results

### Benchmark Performance

On continual learning benchmarks with LLaMA-based models:

| Method | Avg. Accuracy | Forgetting | Parameters |
|--------|---------------|------------|------------|
| Sequential LoRA | 72.3% | 15.2% | $O(t \cdot r)$ |
| EWC + LoRA | 76.8% | 8.4% | $O(t \cdot r)$ |
| Shared LoRA (ours) | 81.2% | 1.3% | $O(r + t \cdot r')$ |

Where $r' \ll r$ for task-specific residuals.

### Scaling Behavior

- Forgetting remains below 2% even after 10+ sequential tasks
- Shared subspace stabilizes after 3-4 tasks
- Parameter efficiency improves with more tasks

## Why This Matters

1. **Practical Deployment**: Single model can handle multiple tasks without degradation
2. **Resource Efficiency**: Avoids storing separate adapters per task
3. **Knowledge Transfer**: Shared subspace enables positive transfer between related tasks
4. **Theoretical Grounding**: Provides guarantees on forgetting bounds

## Takeaways

1. **LoRA subspaces can be shared**: Common low-rank structure exists across diverse tasks
2. **Orthogonal projection prevents forgetting**: New learning in orthogonal directions preserves old knowledge
3. **Sublinear parameter growth**: Shared subspace amortizes storage across tasks
4. **Almost strict is achievable**: Near-zero forgetting with bounded theoretical guarantees
5. **Practical for LLM deployment**: Enables efficient multi-task models without separate fine-tuning
:::

:::zh
**论文**: [2602.06043](https://arxiv.org/abs/2602.06043)
**分类**: cs.LG, cs.CL

## 摘要

大语言模型的持续学习面临灾难性遗忘的根本挑战——学习新任务时会降低先前学习任务的性能。本文介绍了共享LoRA子空间方法，利用LoRA适配器的低秩结构实现近乎严格的持续学习，在高效学习新任务的同时保留先前任务的知识。

## 问题：灾难性遗忘

当在多个任务上顺序微调LLM时：
- 学习任务 $T_2$ 会覆盖为任务 $T_1$ 学习的权重
- 早期任务的性能显著下降
- 朴素解决方案需要为每个任务存储单独的模型

### 传统方法

| 方法 | 存储 | 遗忘 | 效率 |
|------|------|------|------|
| 全量微调 | 高 | 严重 | 低 |
| 每任务单独LoRA | 线性增长 | 无 | 中等 |
| 正则化方法 | 低 | 中等 | 高 |

## 主要贡献

- **共享子空间发现**：识别可跨任务重用的公共低秩子空间
- **近乎严格的持续学习**：在保持可塑性的同时实现近零遗忘
- **参数效率**：随着任务增加，参数亚线性增长
- **理论基础**：证明共享子空间保持任务性能的条件

## 方法论

### LoRA回顾

标准LoRA将权重更新分解为：

$$\Delta W = BA$$

其中 $B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，秩 $r \ll \min(d, k)$。

### 共享子空间公式

不是为每个任务使用独立的LoRA适配器，而是分解为：

$$\Delta W_t = B_{\text{shared}} A_t + B_t A_t^{\text{task}}$$

其中：
- $B_{\text{shared}}$ 捕获跨任务的公共结构
- $A_t$ 将任务特定信息投影到共享空间
- $B_t A_t^{\text{task}}$ 处理任务特定的残差

### 子空间识别

通过分析跨任务LoRA更新的主成分来识别共享子空间：

$$B_{\text{shared}} = \text{SVD}_r\left(\sum_{i=1}^{t-1} B_i A_i\right)$$

这捕获了跨任务泛化的主要适应方向。

### 正交投影防止遗忘

为防止遗忘，新任务更新被正交投影到共享子空间：

$$\Delta W_t^{\perp} = (I - B_{\text{shared}} B_{\text{shared}}^T) \Delta W_t$$

这确保新学习不会干扰先前学习的表示。

## 近乎严格的持续学习

该方法实现"近乎严格"的持续学习：

$$\mathcal{L}_{T_i}(\theta + \Delta\theta_t) \leq \mathcal{L}_{T_i}(\theta + \Delta\theta_i) + \epsilon$$

其中 $\epsilon$ 是有界的，并随着共享子空间更好地捕获任务结构而减小。

### 理论保证

在任务相似性的温和假设下：

$$\|\Delta W_{\text{shared}}\|_F \leq \sqrt{\sum_{i=1}^t \|\Delta W_i\|_F^2 / t}$$

共享表示随任务数量亚线性增长。

## 实验结果

### 基准性能

在基于LLaMA模型的持续学习基准上：

| 方法 | 平均准确率 | 遗忘率 | 参数量 |
|------|------------|--------|--------|
| 顺序LoRA | 72.3% | 15.2% | $O(t \cdot r)$ |
| EWC + LoRA | 76.8% | 8.4% | $O(t \cdot r)$ |
| 共享LoRA（本文） | 81.2% | 1.3% | $O(r + t \cdot r')$ |

其中 $r' \ll r$ 用于任务特定残差。

### 扩展行为

- 即使在10+个顺序任务后，遗忘率仍保持在2%以下
- 共享子空间在3-4个任务后稳定
- 参数效率随任务增多而提高

## 重要意义

1. **实际部署**：单一模型可以处理多个任务而不降级
2. **资源效率**：避免为每个任务存储单独的适配器
3. **知识迁移**：共享子空间使相关任务之间能够正向迁移
4. **理论基础**：提供遗忘界限的保证

## 要点总结

1. **LoRA子空间可以共享**：跨不同任务存在公共低秩结构
2. **正交投影防止遗忘**：在正交方向上的新学习保留旧知识
3. **参数亚线性增长**：共享子空间在任务间分摊存储
4. **近乎严格是可实现的**：具有有界理论保证的近零遗忘
5. **适用于LLM部署**：无需单独微调即可实现高效多任务模型
:::
