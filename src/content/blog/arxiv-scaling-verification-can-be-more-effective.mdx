---
title:
  en: "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment"
  zh: "扩展验证比扩展策略学习更有效:视觉-语言-动作对齐的新范式"
description:
  en: "This paper introduces CoVer, a contrastive verifier that demonstrates test-time verification scaling can outperform policy pre-training scaling for robot instruction following, achieving up to 45% improvement in real-world experiments."
  zh: "本文提出CoVer对比验证器,证明测试时验证扩展在机器人指令跟随任务中优于策略预训练扩展,在真实世界实验中实现高达45%的性能提升。"
date: 2026-02-13
tags: ["arxiv", "ai", "cs.ro", "cs.ai", "eess.sy"]
image: "/arxiv-visuals/arxiv-scaling-verification-can-be-more-effective.png"
---

:::en
**Paper**: [2602.12281](https://arxiv.org/abs/2602.12281)
**Authors**: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
**Categories**: cs.RO, cs.AI, eess.SY

## Abstract

Vision-Language-Action (VLA) models represent a significant step toward general-purpose robots that can understand and execute natural language instructions. However, a persistent challenge remains: the "intention-action gap" where generated actions misalign with given instructions. This paper investigates test-time verification as a solution, characterizing scaling laws for embodied instruction following and demonstrating that jointly scaling rephrased instructions and generated actions increases sample diversity more efficiently than independent scaling. The authors present CoVer, a contrastive verifier for vision-language-action alignment that scales gracefully with computational resources and data. Their framework introduces "boot-time compute" and hierarchical verification, precomputing diverse instruction rephrasings, generating multiple action candidates, and using verification to select optimal prompts and action chunks. Results show 22% in-distribution and 13% out-of-distribution gains on SIMPLER benchmark, with 45% improvement in real-world experiments compared to scaling policy pre-training.

## Key Contributions

- **Test-Time Scaling Laws**: Characterization of scaling behavior for embodied instruction following, showing that joint scaling of instruction rephrasings and action candidates outperforms independent scaling
- **CoVer Architecture**: A contrastive verifier designed specifically for vision-language-action alignment that demonstrates strong scaling properties with additional compute and data
- **Boot-Time Compute Framework**: Novel deployment strategy that precomputes diverse instruction rephrasings and generates multiple action candidates for hierarchical verification
- **Empirical Validation**: Comprehensive evaluation showing verification scaling outperforms policy pre-training scaling across multiple benchmarks and real-world scenarios

## Methodology and Technical Approach

The paper's core insight revolves around shifting computational investment from pre-training to test-time verification. Traditional approaches focus on scaling policy models during pre-training, but this work demonstrates that test-time compute can be more efficiently allocated.

**CoVer Architecture**: The contrastive verifier operates by learning to distinguish between aligned and misaligned vision-language-action triplets. Unlike generative models that must learn the full action distribution, the verifier only needs to rank candidates, making it more sample-efficient and scalable.

**Hierarchical Verification Pipeline**: The inference process operates in two stages:
1. **High-level verification**: Select the best instruction rephrasing from VLM-generated alternatives
2. **Low-level verification**: Choose optimal action chunks from multiple policy-generated candidates

This hierarchical approach allows the system to correct misalignments at both the semantic (instruction understanding) and execution (action generation) levels.

**Boot-Time Compute**: By precomputing instruction rephrasings before deployment, the system amortizes computational cost across multiple queries while maintaining diversity in the action space. This strategy proves particularly effective when deployment scenarios allow for preprocessing.

## Experimental Results and Analysis

The empirical evaluation spans simulation benchmarks and real-world robotic experiments, providing comprehensive evidence for the verification scaling hypothesis.

**SIMPLER Benchmark Performance**:
- 22% improvement in-distribution over baseline policy scaling
- 13% improvement out-of-distribution, demonstrating generalization
- Consistent gains across different task categories and complexity levels

**PolaRiS Benchmark Results**:
- 14% increase in task progress metrics
- 9% improvement in overall success rate
- Particularly strong performance on long-horizon manipulation tasks

**Real-World Experiments**: The 45% improvement in physical robot experiments represents the most compelling evidence. Real-world settings introduce noise, uncertainty, and distribution shift that simulation cannot fully capture, making these gains especially significant.

**Scaling Analysis**: The paper demonstrates that verification scales more favorably than policy pre-training when given equivalent computational budgets. The test-time scaling curve shows continued improvement with additional samples, while policy scaling exhibits diminishing returns beyond certain model sizes.

## Implications for Robotics and AI

This work challenges the prevailing paradigm in embodied AI that emphasizes ever-larger pre-trained models. Several implications emerge:

**Computational Efficiency**: Test-time verification offers a more accessible path to improved performance, as it requires less infrastructure than massive pre-training runs. Organizations with limited computational resources can leverage verification to enhance existing models.

**Deployment Flexibility**: The boot-time compute framework enables adaptation to specific deployment contexts without retraining. Different environments or task distributions can be addressed by adjusting verification strategies rather than model parameters.

**Interpretability and Safety**: Verification provides explicit reasoning about action selection, offering transparency that pure end-to-end policies lack. This interpretability becomes crucial for safety-critical applications where understanding failure modes matters.

**Future Research Directions**: The success of verification scaling suggests investigating similar approaches in other domains where test-time compute can be strategically allocated, such as code generation, mathematical reasoning, or multi-agent coordination.

## Takeaways

1. Test-time verification scaling can outperform policy pre-training scaling for vision-language-action alignment, achieving 22% in-distribution and 45% real-world improvements
2. Joint scaling of instruction rephrasings and action candidates proves more efficient than scaling either dimension independently
3. CoVer's contrastive verification architecture demonstrates strong scaling properties with additional computational resources and training data
4. Boot-time compute strategies enable preprocessing that amortizes computational cost while maintaining action diversity
5. Hierarchical verification at both instruction and action levels effectively addresses the intention-action gap in embodied AI systems
:::

:::zh
**论文**: [2602.12281](https://arxiv.org/abs/2602.12281)
**作者**: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
**分类**: cs.RO, cs.AI, eess.SY

## 摘要

视觉-语言-动作(VLA)模型代表了通用机器人发展的重要一步,这类机器人能够理解并执行自然语言指令。然而,一个持续存在的挑战是"意图-动作鸿沟",即生成的动作与给定指令不匹配。本文研究测试时验证作为解决方案,刻画了具身指令跟随的扩展定律,并证明联合扩展重述指令和生成动作比独立扩展更能有效增加样本多样性。作者提出CoVer,一个专门用于视觉-语言-动作对齐的对比验证器,能够随计算资源和数据优雅扩展。该框架引入"启动时计算"和层次化验证,预计算多样化的指令重述,生成多个动作候选,并使用验证器选择最优提示和动作块。结果显示在SIMPLER基准上分布内提升22%、分布外提升13%,在真实世界实验中相比扩展策略预训练提升45%。

## 主要贡献

- **测试时扩展定律**:刻画具身指令跟随的扩展行为,表明指令重述和动作候选的联合扩展优于独立扩展
- **CoVer架构**:专门为视觉-语言-动作对齐设计的对比验证器,展现出随额外计算和数据的强扩展特性
- **启动时计算框架**:新颖的部署策略,预计算多样化指令重述并生成多个动作候选用于层次化验证
- **实证验证**:全面评估显示验证扩展在多个基准和真实场景中优于策略预训练扩展

## 方法论与技术路径

本文的核心洞察在于将计算投资从预训练转移到测试时验证。传统方法专注于预训练期间扩展策略模型,但这项工作证明测试时计算可以更高效地分配。

**CoVer架构**:对比验证器通过学习区分对齐和不对齐的视觉-语言-动作三元组来运作。与必须学习完整动作分布的生成模型不同,验证器只需对候选进行排序,使其更具样本效率和可扩展性。

**层次化验证流程**:推理过程分两个阶段运行:
1. **高层验证**:从VLM生成的备选方案中选择最佳指令重述
2. **低层验证**:从多个策略生成的候选中选择最优动作块

这种层次化方法使系统能够在语义(指令理解)和执行(动作生成)两个层面纠正不对齐。

**启动时计算**:通过在部署前预计算指令重述,系统在多个查询中分摊计算成本,同时保持动作空间的多样性。当部署场景允许预处理时,这种策略特别有效。

## 实验结果与分析

实证评估涵盖仿真基准和真实世界机器人实验,为验证扩展假设提供了全面证据。

**SIMPLER基准性能**:
- 相比基线策略扩展,分布内提升22%
- 分布外提升13%,展现泛化能力
- 在不同任务类别和复杂度级别上持续获得提升

**PolaRiS基准结果**:
- 任务进度指标提升14%
- 整体成功率提升9%
- 在长时域操作任务上表现尤为突出

**真实世界实验**:物理机器人实验中45%的提升代表了最有说服力的证据。真实世界环境引入了仿真无法完全捕捉的噪声、不确定性和分布偏移,使这些提升尤为显著。

**扩展分析**:论文证明在给定等效计算预算时,验证扩展比策略预训练更有利。测试时扩展曲线显示随着额外样本持续改进,而策略扩展在超过某些模型规模后表现出收益递减。

## 对机器人学和人工智能的影响

这项工作挑战了具身AI中强调越来越大的预训练模型的主流范式。几个影响浮现:

**计算效率**:测试时验证提供了一条更易获得的性能改进路径,因为它比大规模预训练运行需要更少的基础设施。计算资源有限的组织可以利用验证来增强现有模型。

**部署灵活性**:启动时计算框架能够适应特定部署环境而无需重新训练。不同环境或任务分布可以通过调整验证策略而非模型参数来解决。

**可解释性与安全性**:验证提供了关于动作选择的明确推理,提供了纯端到端策略所缺乏的透明度。这种可解释性对于理解失败模式至关重要的安全关键应用变得关键。

**未来研究方向**:验证扩展的成功表明在其他可以策略性分配测试时计算的领域研究类似方法,如代码生成、数学推理或多智能体协调。

## 要点总结

1. 测试时验证扩展在视觉-语言-动作对齐中可以优于策略预训练扩展,实现分布内22%和真实世界45%的提升
2. 指令重述和动作候选的联合扩展比独立扩展任一维度更高效
3. CoVer的对比验证架构展现出随额外计算资源和训练数据的强扩展特性
4. 启动时计算策略能够实现预处理,在保持动作多样性的同时分摊计算成本
5. 指令和动作两个层面的层次化验证有效解决了具身AI系统中的意图-动作鸿沟
:::
