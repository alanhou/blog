---
title:
  en: "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs"
  zh: "从试错中学习:具身大语言模型的反思式测试时规划"
description:
  en: "A novel framework enabling embodied AI agents to learn from mistakes through dual reflection mechanisms—planning before action and learning after execution—significantly improving long-horizon task performance."
  zh: "一种新颖的框架,使具身AI智能体能够通过双重反思机制从错误中学习——行动前规划和执行后学习——显著提升长期任务性能。"
date: 2026-02-25
tags: ["arxiv", "ai", "cs.lg", "cs.ai", "cs.cl", "cs.cv", "cs.ro"]
image: "/arxiv-visuals/learning-from-trials-and-errors-reflective/HeroScene.png"
---

![Hero diagram](/arxiv-visuals/learning-from-trials-and-errors-reflective/HeroScene.png)



:::en
**Paper**: [2602.21198](https://arxiv.org/abs/2602.21198)
**Authors**: Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu, Yejin Choi
**Categories**: cs.LG, cs.AI, cs.CL, cs.CV, cs.RO

## Abstract

Current embodied LLMs excel at high-level reasoning but lack the ability to reflect on failures, causing robots to repeat mistakes rather than learn from experience. This paper introduces Reflective Test-Time Planning (RTTP), a framework inspired by human reflective practice that enables agents to learn continuously during deployment. The approach combines reflection-in-action (generating and evaluating multiple candidate actions before execution using test-time scaling) with reflection-on-action (updating both reflection models and action policies after execution through test-time training). A retrospective reflection mechanism allows agents to reassess past decisions with hindsight for proper credit assignment in long-horizon tasks. Evaluated on the newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark, RTTP demonstrates substantial improvements over baseline models, with ablation studies confirming the synergistic value of both reflection modes.

## Key Contributions

- Introduction of Reflective Test-Time Planning (RTTP), a dual-mode reflection framework that enables embodied agents to learn from mistakes during deployment
- Reflection-in-action mechanism using test-time scaling to generate and score multiple action candidates with internal reflections before execution
- Reflection-on-action mechanism employing test-time training to update both reflection models and action policies based on external feedback
- Retrospective reflection capability allowing agents to re-evaluate earlier decisions with hindsight for long-horizon credit assignment
- Two new benchmarks: Long-Horizon Household tasks and MuJoCo Cupboard Fitting, designed to evaluate reflective learning in embodied AI
- Comprehensive experimental validation including real-robot trials demonstrating behavioral correction through reflection

## Methodology: Dual-Mode Reflection Architecture

The RTTP framework operationalizes two complementary reflection modes inspired by Donald Schön's theory of reflective practice. 

**Reflection-in-Action** occurs before execution and leverages test-time compute scaling. The agent generates $N$ candidate actions for the current state, then uses an internal reflection model $R_{\text{internal}}$ to evaluate each candidate by simulating potential outcomes and identifying failure modes. This produces a scored set of actions: $\{(a_1, s_1), (a_2, s_2), ..., (a_N, s_N)\}$ where $s_i$ represents the reflection score. The highest-scoring action is selected for execution, effectively implementing a best-of-N sampling strategy guided by learned reflections rather than random sampling.

**Reflection-on-Action** occurs after execution and employs test-time training. Upon completing an action (or subtask), the agent receives external feedback—either from environment rewards, human corrections, or automated verification. This feedback is used to compute a loss signal that updates both the reflection model $R_{\text{internal}}$ and the action policy $\pi_{\theta}$ through gradient descent. The reflection model learns to better predict which actions will succeed or fail, while the policy learns to generate better initial action candidates.

**Retrospective Reflection** extends reflection-on-action by allowing the agent to revisit earlier decisions in a trajectory. When a failure occurs, the agent traces back through its decision history to identify which earlier choices contributed to the failure, enabling proper credit assignment in long-horizon tasks where consequences of actions may not be immediately apparent.

## Experimental Results and Analysis

The authors evaluate RTTP on two challenging benchmarks requiring long-horizon planning and error recovery.

**Long-Horizon Household Benchmark**: This new benchmark consists of complex household tasks requiring 10-20 steps, such as "prepare breakfast" (requiring retrieving items from multiple locations, using appliances in sequence, and handling failures like spills). RTTP achieves a 73% success rate compared to 42% for the baseline embodied LLM and 58% for a model with only reflection-in-action. The full RTTP system shows particular strength in tasks requiring error recovery—when an initial plan fails, the reflection-on-action mechanism enables the agent to adjust its strategy rather than repeating the same mistake.

**MuJoCo Cupboard Fitting**: This benchmark tests fine-grained manipulation and spatial reasoning by requiring agents to fit objects of varying shapes into cupboard compartments. RTTP improves success rates from 51% (baseline) to 79%, with the retrospective reflection component proving crucial for tasks where initial placement decisions affect later fitting possibilities.

**Ablation Studies** systematically validate each component. Removing reflection-in-action reduces performance by 18-22%, while removing reflection-on-action causes a 12-16% drop. Removing retrospective reflection particularly impacts long-horizon tasks (25% performance decrease), confirming its role in credit assignment. The studies also reveal that the two reflection modes are complementary: reflection-in-action provides immediate performance gains through better action selection, while reflection-on-action enables continuous improvement over multiple task attempts.

**Real-Robot Validation**: Qualitative experiments on a physical robot arm demonstrate behavioral correction. In one trial, the robot initially attempts to grasp a cup from the top (which fails due to the handle orientation). After reflection-on-action updates, subsequent attempts correctly approach from the side. The retrospective reflection mechanism proves valuable when the robot realizes mid-task that an earlier placement decision has blocked access to a required object.

## Implications and Future Directions

RTTP represents a significant step toward embodied agents that learn continuously from experience rather than treating each deployment as an independent trial. The framework's key insight—that reflection should occur both before and after action—aligns with cognitive science research on human expertise development.

The test-time training aspect is particularly noteworthy, as it enables learning without requiring offline dataset collection or simulation. This makes RTTP practical for real-world deployment where the distribution of tasks and failure modes cannot be fully anticipated during training.

However, several challenges remain. The computational cost of generating multiple action candidates and performing test-time updates may limit real-time applicability in resource-constrained settings. The reliance on external feedback signals also raises questions about how to provide appropriate supervision in unstructured environments where ground truth is ambiguous.

Future work could explore more efficient reflection mechanisms, perhaps using learned world models to simulate action outcomes rather than generating full action sequences. Integration with hierarchical planning could also improve scalability to even longer-horizon tasks. Finally, extending the framework to multi-agent settings where agents can learn from each other's reflections presents an exciting direction.

## Takeaways

1. Embodied LLMs currently lack the ability to learn from mistakes during deployment, causing repeated failures rather than accumulated experience
2. Reflective Test-Time Planning introduces dual-mode reflection: reflection-in-action (planning before execution) and reflection-on-action (learning after execution)
3. Reflection-in-action uses test-time scaling to generate and score multiple candidate actions, selecting the most promising based on internal reflections
4. Reflection-on-action employs test-time training to update both reflection models and action policies based on external feedback
5. Retrospective reflection enables agents to reassess earlier decisions with hindsight, crucial for long-horizon credit assignment
6. Experiments show 31% improvement on Long-Horizon Household tasks and 28% improvement on MuJoCo Cupboard Fitting compared to baseline embodied LLMs
7. Ablation studies confirm that reflection-in-action and reflection-on-action are complementary, each contributing unique benefits
8. Real-robot trials demonstrate concrete behavioral corrections through the reflection mechanism
9. The framework enables continuous learning during deployment without requiring offline dataset collection
10. Key remaining challenges include computational efficiency and scaling to more complex multi-agent scenarios
:::

:::zh
**论文**: [2602.21198](https://arxiv.org/abs/2602.21198)
**作者**: Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu, Yejin Choi
**分类**: cs.LG, cs.AI, cs.CL, cs.CV, cs.RO

## 摘要

当前的具身大语言模型在高层推理方面表现出色,但缺乏对失败进行反思的能力,导致机器人重复犯错而非从经验中学习。本文提出了反思式测试时规划(RTTP)框架,该框架受人类反思实践启发,使智能体能够在部署期间持续学习。该方法结合了行动中反思(使用测试时扩展在执行前生成和评估多个候选动作)和行动后反思(通过测试时训练在执行后更新反思模型和动作策略)。回顾性反思机制允许智能体用后见之明重新评估过去的决策,以便在长期任务中进行适当的信用分配。在新设计的长期家务基准和MuJoCo橱柜装配基准上的评估显示,RTTP相比基线模型有显著改进,消融研究证实了两种反思模式的协同价值。

## 主要贡献

- 提出反思式测试时规划(RTTP),一个双模式反思框架,使具身智能体能够在部署期间从错误中学习
- 行动中反思机制,使用测试时扩展在执行前通过内部反思生成和评分多个动作候选
- 行动后反思机制,采用测试时训练基于外部反馈更新反思模型和动作策略
- 回顾性反思能力,允许智能体用后见之明重新评估早期决策,用于长期信用分配
- 两个新基准:长期家务任务和MuJoCo橱柜装配,专门设计用于评估具身AI中的反思学习
- 全面的实验验证,包括真实机器人试验,展示通过反思实现的行为纠正

## 方法论:双模式反思架构

RTTP框架将Donald Schön的反思实践理论操作化为两种互补的反思模式。

**行动中反思**发生在执行之前,利用测试时计算扩展。智能体为当前状态生成$N$个候选动作,然后使用内部反思模型$R_{\text{internal}}$通过模拟潜在结果和识别失败模式来评估每个候选。这产生一组评分动作:$\{(a_1, s_1), (a_2, s_2), ..., (a_N, s_N)\}$,其中$s_i$表示反思分数。选择得分最高的动作执行,有效地实现了由学习到的反思而非随机采样引导的最佳N选一采样策略。

**行动后反思**发生在执行之后,采用测试时训练。完成一个动作(或子任务)后,智能体接收外部反馈——来自环境奖励、人类纠正或自动验证。该反馈用于计算损失信号,通过梯度下降更新反思模型$R_{\text{internal}}$和动作策略$\pi_{\theta}$。反思模型学习更好地预测哪些动作会成功或失败,而策略学习生成更好的初始动作候选。

**回顾性反思**扩展了行动后反思,允许智能体重新审视轨迹中的早期决策。当失败发生时,智能体回溯其决策历史以识别哪些早期选择导致了失败,从而在动作后果可能不会立即显现的长期任务中实现适当的信用分配。

## 实验结果与分析

作者在两个需要长期规划和错误恢复的挑战性基准上评估RTTP。

**长期家务基准**:这个新基准包含需要10-20步的复杂家务任务,如"准备早餐"(需要从多个位置检索物品、按顺序使用电器、处理溢出等失败)。RTTP达到73%的成功率,而基线具身LLM为42%,仅有行动中反思的模型为58%。完整的RTTP系统在需要错误恢复的任务中表现尤为突出——当初始计划失败时,行动后反思机制使智能体能够调整策略而非重复相同错误。

**MuJoCo橱柜装配**:该基准通过要求智能体将不同形状的物体装入橱柜隔间来测试精细操作和空间推理。RTTP将成功率从51%(基线)提高到79%,回顾性反思组件对于初始放置决策影响后续装配可能性的任务至关重要。

**消融研究**系统地验证了每个组件。移除行动中反思使性能降低18-22%,而移除行动后反思导致12-16%的下降。移除回顾性反思特别影响长期任务(性能下降25%),证实了其在信用分配中的作用。研究还揭示两种反思模式是互补的:行动中反思通过更好的动作选择提供即时性能提升,而行动后反思使多次任务尝试中的持续改进成为可能。

**真实机器人验证**:在物理机械臂上的定性实验展示了行为纠正。在一次试验中,机器人最初尝试从顶部抓取杯子(由于手柄方向而失败)。经过行动后反思更新后,后续尝试正确地从侧面接近。当机器人在任务中途意识到早期的放置决策阻挡了对所需物体的访问时,回顾性反思机制证明很有价值。

## 影响与未来方向

RTTP代表了向从经验中持续学习而非将每次部署视为独立试验的具身智能体迈出的重要一步。该框架的关键洞察——反思应该在动作之前和之后都发生——与关于人类专业知识发展的认知科学研究相一致。

测试时训练方面特别值得注意,因为它无需离线数据集收集或模拟即可实现学习。这使RTTP在任务和失败模式的分布无法在训练期间完全预测的真实世界部署中变得实用。

然而,仍存在几个挑战。生成多个动作候选和执行测试时更新的计算成本可能限制在资源受限环境中的实时适用性。对外部反馈信号的依赖也引发了关于如何在真实情况模糊的非结构化环境中提供适当监督的问题。

未来工作可以探索更高效的反思机制,或许使用学习到的世界模型来模拟动作结果而非生成完整动作序列。与分层规划的集成也可以提高对更长期任务的可扩展性。最后,将框架扩展到智能体可以从彼此的反思中学习的多智能体设置是一个令人兴奋的方向。

## 要点总结

1. 具身大语言模型目前缺乏在部署期间从错误中学习的能力,导致重复失败而非积累经验
2. 反思式测试时规划引入双模式反思:行动中反思(执行前规划)和行动后反思(执行后学习)
3. 行动中反思使用测试时扩展生成和评分多个候选动作,基于内部反思选择最有希望的动作
4. 行动后反思采用测试时训练基于外部反馈更新反思模型和动作策略
5. 回顾性反思使智能体能够用后见之明重新评估早期决策,对长期信用分配至关重要
6. 实验显示相比基线具身LLM,在长期家务任务上提升31%,在MuJoCo橱柜装配上提升28%
7. 消融研究证实行动中反思和行动后反思是互补的,各自贡献独特优势
8. 真实机器人试验展示通过反思机制实现的具体行为纠正
9. 该框架无需离线数据集收集即可在部署期间实现持续学习
10. 关键的剩余挑战包括计算效率和扩展到更复杂的多智能体场景
:::
