---
title:
  en: "Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval"
  zh: "挖掘与精炼:电商搜索检索中的分级相关性优化"
description:
  en: "A two-stage contrastive learning framework that improves e-commerce search by mining hard samples and refining embeddings to explicitly separate graded relevance levels."
  zh: "一个两阶段对比学习框架,通过挖掘困难样本和精炼嵌入来显式分离分级相关性层次,从而改进电商搜索效果。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.ir", "cs.lg"]
image: "/arxiv-visuals/arxiv-mine-and-refine-optimizing-graded-relevance.png"
---

:::en
**Paper**: [2602.17654](https://arxiv.org/abs/2602.17654)
**Authors**: Jiaqi Xi, Raghav Saboo, Luming Chen, Martin Wang, Sudeep Das
**Categories**: cs.IR, cs.LG

## Abstract

This paper introduces a two-stage "Mine and Refine" framework for training semantic embeddings in large-scale e-commerce search systems. The approach addresses the challenge of graded relevance, where search results can be exact matches, substitutes, or complements. Stage 1 trains a multilingual Siamese two-tower retriever using label-aware supervised contrastive learning to establish a robust semantic space. Stage 2 mines hard negative samples via approximate nearest neighbor search, re-annotates them using a policy-aligned LLM, and applies a multi-class extension of circle loss to sharpen boundaries between relevance levels. The framework incorporates spelling augmentation and synthetic query generation for robustness, demonstrating significant improvements in both offline metrics and production A/B tests.

## Key Contributions

- A two-stage training framework that explicitly optimizes for graded relevance in e-commerce search retrieval
- Policy-consistent supervision at scale through fine-tuning a lightweight LLM on human annotations with three-level relevance guidelines
- Multi-class extension of circle loss that sharpens similarity score boundaries between different relevance strata
- Hard sample mining strategy using ANN followed by LLM-based re-annotation to refine the embedding space
- Robustness improvements through additive spelling augmentation and synthetic query generation
- Demonstrated production impact with statistically significant gains in engagement and business metrics

## Methodology: Two-Stage Training Pipeline

The framework operates in two distinct stages, each addressing specific challenges in e-commerce search retrieval.

**Stage 1: Global Semantic Space Construction**

The first stage establishes a robust multilingual semantic space using a Siamese two-tower architecture. The model employs a label-aware supervised contrastive objective that considers the graded nature of relevance. Unlike binary relevance approaches, this objective function explicitly accounts for three relevance levels: exact matches, substitutes/complements, and irrelevant items.

The training data is obtained through a carefully designed supervision pipeline. A lightweight LLM is fine-tuned on human annotations following a three-level relevance guideline that aligns with product policies and business constraints. This ensures consistency and scalability while maintaining quality. The supervision is further refined through engagement-driven auditing, where user interaction signals help identify and correct residual annotation noise.

**Stage 2: Hard Sample Mining and Boundary Refinement**

The second stage focuses on refining the embedding space by explicitly sharpening the boundaries between relevance levels. Hard samples are mined using approximate nearest neighbor (ANN) search, identifying cases where the model's current embeddings place items of different relevance levels too close together in the semantic space.

These hard samples are re-annotated using the policy-aligned LLM to ensure label quality. The key innovation is a multi-class extension of circle loss, which explicitly optimizes for clear separation between the three relevance strata. The loss function can be expressed as:

$$\mathcal{L} = \log[1 + \sum_{i \in \Omega_p} \sum_{j \in \Omega_n} \exp(\gamma(s_j - s_i + m))]$$

where $s_i$ and $s_j$ are similarity scores for positive and negative pairs, $\gamma$ is a scale factor, and $m$ is a margin parameter. The multi-class extension ensures that exact matches have higher similarity scores than substitutes, which in turn score higher than irrelevant items.

## Robustness and Generalization

E-commerce search faces unique challenges including long-tail queries, spelling variations, and noisy user inputs. The framework addresses these through two complementary strategies:

**Additive Spelling Augmentation**: During training, the system introduces controlled spelling variations to queries, helping the model learn robust representations that generalize to misspelled or noisy inputs. This is particularly important for e-commerce where users often type quickly on mobile devices.

**Synthetic Query Generation**: To improve coverage of the long tail, the framework generates synthetic queries for products using language models. This augmentation ensures the retriever can handle diverse query formulations and rare product categories that may have limited organic training data.

The multilingual nature of the two-tower architecture also enables the system to generalize across languages, which is essential for global e-commerce platforms serving diverse markets.

## Experimental Results and Production Impact

The paper reports comprehensive evaluation across both offline metrics and production A/B tests. Offline evaluations demonstrate improvements in standard retrieval metrics including precision, recall, and normalized discounted cumulative gain (NDCG) across all three relevance levels.

The multi-class circle loss shows particular effectiveness in creating clear separation between relevance strata. Analysis of similarity score distributions reveals that the refined embeddings produce distinct clusters for exact matches, substitutes, and irrelevant items, enabling more stable hybrid ranking and clearer thresholding in production systems.

Production A/B tests validate the offline improvements, showing statistically significant gains in user engagement metrics including click-through rate, add-to-cart rate, and conversion rate. The business impact extends beyond engagement, with measurable improvements in revenue metrics. The clear separation of relevance levels also improves system stability, reducing the need for manual threshold tuning and making the retrieval system more maintainable.

## Takeaways

1. Graded relevance is a critical consideration for e-commerce search systems, where users often accept substitutes and complements beyond exact matches
2. Two-stage training with hard sample mining and refinement can significantly improve embedding quality compared to single-stage approaches
3. Policy-aligned LLM supervision provides a scalable path to high-quality training data that respects business constraints
4. Multi-class extensions of contrastive losses can explicitly optimize for clear separation between multiple relevance levels
5. Robustness techniques including spelling augmentation and synthetic query generation are essential for handling real-world e-commerce search challenges
6. Clear separation of similarity scores across relevance strata improves not just retrieval quality but also system stability and maintainability in production
:::

:::zh
**论文**: [2602.17654](https://arxiv.org/abs/2602.17654)
**作者**: Jiaqi Xi, Raghav Saboo, Luming Chen, Martin Wang, Sudeep Das
**分类**: cs.IR, cs.LG

## 摘要

本文提出了一个用于大规模电商搜索系统语义嵌入训练的两阶段"挖掘与精炼"框架。该方法解决了分级相关性的挑战,即搜索结果可以是精确匹配、替代品或互补品。第一阶段使用标签感知的监督对比学习训练多语言孪生双塔检索器,建立稳健的语义空间。第二阶段通过近似最近邻搜索挖掘困难负样本,使用策略对齐的大语言模型重新标注,并应用圆损失的多类扩展来锐化相关性层次之间的边界。该框架结合拼写增强和合成查询生成来提高鲁棒性,在离线指标和生产A/B测试中均展现出显著改进。

## 主要贡献

- 提出两阶段训练框架,显式优化电商搜索检索中的分级相关性
- 通过在三级相关性指南下微调轻量级大语言模型,实现大规模策略一致的监督
- 提出圆损失的多类扩展,锐化不同相关性层次之间的相似度分数边界
- 采用基于近似最近邻的困难样本挖掘策略,结合大语言模型重新标注来精炼嵌入空间
- 通过加性拼写增强和合成查询生成提高鲁棒性
- 在生产环境中展现出统计显著的用户参与度和业务指标提升

## 方法论:两阶段训练流程

该框架分为两个不同阶段,每个阶段针对电商搜索检索中的特定挑战。

**第一阶段:全局语义空间构建**

第一阶段使用孪生双塔架构建立稳健的多语言语义空间。模型采用标签感知的监督对比目标函数,考虑相关性的分级特性。与二元相关性方法不同,该目标函数显式考虑三个相关性级别:精确匹配、替代品/互补品和不相关项。

训练数据通过精心设计的监督流程获得。在遵循三级相关性指南的人工标注基础上微调轻量级大语言模型,该指南与产品策略和业务约束保持一致。这确保了一致性和可扩展性,同时保持质量。监督信号通过参与度驱动的审计进一步精炼,用户交互信号帮助识别和纠正残留的标注噪声。

**第二阶段:困难样本挖掘与边界精炼**

第二阶段专注于通过显式锐化相关性层次之间的边界来精炼嵌入空间。使用近似最近邻(ANN)搜索挖掘困难样本,识别模型当前嵌入将不同相关性级别的项目在语义空间中放置得过于接近的情况。

这些困难样本使用策略对齐的大语言模型重新标注以确保标签质量。关键创新是圆损失的多类扩展,显式优化三个相关性层次之间的清晰分离。损失函数可表示为:

$$\mathcal{L} = \log[1 + \sum_{i \in \Omega_p} \sum_{j \in \Omega_n} \exp(\gamma(s_j - s_i + m))]$$

其中$s_i$和$s_j$是正负样本对的相似度分数,$\gamma$是尺度因子,$m$是边界参数。多类扩展确保精确匹配具有比替代品更高的相似度分数,而替代品又比不相关项得分更高。

## 鲁棒性与泛化能力

电商搜索面临独特挑战,包括长尾查询、拼写变体和嘈杂的用户输入。该框架通过两个互补策略解决这些问题:

**加性拼写增强**:训练期间,系统向查询引入受控的拼写变体,帮助模型学习能够泛化到拼写错误或嘈杂输入的稳健表示。这对于用户经常在移动设备上快速输入的电商场景尤为重要。

**合成查询生成**:为改善长尾覆盖,框架使用语言模型为产品生成合成查询。这种增强确保检索器能够处理多样化的查询表述和可能缺乏有机训练数据的稀有产品类别。

双塔架构的多语言特性还使系统能够跨语言泛化,这对于服务多元化市场的全球电商平台至关重要。

## 实验结果与生产影响

论文报告了离线指标和生产A/B测试的全面评估。离线评估显示在所有三个相关性级别上,标准检索指标包括精确率、召回率和归一化折损累积增益(NDCG)均有改进。

多类圆损失在创建相关性层次之间的清晰分离方面表现出特别的有效性。相似度分数分布分析显示,精炼后的嵌入为精确匹配、替代品和不相关项产生了明显的聚类,使生产系统中的混合排序更加稳定,阈值设置更加清晰。

生产A/B测试验证了离线改进,显示用户参与度指标包括点击率、加购率和转化率均有统计显著提升。业务影响超越了参与度,在收入指标上也有可衡量的改进。相关性层次的清晰分离还提高了系统稳定性,减少了人工阈值调优的需求,使检索系统更易于维护。

## 要点总结

1. 分级相关性是电商搜索系统的关键考虑因素,用户通常接受精确匹配之外的替代品和互补品
2. 采用困难样本挖掘和精炼的两阶段训练相比单阶段方法能显著提高嵌入质量
3. 策略对齐的大语言模型监督提供了一条尊重业务约束的高质量训练数据可扩展路径
4. 对比损失的多类扩展可以显式优化多个相关性级别之间的清晰分离
5. 包括拼写增强和合成查询生成在内的鲁棒性技术对于处理真实世界电商搜索挑战至关重要
6. 相关性层次间相似度分数的清晰分离不仅提高了检索质量,还改善了生产系统的稳定性和可维护性
:::
