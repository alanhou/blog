---
title:
  en: "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames"
  zh: "BPP: 通过关注关键历史帧实现长上下文机器人模仿学习"
description:
  en: "A novel approach that uses vision-language models to identify keyframes in robot task histories, achieving 70% higher success rates by reducing spurious correlations in long-context imitation learning."
  zh: "一种利用视觉-语言模型识别机器人任务历史中关键帧的新方法,通过减少长上下文模仿学习中的虚假关联,实现了70%的成功率提升。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.ro", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15010](https://arxiv.org/abs/2602.15010)
**Authors**: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar
**Categories**: cs.RO, cs.LG

## Abstract

Many robotic manipulation tasks inherently require reasoning over historical observations—such as remembering which drawers have been searched when looking for an object. While current state-of-the-art robot policies excel at reactive behaviors conditioned on current observations, they struggle with tasks requiring memory of past states. This paper identifies that naively conditioning on observation history leads to spurious correlations: policies overfit to incidental features in training trajectories that don't generalize at deployment. The root cause is exponentially growing history space that cannot be adequately covered during training. The authors propose Big Picture Policies (BPP), which uses vision-language models to extract a minimal set of task-relevant keyframes from observation histories. By compressing diverse trajectories into compact event representations, BPP achieves 70% higher success rates than baselines on real-world manipulation tasks.

## Key Contributions

- **Problem Analysis**: Identifies that spurious correlations in history-conditioned policies stem from insufficient coverage of the exponentially large history space during training
- **BPP Framework**: Introduces a method that uses vision-language models (VLMs) to detect meaningful keyframes, projecting diverse rollouts onto a compact set of task-relevant events
- **Distribution Shift Reduction**: Demonstrates that keyframe-based conditioning substantially reduces the gap between training and deployment distributions without sacrificing policy expressivity
- **Empirical Validation**: Achieves 70% improvement over best baselines across four real-world manipulation tasks and three simulation environments

## Methodology

The BPP approach consists of three main components:

**Keyframe Detection**: A vision-language model analyzes the observation history to identify frames corresponding to significant task events. Rather than conditioning on all past observations, BPP selects only those frames where meaningful state changes occur. This is formulated as detecting frames $\{o_{t_1}, o_{t_2}, ..., o_{t_k}\}$ where $k \ll T$ and each $o_{t_i}$ represents a semantically important moment.

**Policy Architecture**: The policy $\pi(a_t | o_t, \{o_{t_i}\}_{i=1}^k)$ conditions on both the current observation and the detected keyframes. This architecture maintains expressivity for complex tasks while dramatically reducing the effective history space that must be covered during training.

**Training Procedure**: BPP is trained using standard imitation learning on demonstration data, but the keyframe detector preprocesses trajectories to extract relevant historical context. This preprocessing step is task-agnostic and leverages the semantic understanding of pre-trained VLMs.

The key insight is that by projecting high-dimensional observation histories onto a low-dimensional space of task-relevant events, BPP achieves better generalization. The exponential growth of possible histories is tamed by focusing only on semantically meaningful transitions.

## Experimental Results

The authors evaluate BPP on seven tasks total: four real-world manipulation tasks and three simulation environments. All tasks require history conditioning to succeed.

**Real-World Tasks**:
- Drawer search and retrieval
- Multi-step assembly requiring memory of completed sub-tasks
- Object rearrangement with occlusions
- Sequential manipulation with state dependencies

**Performance Metrics**: BPP achieves an average 70% improvement in success rate over the best baseline methods. Notably, naive history conditioning (using all past frames) performs worse than current-observation-only policies on several tasks, confirming the spurious correlation problem.

**Ablation Studies**: The paper demonstrates that:
- Keyframe selection is crucial—random frame sampling performs poorly
- VLM-based detection outperforms hand-crafted heuristics
- The number of keyframes can be task-adaptive without manual tuning

**Generalization**: BPP shows strong out-of-distribution generalization, succeeding on trajectories with novel orderings of sub-tasks and environmental variations not seen during training.

## Technical Insights

The paper provides several important theoretical and practical insights:

**Coverage Problem**: The authors formalize why history-conditioned policies fail. Given a horizon $T$, the space of possible observation histories grows as $|\mathcal{O}|^T$. Training data provides limited coverage, leading policies to memorize spurious patterns. Traditional regularization (dropout, weight decay) doesn't address this fundamental coverage issue.

**Keyframe Compression**: By reducing history to $k$ keyframes where $k \ll T$, the effective history space becomes $|\mathcal{O}|^k$, which is exponentially smaller and more tractable for training data to cover.

**VLM Advantages**: Pre-trained vision-language models bring semantic understanding that enables zero-shot keyframe detection. The VLM identifies "important" frames based on task-relevant visual changes, without requiring task-specific training.

**Comparison to Alternatives**: The paper compares against several baselines including recurrent policies (LSTMs, Transformers), attention mechanisms, and various regularization schemes. BPP's advantage comes from explicitly addressing the coverage problem rather than just adding model capacity or regularization.

## Takeaways

1. History-conditioned robot policies fail primarily due to insufficient coverage of the exponentially large history space, not model capacity limitations
2. Vision-language models can effectively identify task-relevant keyframes in observation histories without task-specific training
3. Compressing histories to minimal keyframe sets reduces distribution shift between training and deployment by 70% in success rate
4. The approach is general across diverse manipulation tasks and doesn't require manual engineering of what constitutes a "keyframe"
5. BPP demonstrates that semantic compression of temporal information is more effective than naive temporal modeling for robot learning
:::

:::zh
**论文**: [2602.15010](https://arxiv.org/abs/2602.15010)
**作者**: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar
**分类**: cs.RO, cs.LG

## 摘要

许多机器人操作任务本质上需要对历史观测进行推理——例如在寻找物体时记住已经搜索过哪些抽屉。虽然当前最先进的机器人策略在基于当前观测的反应性行为上表现出色,但它们在需要记忆过去状态的任务上表现不佳。本文指出,简单地以观测历史为条件会导致虚假关联:策略过拟合训练轨迹中的偶然特征,这些特征在部署时无法泛化。根本原因是历史空间呈指数级增长,训练期间无法充分覆盖。作者提出了大局策略(BPP),它使用视觉-语言模型从观测历史中提取最小的任务相关关键帧集。通过将多样化的轨迹压缩为紧凑的事件表示,BPP在真实世界操作任务上实现了比基线高70%的成功率。

## 主要贡献

- **问题分析**: 识别出历史条件策略中的虚假关联源于训练期间对指数级大的历史空间覆盖不足
- **BPP框架**: 引入一种使用视觉-语言模型检测有意义关键帧的方法,将多样化的展开投影到紧凑的任务相关事件集上
- **分布偏移减少**: 证明基于关键帧的条件化在不牺牲策略表达能力的情况下,大幅减少了训练和部署分布之间的差距
- **实证验证**: 在四个真实世界操作任务和三个仿真环境中,相比最佳基线实现了70%的改进

## 方法论

BPP方法由三个主要组件构成:

**关键帧检测**: 视觉-语言模型分析观测历史以识别对应重要任务事件的帧。BPP不是以所有过去观测为条件,而是只选择发生有意义状态变化的帧。这被形式化为检测帧 $\{o_{t_1}, o_{t_2}, ..., o_{t_k}\}$,其中 $k \ll T$,每个 $o_{t_i}$ 代表一个语义上重要的时刻。

**策略架构**: 策略 $\pi(a_t | o_t, \{o_{t_i}\}_{i=1}^k)$ 同时以当前观测和检测到的关键帧为条件。这种架构为复杂任务保持了表达能力,同时显著减少了训练期间必须覆盖的有效历史空间。

**训练过程**: BPP使用标准模仿学习在演示数据上训练,但关键帧检测器预处理轨迹以提取相关的历史上下文。这个预处理步骤与任务无关,并利用预训练VLM的语义理解能力。

关键洞察是,通过将高维观测历史投影到任务相关事件的低维空间,BPP实现了更好的泛化。通过只关注语义上有意义的转换,可能历史的指数级增长得到了控制。

## 实验结果

作者在总共七个任务上评估BPP:四个真实世界操作任务和三个仿真环境。所有任务都需要历史条件化才能成功。

**真实世界任务**:
- 抽屉搜索和检索
- 需要记忆已完成子任务的多步骤组装
- 带遮挡的物体重排
- 具有状态依赖性的顺序操作

**性能指标**: BPP在成功率上比最佳基线方法平均提高70%。值得注意的是,简单的历史条件化(使用所有过去帧)在几个任务上的表现比仅使用当前观测的策略更差,证实了虚假关联问题。

**消融研究**: 论文证明:
- 关键帧选择至关重要——随机帧采样表现不佳
- 基于VLM的检测优于手工设计的启发式方法
- 关键帧数量可以自适应任务而无需手动调整

**泛化能力**: BPP显示出强大的分布外泛化能力,在具有新颖子任务顺序和训练期间未见过的环境变化的轨迹上取得成功。

## 技术洞察

论文提供了几个重要的理论和实践洞察:

**覆盖问题**: 作者形式化了为什么历史条件策略会失败。给定时间范围 $T$,可能的观测历史空间增长为 $|\mathcal{O}|^T$。训练数据提供的覆盖有限,导致策略记忆虚假模式。传统正则化(dropout、权重衰减)无法解决这个根本的覆盖问题。

**关键帧压缩**: 通过将历史减少到 $k$ 个关键帧(其中 $k \ll T$),有效历史空间变为 $|\mathcal{O}|^k$,这在指数级上更小,训练数据更容易覆盖。

**VLM优势**: 预训练的视觉-语言模型带来语义理解能力,实现零样本关键帧检测。VLM基于任务相关的视觉变化识别"重要"帧,无需特定任务的训练。

**与替代方案的比较**: 论文与几个基线进行比较,包括循环策略(LSTM、Transformer)、注意力机制和各种正则化方案。BPP的优势来自明确解决覆盖问题,而不仅仅是增加模型容量或正则化。

## 要点总结

1. 历史条件机器人策略失败主要是由于对指数级大的历史空间覆盖不足,而非模型容量限制
2. 视觉-语言模型可以有效识别观测历史中的任务相关关键帧,无需特定任务的训练
3. 将历史压缩为最小关键帧集可将训练和部署之间的分布偏移减少70%的成功率
4. 该方法在不同操作任务中具有通用性,不需要手动设计什么构成"关键帧"
5. BPP证明了时间信息的语义压缩比简单的时间建模对机器人学习更有效
:::
