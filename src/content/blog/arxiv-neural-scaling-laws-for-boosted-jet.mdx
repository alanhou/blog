---
title:
  en: "Neural Scaling Laws for Boosted Jet Tagging"
  zh: "增强喷注标记的神经网络缩放定律"
description:
  en: "First comprehensive study of neural scaling laws in high energy physics, revealing compute-optimal training strategies and performance limits for jet classification tasks."
  zh: "首次全面研究高能物理中的神经网络缩放定律,揭示喷注分类任务的计算最优训练策略和性能极限。"
date: 2026-02-18
tags: ["arxiv", "ai", "hep-ex", "cs.lg", "hep-ph", "physics.data-an"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15781](https://arxiv.org/abs/2602.15781)
**Authors**: Matthias Vigl, Nicole Hartman, Michael Kagan, Lukas Heinrich
**Categories**: hep-ex, cs.LG, hep-ph, physics.data-an

## Abstract

This paper establishes the first systematic investigation of neural scaling laws for high energy physics, specifically focusing on boosted jet classification using the JetClass dataset. The authors demonstrate that the principles driving Large Language Model success—scaling compute through increased model capacity and dataset size—apply equally to particle physics tasks. They derive compute-optimal scaling laws that reveal consistent performance limits achievable through increased computational resources, quantify the benefits of data repetition in simulation-constrained environments, and show how feature representation choices fundamentally affect both scaling behavior and asymptotic performance ceilings.

## Key Contributions

- **Compute-Optimal Scaling Laws**: Derived power-law relationships between compute budget $C$, model parameters $N$, and dataset size $D$ for jet tagging, following the form $L(C) = L_{\infty} + \alpha C^{-\beta}$ where $L_{\infty}$ represents the irreducible error floor
- **Data Repetition Analysis**: Quantified how repeated training on expensive simulation data affects scaling, introducing an effective dataset size $D_{\text{eff}}$ that accounts for diminishing returns from repetition
- **Feature Representation Impact**: Demonstrated that low-level particle features (4-vectors) achieve better asymptotic limits than high-level engineered features, with performance gaps widening at scale
- **Particle Multiplicity Effects**: Showed that models trained on higher particle multiplicities exhibit improved scaling coefficients and lower performance floors

## Methodology and Experimental Design

The research employs ParticleNet architectures trained on the JetClass dataset, which contains 100 million simulated jets across 10 classification categories. The authors systematically vary three key dimensions:

**Compute Budget Scaling**: Models range from 10¹⁸ to 10²¹ FLOPs, spanning four orders of magnitude. For each compute budget, they optimize the allocation between model size and training tokens following Chinchilla-style scaling principles.

**Feature Representations**: Three input modalities are compared:
- High-level features (16 engineered variables)
- Particle 4-vectors (energy-momentum representations)
- Particle 4-vectors with additional substructure variables

**Data Repetition Regimes**: Training extends from single-epoch to 100-epoch scenarios, modeling the reality of expensive HEP simulations where generating new data is computationally prohibitive.

The scaling law formulation follows:

$$L(N, D) = L_{\infty} + \frac{\alpha_N}{N^{\beta_N}} + \frac{\alpha_D}{D^{\beta_D}}$$

where the compute-optimal trajectory satisfies $N \propto C^a$ and $D \propto C^b$ with $a + b = 1$.

## Results and Scaling Behavior

The empirical results reveal several critical insights:

**Power-Law Scaling Holds**: Across all feature representations and particle multiplicities, performance follows predictable power laws with compute. The exponent $\beta$ typically ranges from 0.05 to 0.15, indicating that each order of magnitude increase in compute yields consistent fractional improvements.

**Asymptotic Limits Vary by Representation**: The irreducible error $L_{\infty}$ differs substantially:
- High-level features: $L_{\infty} \approx 0.45$
- Particle 4-vectors: $L_{\infty} \approx 0.35$
- 4-vectors + substructure: $L_{\infty} \approx 0.32$

This demonstrates that feature expressiveness directly determines the performance ceiling, regardless of compute invested.

**Data Repetition Provides Effective Size Gains**: When training data is repeated $R$ times, the effective dataset size follows $D_{\text{eff}} = D \cdot R^{\gamma}$ where $\gamma \approx 0.6$. This means 100 epochs on 1M samples performs comparably to 16 epochs on 10M samples—a crucial finding for simulation-limited domains.

**Particle Multiplicity Matters**: Models trained on jets with 128 particles consistently outperform those trained on 64 particles, with both better scaling coefficients and lower asymptotic limits. This suggests that preserving fine-grained particle information improves both sample efficiency and ultimate performance.

## Implications for High Energy Physics

This work has profound implications for how the HEP community should approach machine learning:

**Resource Allocation**: The compute-optimal scaling laws provide concrete guidance for balancing model size against dataset size. For a fixed compute budget, the optimal allocation typically favors larger models trained on more data than current practice suggests.

**Simulation Strategy**: The quantified benefits of data repetition justify training longer on existing simulations rather than immediately generating new samples. A 10x increase in epochs provides roughly 4x effective data, which may be more efficient than generating 4x more simulation.

**Feature Engineering vs. Raw Data**: The superior asymptotic performance of low-level features challenges the HEP tradition of hand-crafted high-level variables. While engineered features may perform better in data-scarce regimes, scaling compute favors learning directly from particle 4-vectors.

**Compute Investment**: Current HEP models use 10¹⁸-10¹⁹ FLOPs, while foundation models use 10²³-10²⁴ FLOPs. The demonstrated scaling laws suggest that HEP could achieve substantial performance gains by increasing compute budgets, potentially approaching the asymptotic limits identified in this work.

## Takeaways

1. Neural scaling laws from LLM research apply to high energy physics tasks, with power-law relationships between compute, model size, dataset size, and performance holding across four orders of magnitude
2. Compute-optimal training requires careful balancing of model parameters and training tokens, with the optimal allocation differing from current HEP practice
3. Data repetition in simulation-constrained environments provides quantifiable effective dataset size gains following $D_{\text{eff}} \propto R^{0.6}$, making extended training on existing data more efficient than previously assumed
4. Low-level particle representations (4-vectors) achieve better asymptotic performance limits than engineered high-level features, with advantages increasing at scale
5. Particle multiplicity directly affects both scaling efficiency and performance ceilings, suggesting that preserving fine-grained detector information improves model capacity
6. The HEP community has significant headroom for performance improvements through increased compute investment, with current models operating far below the identified asymptotic limits
:::

:::zh
**论文**: [2602.15781](https://arxiv.org/abs/2602.15781)
**作者**: Matthias Vigl, Nicole Hartman, Michael Kagan, Lukas Heinrich
**分类**: hep-ex, cs.LG, hep-ph, physics.data-an

## 摘要

本文首次系统性地研究了高能物理中的神经网络缩放定律,特别聚焦于使用JetClass数据集的增强喷注分类任务。作者证明了驱动大语言模型成功的原理——通过增加模型容量和数据集规模来扩展计算——同样适用于粒子物理任务。他们推导出计算最优的缩放定律,揭示了通过增加计算资源可以达到的一致性能极限,量化了在模拟受限环境中数据重复的收益,并展示了特征表示选择如何从根本上影响缩放行为和渐近性能上限。

## 主要贡献

- **计算最优缩放定律**:推导了喷注标记任务中计算预算$C$、模型参数$N$和数据集大小$D$之间的幂律关系,形式为$L(C) = L_{\infty} + \alpha C^{-\beta}$,其中$L_{\infty}$代表不可约误差下限
- **数据重复分析**:量化了在昂贵模拟数据上重复训练如何影响缩放,引入了有效数据集大小$D_{\text{eff}}$来解释重复带来的边际收益递减
- **特征表示影响**:证明了低层次粒子特征(四矢量)比高层次工程特征达到更好的渐近极限,且性能差距随规模扩大而增大
- **粒子多重性效应**:显示在更高粒子多重性上训练的模型表现出更好的缩放系数和更低的性能下限

## 方法论与实验设计

研究采用ParticleNet架构在JetClass数据集上训练,该数据集包含1亿个模拟喷注,涵盖10个分类类别。作者系统性地变化三个关键维度:

**计算预算缩放**:模型范围从10¹⁸到10²¹ FLOPs,跨越四个数量级。对于每个计算预算,他们遵循Chinchilla式缩放原则优化模型大小和训练token之间的分配。

**特征表示**:比较三种输入模态:
- 高层次特征(16个工程变量)
- 粒子四矢量(能量-动量表示)
- 粒子四矢量加额外子结构变量

**数据重复机制**:训练从单轮次扩展到100轮次场景,模拟高能物理模拟昂贵、生成新数据计算成本高昂的现实情况。

缩放定律公式遵循:

$$L(N, D) = L_{\infty} + \frac{\alpha_N}{N^{\beta_N}} + \frac{\alpha_D}{D^{\beta_D}}$$

其中计算最优轨迹满足$N \propto C^a$和$D \propto C^b$,且$a + b = 1$。

## 结果与缩放行为

实证结果揭示了几个关键洞察:

**幂律缩放成立**:在所有特征表示和粒子多重性下,性能都遵循与计算量的可预测幂律关系。指数$\beta$通常在0.05到0.15之间,表明计算量每增加一个数量级都会产生一致的分数改进。

**渐近极限因表示而异**:不可约误差$L_{\infty}$存在显著差异:
- 高层次特征:$L_{\infty} \approx 0.45$
- 粒子四矢量:$L_{\infty} \approx 0.35$
- 四矢量+子结构:$L_{\infty} \approx 0.32$

这表明特征表达能力直接决定了性能上限,与投入的计算量无关。

**数据重复提供有效规模增益**:当训练数据重复$R$次时,有效数据集大小遵循$D_{\text{eff}} = D \cdot R^{\gamma}$,其中$\gamma \approx 0.6$。这意味着在100万样本上训练100轮的效果相当于在1000万样本上训练16轮——这对模拟受限领域是一个关键发现。

**粒子多重性很重要**:在128个粒子的喷注上训练的模型始终优于在64个粒子上训练的模型,具有更好的缩放系数和更低的渐近极限。这表明保留细粒度粒子信息可以提高样本效率和最终性能。

## 对高能物理的影响

这项工作对高能物理社区如何应用机器学习具有深远影响:

**资源分配**:计算最优缩放定律为平衡模型大小与数据集大小提供了具体指导。对于固定的计算预算,最优分配通常倾向于在更多数据上训练更大的模型,这与当前实践不同。

**模拟策略**:数据重复的量化收益证明了在现有模拟上训练更长时间比立即生成新样本更合理。轮次增加10倍提供约4倍的有效数据,这可能比生成4倍模拟更高效。

**特征工程vs原始数据**:低层次特征的优越渐近性能挑战了高能物理手工制作高层次变量的传统。虽然工程特征在数据稀缺情况下可能表现更好,但扩展计算更倾向于直接从粒子四矢量学习。

**计算投资**:当前高能物理模型使用10¹⁸-10¹⁹ FLOPs,而基础模型使用10²³-10²⁴ FLOPs。已证明的缩放定律表明,高能物理可以通过增加计算预算获得显著性能提升,有可能接近本研究中识别的渐近极限。

## 要点总结

1. 大语言模型研究中的神经网络缩放定律适用于高能物理任务,计算量、模型大小、数据集大小和性能之间的幂律关系在四个数量级范围内成立
2. 计算最优训练需要仔细平衡模型参数和训练token,最优分配与当前高能物理实践不同
3. 模拟受限环境中的数据重复提供可量化的有效数据集大小增益,遵循$D_{\text{eff}} \propto R^{0.6}$,使得在现有数据上扩展训练比之前假设的更高效
4. 低层次粒子表示(四矢量)比工程高层次特征达到更好的渐近性能极限,优势随规模增加而扩大
5. 粒子多重性直接影响缩放效率和性能上限,表明保留细粒度探测器信息可以提高模型能力
6. 高能物理社区通过增加计算投资有显著的性能改进空间,当前模型运行远低于已识别的渐近极限
:::
