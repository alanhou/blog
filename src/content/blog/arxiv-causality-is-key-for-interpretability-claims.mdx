---
title:
  en: "Causality is Key for Interpretability Claims to Generalise"
  zh: "因果性是可解释性主张泛化的关键"
description:
  en: "A framework grounding LLM interpretability research in Pearl's causal hierarchy, distinguishing what observations, interventions, and counterfactuals can actually justify."
  zh: "基于Pearl因果层级的大语言模型可解释性研究框架,明确区分观察、干预和反事实推理各自能够支持的主张。"
date: 2026-02-19
tags: ["arxiv", "ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.16698](https://arxiv.org/abs/2602.16698)
**Authors**: Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar
**Categories**: cs.LG

## Abstract

This paper addresses a critical gap in LLM interpretability research: the tendency to make causal claims that exceed what the evidence can support. The authors argue that many interpretability findings fail to generalize because researchers conflate correlation with causation and make counterfactual claims without proper justification. By grounding interpretability in Pearl's causal hierarchy and causal representation learning (CRL), they propose a diagnostic framework that matches interpretability methods to the strength of claims they can legitimately support—ensuring findings are both rigorous and generalizable.

## Key Contributions

- **Causal hierarchy for interpretability**: Maps Pearl's three-level causal hierarchy (association, intervention, counterfactual) to common interpretability practices, clarifying what each level of analysis can justify
- **Critique of current methods**: Demonstrates how popular techniques like activation patching and ablation studies support only interventional claims, not the counterfactual interpretations often attributed to them
- **CRL integration**: Shows how causal representation learning provides formal conditions under which high-level variables can be recovered from neural activations
- **Diagnostic framework**: Offers practitioners a systematic approach to select appropriate methods and evaluations that align claims with available evidence

## The Causal Hierarchy in Interpretability

The paper leverages Pearl's causal ladder to categorize interpretability research:

**Level 1: Association** - Observational studies that identify correlations between model activations and behaviors. Examples include probing classifiers and representation similarity analysis. These methods answer "what is?" questions but cannot establish causation.

**Level 2: Intervention** - Experimental manipulations like ablations, activation patching, or steering vectors. These answer "what if we do?" questions, showing how changes to model components affect outputs across a distribution of inputs. Crucially, interventional claims are about average effects over datasets, not specific counterfactuals.

**Level 3: Counterfactual** - The strongest claims, answering "what would have happened?" for specific instances under unobserved conditions. The authors argue these require either controlled supervision during training or strong untestable assumptions—making most counterfactual interpretability claims unverifiable.

## Why Current Interpretability Falls Short

Many interpretability studies implicitly make counterfactual claims when their methods only support interventional ones. For example:

- **Activation patching** shows that replacing activations from prompt A with those from prompt B changes the output. This is interventional evidence about the effect of that swap.
- However, researchers often interpret this as "this activation encodes concept X for this specific prompt"—a counterfactual claim about what would happen under a hypothetical intervention that wasn't actually performed.

The distinction matters because interventional findings may not generalize to out-of-distribution prompts or different model states. Without causal grounding, we risk building interpretability tools that work on benchmarks but fail in deployment.

## Causal Representation Learning as Foundation

CRL provides the theoretical machinery to formalize when and how high-level causal variables can be identified from low-level representations. The key insight: under certain assumptions (like sufficient variability in the data or known intervention targets), we can provably recover latent causal structures from observations.

For interpretability, this means:
- Specifying which semantic concepts are theoretically recoverable from activations
- Identifying what data or assumptions are needed for valid recovery
- Distinguishing between identifiable causal variables and mere correlates

The authors advocate for interpretability research to explicitly state its position in this framework, making assumptions transparent and claims falsifiable.

## Practical Implications

The diagnostic framework guides researchers to:

1. **Match methods to claims**: Use observational methods for exploratory analysis, interventional methods for causal effect estimation, and recognize when counterfactual claims require additional evidence
2. **Evaluate generalization explicitly**: Test whether findings hold across different prompts, model checkpoints, and task variations
3. **State assumptions clearly**: Make explicit what causal assumptions underlie interpretability claims
4. **Design better benchmarks**: Create evaluation protocols that test whether discovered mechanisms generalize beyond the training distribution

## Takeaways

1. Most current interpretability methods support interventional claims at best—counterfactual interpretations require much stronger evidence than typically provided
2. Pearl's causal hierarchy offers a principled framework for categorizing interpretability research and matching claims to evidence
3. Causal representation learning formalizes when high-level concepts can be reliably extracted from neural activations
4. Generalization failures in interpretability often stem from treating interventional findings as counterfactual truths
5. The field needs standardized practices for stating assumptions, testing generalization, and aligning method capabilities with research claims
:::

:::zh
**论文**: [2602.16698](https://arxiv.org/abs/2602.16698)
**作者**: Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar
**分类**: cs.LG

## 摘要

本文针对大语言模型可解释性研究中的关键问题:研究者往往做出超出证据支持范围的因果主张。作者指出,许多可解释性发现无法泛化,根源在于混淆了相关性与因果性,并在缺乏充分依据的情况下做出反事实推断。通过将可解释性研究建立在Pearl因果层级和因果表征学习(CRL)的基础上,他们提出了一个诊断框架,使可解释性方法与其能够合理支持的主张相匹配,从而确保研究发现既严谨又具有泛化性。

## 主要贡献

- **可解释性的因果层级映射**: 将Pearl的三层因果层级(关联、干预、反事实)映射到常见可解释性实践,明确各层次分析能够证明什么
- **现有方法的批判性分析**: 展示了激活修补和消融研究等流行技术仅支持干预性主张,而非常被赋予的反事实解释
- **因果表征学习整合**: 说明CRL如何提供形式化条件,确定何时可以从神经激活中恢复高层变量
- **诊断框架**: 为实践者提供系统化方法,选择与可用证据相匹配的适当方法和评估手段

## 可解释性中的因果层级

论文利用Pearl的因果阶梯对可解释性研究进行分类:

**第一层:关联** - 识别模型激活与行为之间相关性的观察性研究。例如探测分类器和表征相似性分析。这些方法回答"是什么?"的问题,但无法建立因果关系。

**第二层:干预** - 实验性操作,如消融、激活修补或引导向量。这些回答"如果我们这样做会怎样?"的问题,展示对模型组件的改变如何影响输入分布上的输出。关键在于,干预性主张是关于数据集上的平均效应,而非特定反事实。

**第三层:反事实** - 最强的主张,回答"在未观察到的条件下会发生什么?"针对特定实例。作者认为这需要训练期间的受控监督或强不可测试假设,使得大多数反事实可解释性主张无法验证。

## 当前可解释性研究的不足

许多可解释性研究在方法仅支持干预性主张时,隐含地做出了反事实主张。例如:

- **激活修补**显示用提示B的激活替换提示A的激活会改变输出。这是关于该交换效果的干预性证据。
- 然而,研究者常将其解释为"该激活为这个特定提示编码了概念X"——这是关于未实际执行的假设干预下会发生什么的反事实主张。

这种区分很重要,因为干预性发现可能无法泛化到分布外提示或不同模型状态。缺乏因果基础,我们可能构建出在基准测试上有效但在实际部署中失效的可解释性工具。

## 因果表征学习作为理论基础

CRL提供了理论机制,形式化何时以及如何从低层表征中识别高层因果变量。核心洞察:在特定假设下(如数据中的充分变异性或已知干预目标),我们可以证明从观察中恢复潜在因果结构。

对于可解释性,这意味着:
- 明确哪些语义概念理论上可从激活中恢复
- 识别有效恢复所需的数据或假设
- 区分可识别的因果变量与单纯的相关物

作者主张可解释性研究应明确说明其在该框架中的位置,使假设透明化,主张可证伪。

## 实践意义

诊断框架指导研究者:

1. **方法与主张匹配**: 使用观察性方法进行探索性分析,干预性方法进行因果效应估计,并认识到反事实主张需要额外证据
2. **明确评估泛化性**: 测试发现是否在不同提示、模型检查点和任务变体中成立
3. **清晰陈述假设**: 明确说明可解释性主张背后的因果假设
4. **设计更好的基准**: 创建评估协议,测试发现的机制是否能泛化到训练分布之外

## 要点总结

1. 当前大多数可解释性方法最多支持干预性主张——反事实解释需要比通常提供的更强证据
2. Pearl的因果层级为可解释性研究分类和主张与证据匹配提供了原则性框架
3. 因果表征学习形式化了何时可以可靠地从神经激活中提取高层概念
4. 可解释性中的泛化失败往往源于将干预性发现当作反事实真理
5. 该领域需要标准化实践来陈述假设、测试泛化性,并使方法能力与研究主张保持一致
:::
