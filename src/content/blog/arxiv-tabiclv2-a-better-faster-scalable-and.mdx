---
title:
  en: "TabICLv2: A Better, Faster, Scalable, and Open Tabular Foundation Model"
  zh: "TabICLv2: 更优、更快、可扩展的开源表格基础模型"
description:
  en: "TabICLv2 achieves state-of-the-art performance on tabular data through novel synthetic data generation, scalable attention mechanisms, and optimized pretraining, surpassing existing models while maintaining efficiency on million-scale datasets."
  zh: "TabICLv2通过创新的合成数据生成、可扩展注意力机制和优化的预训练协议,在表格数据上实现最先进性能,超越现有模型的同时保持百万级数据集的高效处理能力。"
date: 2026-02-12
tags: ["arxiv", "ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.11139](https://arxiv.org/abs/2602.11139)
**Authors**: Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan
**Categories**: cs.LG

## Abstract

TabICLv2 represents a significant advancement in tabular foundation models, demonstrating that in-context learning can outperform traditional gradient-boosted trees on predictive benchmarks. The model achieves state-of-the-art results through three core innovations: a sophisticated synthetic data generation engine that ensures high pretraining diversity, architectural improvements including a novel scalable softmax mechanism for attention layers, and optimized pretraining protocols featuring the Muon optimizer. Without any hyperparameter tuning, TabICLv2 surpasses RealTabPFN-2.5 on TabArena and TALENT benchmarks while efficiently handling million-scale datasets under 50GB GPU memory constraints and delivering faster inference speeds.

## Key Contributions

- Novel synthetic data generation engine designed to maximize pretraining diversity and improve generalization across varied tabular data distributions
- Scalable softmax mechanism in attention layers that enables effective generalization to larger datasets without requiring prohibitively expensive long-sequence pretraining
- Integration of the Muon optimizer replacing AdamW, leading to improved convergence and model performance
- State-of-the-art results on TabArena and TALENT benchmarks without hyperparameter tuning, ensembling, or fine-tuning on real data
- Efficient scaling to million-row datasets with memory footprint under 50GB and significantly faster inference than competing models
- Commitment to open research with released inference code, model weights, and planned release of synthetic data engine and pretraining code

## Technical Architecture and Innovations

The architecture of TabICLv2 builds upon transformer-based in-context learning principles but introduces several critical modifications. The scalable softmax mechanism addresses a fundamental challenge in tabular foundation models: generalizing to datasets larger than those seen during pretraining. Traditional attention mechanisms struggle with this extrapolation, but TabICLv2's approach allows the model to handle varying dataset sizes without retraining on longer sequences.

The synthetic data generation engine represents a departure from previous approaches that relied heavily on real data augmentation or simple synthetic distributions. By carefully designing the data generation process to cover a wide range of statistical properties, feature interactions, and data distributions, TabICLv2 learns robust representations that transfer effectively to real-world tabular datasets. This diversity in pretraining data appears crucial for the model's strong zero-shot performance.

The adoption of the Muon optimizer over the standard AdamW represents an interesting choice in the optimization landscape. While AdamW has been the default for most transformer pretraining, the Muon optimizer's properties evidently provide benefits for the specific characteristics of tabular data and the in-context learning paradigm.

## Benchmark Performance and Scalability

TabICLv2's performance on TabArena and TALENT benchmarks is particularly noteworthy because it achieves superior results compared to RealTabPFN-2.5 without the latter's extensive post-processing. RealTabPFN-2.5 employs hyperparameter tuning, ensembling, and fine-tuning on real data—all computationally expensive techniques that TabICLv2 avoids while still delivering better performance. This suggests that the model's pretraining approach and architectural choices are fundamentally more effective.

The scalability characteristics are equally impressive. Many foundation models struggle with memory constraints when applied to large datasets, but TabICLv2 maintains a memory footprint under 50GB even for million-scale datasets. This efficiency, combined with faster inference speeds than RealTabPFN-2.5, makes the model practical for real-world deployment scenarios where computational resources are constrained.

The model's ability to generalize to larger datasets than those seen during pretraining is particularly valuable. Traditional machine learning approaches require retraining or fine-tuning when dataset sizes change significantly, but TabICLv2's scalable attention mechanism enables zero-shot application to varied dataset sizes.

## Implications for Tabular Machine Learning

TabICLv2's success has several important implications for the field of tabular machine learning. First, it provides further evidence that foundation models with in-context learning can compete with and surpass traditional methods like gradient-boosted trees, which have dominated tabular benchmarks for years. This shift suggests that the paradigm of pretraining on diverse synthetic data and applying zero-shot may become standard practice.

Second, the emphasis on synthetic data generation challenges the conventional wisdom that foundation models require massive amounts of real data. By carefully engineering the synthetic data distribution, TabICLv2 achieves strong performance with only moderate pretraining compute. This approach may be more accessible to researchers and practitioners who lack access to large proprietary datasets.

Third, the commitment to open research—releasing code, weights, and eventually the full pretraining pipeline—could accelerate progress in tabular foundation models. The field has sometimes been hampered by closed-source models and proprietary techniques, so TabICLv2's openness is a welcome development.

## Takeaways

1. TabICLv2 achieves state-of-the-art performance on tabular benchmarks without hyperparameter tuning, ensembling, or fine-tuning, demonstrating the power of well-designed pretraining
2. The scalable softmax mechanism enables generalization to datasets larger than those seen during pretraining, addressing a key limitation of previous foundation models
3. Synthetic data generation with high diversity can be more effective than relying primarily on real data augmentation for pretraining tabular models
4. The Muon optimizer provides advantages over AdamW for tabular foundation model pretraining, suggesting optimizer choice matters significantly in this domain
5. Efficient scaling to million-row datasets under 50GB memory makes TabICLv2 practical for real-world deployment scenarios
6. Open release of code and weights promotes reproducibility and accelerates research progress in tabular machine learning
:::

:::zh
**论文**: [2602.11139](https://arxiv.org/abs/2602.11139)
**作者**: Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan
**分类**: cs.LG

## 摘要

TabICLv2代表了表格基础模型的重大进展,证明了上下文学习能够在预测基准测试中超越传统的梯度提升树。该模型通过三大核心创新实现了最先进的结果:一个确保高预训练多样性的复杂合成数据生成引擎、包括用于注意力层的新型可扩展softmax机制在内的架构改进,以及采用Muon优化器的优化预训练协议。在无需任何超参数调优的情况下,TabICLv2在TabArena和TALENT基准测试中超越了RealTabPFN-2.5,同时能够在50GB GPU内存约束下高效处理百万级数据集,并提供更快的推理速度。

## 主要贡献

- 创新的合成数据生成引擎,旨在最大化预训练多样性并改善跨不同表格数据分布的泛化能力
- 注意力层中的可扩展softmax机制,使模型能够有效泛化到更大数据集,而无需进行代价高昂的长序列预训练
- 集成Muon优化器替代AdamW,实现更好的收敛性和模型性能
- 在TabArena和TALENT基准测试中实现最先进结果,无需超参数调优、集成或在真实数据上微调
- 高效扩展到百万行数据集,内存占用低于50GB,推理速度显著快于竞争模型
- 致力于开放研究,已发布推理代码和模型权重,计划发布合成数据引擎和预训练代码

## 技术架构与创新

TabICLv2的架构建立在基于transformer的上下文学习原理之上,但引入了几项关键改进。可扩展softmax机制解决了表格基础模型的一个根本挑战:泛化到比预训练期间见过的数据集更大的数据集。传统注意力机制在这种外推上表现不佳,但TabICLv2的方法允许模型处理不同大小的数据集而无需在更长序列上重新训练。

合成数据生成引擎代表了与以往方法的重大突破,以往方法主要依赖真实数据增强或简单的合成分布。通过精心设计数据生成过程以覆盖广泛的统计特性、特征交互和数据分布,TabICLv2学习到了能够有效迁移到真实世界表格数据集的鲁棒表示。预训练数据的这种多样性对模型强大的零样本性能至关重要。

采用Muon优化器而非标准的AdamW代表了优化领域的一个有趣选择。虽然AdamW一直是大多数transformer预训练的默认选择,但Muon优化器的特性显然为表格数据的特定特征和上下文学习范式提供了优势。

## 基准性能与可扩展性

TabICLv2在TabArena和TALENT基准测试上的性能特别值得注意,因为它在没有后者大量后处理的情况下实现了优于RealTabPFN-2.5的结果。RealTabPFN-2.5采用了超参数调优、集成和在真实数据上微调——这些都是计算成本高昂的技术,而TabICLv2在避免这些技术的同时仍能提供更好的性能。这表明该模型的预训练方法和架构选择从根本上更加有效。

可扩展性特征同样令人印象深刻。许多基础模型在应用于大型数据集时会遇到内存限制,但TabICLv2即使对于百万级数据集也能将内存占用保持在50GB以下。这种效率,加上比RealTabPFN-2.5更快的推理速度,使该模型在计算资源受限的实际部署场景中具有实用性。

模型泛化到比预训练期间见过的更大数据集的能力特别有价值。传统机器学习方法在数据集大小发生显著变化时需要重新训练或微调,但TabICLv2的可扩展注意力机制使其能够零样本应用于不同大小的数据集。

## 对表格机器学习的影响

TabICLv2的成功对表格机器学习领域有几个重要影响。首先,它进一步证明了具有上下文学习能力的基础模型可以与传统方法(如梯度提升树)竞争并超越它们,而梯度提升树多年来一直主导着表格基准测试。这种转变表明,在多样化合成数据上预训练并进行零样本应用的范式可能成为标准实践。

其次,对合成数据生成的重视挑战了基础模型需要大量真实数据的传统观念。通过精心设计合成数据分布,TabICLv2仅用适度的预训练计算就实现了强大的性能。这种方法对于缺乏大型专有数据集访问权限的研究人员和从业者来说可能更容易获得。

第三,对开放研究的承诺——发布代码、权重以及最终的完整预训练流程——可能会加速表格基础模型的进展。该领域有时受到闭源模型和专有技术的阻碍,因此TabICLv2的开放性是一个值得欢迎的发展。

## 要点总结

1. TabICLv2在表格基准测试中实现最先进性能,无需超参数调优、集成或微调,展示了精心设计的预训练的强大力量
2. 可扩展softmax机制使模型能够泛化到比预训练期间见过的更大数据集,解决了以往基础模型的关键局限
3. 具有高多样性的合成数据生成比主要依赖真实数据增强的方法更有效地预训练表格模型
4. Muon优化器在表格基础模型预训练中相比AdamW提供了优势,表明优化器选择在该领域非常重要
5. 在50GB内存下高效扩展到百万行数据集使TabICLv2在实际部署场景中具有实用性
6. 开放发布代码和权重促进了可重复性并加速了表格机器学习的研究进展
:::
