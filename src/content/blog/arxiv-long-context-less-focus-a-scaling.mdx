---
title:
  en: "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization"
  zh: "长上下文,弱聚焦:大语言模型在隐私与个性化中暴露的扩展性缺陷"
description:
  en: "This paper introduces PAPerBench, a large-scale benchmark revealing that LLMs suffer from attention dilution as context length increases, leading to degraded personalization and privacy protection."
  zh: "本文提出PAPerBench基准测试,揭示大语言模型随上下文长度增加会出现注意力稀释现象,导致个性化能力和隐私保护性能同步下降。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "/arxiv-visuals/arxiv-long-context-less-focus-a-scaling.png"
---

:::en
**Paper**: [2602.15028](https://arxiv.org/abs/2602.15028)
**Authors**: Shangding Gu
**Categories**: cs.LG, cs.AI

## Abstract

As large language models are deployed in increasingly privacy-sensitive and personalization-critical applications, understanding how context length affects their behavior becomes crucial. This paper presents PAPerBench, a comprehensive benchmark with ~29,000 instances spanning context lengths from 1K to 256K tokens (377K total evaluation questions), designed to jointly assess personalization quality and privacy leakage in LLMs. The evaluation reveals a consistent and counterintuitive finding: both personalization performance and privacy protection degrade as context length increases across state-of-the-art models. The authors provide theoretical analysis attributing this phenomenon to attention dilution in fixed-capacity Transformers, where soft attention mechanisms struggle to maintain focus as context scales. This "long context, less focus" scaling gap represents a fundamental limitation in current architectures.

## Key Contributions

- Introduction of PAPerBench, the first large-scale benchmark jointly evaluating personalization and privacy across varying context lengths (1K-256K tokens)
- Empirical demonstration that both personalization effectiveness and privacy protection degrade consistently as context length increases across multiple state-of-the-art LLMs
- Theoretical analysis of attention dilution mechanisms, explaining performance degradation as an inherent limitation of soft attention in fixed-capacity Transformers
- Identification of a fundamental "scaling gap" in current LLM architectures where longer contexts lead to reduced focus and capability
- Open-source release of benchmark data and evaluation code to enable reproducible research

## The PAPerBench Benchmark Design

PAPerBench is structured to systematically probe the intersection of context length, personalization, and privacy. The benchmark contains approximately 29,000 base instances, each evaluated at multiple context lengths, generating 377K total evaluation questions. Context lengths range from 1K tokens (short context) to 256K tokens (ultra-long context), covering the spectrum of modern LLM capabilities.

The benchmark evaluates two critical dimensions simultaneously:

**Personalization Performance**: Measures how well models utilize user-specific information embedded in context to provide tailored responses. Tasks include preference-based recommendations, user history-aware suggestions, and context-dependent decision making.

**Privacy Protection**: Assesses whether models inadvertently leak sensitive information from the context when responding to queries. This includes direct information extraction attacks, inference attacks, and membership inference scenarios.

The dual evaluation framework enables controlled analysis of the trade-offs and correlations between these two objectives as context scales.

## Empirical Findings: The Degradation Pattern

Extensive evaluation across state-of-the-art LLMs reveals a striking and consistent pattern: both personalization quality and privacy protection deteriorate as context length increases. This finding holds across different model families, sizes, and architectures.

For personalization tasks, models show progressively weaker ability to leverage user-specific information as context grows. At 1K tokens, models effectively utilize personal preferences and history. However, at 128K-256K tokens, the same information becomes effectively "invisible" to the model, resulting in generic responses that ignore user context.

Privacy metrics show a parallel degradation. Shorter contexts enable models to better compartmentalize and protect sensitive information. As context expands, the probability of privacy leakage increases—models become more likely to inadvertently expose private details when answering queries, even when such information should remain confidential.

The correlation between these two degradation patterns suggests a common underlying mechanism rather than independent failure modes.

## Theoretical Analysis: Attention Dilution

The paper provides theoretical grounding for the empirical observations through analysis of attention mechanisms in Transformers. The core insight centers on attention dilution under context scaling.

In standard Transformer architectures, attention weights are computed via softmax over all context positions:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

As context length $L$ increases, the attention distribution becomes increasingly diffuse. For a fixed model capacity (fixed number of parameters and attention heads), the attention "budget" must be distributed across more positions. This creates a fundamental tension: longer contexts require more selective attention, but the soft attention mechanism inherently spreads probability mass across all positions.

The authors formalize this as attention dilution: even when relevant information exists in context, the attention weight assigned to critical tokens decreases as $O(1/L)$ in the worst case. This dilution effect is particularly pronounced for:

- Rare or unique information (like personal details) that appears infrequently in context
- Information that requires precise retrieval rather than aggregate understanding
- Tasks requiring fine-grained discrimination between similar context elements

The theoretical analysis suggests this is not merely an optimization or training issue, but an architectural limitation of soft attention in fixed-capacity models. As context scales, the model's effective "focus" diminishes, leading to both weaker personalization (inability to attend to user-specific details) and weaker privacy (inability to selectively withhold sensitive information).

## Implications and Future Directions

The "long context, less focus" phenomenon has significant implications for LLM deployment and development:

**For Privacy-Critical Applications**: The finding that privacy protection degrades with context length challenges the assumption that more context is always better. Applications handling sensitive data may need to carefully limit context length or employ alternative architectures.

**For Personalization Systems**: The degradation in personalization effectiveness suggests that simply extending context windows is insufficient for improving user-specific adaptation. New mechanisms for selective attention or explicit memory structures may be necessary.

**For Architecture Design**: The theoretical analysis points toward fundamental limitations of soft attention that cannot be resolved through scaling alone. Future architectures may need sparse attention patterns, explicit retrieval mechanisms, or hybrid approaches combining parametric and non-parametric memory.

**For Evaluation Methodology**: PAPerBench demonstrates the importance of joint evaluation across multiple objectives. Optimizing for context length alone without considering downstream task performance and safety properties can lead to misleading conclusions about model capabilities.

The benchmark and findings open several research directions: developing attention mechanisms that maintain focus at scale, designing architectures with explicit privacy-preserving components, and creating training objectives that balance context utilization with selective information processing.

## Takeaways

1. Both personalization quality and privacy protection in LLMs degrade consistently as context length increases from 1K to 256K tokens
2. This degradation pattern holds across multiple state-of-the-art models, suggesting a fundamental architectural limitation rather than model-specific issues
3. Attention dilution in fixed-capacity Transformers provides theoretical explanation for the empirical observations—soft attention spreads focus too thinly across long contexts
4. The "long context, less focus" scaling gap represents a critical challenge for deploying LLMs in privacy-sensitive and personalization-critical applications
5. PAPerBench provides a reproducible benchmark with 377K evaluation questions for future research on scalable privacy and personalization in long-context models
:::

:::zh
**论文**: [2602.15028](https://arxiv.org/abs/2602.15028)
**作者**: Shangding Gu
**分类**: cs.LG, cs.AI

## 摘要

随着大语言模型在隐私敏感和个性化关键应用中的部署日益广泛,理解上下文长度如何影响模型行为变得至关重要。本文提出PAPerBench综合基准测试,包含约29,000个实例,涵盖1K到256K token的上下文长度(总计377K个评估问题),旨在联合评估大语言模型的个性化质量和隐私泄露风险。评估揭示了一个一致且反直觉的发现:在最先进的模型中,个性化性能和隐私保护能力都随上下文长度增加而退化。作者提供理论分析,将这一现象归因于固定容量Transformer中的注意力稀释效应,其中软注意力机制在上下文扩展时难以保持聚焦。这种"长上下文,弱聚焦"的扩展性缺陷代表了当前架构的根本性限制。

## 主要贡献

- 提出PAPerBench,首个在不同上下文长度(1K-256K token)下联合评估个性化和隐私的大规模基准测试
- 通过实证研究证明,在多个最先进的大语言模型中,个性化效果和隐私保护能力都随上下文长度增加而一致性退化
- 对注意力稀释机制进行理论分析,将性能退化解释为固定容量Transformer中软注意力的内在局限
- 识别当前大语言模型架构中的根本性"扩展性缺陷",即更长的上下文导致聚焦能力和整体能力下降
- 开源发布基准数据和评估代码,支持可复现研究

## PAPerBench基准测试设计

PAPerBench的结构设计旨在系统性探究上下文长度、个性化和隐私之间的交互关系。该基准包含约29,000个基础实例,每个实例在多个上下文长度下进行评估,生成总计377K个评估问题。上下文长度范围从1K token(短上下文)到256K token(超长上下文),覆盖现代大语言模型的能力谱系。

基准同时评估两个关键维度:

**个性化性能**:测量模型利用上下文中嵌入的用户特定信息提供定制化响应的能力。任务包括基于偏好的推荐、用户历史感知建议和上下文依赖决策。

**隐私保护**:评估模型在回答查询时是否无意中泄露上下文中的敏感信息。这包括直接信息提取攻击、推理攻击和成员推断场景。

双重评估框架使得能够对这两个目标在上下文扩展过程中的权衡和相关性进行受控分析。

## 实证发现:退化模式

对最先进大语言模型的广泛评估揭示了一个显著且一致的模式:个性化质量和隐私保护能力都随上下文长度增加而退化。这一发现在不同的模型家族、规模和架构中均成立。

对于个性化任务,模型随着上下文增长表现出逐渐减弱的用户特定信息利用能力。在1K token时,模型能有效利用个人偏好和历史。然而,在128K-256K token时,相同的信息对模型而言实际上变得"不可见",导致忽略用户上下文的通用响应。

隐私指标显示出平行的退化趋势。较短的上下文使模型能够更好地分隔和保护敏感信息。随着上下文扩展,隐私泄露的概率增加——即使这些信息应该保密,模型在回答查询时也更可能无意中暴露私密细节。

这两种退化模式之间的相关性表明存在共同的底层机制,而非独立的失效模式。

## 理论分析:注意力稀释

论文通过分析Transformer中的注意力机制为实证观察提供理论基础。核心洞察集中在上下文扩展下的注意力稀释现象。

在标准Transformer架构中,注意力权重通过对所有上下文位置的softmax计算得出:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

随着上下文长度$L$增加,注意力分布变得越来越分散。对于固定的模型容量(固定的参数数量和注意力头数),注意力"预算"必须分配到更多位置。这产生了根本性张力:更长的上下文需要更具选择性的注意力,但软注意力机制本质上会将概率质量分散到所有位置。

作者将此形式化为注意力稀释:即使相关信息存在于上下文中,分配给关键token的注意力权重在最坏情况下以$O(1/L)$的速度递减。这种稀释效应对以下情况尤为明显:

- 在上下文中不频繁出现的罕见或独特信息(如个人详情)
- 需要精确检索而非聚合理解的信息
- 需要在相似上下文元素之间进行细粒度区分的任务

理论分析表明,这不仅仅是优化或训练问题,而是固定容量模型中软注意力的架构性限制。随着上下文扩展,模型的有效"聚焦"能力减弱,导致个性化能力减弱(无法关注用户特定细节)和隐私保护减弱(无法选择性地隐藏敏感信息)。

## 影响与未来方向

"长上下文,弱聚焦"现象对大语言模型的部署和开发具有重要影响:

**对于隐私关键应用**:隐私保护随上下文长度退化的发现挑战了"更多上下文总是更好"的假设。处理敏感数据的应用可能需要谨慎限制上下文长度或采用替代架构。

**对于个性化系统**:个性化效果的退化表明,仅仅扩展上下文窗口不足以改善用户特定适应能力。可能需要新的选择性注意力机制或显式记忆结构。

**对于架构设计**:理论分析指向软注意力的根本性限制,这些限制无法仅通过扩展来解决。未来架构可能需要稀疏注意力模式、显式检索机制,或结合参数化和非参数化记忆的混合方法。

**对于评估方法论**:PAPerBench展示了跨多个目标联合评估的重要性。仅针对上下文长度进行优化而不考虑下游任务性能和安全属性,可能导致对模型能力的误导性结论。

该基准和发现开启了多个研究方向:开发在大规模下保持聚焦的注意力机制、设计具有显式隐私保护组件的架构,以及创建平衡上下文利用与选择性信息处理的训练目标。

## 要点总结

1. 大语言模型的个性化质量和隐私保护能力在上下文长度从1K增加到256K token时都出现一致性退化
2. 这种退化模式在多个最先进模型中均成立,表明这是根本性架构限制而非特定模型问题
3. 固定容量Transformer中的注意力稀释为实证观察提供理论解释——软注意力在长上下文中将聚焦分散得过于稀薄
4. "长上下文,弱聚焦"的扩展性缺陷对在隐私敏感和个性化关键应用中部署大语言模型构成重大挑战
5. PAPerBench提供包含377K个评估问题的可复现基准,支持未来在长上下文模型的可扩展隐私和个性化方面的研究
:::
