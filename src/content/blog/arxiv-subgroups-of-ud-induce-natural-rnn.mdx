---
title:
  en: "Subgroups of U(d) Induce Natural RNN and Transformer Architectures"
  zh: "U(d)的子群诱导自然的RNN和Transformer架构"
description:
  en: "A unified framework deriving RNN and transformer architectures from closed subgroups of U(d), with experimental validation on orthogonal-state models."
  zh: "从U(d)的闭子群出发构建统一框架,推导RNN和transformer架构,并在正交态模型上进行实验验证。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.lg", "cs.cl"]
image: "/arxiv-visuals/arxiv-subgroups-of-ud-induce-natural-rnn.png"
---

:::en
**Paper**: [2602.18417](https://arxiv.org/abs/2602.18417)
**Authors**: Joshua Nunley
**Categories**: cs.LG, cs.CL

## Abstract

This paper introduces a mathematical framework that unifies recurrent neural networks (RNNs) and transformers through the lens of Lie group theory. By constraining hidden states to closed subgroups of the unitary group $U(d)$, the authors derive both recurrent and attention-based architectures from a shared mathematical skeleton. The framework treats subgroup selection as a modular design choice that simultaneously determines the state space geometry, tangent space projections, and update mechanisms. Experimental validation focuses on orthogonal group $O(d)$ instantiations, demonstrating competitive performance on language modeling benchmarks while introducing a tangent-space linear mixing extension that enhances learning efficiency.

## Key Contributions

- A minimal axiomatic framework deriving sequence models from closed subgroups of $U(d)$
- Unified mathematical skeleton generating both RNN and transformer architectures through subgroup choice
- Concrete $O(d)$-based implementations with orthogonal hidden states
- Tangent-space linear mixing mechanism applicable across different subgroup selections
- Empirical validation on Tiny Shakespeare and Penn Treebank under parameter-matched conditions

## Mathematical Framework and Architecture Derivation

The core insight lies in recognizing that many successful neural architectures implicitly constrain their hidden states to structured manifolds. This paper makes these constraints explicit by working directly with closed subgroups $G \subset U(d)$. The framework begins with three axioms: (1) hidden states live in $G$, (2) updates occur via exponential maps from the tangent space $\mathfrak{g}$, and (3) learned parameters generate tangent vectors through projection operators.

From these axioms, the recurrent template emerges naturally. Given input $x_t$ and previous state $h_{t-1} \in G$, the model computes a tangent vector $v_t \in \mathfrak{g}$ via learned projection $\pi_\theta(x_t, h_{t-1})$, then updates the state through the exponential map: $h_t = h_{t-1} \exp(v_t)$. This formulation generalizes standard RNNs while maintaining geometric structure.

The transformer variant replaces sequential updates with parallel attention mechanisms. Query, key, and value projections are constrained to respect the subgroup structure, with attention weights computed over tangent-space representations. The self-attention operation becomes a structured mixing of tangent vectors before exponential mapping back to the group manifold. This geometric perspective reveals why certain architectural choices (like residual connections) emerge naturally from the group structure.

## Orthogonal Instantiation and Experimental Results

Specializing to $O(d)$ provides concrete implementations with orthogonal hidden states. The tangent space becomes the space of skew-symmetric matrices, and the exponential map reduces to matrix exponentiation. This choice offers computational advantages: orthogonal matrices preserve norms, preventing gradient explosion/vanishing, and the skew-symmetric constraint reduces parameter count.

Experiments on Tiny Shakespeare (character-level) and Penn Treebank (word-level) compare orthogonal-state RNNs and transformers against standard baselines under matched parameter budgets. The orthogonal RNN achieves comparable perplexity to LSTM while using fewer parameters, suggesting the geometric constraint acts as effective regularization. The orthogonal transformer shows competitive performance with standard transformers, with the gap narrowing as model depth increases.

The linear mixing extension introduces learnable combinations of tangent vectors before exponential mapping: $h_t = h_{t-1} \exp(\sum_i \alpha_i v_i)$ where $\alpha_i$ are learned coefficients. This mechanism, applicable to any subgroup choice, consistently improves performance across experiments, particularly in low-parameter regimes. The improvement suggests that richer tangent-space dynamics enhance the model's expressive capacity without sacrificing geometric structure.

## Theoretical Implications and Future Directions

This work bridges differential geometry and deep learning, suggesting that successful architectures may be understood as discrete approximations to flows on Lie groups. The framework raises several theoretical questions: What properties of a subgroup $G$ determine the expressiveness of the resulting architecture? Can we characterize the function classes learnable by different subgroup choices?

The modular nature of the framework invites systematic exploration of the subgroup zoo. Beyond $O(d)$, natural candidates include $SU(d)$ (special unitary), $Sp(d)$ (symplectic), and various product groups. Each choice induces different geometric constraints and computational properties. The paper's methodology provides a principled way to evaluate these alternatives.

From a practical standpoint, the tangent-space mixing mechanism deserves further investigation. Its consistent improvements suggest that enriching the tangent algebra—while maintaining group structure—may be a general principle for architecture design. Future work might explore adaptive mixing schemes, hierarchical tangent spaces, or connections to other geometric deep learning frameworks.

## Takeaways

1. Closed subgroups of $U(d)$ provide a unified mathematical foundation for deriving both RNN and transformer architectures through geometric constraints on hidden states.

2. The framework treats subgroup selection as a modular design choice that simultaneously determines state space, update mechanisms, and projection operators.

3. Orthogonal-state models ($O(d)$ instantiation) achieve competitive performance on language modeling benchmarks while offering computational advantages through norm preservation.

4. Linear mixing in tangent space improves performance across subgroup choices, particularly in parameter-constrained settings, suggesting a general principle for architecture enhancement.

5. The geometric perspective reveals deep connections between Lie group theory and neural architecture design, opening pathways for principled exploration of structured state spaces.
:::

:::zh
**论文**: [2602.18417](https://arxiv.org/abs/2602.18417)
**作者**: Joshua Nunley
**分类**: cs.LG, cs.CL

## 摘要

本文通过李群理论的视角,提出了一个统一循环神经网络(RNN)和transformer的数学框架。通过将隐藏状态约束到酉群$U(d)$的闭子群上,作者从共享的数学骨架推导出循环架构和注意力架构。该框架将子群选择视为模块化设计选项,同时决定状态空间几何、切空间投影和更新机制。实验验证聚焦于正交群$O(d)$的实例化,在语言建模基准上展示了竞争力,并引入了切空间线性混合扩展以提升学习效率。

## 主要贡献

- 从$U(d)$的闭子群出发构建最小公理化框架,推导序列模型
- 统一的数学骨架通过子群选择生成RNN和transformer架构
- 基于$O(d)$的具体实现,采用正交隐藏状态
- 适用于不同子群选择的切空间线性混合机制
- 在参数匹配条件下对Tiny Shakespeare和Penn Treebank进行实证验证

## 数学框架与架构推导

核心洞察在于认识到许多成功的神经架构隐式地将隐藏状态约束到结构化流形上。本文通过直接使用闭子群$G \subset U(d)$使这些约束显式化。框架从三个公理出发:(1)隐藏状态位于$G$中,(2)更新通过切空间$\mathfrak{g}$的指数映射进行,(3)学习参数通过投影算子生成切向量。

从这些公理自然涌现出循环模板。给定输入$x_t$和前一状态$h_{t-1} \in G$,模型通过学习投影$\pi_\theta(x_t, h_{t-1})$计算切向量$v_t \in \mathfrak{g}$,然后通过指数映射更新状态:$h_t = h_{t-1} \exp(v_t)$。这一表述推广了标准RNN同时保持几何结构。

transformer变体用并行注意力机制替换顺序更新。查询、键、值投影被约束以尊重子群结构,注意力权重在切空间表示上计算。自注意力操作变成切向量的结构化混合,然后通过指数映射返回群流形。这种几何视角揭示了为什么某些架构选择(如残差连接)从群结构中自然涌现。

## 正交实例化与实验结果

特化到$O(d)$提供了具有正交隐藏状态的具体实现。切空间变成反对称矩阵空间,指数映射简化为矩阵指数。这一选择提供计算优势:正交矩阵保持范数,防止梯度爆炸/消失,反对称约束减少参数数量。

在Tiny Shakespeare(字符级)和Penn Treebank(词级)上的实验,在匹配参数预算下比较正交态RNN和transformer与标准基线。正交RNN使用更少参数达到与LSTM相当的困惑度,表明几何约束起到有效正则化作用。正交transformer与标准transformer表现竞争力相当,随着模型深度增加差距缩小。

线性混合扩展在指数映射前引入切向量的可学习组合:$h_t = h_{t-1} \exp(\sum_i \alpha_i v_i)$,其中$\alpha_i$是学习系数。这一机制适用于任何子群选择,在实验中持续改善性能,特别是在低参数区域。改进表明更丰富的切空间动力学在不牺牲几何结构的情况下增强了模型表达能力。

## 理论意义与未来方向

这项工作连接了微分几何与深度学习,表明成功的架构可以理解为李群上流的离散近似。该框架提出几个理论问题:子群$G$的哪些性质决定了所得架构的表达能力?我们能否刻画不同子群选择可学习的函数类?

框架的模块化特性邀请对子群动物园的系统探索。除了$O(d)$,自然候选包括$SU(d)$(特殊酉群)、$Sp(d)$(辛群)和各种乘积群。每个选择诱导不同的几何约束和计算性质。本文方法提供了评估这些替代方案的原则性途径。

从实践角度看,切空间混合机制值得进一步研究。其持续改进表明,在保持群结构的同时丰富切代数可能是架构设计的一般原则。未来工作可能探索自适应混合方案、层次化切空间或与其他几何深度学习框架的联系。

## 要点总结

1. $U(d)$的闭子群通过对隐藏状态的几何约束,为推导RNN和transformer架构提供了统一的数学基础。

2. 框架将子群选择视为模块化设计选项,同时决定状态空间、更新机制和投影算子。

3. 正交态模型($O(d)$实例化)在语言建模基准上达到竞争性能,同时通过范数保持提供计算优势。

4. 切空间中的线性混合在不同子群选择中改善性能,特别是在参数受限设置下,表明这是架构增强的一般原则。

5. 几何视角揭示了李群理论与神经架构设计之间的深层联系,为结构化状态空间的原则性探索开辟了路径。
:::
