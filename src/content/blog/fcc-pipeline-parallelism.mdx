---
title:
  en: "Pipeline Parallelism from Scratch – Building Distributed AI Training"
  zh: "从零构建流水线并行 - 分布式AI训练"
description:
  en: "Learn to build pipeline parallelism for training large AI models across multiple GPUs from freeCodeCamp"
  zh: "学习构建流水线并行以在多GPU上训练大型AI模型"
date: 2026-01-26
tags: ["ai", "distributed-training", "pipeline-parallelism", "gpu", "pytorch", "freecodecamp"]
image: "https://i2.ytimg.com/vi/D5F8kp_azzw/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="D5F8kp_azzw" title="Let's Build Pipeline Parallelism from Scratch – Tutorial" />

:::en
This 3+ hour freeCodeCamp tutorial teaches you to build pipeline parallelism from scratch - a technique for training massive AI models that don't fit on a single GPU.

## Why Pipeline Parallelism?

Modern LLMs have billions of parameters:
- GPT-3: 175B parameters
- LLaMA 70B: 70B parameters
- A single GPU (even 80GB A100) can't hold these models

**Solution:** Split the model across multiple GPUs and process data like an assembly line.

## Types of Parallelism

### Data Parallelism
- Same model on each GPU
- Different data batches
- Gradients synchronized
- Limited by model size

### Model Parallelism
- Model split across GPUs
- **Tensor Parallelism**: Split individual layers
- **Pipeline Parallelism**: Split by layer groups

### Pipeline Parallelism
- Model divided into stages
- Each stage on different GPU
- Data flows through stages like assembly line
- Enables training models larger than single GPU memory

## The Pipeline Concept

```
GPU 0: Layers 1-4   →
GPU 1: Layers 5-8   →
GPU 2: Layers 9-12  →
GPU 3: Layers 13-16 →
```

**Forward Pass:** Data flows left to right
**Backward Pass:** Gradients flow right to left

## The Bubble Problem

Naive pipeline has idle time (bubbles):
- GPU 0 processes, others wait
- GPU 1 processes, GPU 0 and others wait
- Significant GPU underutilization

**Solution:** Micro-batching

## Micro-batching

Split batch into smaller micro-batches:
1. Send micro-batch 1 to GPU 0
2. While GPU 1 processes micro-batch 1, GPU 0 processes micro-batch 2
3. Pipeline stays full, reducing bubbles

**GPipe Schedule:**
- All forward passes first
- Then all backward passes
- Simple but has memory overhead

**1F1B Schedule (One Forward One Backward):**
- Interleave forward and backward
- Better memory efficiency
- More complex implementation

## Implementation Steps

### 1. Model Partitioning
```python
# Split model into stages
stage_0 = nn.Sequential(layers[0:4])
stage_1 = nn.Sequential(layers[4:8])
# ... etc
```

### 2. Communication
- Send activations between GPUs
- Use `torch.distributed` for communication
- Handle tensor serialization

### 3. Scheduling
- Manage micro-batch flow
- Coordinate forward/backward passes
- Handle gradient accumulation

### 4. Gradient Synchronization
- Accumulate gradients across micro-batches
- Apply optimizer step after full batch

## Key Concepts

### Activation Checkpointing
- Don't store all activations
- Recompute during backward pass
- Trades compute for memory

### Gradient Accumulation
- Accumulate gradients over micro-batches
- Single optimizer step per batch
- Effective larger batch size

### Communication Overlap
- Overlap computation with communication
- Hide transfer latency
- Maximize GPU utilization

## Practical Considerations

### Memory Management
- Activations consume memory
- Balance stage sizes
- Consider activation checkpointing

### Load Balancing
- Equal compute per stage
- Avoid bottlenecks
- Profile and adjust

### Fault Tolerance
- Handle GPU failures
- Checkpoint regularly
- Recovery strategies

## Frameworks

While building from scratch teaches concepts, production uses:
- **DeepSpeed**: Microsoft's library
- **Megatron-LM**: NVIDIA's solution
- **PyTorch FSDP**: Native PyTorch
- **FairScale**: Facebook's library

## Performance Tips

1. **Minimize bubble time** with proper scheduling
2. **Balance stages** for equal compute
3. **Overlap communication** with computation
4. **Use activation checkpointing** for memory
5. **Profile extensively** to find bottlenecks

## When to Use Pipeline Parallelism

**Good for:**
- Models too large for single GPU
- When you have multiple GPUs
- Training, not just inference

**Consider alternatives:**
- Data parallelism for smaller models
- Tensor parallelism for very wide layers
- Combination approaches for largest models
:::

:::zh
这个3小时以上的freeCodeCamp教程教你从零构建流水线并行——一种用于训练无法放入单个GPU的大型AI模型的技术。

## 为什么需要流水线并行？

现代LLM有数十亿参数：
- GPT-3：1750亿参数
- LLaMA 70B：700亿参数
- 单个GPU（即使是80GB A100）无法容纳这些模型

**解决方案：** 将模型分割到多个GPU上，像流水线一样处理数据。

## 并行类型

### 数据并行
- 每个GPU上相同的模型
- 不同的数据批次
- 梯度同步
- 受模型大小限制

### 模型并行
- 模型分割到多个GPU
- **张量并行**：分割单个层
- **流水线并行**：按层组分割

### 流水线并行
- 模型分成多个阶段
- 每个阶段在不同GPU上
- 数据像流水线一样流过各阶段
- 能训练超过单GPU内存的模型

## 流水线概念

```
GPU 0: 层1-4   →
GPU 1: 层5-8   →
GPU 2: 层9-12  →
GPU 3: 层13-16 →
```

**前向传播：** 数据从左到右流动
**反向传播：** 梯度从右到左流动

## 气泡问题

朴素流水线有空闲时间（气泡）：
- GPU 0处理时，其他等待
- GPU 1处理时，GPU 0和其他等待
- GPU利用率显著不足

**解决方案：** 微批次

## 微批次

将批次分成更小的微批次：
1. 发送微批次1到GPU 0
2. GPU 1处理微批次1时，GPU 0处理微批次2
3. 流水线保持满载，减少气泡

**GPipe调度：**
- 先完成所有前向传播
- 然后所有反向传播
- 简单但有内存开销

**1F1B调度（一前一后）：**
- 交错前向和反向
- 更好的内存效率
- 实现更复杂

## 实现步骤

### 1. 模型分区
```python
# 将模型分成阶段
stage_0 = nn.Sequential(layers[0:4])
stage_1 = nn.Sequential(layers[4:8])
# ... 等等
```

### 2. 通信
- 在GPU之间发送激活值
- 使用`torch.distributed`进行通信
- 处理张量序列化

### 3. 调度
- 管理微批次流
- 协调前向/反向传播
- 处理梯度累积

### 4. 梯度同步
- 跨微批次累积梯度
- 完整批次后应用优化器步骤

## 关键概念

### 激活检查点
- 不存储所有激活值
- 反向传播时重新计算
- 用计算换内存

### 梯度累积
- 跨微批次累积梯度
- 每批次单次优化器步骤
- 有效增大批次大小

### 通信重叠
- 计算与通信重叠
- 隐藏传输延迟
- 最大化GPU利用率

## 实践考虑

### 内存管理
- 激活值消耗内存
- 平衡阶段大小
- 考虑激活检查点

### 负载均衡
- 每阶段计算量相等
- 避免瓶颈
- 分析并调整

### 容错
- 处理GPU故障
- 定期检查点
- 恢复策略

## 框架

虽然从零构建能学习概念，生产环境使用：
- **DeepSpeed**：微软的库
- **Megatron-LM**：NVIDIA的方案
- **PyTorch FSDP**：原生PyTorch
- **FairScale**：Facebook的库

## 性能技巧

1. **通过适当调度最小化气泡时间**
2. **平衡阶段**以获得相等计算量
3. **重叠通信**与计算
4. **使用激活检查点**节省内存
5. **广泛分析**以找到瓶颈

## 何时使用流水线并行

**适用于：**
- 模型太大无法放入单GPU
- 当你有多个GPU时
- 训练，而非仅推理

**考虑替代方案：**
- 较小模型用数据并行
- 非常宽的层用张量并行
- 最大模型用组合方法
:::
