---
title:
  en: "When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift"
  zh: "当基准测试说谎时:真实分布偏移下的恶意提示词分类器评估"
description:
  en: "This paper exposes critical flaws in current prompt injection detection evaluation methods and proposes Leave-One-Dataset-Out (LODO) evaluation to measure true generalization, revealing that standard practices overestimate performance by up to 25% accuracy."
  zh: "本文揭示了当前提示词注入检测评估方法的关键缺陷,提出留一数据集交叉验证(LODO)方法来衡量真实泛化能力,发现标准做法高估性能达25%准确率。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.lg"]
image: "/arxiv-visuals/arxiv-when-benchmarks-lie-evaluating-malicious-prompt.png"
---

:::en
**Paper**: [2602.14161](https://arxiv.org/abs/2602.14161)
**Authors**: Max Fomin
**Categories**: cs.LG

## Abstract

As LLM-based agents increasingly handle untrusted data from diverse sources, detecting prompt injection and jailbreak attacks becomes critical for safe deployment. This paper presents a comprehensive analysis revealing fundamental limitations in current evaluation practices and production systems. Using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks, the authors propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization. The findings are striking: standard train-test splits from the same dataset sources severely overestimate performance, with aggregate metrics showing 8.4 percentage point AUC inflation and per-dataset gaps ranging from 1% to 25% accuracy. Through Sparse Auto-Encoder (SAE) analysis, the research identifies that 28% of top classifier features are dataset-dependent shortcuts rather than semantic signals. Production guardrails like PromptGuard 2 and LlamaGuard fail dramatically on indirect attacks (7-37% detection) and cannot evaluate agentic tool injection due to architectural constraints.

## Key Contributions

- **LODO Evaluation Protocol**: Introduces Leave-One-Dataset-Out cross-validation as the gold standard for measuring true generalization in prompt attack detection, exposing severe performance overestimation in standard evaluation practices
- **Comprehensive Benchmark**: Assembles 18 diverse datasets covering harmful requests, jailbreaks, indirect prompt injections, and extraction attacks to enable rigorous out-of-distribution testing
- **Dataset Shortcut Analysis**: Uses Sparse Auto-Encoder feature analysis to identify that 28% of top classifier features are dataset-dependent artifacts rather than genuine semantic signals
- **Production System Evaluation**: Systematically benchmarks PromptGuard 2, LlamaGuard, and LLM-as-judge approaches, revealing critical failures on indirect attacks and architectural limitations for agentic scenarios
- **Open-Source Framework**: Releases evaluation framework at https://github.com/maxf-zn/prompt-mining to establish reproducible standards for the research community

## The Evaluation Crisis in Prompt Security

The paper begins by highlighting a critical disconnect between evaluation practices and real-world deployment scenarios. Current research typically evaluates prompt injection classifiers using random train-test splits from the same dataset sources. While this approach yields impressive metrics, it fundamentally misrepresents how these systems perform when encountering novel attack patterns in production.

The LODO evaluation protocol addresses this by training classifiers on $n-1$ datasets and testing on the held-out dataset, repeating this process for each dataset. This methodology mirrors the actual deployment challenge: classifiers must generalize to attack patterns they haven't seen during training. The results are sobering—aggregate AUC drops by 8.4 percentage points under LODO evaluation, but this masks heterogeneous failure modes where individual datasets show accuracy gaps ranging from 1% to 25%.

This performance degradation isn't uniform across attack types. The research identifies that classifiers struggle most with indirect prompt injections targeting agentic systems, where attackers embed malicious instructions within seemingly benign content that agents process from external sources. This finding has immediate implications for the rapidly growing ecosystem of LLM-based agents that interact with emails, documents, and API responses.

## Mechanistic Analysis: Why Classifiers Fail

To understand the root causes of generalization failure, the authors employ Sparse Auto-Encoder (SAE) analysis to decompose classifier decision-making into interpretable features. SAEs learn to represent neural network activations as sparse linear combinations of learned features, enabling researchers to identify which features drive classification decisions.

The analysis reveals a troubling pattern: 28% of the top features used by classifiers are dataset-dependent shortcuts. These features exhibit strong class signal (correlation with malicious vs. benign labels) within specific datasets but fail to generalize across datasets. In other words, classifiers learn to exploit statistical artifacts of dataset construction rather than semantic properties of attacks.

For example, a feature might strongly correlate with malicious prompts in Dataset A because that dataset's collection methodology introduced systematic differences in prompt length, formatting, or vocabulary. When the classifier encounters Dataset B with different collection artifacts, this feature becomes unreliable or even misleading.

The paper introduces the concept of LODO-stable features—features that maintain consistent class signal across all LODO folds. These features represent genuine semantic patterns of malicious prompts rather than dataset artifacts. By filtering explanations to only include LODO-stable features, the research demonstrates more reliable interpretability for classifier decisions.

## Production Systems Under Scrutiny

The systematic evaluation of production guardrails reveals critical gaps in current defensive capabilities. PromptGuard 2 and LlamaGuard, two widely deployed prompt security systems, show detection rates of only 7-37% on indirect attacks targeting agents. This failure rate is particularly concerning given that indirect attacks represent the primary threat vector for agentic systems processing untrusted external data.

The architectural analysis exposes fundamental limitations: both PromptGuard 2 and LlamaGuard are designed to evaluate single prompts in isolation. They cannot assess whether a prompt attempts to manipulate an agent's tool usage or decision-making process because they lack context about the agent's capabilities, current state, and intended workflow. This architectural constraint means these systems are blind to an entire class of attacks specifically designed for agentic scenarios.

LLM-as-judge approaches, which use large language models to evaluate prompt safety, show somewhat better performance on indirect attacks but still fail to achieve reliable detection rates. The paper suggests this reflects a fundamental challenge: distinguishing between legitimate instructions for an agent and malicious attempts to hijack agent behavior requires deep understanding of context, intent, and potential consequences—capabilities that current systems lack.

## Implications for AI Safety

The research carries significant implications for AI safety practices and deployment strategies. First, it establishes that current evaluation methodologies provide false confidence in prompt security systems. Organizations deploying LLM-based agents based on published benchmark results may be operating under dangerous misconceptions about their actual security posture.

Second, the identification of dataset-dependent shortcuts suggests that simply collecting more training data won't solve the generalization problem. Without careful attention to dataset construction and evaluation methodology, larger datasets may simply encode more sophisticated shortcuts that fail in novel deployment contexts.

Third, the failure of production systems on indirect attacks highlights an urgent need for architectural innovation. Effective prompt security for agentic systems requires context-aware evaluation that understands agent capabilities, current state, and potential attack surfaces. This likely requires tight integration between security systems and agent architectures rather than bolt-on classifiers.

The paper's release of the evaluation framework and benchmark datasets provides the research community with tools to develop and validate more robust approaches. By establishing LODO as the standard evaluation protocol, the work creates accountability for claims about prompt security system performance.

## Takeaways

1. Standard evaluation practices using same-dataset train-test splits overestimate prompt classifier performance by 8.4 percentage points AUC on average, with individual dataset gaps reaching 25% accuracy
2. Leave-One-Dataset-Out (LODO) evaluation provides a more realistic measure of out-of-distribution generalization and should become the standard protocol for prompt security research
3. 28% of top classifier features are dataset-dependent shortcuts that exploit statistical artifacts rather than semantic attack patterns, explaining poor generalization
4. Production systems PromptGuard 2 and LlamaGuard achieve only 7-37% detection on indirect attacks and cannot evaluate agentic tool injection due to architectural limitations
5. LODO-stable SAE features enable more reliable interpretability by filtering dataset artifacts from classifier explanations
6. Effective prompt security for agentic systems requires architectural innovation beyond current single-prompt evaluation approaches
7. The open-source evaluation framework at https://github.com/maxf-zn/prompt-mining enables reproducible research and standardized benchmarking for the community
:::

:::zh
**论文**: [2602.14161](https://arxiv.org/abs/2602.14161)
**作者**: Max Fomin
**分类**: cs.LG

## 摘要

随着基于大语言模型的智能体越来越多地处理来自不同来源的不可信数据,检测提示词注入和越狱攻击对于安全部署变得至关重要。本文通过全面分析揭示了当前评估实践和生产系统的根本性局限。作者使用涵盖有害请求、越狱攻击、间接提示词注入和提取攻击的18个数据集构建了多样化基准,提出留一数据集交叉验证(LODO)方法来衡量真实的分布外泛化能力。研究发现令人震惊:来自相同数据集源的标准训练-测试划分严重高估了性能,聚合指标显示AUC膨胀8.4个百分点,而单个数据集的准确率差距从1%到25%不等。通过稀疏自编码器(SAE)分析,研究发现28%的顶级分类器特征是数据集依赖的捷径而非语义信号。对PromptGuard 2、LlamaGuard等生产级防护系统的评估显示,它们在间接攻击上表现糟糕(检测率7-37%),且由于架构限制无法评估智能体工具注入。

## 主要贡献

- **LODO评估协议**: 引入留一数据集交叉验证作为衡量提示词攻击检测真实泛化能力的黄金标准,揭示标准评估实践中的严重性能高估问题
- **综合基准数据集**: 汇集18个涵盖有害请求、越狱攻击、间接提示词注入和提取攻击的多样化数据集,实现严格的分布外测试
- **数据集捷径分析**: 使用稀疏自编码器特征分析识别出28%的顶级分类器特征是数据集依赖的伪影而非真实语义信号
- **生产系统评估**: 系统性地对PromptGuard 2、LlamaGuard和LLM评判方法进行基准测试,揭示在间接攻击上的关键失败和智能体场景的架构局限
- **开源框架**: 在https://github.com/maxf-zn/prompt-mining发布评估框架,为研究社区建立可复现的标准

## 提示词安全评估的危机

论文首先强调了评估实践与实际部署场景之间的关键脱节。当前研究通常使用来自相同数据集源的随机训练-测试划分来评估提示词注入分类器。虽然这种方法产生了令人印象深刻的指标,但它从根本上误导了这些系统在生产环境中遇到新型攻击模式时的实际表现。

LODO评估协议通过在$n-1$个数据集上训练分类器并在保留的数据集上测试来解决这个问题,对每个数据集重复此过程。这种方法论反映了实际部署挑战:分类器必须泛化到训练期间未见过的攻击模式。结果令人清醒——在LODO评估下,聚合AUC下降8.4个百分点,但这掩盖了异质性失败模式,个别数据集显示1%到25%的准确率差距。

这种性能下降在不同攻击类型上并不均匀。研究发现分类器在针对智能体系统的间接提示词注入上最为困难,攻击者将恶意指令嵌入到智能体从外部源处理的看似良性的内容中。这一发现对快速增长的与电子邮件、文档和API响应交互的基于LLM的智能体生态系统具有直接影响。

## 机制分析:分类器为何失败

为了理解泛化失败的根本原因,作者采用稀疏自编码器(SAE)分析将分类器决策分解为可解释的特征。SAE学习将神经网络激活表示为学习特征的稀疏线性组合,使研究人员能够识别哪些特征驱动分类决策。

分析揭示了一个令人不安的模式:分类器使用的顶级特征中有28%是数据集依赖的捷径。这些特征在特定数据集内表现出强烈的类别信号(与恶意vs良性标签的相关性),但无法跨数据集泛化。换句话说,分类器学会利用数据集构建的统计伪影,而不是攻击的语义属性。

例如,某个特征可能与数据集A中的恶意提示词强相关,因为该数据集的收集方法在提示词长度、格式或词汇方面引入了系统性差异。当分类器遇到具有不同收集伪影的数据集B时,这个特征变得不可靠甚至误导。

论文引入了LODO稳定特征的概念——在所有LODO折叠中保持一致类别信号的特征。这些特征代表恶意提示词的真实语义模式而非数据集伪影。通过过滤解释以仅包含LODO稳定特征,研究展示了分类器决策更可靠的可解释性。

## 生产系统的审视

对生产级防护系统的系统评估揭示了当前防御能力的关键差距。PromptGuard 2和LlamaGuard这两个广泛部署的提示词安全系统,在针对智能体的间接攻击上仅显示7-37%的检测率。考虑到间接攻击代表处理不可信外部数据的智能体系统的主要威胁向量,这种失败率尤其令人担忧。

架构分析暴露了根本性限制:PromptGuard 2和LlamaGuard都被设计为孤立地评估单个提示词。它们无法评估提示词是否试图操纵智能体的工具使用或决策过程,因为它们缺乏关于智能体能力、当前状态和预期工作流程的上下文。这种架构约束意味着这些系统对专门为智能体场景设计的整类攻击视而不见。

LLM评判方法使用大语言模型评估提示词安全性,在间接攻击上表现稍好,但仍未能实现可靠的检测率。论文认为这反映了一个根本挑战:区分智能体的合法指令和恶意劫持智能体行为的尝试需要对上下文、意图和潜在后果的深刻理解——这是当前系统所缺乏的能力。

## 对AI安全的影响

这项研究对AI安全实践和部署策略具有重要意义。首先,它确立了当前评估方法论对提示词安全系统提供了虚假信心。基于已发表基准结果部署基于LLM的智能体的组织可能在对其实际安全态势的危险误解下运作。

其次,数据集依赖捷径的识别表明,简单地收集更多训练数据不会解决泛化问题。如果不仔细关注数据集构建和评估方法论,更大的数据集可能只是编码更复杂的捷径,在新的部署环境中失败。

第三,生产系统在间接攻击上的失败突显了架构创新的迫切需求。智能体系统的有效提示词安全需要理解智能体能力、当前状态和潜在攻击面的上下文感知评估。这可能需要安全系统与智能体架构之间的紧密集成,而不是附加的分类器。

论文发布的评估框架和基准数据集为研究社区提供了开发和验证更强大方法的工具。通过将LODO确立为标准评估协议,这项工作为提示词安全系统性能的声明创建了问责制。

## 要点总结

1. 使用同数据集训练-测试划分的标准评估实践平均高估提示词分类器性能8.4个百分点AUC,个别数据集差距达25%准确率
2. 留一数据集交叉验证(LODO)评估提供了更现实的分布外泛化度量,应成为提示词安全研究的标准协议
3. 28%的顶级分类器特征是利用统计伪影而非语义攻击模式的数据集依赖捷径,解释了泛化能力差的原因
4. 生产系统PromptGuard 2和LlamaGuard在间接攻击上仅实现7-37%检测率,且由于架构限制无法评估智能体工具注入
5. LODO稳定SAE特征通过过滤数据集伪影实现更可靠的分类器解释
6. 智能体系统的有效提示词安全需要超越当前单提示词评估方法的架构创新
7. https://github.com/maxf-zn/prompt-mining的开源评估框架为社区实现可复现研究和标准化基准测试
:::
