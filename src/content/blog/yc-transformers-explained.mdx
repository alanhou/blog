---
title:
  en: "Notes: Transformers Explained - The Discovery That Changed AI Forever"
  zh: "笔记：Transformer 详解 - 永远改变 AI 的发现"
description:
  en: "Y Combinator's explainer on the evolution from RNNs to LSTMs to seq2seq with attention, and finally to the Transformer architecture that powers modern AI"
  zh: "Y Combinator 讲解从 RNN 到 LSTM 再到带注意力机制的 seq2seq，最终到驱动现代 AI 的 Transformer 架构的演变历程"
date: 2025-10-23
tags: ["ycombinator", "ai", "transformers", "attention", "deep-learning", "tutorial"]
image: "https://i2.ytimg.com/vi/JZLZQVmfGn8/hqdefault.jpg"
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="JZLZQVmfGn8" title="Transformers Explained: The Discovery That Changed AI Forever" />

:::en
This post summarizes Y Combinator's explainer video on the history and development of the Transformer architecture that powers nearly every state-of-the-art AI system today.

## Introduction

Nearly every state-of-the-art AI system - whether it's ChatGPT, Claude, Gemini, or Grok - is built on the same underlying model architecture: the Transformer. But where did the Transformer architecture come from? And what can its development teach us about the way breakthroughs in AI happen?

A Transformer is a neural network that uses self-attention to take input data (like text or images), model the relationships between that data, and generate outputs like meaningful text responses, translations, or classifications.

The original Transformer architecture was introduced in a now-famous 2017 paper from Google called "Attention Is All You Need." But there were three key developments that made this breakthrough possible:
1. Long Short-Term Memory (LSTM)
2. Seq2seq with Attention
3. Transformers

## The Challenge: Understanding Sequences

One of the core challenges motivating early AI research was getting neural networks to understand sequences. Natural language is inherently sequential - the meaning of a word depends on what comes before or after it, and understanding an entire sentence requires maintaining context across many words.

### Feedforward Neural Networks: The Limitation

Early architectures like feedforward neural networks process each input in isolation. They weren't capable of understanding context, or they required looking at inputs of a fixed length.

### Recurrent Neural Networks (RNNs): A Solution

Researchers developed RNNs as a solution. In simple terms, an RNN iterates over inputs in order, one at a time, and consumes the previous outputs as additional input at each step. If an input is of length N, there are N feedforward pass steps.

**The Vanishing Gradient Problem**: During the backwards pass, the gradient with respect to early inputs is the result of N matrix multiplications. In practice, this meant we often faced vanishing gradients - early inputs in a sequence had less and less influence on the network's output as the sequence grew longer.

## Long Short-Term Memory (LSTM)

In the 1990s, Hochreiter and Schmidhuber proposed a solution: Long Short-Term Memory networks (LSTMs).

**Key Innovation**: LSTMs were a type of RNN that attempted to fix the vanishing gradient problem by introducing **gates**, which could learn what information to keep, update, or forget. This made it possible to learn long-range dependencies, something vanilla RNNs struggled with.

**The Compute Problem**: LSTMs were too expensive to train at scale in the '90s, and progress stalled.

**The Revival**: Fast forward to the early 2010s - GPU acceleration, better optimization techniques, and new large-scale datasets brought LSTMs back into the spotlight. Suddenly, this relatively old architecture was viable again and began to dominate natural language processing.

LSTMs were quickly adopted for everything from speech recognition to language modeling. In these years, NLP and computer vision were somewhat separate worlds - RNNs and LSTMs were preeminent in language tasks, while CNNs were winning in vision.

### The Fixed-Length Bottleneck

Despite their success, LSTMs still had limitations. The most fundamental was the **fixed-length bottleneck**.

For sequence-to-sequence tasks like translation:
1. Take the input sentence
2. Feed it into an encoder LSTM
3. Boil the input down to a single fixed-size vector
4. A decoder LSTM takes that vector and constructs the target sentence word by word

**The Problem**: That single vector was still unable to accurately capture the meaning of long or complex sentences. There wasn't a great way to encode the concept of order into a fixed-size vector.

**Example**: In English, we put adjectives before nouns. In Spanish, adjectives often come after nouns. These models worked okay on short inputs but quickly fell apart as sequences got longer.

This pointed to a deeper architectural problem: allowing the decoder to only see one static summary of the input was a fundamental limitation. Why not give it access to all the intermediate information that the encoder saw?

## Seq2seq with Attention

In 2014, a paper introduced what would become the new standard for sequence translation: **sequence-to-sequence (seq2seq) models with attention**.

### The Core Idea

Like before, the core idea was to train two neural networks jointly:
- **Encoder**: Reads the input sequence and builds a representation of it
- **Decoder**: Generates the output sequence one step at a time

Both models were LSTMs, trained together end to end.

### The Key Insight: Attention

Even though seq2seq used a fixed-length vector, researchers realized that if you could let the decoder **look back** (or attend to) the encoder's hidden states, you could let the model learn how to align parts of the input to parts of the output.

**Bahdanau, Cho, and Bengio** showed that these models could significantly outperform traditional rule-based systems and existing seq2seq models on tasks like machine translation.

### Real-World Impact

- These models showed near state-of-the-art performance, beating even the best statistical systems at the time
- It was a sign that neural models could compete head-to-head with mature production-grade systems
- Google Translate adopted a neural seq2seq architecture around this time - you may remember this as the era when Google Translate started to finally work well

### Beyond NLP

This insight - learning to align and translate at the same time - was transformative, and it wouldn't just stay in NLP. Yoshua Bengio soon applied similar alignment-based architectures to computer vision. This was the first sign that these sequence models might be useful beyond language.

### Remaining Limitations

Even when augmented with attention, RNNs were still constrained by their sequential architecture:
- Processing tokens one at a time made it challenging to run computations in parallel
- Runtimes scaled linearly with sequence length
- Training models on large datasets became intractably slow

Researchers developed techniques like factorizing LSTM matrices or conditionally activating only parts of a network, but the fundamental linear runtime constraint remained.

## The Transformer Breakthrough (2017)

Then came the big breakthrough in 2017, when a team of researchers at Google published "Attention Is All You Need," proposing a new machine translation architecture called the **Transformer**.

### The Revolutionary Change

Transformers **scrapped recurrence entirely**, instead relying solely on an attention mechanism to generate outputs.

### How Transformers Work

At a high level, Transformers use a modified version of the encoder-decoder architecture originally proposed in seq2seq:
- Instead of compressing inputs into a single vector embedding, Transformers kept **separate embeddings for each input token**
- These embeddings are updated through **self-attention** - a mechanism that updates token representations based on a learned weighted dot product over the embeddings of all other tokens in the sequence

### The Key Advantage: Parallelization

Because each token in this architecture could attend to all others simultaneously, Transformers could **process an entire sequence in parallel**, making them dramatically faster than RNNs.

Remarkably, they were also much more accurate on machine translation benchmarks.

## Evolution of Transformer Architectures

Over the next few years, researchers experimented with different variations:

### The Original Architecture
The architecture described in the original Google paper featured an encoder and decoder that each had self-attention and cross-attention between the two - resembling the original seq2seq architectures, but without the recurrence.

### BERT: Encoder-Only
A series of models called BERT focused on using only the encoder to do masked language modeling.

### GPT: Decoder-Only
Efforts to use only the decoder for autoregressive modeling gave rise to OpenAI's GPT series of models.

Both model series can be described as subsets of the original "Attention Is All You Need" Transformer model.

### Scaling Up

It quickly became clear that these models could scale to large numbers of parameters. Ultimately, one model type - the **Generative Pre-trained Transformer (GPT)** - would be scaled up to create the LLMs that we regularly use today in products like ChatGPT or Claude.

## From Single-Task to General Intelligence

Not long ago, it wasn't obvious that there might be one model to rule them all. People were training variants of model architectures for every task:
- One for machine translation
- Another for named entity recognition
- And so on

Each had a shared backbone but slight differences in the final model layers. These models were intelligent (high accuracy), but they were largely single-task models.

At this point, there wasn't really a concept of prompting the models because there was no chat interface. Instead, people interacted with models through domain-specific inputs.

It was only as labs started to experiment with training autoregressive models on much larger datasets that they began to look and feel more like **generally intelligent systems**.

## Conclusion

This history helps contextualize what it took to get these models to a place where we could scale them:
- From the vanishing gradient problem to LSTMs
- From the fixed-length bottleneck to attention mechanisms
- From sequential processing to parallel Transformers
- From single-task models to general-purpose LLMs

The next frontier involves the architectural and engineering innovations needed to push these models to even higher performance levels.
:::

:::zh
本文总结了 Y Combinator 关于 Transformer 架构历史和发展的讲解视频，这一架构驱动着当今几乎所有最先进的 AI 系统。

## 引言

几乎每一个最先进的 AI 系统 - 无论是 ChatGPT、Claude、Gemini 还是 Grok - 都建立在相同的底层模型架构上：Transformer。但 Transformer 架构从何而来？它的发展历程能教给我们什么关于 AI 突破如何发生的道理？

Transformer 是一种神经网络，它使用自注意力机制来处理输入数据（如文本或图像），建模数据之间的关系，并生成有意义的文本响应、翻译或分类等输出。

最初的 Transformer 架构是在 Google 2017 年发表的著名论文"Attention Is All You Need"中提出的。但有三个关键发展使这一突破成为可能：
1. 长短期记忆网络（LSTM）
2. 带注意力机制的 Seq2seq
3. Transformer

## 挑战：理解序列

早期 AI 研究的核心挑战之一是让神经网络理解序列。自然语言本质上是序列化的 - 一个词的含义取决于它前后的内容，理解整个句子需要在多个词之间保持上下文。

### 前馈神经网络：局限性

早期的架构如前馈神经网络独立处理每个输入。它们无法理解上下文，或者需要查看固定长度的输入。

### 循环神经网络（RNN）：一种解决方案

研究人员开发了 RNN 作为解决方案。简单来说，RNN 按顺序逐个迭代输入，并在每一步将之前的输出作为额外输入。如果输入长度为 N，就有 N 个前向传播步骤。

**梯度消失问题**：在反向传播过程中，相对于早期输入的梯度是 N 次矩阵乘法的结果。实际上，这意味着我们经常面临梯度消失 - 随着序列变长，序列中早期的输入对网络输出的影响越来越小。

## 长短期记忆网络（LSTM）

在 1990 年代，Hochreiter 和 Schmidhuber 提出了一个解决方案：长短期记忆网络（LSTM）。

**关键创新**：LSTM 是一种 RNN，它试图通过引入**门控机制**来解决梯度消失问题，这些门可以学习保留、更新或遗忘哪些信息。这使得学习长距离依赖成为可能，而这正是普通 RNN 所困难的。

**计算问题**：LSTM 在 90 年代大规模训练成本太高，进展停滞。

**复兴**：快进到 2010 年代初 - GPU 加速、更好的优化技术和新的大规模数据集使 LSTM 重新回到聚光灯下。突然间，这个相对古老的架构再次变得可行，并开始主导自然语言处理领域。

LSTM 很快被用于从语音识别到语言建模的各种任务。在这些年里，NLP 和计算机视觉是相对独立的领域 - RNN 和 LSTM 在语言任务中占主导地位，而 CNN 在视觉领域获胜。

### 固定长度瓶颈

尽管取得了成功，LSTM 仍然有局限性。最根本的是**固定长度瓶颈**。

对于翻译等序列到序列任务：
1. 获取输入句子
2. 将其输入编码器 LSTM
3. 将输入压缩成单个固定大小的向量
4. 解码器 LSTM 获取该向量并逐词构建目标句子

**问题**：单个向量仍然无法准确捕获长句或复杂句子的含义。没有很好的方法将顺序概念编码到固定大小的向量中。

**例子**：在英语中，形容词放在名词前面。在西班牙语中，形容词通常放在名词后面。这些模型在短输入上表现还可以，但随着序列变长很快就会崩溃。

这指向了一个更深层的架构问题：只让解码器看到输入的一个静态摘要是一个根本性的限制。为什么不让它访问编码器看到的所有中间信息呢？

## 带注意力机制的 Seq2seq

2014 年，一篇论文引入了将成为序列翻译新标准的方法：**带注意力机制的序列到序列（seq2seq）模型**。

### 核心思想

与之前一样，核心思想是联合训练两个神经网络：
- **编码器**：读取输入序列并构建其表示
- **解码器**：逐步生成输出序列

两个模型都是 LSTM，端到端一起训练。

### 关键洞察：注意力机制

尽管 seq2seq 使用固定长度向量，研究人员意识到，如果能让解码器**回顾**（或关注）编码器的隐藏状态，就可以让模型学习如何将输入的部分与输出的部分对齐。

**Bahdanau、Cho 和 Bengio** 表明，这些模型可以在机器翻译等任务上显著优于传统的基于规则的系统和现有的 seq2seq 模型。

### 现实世界的影响

- 这些模型展示了接近最先进的性能，甚至击败了当时最好的统计系统
- 这表明神经模型可以与成熟的生产级系统正面竞争
- Google 翻译在这个时期采用了神经 seq2seq 架构 - 你可能记得这是 Google 翻译终于开始好用的时代

### 超越 NLP

这个洞察 - 同时学习对齐和翻译 - 是变革性的，它不会只停留在 NLP 领域。Yoshua Bengio 很快将类似的基于对齐的架构应用到计算机视觉。这是这些序列模型可能在语言之外有用的第一个迹象。

### 剩余的局限性

即使增加了注意力机制，RNN 仍然受到其序列架构的限制：
- 逐个处理 token 使得并行计算变得困难
- 运行时间随序列长度线性增长
- 在大型数据集上训练模型变得难以处理地慢

研究人员开发了诸如分解 LSTM 矩阵或有条件地只激活网络的部分等技术，但根本的线性运行时间约束仍然存在。

## Transformer 突破（2017）

然后在 2017 年迎来了重大突破，Google 的一个研究团队发表了"Attention Is All You Need"，提出了一种名为 **Transformer** 的新机器翻译架构。

### 革命性的改变

Transformer **完全抛弃了循环**，而是完全依赖注意力机制来生成输出。

### Transformer 如何工作

在高层次上，Transformer 使用了最初在 seq2seq 中提出的编码器-解码器架构的修改版本：
- 不是将输入压缩成单个向量嵌入，Transformer 为**每个输入 token 保留单独的嵌入**
- 这些嵌入通过**自注意力**更新 - 一种基于对序列中所有其他 token 嵌入的学习加权点积来更新 token 表示的机制

### 关键优势：并行化

因为这种架构中的每个 token 可以同时关注所有其他 token，Transformer 可以**并行处理整个序列**，使其比 RNN 快得多。

值得注意的是，它们在机器翻译基准测试上也更加准确。

## Transformer 架构的演变

在接下来的几年里，研究人员尝试了不同的变体：

### 原始架构
原始 Google 论文中描述的架构具有编码器和解码器，每个都有自注意力和两者之间的交叉注意力 - 类似于原始的 seq2seq 架构，但没有循环。

### BERT：仅编码器
一系列名为 BERT 的模型专注于仅使用编码器进行掩码语言建模。

### GPT：仅解码器
仅使用解码器进行自回归建模的努力催生了 OpenAI 的 GPT 系列模型。

这两个模型系列都可以被描述为原始"Attention Is All You Need"Transformer 模型的子集。

### 扩大规模

很快就清楚这些模型可以扩展到大量参数。最终，一种模型类型 - **生成式预训练 Transformer（GPT）** - 被扩大规模以创建我们今天在 ChatGPT 或 Claude 等产品中经常使用的 LLM。

## 从单任务到通用智能

不久前，一个模型统治所有任务并不明显。人们为每个任务训练模型架构的变体：
- 一个用于机器翻译
- 另一个用于命名实体识别
- 等等

每个都有共享的骨干网络，但在最终模型层有细微差异。这些模型是智能的（准确率高），但它们主要是单任务模型。

在这一点上，实际上没有提示模型的概念，因为没有聊天界面。相反，人们通过特定领域的输入与模型交互。

只有当实验室开始尝试在更大的数据集上训练自回归模型时，它们才开始看起来和感觉更像**通用智能系统**。

## 结论

这段历史帮助我们理解将这些模型发展到可以扩展的程度需要什么：
- 从梯度消失问题到 LSTM
- 从固定长度瓶颈到注意力机制
- 从序列处理到并行 Transformer
- 从单任务模型到通用 LLM

下一个前沿涉及将这些模型推向更高性能水平所需的架构和工程创新。
:::
