---
title:
  en: "Human-level 3D Shape Perception Emerges from Multi-view Learning"
  zh: "通过多视角学习实现人类水平的三维形状感知"
description:
  en: "A novel neural network framework achieves human-level 3D shape perception by learning from naturalistic multi-view visual-spatial data, without task-specific training or object-related inductive biases."
  zh: "一种新型神经网络框架通过从自然多视角视觉空间数据中学习,在无需任务特定训练或物体相关归纳偏置的情况下,实现了人类水平的三维形状感知能力。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/arxiv-human-level-3d-shape-perception-emerges.png"
---

:::en
**Paper**: [2602.17650](https://arxiv.org/abs/2602.17650)
**Authors**: Tyler Bonnen, Jitendra Malik, Angjoo Kanazawa
**Categories**: cs.CV

## Abstract

This paper presents a breakthrough in computational 3D perception by developing neural networks that match human-level performance on 3D shape inference tasks. The key innovation lies in training models with a visual-spatial objective over naturalistic multi-view data—learning to predict camera locations and visual depth from images taken at different positions within natural scenes. Remarkably, these models achieve human-level accuracy on established 3D perception benchmarks without any task-specific training or fine-tuning. The framework not only matches human accuracy but also predicts fine-grained behavioral patterns including error distributions and reaction times, suggesting a fundamental correspondence between the model's learned representations and human perceptual mechanisms.

## Key Contributions

- **First computational model to achieve human-level accuracy** on 3D shape inference tasks without task-specific training
- **Novel multi-view learning framework** that learns from visual-spatial objectives over naturalistic sensory data
- **Zero-shot evaluation methodology** demonstrating generalization to arbitrary 3D perception tasks
- **Behavioral correspondence analysis** showing models predict human error patterns and reaction times
- **Elimination of object-specific inductive biases**, relying instead on general visual-spatial learning principles

## Methodology and Architecture

The research introduces a paradigm shift in how we approach 3D perception modeling. Traditional methods have relied heavily on object-specific priors, geometric constraints, or supervised learning with 3D annotations. This work takes a fundamentally different approach inspired by the sensory information available to biological vision systems.

The multi-view learning framework operates on a simple principle: given multiple images from different viewpoints within a natural scene, the model learns to predict spatial relationships between these views. Specifically, the training objective involves predicting camera pose (location and orientation) and depth information without explicit 3D shape supervision. This visual-spatial objective mirrors the kind of information humans naturally extract from visual experience as they move through environments.

The neural architecture processes sets of images and learns representations that encode viewpoint-invariant shape information as an emergent property. Critically, the models contain no explicit 3D shape representations, object categories, or geometric primitives—all structure emerges from the multi-view consistency constraints implicit in the training data.

## Experimental Results and Human Comparison

The evaluation framework centers on a well-established psychophysical paradigm for measuring 3D shape perception. Human participants and models are presented with identical visual stimuli and must make judgments about 3D object structure. The zero-shot nature of the evaluation is crucial: models are tested on this task without any exposure to the specific stimuli or task format during training.

The results demonstrate that multi-view trained models achieve accuracy statistically indistinguishable from human performance. This represents the first time a computational model has reached human-level performance on this benchmark without task-specific optimization.

Beyond aggregate accuracy, the analysis reveals striking correspondences in error patterns. When humans make mistakes on particular stimuli, the models tend to make similar errors. This correlation extends to reaction time predictions—model processing dynamics (measured through iterative inference or layer depth) correlate with human response latencies. These behavioral signatures suggest the models have discovered computational strategies similar to those employed by human visual systems.

## Implications for Vision Science and AI

The success of this approach carries profound implications for both neuroscience and artificial intelligence. From a scientific perspective, the results suggest that human-level 3D perception may not require the elaborate geometric reasoning, object knowledge, or physical intuition often hypothesized as necessary. Instead, 3D shape understanding can emerge from a relatively simple learning objective applied to naturalistic visual-spatial data.

This finding aligns with ecological theories of perception emphasizing the information available in natural viewing conditions. As humans move through environments, they naturally experience multi-view visual information. The correspondence between model and human behavior suggests that human 3D perception may be fundamentally grounded in learning from such multi-view experiences.

For AI systems, the work demonstrates a scalable path toward human-level perceptual capabilities. The training approach requires only multi-view image data—no manual 3D annotations, object labels, or hand-crafted geometric constraints. This makes the framework applicable to diverse visual domains and potentially extensible to other perceptual modalities.

The behavioral correspondence between models and humans also opens new possibilities for using these models as computational hypotheses in vision science. The models can generate predictions about human perception in novel conditions, potentially guiding experimental design and theory development.

## Takeaways

1. Human-level 3D shape perception can emerge from multi-view learning over naturalistic visual-spatial data, without object-specific inductive biases or task-specific training.

2. Models trained with visual-spatial objectives (predicting camera pose and depth from multi-view images) achieve zero-shot performance matching human accuracy on established 3D perception benchmarks.

3. The correspondence extends beyond accuracy to fine-grained behavioral patterns—models predict human error distributions and reaction times, suggesting shared computational principles.

4. This approach eliminates the need for 3D annotations, geometric constraints, or object category labels, offering a scalable framework for perceptual AI systems.

5. The findings support ecological theories of perception, suggesting human 3D vision may be fundamentally grounded in learning from natural multi-view visual experience rather than explicit geometric reasoning.
:::

:::zh
**论文**: [2602.17650](https://arxiv.org/abs/2602.17650)
**作者**: Tyler Bonnen, Jitendra Malik, Angjoo Kanazawa
**分类**: cs.CV

## 摘要

本文在计算三维感知领域取得突破性进展,开发出在三维形状推理任务上达到人类水平表现的神经网络。核心创新在于使用视觉空间目标在自然多视角数据上训练模型——学习从自然场景中不同位置拍摄的图像预测相机位置和视觉深度。值得注意的是,这些模型在既定的三维感知基准测试中达到人类水平的准确率,且无需任何任务特定训练或微调。该框架不仅在准确率上与人类匹配,还能预测包括错误分布和反应时间在内的细粒度行为模式,表明模型学习到的表征与人类感知机制之间存在根本性对应关系。

## 主要贡献

- **首个在无任务特定训练情况下达到人类水平准确率**的三维形状推理计算模型
- **新颖的多视角学习框架**,从自然感官数据的视觉空间目标中学习
- **零样本评估方法**,展示了对任意三维感知任务的泛化能力
- **行为对应性分析**,显示模型能预测人类错误模式和反应时间
- **消除物体特定归纳偏置**,转而依赖通用视觉空间学习原则

## 方法论与架构设计

本研究在三维感知建模方法上引入了范式转变。传统方法严重依赖物体特定先验、几何约束或带有三维标注的监督学习。这项工作采用了根本不同的方法,其灵感来自生物视觉系统可获得的感官信息。

多视角学习框架基于一个简单原则:给定自然场景中不同视点的多张图像,模型学习预测这些视图之间的空间关系。具体而言,训练目标涉及预测相机姿态(位置和方向)和深度信息,而无需显式的三维形状监督。这种视觉空间目标反映了人类在环境中移动时自然提取的信息类型。

神经架构处理图像集合,学习将视点不变的形状信息编码为涌现属性的表征。关键在于,模型不包含显式的三维形状表示、物体类别或几何基元——所有结构都从训练数据中隐含的多视角一致性约束中涌现。

## 实验结果与人类对比

评估框架围绕一个成熟的心理物理学范式展开,用于测量三维形状感知。人类参与者和模型接受相同的视觉刺激,必须对三维物体结构做出判断。评估的零样本性质至关重要:模型在训练期间未接触过特定刺激或任务格式的情况下接受测试。

结果表明,多视角训练的模型达到了与人类表现在统计上无法区分的准确率。这是首次有计算模型在无任务特定优化的情况下在该基准测试上达到人类水平表现。

除了总体准确率,分析还揭示了错误模式中的显著对应性。当人类在特定刺激上犯错时,模型倾向于犯类似的错误。这种相关性延伸到反应时间预测——模型处理动态(通过迭代推理或层深度测量)与人类响应延迟相关。这些行为特征表明模型发现了与人类视觉系统类似的计算策略。

## 对视觉科学和人工智能的启示

这种方法的成功对神经科学和人工智能都具有深远影响。从科学角度看,结果表明人类水平的三维感知可能不需要通常假设必需的复杂几何推理、物体知识或物理直觉。相反,三维形状理解可以从应用于自然视觉空间数据的相对简单的学习目标中涌现。

这一发现与强调自然观察条件下可用信息的生态感知理论相一致。当人类在环境中移动时,自然会体验多视角视觉信息。模型与人类行为之间的对应性表明,人类三维感知可能从根本上基于从这种多视角体验中学习。

对于人工智能系统,这项工作展示了通往人类水平感知能力的可扩展路径。训练方法仅需要多视角图像数据——无需手动三维标注、物体标签或手工制作的几何约束。这使得框架适用于多样化的视觉领域,并可能扩展到其他感知模态。

模型与人类之间的行为对应性也为将这些模型用作视觉科学中的计算假设开辟了新可能性。模型可以生成关于新条件下人类感知的预测,潜在地指导实验设计和理论发展。

## 要点总结

1. 人类水平的三维形状感知可以从自然视觉空间数据的多视角学习中涌现,无需物体特定归纳偏置或任务特定训练。

2. 使用视觉空间目标(从多视角图像预测相机姿态和深度)训练的模型在既定三维感知基准测试上实现了与人类准确率匹配的零样本性能。

3. 对应性超越准确率延伸到细粒度行为模式——模型预测人类错误分布和反应时间,表明共享的计算原则。

4. 该方法消除了对三维标注、几何约束或物体类别标签的需求,为感知人工智能系统提供了可扩展框架。

5. 研究结果支持生态感知理论,表明人类三维视觉可能从根本上基于从自然多视角视觉体验中学习,而非显式几何推理。
:::
