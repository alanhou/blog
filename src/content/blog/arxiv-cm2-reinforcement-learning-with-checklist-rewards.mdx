---
title:
  en: "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use"
  zh: "CM2: 基于检查清单奖励的多轮多步骤智能体工具使用强化学习"
description:
  en: "A novel RL framework that uses fine-grained checklist rewards instead of verifiable outcomes to train AI agents for complex multi-turn tool use, achieving significant improvements over supervised fine-tuning."
  zh: "一种创新的强化学习框架,使用细粒度检查清单奖励替代可验证结果,用于训练复杂多轮工具使用的AI智能体,相比监督微调实现显著提升。"
date: 2026-02-13
tags: ["arxiv", "ai", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.12268](https://arxiv.org/abs/2602.12268)
**Authors**: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu
**Categories**: cs.AI

## Abstract

AI agents are increasingly deployed to handle complex real-world tasks requiring multi-turn interactions and external tool invocation. However, applying reinforcement learning to such scenarios faces fundamental challenges: realistic objectives often lack verifiable rewards, multi-turn and multi-step agentic tool use remains underexplored in RL contexts, and building executable tool environments is prohibitively expensive. This paper introduces CM2 (Checklist rewards for Multi-turn and Multi-step tool use), an RL framework that replaces traditional verifiable outcome rewards with structured checklist rewards. By decomposing intended behaviors into fine-grained binary criteria with explicit evidence grounding, CM2 transforms open-ended evaluation into stable classification decisions. The framework employs sparse reward assignment with dense evaluation criteria and trains in scalable LLM-simulated environments. Experimental results demonstrate consistent improvements over supervised fine-tuning, with an 8B model achieving 8-point gains on $\tau$-Bench, 10 points on BFCL-V4, and 12 points on ToolSandbox, matching or exceeding similarly-sized open-source baselines.

## Key Contributions

- **Checklist Reward Framework**: Introduces a novel reward mechanism that decomposes complex agent behaviors into fine-grained, verifiable binary criteria with explicit evidence grounding, making RL training more stable and interpretable
- **Sparse-Dense Strategy**: Balances training stability and informativeness through sparse reward assignment combined with dense evaluation criteria across multiple behavioral dimensions
- **Scalable LLM-Simulated Environment**: Eliminates the need for costly executable tool environments by leveraging LLM simulation, enabling training on diverse tool sets without heavy engineering overhead
- **Strong Empirical Results**: Demonstrates consistent improvements over SFT baselines across multiple benchmarks, with the 8B model achieving performance comparable to or exceeding larger open-source models

## Methodology

The CM2 framework addresses the core challenge of reward design for multi-turn, multi-step tool-using agents through several key innovations:

**Checklist Reward Structure**: Instead of relying on verifiable final outcomes, CM2 decomposes each turn's intended behavior into a structured checklist of binary criteria. Each criterion includes: (1) a clear behavioral expectation, (2) explicit evidence requirements from the agent's output, and (3) structured metadata for systematic evaluation. This decomposition converts subjective, open-ended judgments into more objective classification tasks.

**Reward Assignment Strategy**: The framework adopts a "sparse assignment, dense criteria" approach. Rewards are assigned sparsely—only at specific decision points—but each assignment considers dense evaluation across multiple criteria. This design prevents reward hacking while maintaining rich feedback signals. The reward function can be formalized as:

$$R(s, a) = \sum_{i=1}^{n} w_i \cdot \mathbb{1}[\text{criterion}_i(s, a) = \text{true}]$$

where $w_i$ represents the weight of criterion $i$, and $\mathbb{1}[\cdot]$ is the indicator function.

**LLM-Simulated Tool Environment**: Rather than building and maintaining executable tool environments, CM2 uses LLM simulation to model tool behaviors and outcomes. This approach dramatically reduces engineering costs while enabling coverage of diverse tool sets. The simulator generates realistic tool responses based on the agent's actions, creating a scalable training environment.

**Training Pipeline**: The RL training process uses Proximal Policy Optimization (PPO) with the checklist-based reward signal. Starting from a supervised fine-tuned base model, the agent explores the simulated environment, receives checklist-based feedback, and iteratively improves its policy for multi-turn tool use.

## Experimental Results

The experiments validate CM2's effectiveness across three challenging benchmarks for agentic tool use:

**Benchmark Performance**: Starting from an 8B parameter base model and training on an 8k-example RL dataset, CM2 achieves:
- **$\tau$-Bench**: 8-point improvement over SFT baseline
- **BFCL-V4**: 10-point improvement over SFT baseline  
- **ToolSandbox**: 12-point improvement over SFT baseline

These gains are particularly notable given the relatively small RL dataset size, suggesting that checklist rewards provide highly informative training signals.

**Comparison with Baselines**: The CM2-trained 8B model matches or outperforms similarly-sized open-source baselines, including models specifically designed for tool use. Remarkably, it even exceeds the performance of the judging model used to generate checklist evaluations in some cases, indicating that the RL process successfully extracts and generalizes behavioral patterns beyond the judge's explicit knowledge.

**Ablation Studies**: The paper demonstrates that both components—checklist decomposition and the sparse-dense reward strategy—are essential. Removing either component leads to degraded performance, with pure outcome-based rewards showing instability and dense reward assignment causing reward hacking behaviors.

**Generalization**: The framework shows strong generalization to unseen tools and task variations, suggesting that the learned policies capture fundamental reasoning patterns for tool use rather than memorizing specific tool behaviors.

## Implications and Future Directions

CM2 represents a significant step toward practical RL for complex agentic systems. The checklist reward paradigm offers several advantages:

**Scalability**: By eliminating the need for executable environments and verifiable outcomes, CM2 enables RL training on much larger and more diverse tool sets than previously feasible. This scalability is crucial for real-world deployment where agents must handle hundreds or thousands of tools.

**Interpretability**: The explicit checklist structure provides transparency into what behaviors are being rewarded, making it easier to diagnose training issues and align agent behavior with intended objectives. Each criterion serves as a interpretable component of the overall reward signal.

**Flexibility**: The framework can accommodate various task types and behavioral requirements by adjusting the checklist criteria. This flexibility makes it applicable beyond tool use to other multi-turn, multi-step agent scenarios.

**Future Research Directions**: Several promising extensions emerge:
- Automated checklist generation from task specifications
- Hierarchical checklists for complex, long-horizon tasks
- Integration with other RL algorithms beyond PPO
- Application to multi-agent scenarios with coordinated tool use
- Combining checklist rewards with sparse verifiable outcomes when available

The work also raises important questions about the role of LLM judges in RL training and the potential for self-improvement loops where agents exceed their evaluators' capabilities.

## Takeaways

1. Checklist rewards provide a practical alternative to verifiable outcomes for RL training of complex agents, decomposing open-ended behaviors into stable binary criteria
2. The sparse-dense reward strategy effectively balances training stability with informative feedback signals in multi-turn, multi-step scenarios
3. LLM-simulated tool environments enable scalable RL training without costly executable infrastructure, dramatically expanding the scope of trainable tool sets
4. An 8B model trained with CM2 achieves 8-12 point improvements over SFT across multiple benchmarks, matching or exceeding larger specialized baselines
5. The framework's success suggests that fine-grained behavioral decomposition may be key to applying RL in realistic agentic settings where traditional reward engineering fails
:::

:::zh
**论文**: [2602.12268](https://arxiv.org/abs/2602.12268)
**作者**: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu
**分类**: cs.AI

## 摘要

AI智能体越来越多地被部署来处理需要多轮交互和外部工具调用的复杂现实任务。然而,在这类场景中应用强化学习面临根本性挑战:现实目标往往缺乏可验证的奖励,多轮多步骤智能体工具使用在强化学习语境下仍未得到充分探索,且构建可执行工具环境的成本高昂。本文提出CM2(多轮多步骤工具使用的检查清单奖励),这是一个用结构化检查清单奖励替代传统可验证结果奖励的强化学习框架。通过将预期行为分解为具有明确证据支撑的细粒度二元标准,CM2将开放式评估转化为稳定的分类决策。该框架采用稀疏奖励分配与密集评估标准相结合的策略,并在可扩展的大语言模型模拟环境中训练。实验结果表明相比监督微调有持续改进,8B模型在$\tau$-Bench上获得8分提升,在BFCL-V4上提升10分,在ToolSandbox上提升12分,达到或超越同等规模的开源基线模型。

## 主要贡献

- **检查清单奖励框架**: 引入新颖的奖励机制,将复杂智能体行为分解为具有明确证据支撑的细粒度可验证二元标准,使强化学习训练更稳定和可解释
- **稀疏-密集策略**: 通过稀疏奖励分配结合跨多个行为维度的密集评估标准,平衡训练稳定性和信息丰富性
- **可扩展的大语言模型模拟环境**: 利用大语言模型模拟消除对昂贵可执行工具环境的需求,无需大量工程开销即可在多样化工具集上训练
- **强劲的实证结果**: 在多个基准测试中展示相比监督微调基线的持续改进,8B模型性能可比拟或超越更大规模的开源模型

## 方法论

CM2框架通过几个关键创新解决了多轮多步骤工具使用智能体奖励设计的核心挑战:

**检查清单奖励结构**: CM2不依赖可验证的最终结果,而是将每轮的预期行为分解为结构化的二元标准检查清单。每个标准包括:(1)明确的行为期望,(2)来自智能体输出的显式证据要求,(3)用于系统评估的结构化元数据。这种分解将主观的开放式判断转化为更客观的分类任务。

**奖励分配策略**: 框架采用"稀疏分配,密集标准"方法。奖励仅在特定决策点稀疏分配,但每次分配都考虑跨多个标准的密集评估。这种设计在保持丰富反馈信号的同时防止奖励欺骗。奖励函数可形式化为:

$$R(s, a) = \sum_{i=1}^{n} w_i \cdot \mathbb{1}[\text{criterion}_i(s, a) = \text{true}]$$

其中$w_i$表示标准$i$的权重,$\mathbb{1}[\cdot]$是指示函数。

**大语言模型模拟工具环境**: CM2使用大语言模型模拟来建模工具行为和结果,而非构建和维护可执行工具环境。这种方法大幅降低工程成本,同时支持多样化工具集的覆盖。模拟器根据智能体的动作生成真实的工具响应,创建可扩展的训练环境。

**训练流程**: 强化学习训练过程使用近端策略优化(PPO)算法配合基于检查清单的奖励信号。从监督微调的基础模型开始,智能体在模拟环境中探索,接收基于检查清单的反馈,并迭代改进其多轮工具使用策略。

## 实验结果

实验在三个具有挑战性的智能体工具使用基准测试中验证了CM2的有效性:

**基准测试性能**: 从8B参数基础模型开始,在8k样本的强化学习数据集上训练,CM2实现:
- **$\tau$-Bench**: 相比监督微调基线提升8分
- **BFCL-V4**: 相比监督微调基线提升10分
- **ToolSandbox**: 相比监督微调基线提升12分

考虑到相对较小的强化学习数据集规模,这些提升尤为显著,表明检查清单奖励提供了高度信息丰富的训练信号。

**与基线的比较**: CM2训练的8B模型达到或超越同等规模的开源基线模型,包括专门为工具使用设计的模型。值得注意的是,它在某些情况下甚至超过了用于生成检查清单评估的评判模型的性能,表明强化学习过程成功提取并泛化了超越评判者显式知识的行为模式。

**消融研究**: 论文证明检查清单分解和稀疏-密集奖励策略两个组件都是必不可少的。移除任一组件都会导致性能下降,纯基于结果的奖励表现出不稳定性,而密集奖励分配会导致奖励欺骗行为。

**泛化能力**: 框架对未见过的工具和任务变体表现出强大的泛化能力,表明学习到的策略捕获了工具使用的基本推理模式,而非记忆特定工具行为。

## 影响与未来方向

CM2代表了复杂智能体系统实用强化学习的重要进展。检查清单奖励范式提供了几个优势:

**可扩展性**: 通过消除对可执行环境和可验证结果的需求,CM2使得在比以往可行规模大得多且更多样化的工具集上进行强化学习训练成为可能。这种可扩展性对于智能体必须处理数百或数千个工具的现实部署至关重要。

**可解释性**: 显式的检查清单结构提供了对哪些行为被奖励的透明度,使诊断训练问题和将智能体行为与预期目标对齐变得更容易。每个标准都作为整体奖励信号的可解释组件。

**灵活性**: 框架可以通过调整检查清单标准来适应各种任务类型和行为要求。这种灵活性使其不仅适用于工具使用,还可应用于其他多轮多步骤智能体场景。

**未来研究方向**: 出现了几个有前景的扩展方向:
- 从任务规范自动生成检查清单
- 用于复杂长期任务的层次化检查清单
- 与PPO之外的其他强化学习算法集成
- 应用于具有协调工具使用的多智能体场景
- 在可用时将检查清单奖励与稀疏可验证结果结合

这项工作还提出了关于大语言模型评判者在强化学习训练中的作用以及智能体超越其评估者能力的自我改进循环潜力的重要问题。

## 要点总结

1. 检查清单奖励为复杂智能体的强化学习训练提供了可验证结果的实用替代方案,将开放式行为分解为稳定的二元标准
2. 稀疏-密集奖励策略在多轮多步骤场景中有效平衡了训练稳定性与信息丰富的反馈信号
3. 大语言模型模拟工具环境实现了无需昂贵可执行基础设施的可扩展强化学习训练,大幅扩展了可训练工具集的范围
4. 使用CM2训练的8B模型在多个基准测试中相比监督微调实现8-12分的提升,达到或超越更大规模的专用基线模型
5. 框架的成功表明,细粒度行为分解可能是在传统奖励工程失效的现实智能体设置中应用强化学习的关键
:::
