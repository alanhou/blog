---
title:
  en: "SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer"
  zh: "SMAC: 基于分数匹配的演员-评论家算法实现稳健的离线到在线迁移"
description:
  en: "A novel offline RL method that enables smooth transition to online learning by regularizing Q-functions to match policy score gradients, achieving 34-58% regret reduction in D4RL benchmarks."
  zh: "一种新型离线强化学习方法,通过正则化Q函数以匹配策略分数梯度,实现平滑的在线学习过渡,在D4RL基准测试中将遗憾值降低34-58%。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "/arxiv-visuals/arxiv-smac-score-matched-actor-critics-for.png"
---

:::en
**Paper**: [2602.17632](https://arxiv.org/abs/2602.17632)
**Authors**: Nathan S. de Lara, Florian Shkurti
**Categories**: cs.LG, cs.AI

## Abstract

This paper addresses a critical challenge in reinforcement learning: the performance collapse that occurs when fine-tuning offline-trained actor-critics with online value-based algorithms. The authors propose Score Matched Actor-Critic (SMAC), which hypothesizes that offline and online performance maxima are separated by low-performance valleys in the loss landscape. SMAC introduces a regularization technique that enforces a first-order derivative equality between the policy score function and the action-gradient of the Q-function during offline training. This approach creates smooth optimization paths between offline and online maxima, enabling seamless transfer to algorithms like Soft Actor-Critic (SAC) and TD3. Experimental results on D4RL benchmarks demonstrate perfect transfer success (6/6 tasks) with substantial regret reduction (34-58% in 4/6 environments).

## Key Contributions

- **Loss Landscape Analysis**: Provides empirical evidence that offline and online maxima are separated by performance valleys, explaining why standard fine-tuning fails
- **Score Matching Regularization**: Introduces a novel regularization term that enforces $\nabla_a \log \pi(a|s) = \nabla_a Q(s,a)$ during offline training
- **Smooth Transfer Guarantee**: Demonstrates that SMAC finds offline solutions connected to online maxima via monotonically improving paths
- **Strong Empirical Results**: Achieves 100% successful transfer rate across D4RL tasks with significant regret reduction over baselines

## Methodology

SMAC's core innovation lies in its regularization approach during the offline training phase. Traditional offline RL methods optimize the actor and critic independently, leading to misalignment when transitioning to online learning. SMAC addresses this by adding a score-matching regularization term to the critic loss:

$$\mathcal{L}_{\text{SMAC}} = \mathcal{L}_{\text{critic}} + \lambda \mathbb{E}_{(s,a) \sim \mathcal{D}} \left[\|\nabla_a Q(s,a) - \nabla_a \log \pi(a|s)\|^2\right]$$

This regularization ensures that the Q-function's action-gradient aligns with the policy's score function. The theoretical motivation stems from the observation that in optimal actor-critic solutions, these gradients should be proportional. By enforcing this relationship during offline training, SMAC creates a Q-function that naturally supports gradient-based policy improvement in the online phase.

The algorithm maintains the standard actor-critic architecture but modifies the critic update to respect this constraint. During offline training, the policy is trained using standard policy gradient methods, while the Q-function is trained with both the Bellman error and the score-matching penalty. This dual objective shapes the loss landscape such that the offline optimum lies on a ridge connecting to better online optima, rather than in an isolated valley.

## Experimental Results

The authors evaluate SMAC on six D4RL benchmark tasks spanning different domains and dataset qualities. The key findings include:

**Transfer Success**: SMAC achieves successful transfer to both SAC and TD3 in all 6/6 tested environments, with "successful" defined as maintaining or improving performance during the initial online training phase. Baseline methods like IQL, CQL, and TD3+BC frequently exhibit immediate performance drops of 20-40% upon online fine-tuning.

**Regret Reduction**: In 4 out of 6 environments (halfcheetah-medium-expert, hopper-medium, walker2d-medium-replay, and antmaze-medium-play), SMAC reduces cumulative regret by 34-58% compared to the best performing baseline. The regret metric measures the area between the optimal policy's performance and the agent's performance during online learning.

**Convergence Speed**: SMAC not only avoids initial performance drops but also converges to higher final performance faster than baselines. The monotonic improvement property means that every gradient step during online fine-tuning leads to measurable reward increases, eliminating the recovery phase required by other methods.

**Ablation Studies**: The authors demonstrate that the score-matching coefficient $\lambda$ is critical—too low fails to prevent valleys, while too high over-constrains the Q-function. The optimal range appears to be $\lambda \in [0.1, 1.0]$ depending on the environment.

## Implications and Future Directions

SMAC represents a significant advance in offline-to-online RL by addressing the fundamental geometric properties of the loss landscape rather than just improving offline performance. This perspective opens several research directions:

**Theoretical Understanding**: While the paper provides empirical evidence for the valley hypothesis, a formal characterization of when and why these valleys form would strengthen the theoretical foundation. Understanding the relationship between dataset coverage, function approximation, and landscape geometry could lead to more principled algorithm design.

**Scalability**: The score-matching regularization requires computing action-gradients of both the Q-function and policy, which adds computational overhead. Investigating efficient approximations or sampling-based methods could make SMAC more practical for high-dimensional action spaces.

**Beyond Actor-Critic**: The core insight about aligning gradients during offline training might extend to other RL paradigms. Model-based methods, for instance, could benefit from similar regularization to ensure smooth transfer when switching from model-based planning to model-free fine-tuning.

**Multi-Task Transfer**: SMAC's success in single-task offline-to-online transfer suggests potential for multi-task scenarios where an agent must transfer knowledge across different tasks or domains. The score-matching principle could help maintain performance across task boundaries.

## Takeaways

1. **Performance drops during offline-to-online transfer stem from loss landscape geometry**, specifically valleys separating offline and online maxima, rather than just poor offline training.

2. **Score-matching regularization creates smooth optimization paths** by enforcing $\nabla_a Q(s,a) \propto \nabla_a \log \pi(a|s)$ during offline training, enabling gradient-based fine-tuning without performance collapse.

3. **SMAC achieves 100% transfer success rate** across D4RL benchmarks with 34-58% regret reduction in most environments, demonstrating practical effectiveness.

4. **The approach is algorithm-agnostic** for online fine-tuning, successfully transferring to both SAC and TD3 without modification.

5. **Geometric considerations in loss landscapes** should be a primary concern when designing offline RL algorithms intended for online deployment, not just an afterthought.
:::

:::zh
**论文**: [2602.17632](https://arxiv.org/abs/2602.17632)
**作者**: Nathan S. de Lara, Florian Shkurti
**分类**: cs.LG, cs.AI

## 摘要

本文解决了强化学习中的一个关键挑战:当使用在线价值函数算法对离线训练的演员-评论家模型进行微调时,会出现性能崩溃现象。作者提出了分数匹配演员-评论家算法(SMAC),该算法假设离线和在线性能最大值在损失景观中被低性能谷地分隔。SMAC引入了一种正则化技术,在离线训练期间强制执行策略分数函数与Q函数动作梯度之间的一阶导数等式。这种方法在离线和在线最大值之间创建了平滑的优化路径,实现了向Soft Actor-Critic(SAC)和TD3等算法的无缝迁移。在D4RL基准测试上的实验结果显示完美的迁移成功率(6/6任务),并大幅降低遗憾值(在4/6环境中降低34-58%)。

## 主要贡献

- **损失景观分析**: 提供了离线和在线最大值被性能谷地分隔的实证证据,解释了标准微调失败的原因
- **分数匹配正则化**: 引入了一个新颖的正则化项,在离线训练期间强制执行 $\nabla_a \log \pi(a|s) = \nabla_a Q(s,a)$
- **平滑迁移保证**: 证明SMAC找到的离线解通过单调改进路径连接到在线最大值
- **强大的实验结果**: 在D4RL任务中实现100%的成功迁移率,相比基线方法显著降低遗憾值

## 方法论

SMAC的核心创新在于离线训练阶段的正则化方法。传统离线强化学习方法独立优化演员和评论家,导致在过渡到在线学习时出现不对齐。SMAC通过在评论家损失中添加分数匹配正则化项来解决这个问题:

$$\mathcal{L}_{\text{SMAC}} = \mathcal{L}_{\text{critic}} + \lambda \mathbb{E}_{(s,a) \sim \mathcal{D}} \left[\|\nabla_a Q(s,a) - \nabla_a \log \pi(a|s)\|^2\right]$$

这种正则化确保Q函数的动作梯度与策略的分数函数对齐。理论动机源于这样的观察:在最优演员-评论家解中,这些梯度应该成比例。通过在离线训练期间强制执行这种关系,SMAC创建了一个在在线阶段自然支持基于梯度的策略改进的Q函数。

该算法保持标准的演员-评论家架构,但修改评论家更新以遵守此约束。在离线训练期间,策略使用标准策略梯度方法训练,而Q函数则同时使用贝尔曼误差和分数匹配惩罚进行训练。这个双重目标塑造了损失景观,使得离线最优点位于连接到更好在线最优点的山脊上,而不是在孤立的谷地中。

## 实验结果

作者在六个D4RL基准任务上评估SMAC,这些任务跨越不同领域和数据集质量。主要发现包括:

**迁移成功率**: SMAC在所有6/6测试环境中成功迁移到SAC和TD3,"成功"定义为在初始在线训练阶段保持或提高性能。基线方法如IQL、CQL和TD3+BC在在线微调时经常出现20-40%的即时性能下降。

**遗憾值降低**: 在6个环境中的4个(halfcheetah-medium-expert、hopper-medium、walker2d-medium-replay和antmaze-medium-play)中,SMAC相比表现最好的基线方法将累积遗憾值降低了34-58%。遗憾值指标衡量最优策略性能与智能体在在线学习期间性能之间的差距面积。

**收敛速度**: SMAC不仅避免了初始性能下降,而且比基线方法更快地收敛到更高的最终性能。单调改进特性意味着在线微调期间的每个梯度步骤都会导致可测量的奖励增加,消除了其他方法所需的恢复阶段。

**消融研究**: 作者证明分数匹配系数 $\lambda$ 至关重要——太低无法防止谷地,太高则过度约束Q函数。最优范围似乎是 $\lambda \in [0.1, 1.0]$,具体取决于环境。

## 影响与未来方向

SMAC通过解决损失景观的基本几何特性而不仅仅是提高离线性能,代表了离线到在线强化学习的重大进展。这一视角开启了几个研究方向:

**理论理解**: 虽然论文为谷地假设提供了实证证据,但对这些谷地何时以及为何形成的正式表征将加强理论基础。理解数据集覆盖、函数近似和景观几何之间的关系可能导致更有原则的算法设计。

**可扩展性**: 分数匹配正则化需要计算Q函数和策略的动作梯度,这增加了计算开销。研究高效近似或基于采样的方法可以使SMAC在高维动作空间中更加实用。

**超越演员-评论家**: 关于在离线训练期间对齐梯度的核心见解可能扩展到其他强化学习范式。例如,基于模型的方法可以从类似的正则化中受益,以确保从基于模型的规划切换到无模型微调时的平滑迁移。

**多任务迁移**: SMAC在单任务离线到在线迁移中的成功表明在多任务场景中的潜力,其中智能体必须跨不同任务或领域迁移知识。分数匹配原则可以帮助在任务边界上保持性能。

## 要点总结

1. **离线到在线迁移期间的性能下降源于损失景观几何**,特别是分隔离线和在线最大值的谷地,而不仅仅是糟糕的离线训练。

2. **分数匹配正则化创建平滑优化路径**,通过在离线训练期间强制执行 $\nabla_a Q(s,a) \propto \nabla_a \log \pi(a|s)$,实现无性能崩溃的基于梯度的微调。

3. **SMAC在D4RL基准测试中实现100%迁移成功率**,在大多数环境中将遗憾值降低34-58%,展示了实际有效性。

4. **该方法对在线微调算法不可知**,成功迁移到SAC和TD3而无需修改。

5. **损失景观中的几何考虑**应该是设计用于在线部署的离线强化学习算法时的首要关注点,而不是事后考虑。
:::
