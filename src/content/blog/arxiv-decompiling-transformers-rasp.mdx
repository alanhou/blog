---
title:
  en: "Discovering Interpretable Algorithms by Decompiling Transformers to RASP"
  zh: "通过将Transformer反编译为RASP发现可解释算法"
description:
  en: "A general method to extract interpretable RASP programs from trained Transformers, providing direct evidence that Transformers internally implement simple algorithms"
  zh: "一种从训练好的Transformer中提取可解释RASP程序的通用方法，提供Transformer内部实现简单算法的直接证据"
date: 2026-02-10
tags: ["arxiv", "ai", "interpretability", "mechanistic-interpretability", "transformer", "rasp", "cs.CL", "cs.LG"]
image: "/arxiv-visuals/arxiv-decompiling-transformers-rasp.png"
---

:::en
**Paper**: [2602.08857](https://arxiv.org/abs/2602.08857)
**Authors**: Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn
**Categories**: cs.CL, cs.LG

## Abstract

This paper presents a general method to extract interpretable RASP (Restricted Access Sequence Processing) programs from trained Transformers. The approach re-parameterizes a Transformer as a RASP program and applies causal interventions to discover a small sufficient sub-program. When applied to small Transformers trained on algorithmic and formal language tasks, the method often recovers simple, interpretable RASP programs from length-generalizing transformers. This provides the most direct evidence to date that Transformers internally implement simple RASP-like programs.

## Key Contributions

- **Transformer-to-RASP decompilation**: A systematic method to convert trained Transformer weights into interpretable RASP programs
- **Causal intervention for simplification**: Uses causal analysis to identify and retain only the essential sub-program
- **Empirical validation**: Successfully recovers known algorithms from Transformers trained on algorithmic tasks
- **Length generalization connection**: Links interpretable internal programs to the ability to generalize to longer sequences
- **Mechanistic interpretability advance**: Provides the strongest evidence yet for the RASP hypothesis of Transformer computation

## Background: RASP and Transformer Computation

### What is RASP?

RASP (Restricted Access Sequence Processing) is a programming language designed to express computations that Transformers can perform. It provides primitives that map directly to Transformer components:

- **select**: Creates attention patterns (maps to attention heads)
- **aggregate**: Combines values based on selection patterns (maps to attention-weighted aggregation)
- **sequence operations**: Element-wise transformations (maps to MLP layers)

A RASP program is a composition of these primitives that describes an algorithm operating on sequences.

### The RASP Hypothesis

The RASP hypothesis posits that trained Transformers internally implement algorithms expressible as RASP programs. If true, this means Transformer computation is fundamentally interpretable — we just need the right tools to extract the underlying program.

## Method

### Step 1: Re-parameterization

The trained Transformer's weights are re-parameterized into RASP-compatible form. Each attention head is expressed as a **select** operation defined by its query-key interaction pattern, and each MLP layer is expressed as element-wise **soperations**:

For an attention head with query matrix $W_Q$, key matrix $W_K$, and value matrix $W_V$:

$$\text{select}(q, k) = \text{pattern derived from } W_Q^T W_K$$
$$\text{aggregate}(s, v) = \text{weighted sum using } W_V$$

### Step 2: Causal Interventions

The full RASP program derived from re-parameterization is typically large and redundant. Causal interventions identify which components are actually necessary:

1. **Ablation**: Systematically remove individual RASP operations and measure task performance
2. **Activation patching**: Replace activations at specific program points with baseline values
3. **Sufficing**: Verify that the retained sub-program alone reproduces the full model's behavior

The goal is to find the **minimal sufficient sub-program** — the smallest set of RASP operations that fully explains the model's computation.

### Step 3: Interpretation

The resulting minimal RASP program is typically small enough for human inspection. Researchers can then:
- Verify it implements a known algorithm
- Discover novel algorithmic strategies
- Understand failure modes and generalization boundaries

## Case Studies

### Sorting

For Transformers trained on sorting tasks, the decompid RASP program reveals a comparison-based algorithm using attention heads as comparators — each head implements a specific comparison operation, and the composition implements a sorting network.

### Parity

For parity computation, the extracted program shows an XOR-accumulation pattern: attention heads aggregate binary values, and the MLP computes the running XOR.

### Formal Language Recognition

For tasks like Dyck language (balanced parentheses) recognition, the decompiled program reveals a stack-like counter mechanism implemented through attention patterns that track nesting depth.

## Connection to Length Generalization
iking finding is that **Transformers with interpretable RASP programs tend to length-generalize**, while those without clean RASP decompositions fail on longer sequences. This suggests that:

- Length generalization requires learning a genuine algorithm (not pattern matching)
- RASP-expressible algorithms are inherently length-invariant
- The decompilation method can predict which models will generalize

## Results

- Successfully extracts interpretable RASP programs from Transformers trained on multiple algorithmic tasks
- Recovered programs match known optimal algorithms in most cases
- Progr from length-generalizing models are simpler and more interpretable
- Provides the most direct evidence that Transformers implement RASP-like computations

## Significance

1. **Mechanistic interpretability milestone**: Moves beyond correlation-based interpretability to actual algorithm extraction
2. **RASP hypothesis validation**: Strongest empirical support for the theoretical framework connecting Transformers and RASP
3. **Practical debugging tool**: Could help identify why models fail on specific inputs by examining the extracted algorithm
4. **Length generalization insight**: Connects interpretability to generalization, suggesting that "undersnd "generalizing" are linked
5. **Foundation for scaling**: While demonstrated on small Transformers, the methodology could potentially scale to larger models with hierarchical decomposition
:::

:::zh
**论文**: [2602.08857](https://arxiv.org/abs/2602.08857)
**作者**: Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn
**分类**: cs.CL, cs.LG

## 摘要

本文提出了一种从训练好的Transformer中提取可解释RASP（受限访问序列处理）程序的通用方法。该方法将Transformer重新参数化为RASP程序，并应用因果干预来发现小的充分子程序。当应用于在算法和形式语言任务上训练的小型Transformer时，该方法通常能从具有长度泛化能力的Transformer中恢复简单、可解释的RASP程序。这提供了迄今为止最直接的证据，表明Trarmer内部实现了简单的类RASP程序。

## 主要贡献

- **Transformer到RASP反编译**：将训练好的Transformer权重转换为可解释RASP程序的系统方法
- **因果干预简化**：使用因果分析识别并仅保留必要的子程序
- **实证验证**：成功从在算法任务上训练的Transformer中恢复已知算法
- **长度泛化联系**：将可解释的内部程序与泛化到更长序列的能力联系起来
- **机制可解释性进展**：为Transformer计算的RASP假说提供了迄今最强的证据

## 背景：RASP与Transformer计算

### 什么是RASP？

RASP（受限访问序列处理）是一种编程语言，旨在表达Transformer可以执行的计算。它提供直接映射到Transformer组件的原语：

- **select**：创建注意力模式（映射到注意力头）
- **aggregate**：基于选择模式组合值（映射到注意力加权聚合）
- **序列操作**：逐元素变换（映射到MLP层）

RASP程序是这些原语的组合，描述了在序列上运行的算法。

### RASP假说

RASP假说认为训练好的Transformer内部实现了可用RASP程序表达的算法。如果这是真的，这意味着Transforme上是可解释的——我们只需要正确的工具来提取底层程序。

## 方法

### 步骤1：重新参数化

训练好的Transformer权重被重新参数化为RASP兼容形式。每个注意力头被表达为由其查询-键交互模式定义的**select**操作，每个MLP层被表达为逐元素的**序列操作**：

对于具有查询矩阵$W_Q$、键矩阵$W_K$和值矩阵$W_V$的注意力头：

$$\text{select}(q, k) = \text{从 } W_Q^T W_K \text{ 推导的模式}$$
$$\text{aggregate}(s, v) = \text{使用 } W_V \text{ 的加权求和}$$

### 步骤2：因果干预

从重新参数化得到的完整RASP程序通常很大且冗余。因果干预识别哪些组件实际上是必要的：

1. **消融**：系统地移除单个RASP操作并测量任务性能
2. **激活修补**：用基线值替换特定程序点的激活
3. **充分性测试**：验证保留的子程序单独能复现完整模型的行为

目标是找到**最小充分子程序**——完全解释模型计算的最小RASP操作集。

### 步骤3：解释

得到的最小RASP程序通常足够小，可以进行人工检查。研究人员可以：
- 验证它实现了已知算法
- 发
- 理解失败模式和泛化边界

## 案例研究

### 排序

对于在排序任务上训练的Transformer，反编译的RASP程序揭示了一种基于比较的算法，使用注意力头作为比较器——每个头实现特定的比较操作，组合实现了排序网络。

### 奇偶校验

对于奇偶校验计算，提取的程序显示了XOR累积模式：注意力头聚合二进制值，MLP计算运行中的XOR。

### 形式语言识别

对于Dyck语言（平衡括号）识别等任务，反编译的程序揭示了通过跟踪嵌套深度的注意力模式实现的类栈计数器机制。

## 与长度泛化的联系

一个引人注目的发现是，**具有可解释RASP程序的Transformer倾向于长度泛化**，而没有干净RASP分解的Transformer在更长序列上失败。这表明：

- 长度泛化需要学习真正的算法（而非模式匹配）
- RASP可表达的算法本质上是长度不变的
- 反编译方法可以预测哪些模型会泛化

## 实验结果

- 成功从在多个算法任务上训练的Transformer中提取可解释的RASP程序
- 恢复的程序在大多数情况下与已知最优算法匹配
- 来自长度泛化模型的程序更简单、更可解释
- 提供了Transformer实现类RASP计算的最直接证据

## 重要意义

1. **机制可解释性里程碑**：从基于相关性的可解释性迈向实际算法提取
2. **RASP假说验证**：对连接Transformer和RASP的理论框架的最强实证支持
3. **实用调试工具**：可以通过检查提取的算法来帮助识别模型在特定输入上失败的原因
4. **长度泛化洞察**：将可解释性与泛化联系起来，表明"理解"和"泛化"是相关的
5. **扩展基础**：虽然在小型Transformer上演示，但该方法论可能通过层次分解扩展到更大的模型
:::
