---
title:
  en: "OmniMoE: Efficient Mixture-of-Experts via Atomic Expert Orchestration"
  zh: "OmniMoE：通过原子专家编排实现高效混合专家模型"
description:
  en: "A fine-grained MoE framework with vector-level atomic experts and Cartesian Product Router achieving 10.9x speedup over comparable approaches"
  zh: "一种细粒度MoE框架，采用向量级原子专家和笛卡尔积路由器，比同类方法快10.9倍"
date: 2026-02-05
tags: ["arxiv", "ai", "mixture-of-experts", "moe", "efficient-inference", "scaling", "cs.CL", "cs.AI"]
image: "/arxiv-visuals/arxiv-omnimoe-atomic-experts.png"
---

:::en
**Paper**: [2602.05711](https://arxiv.org/abs/2602.05711)
**Authors**: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu, Yuyu Luo
**Affiliations**: BAAI (Beijing Academy of Artificial Intelligence)
**Categories**: cs.CL, cs.AI

## Abstract

OmniMoE addresses a fundamental challenge in Mixture-of-Experts architectures: balancing expert specialization granularity with hardware efficiency. The framework introduces vector-level Atomic Experts that enable fine-grained routing while maintaining computational speed, achieving 50.9% zero-shot accuracy across seven benchmarks with 1.7B active parameters and a 10.9-fold speedup in inference latency.

## Key Contributions

- **Vector-level Atomic Experts**: Fine-grained expert decomposition at the vector level rather than full FFN blocks
- **Cartesian Product Router**: Reduces routing complexity from O(N) to O(√N)
- **Expert-Centric Scheduling**: Converts scattered memory operations into dense matrix computations

## The Granularity-Efficiency Tradeoff

Traditional MoE architectures face a fundamental tension:

- **Coarse-grained experts** (full FFN blocks): Hardware-efficient but limited routing flexibility
- **Fine-grained experts** (neuron-level): Better specialization but scattered memory access patterns that kill GPU throughput

OmniMoE resolves this by decomposing experts at the **vector level** — finer than FFN blocks but coarser than individual neurons — and introducing hardware-aware scheduling.

## Architecture

### Atomic Experts

Each expert is decomposed into vector-level atomic units that can be independently routed. This enables combinatorial expert compositions without the memory overhead of maintaining many full-sized experts.

### Cartesian Product Router

Instead of routing over N experts directly (O(N) complexity), the router operates over two smaller sets whose Cartesian product defines the expert space. This reduces routing complexity to O(√N) while maintaining the same expressiveness.

### Expert-Centric Scheduling

Converts the scattered memory access patterns inherent in fine-grained routing into dense matrix operations that GPUs can execute efficiently. This is the key to achieving the 10.9x speedup.

## Results

- **50.9% zero-shot accuracy** across seven benchmarks with 1.7B active parameters
- **10.9x speedup** in inference latency (73ms → 6.7ms) vs comparable fine-grained approaches
- Open-source code available

## Takeaways

1. **Granularity doesn't require inefficiency**: Hardware-aware design can make fine-grained routing practical
2. **Routing complexity is reducible**: Cartesian product decomposition is an elegant solution
3. **Memory access patterns matter**: Expert-centric scheduling is as important as the routing algorithm itself
:::

:::zh
**论文**: [2602.05711](https://arxiv.org/abs/2602.05711)
**作者**: Jingze Shi, Zhangyang Peng, Yizhang Zhu, Yifan Wu, Guang Liu, Yuyu Luo
**机构**: BAAI（北京人工智能研究院）
**分类**: cs.CL, cs.AI

## 摘要

OmniMoE解决了混合专家架构中的一个根本挑战：平衡专家专业化粒度与硬件效率。该框架引入了向量级原子专家，在保持计算速度的同时实现细粒度路由，以1.7B活跃参数在七个基准测试中达到50.9%零样本准确率，推理延迟加速10.9倍。

## 主要贡献

- **向量级原子专家**：在向量级别而非完整FFN块进行细粒度专家分解
- **笛卡尔积路由器**：将路由复杂度从O(N)降低到O(√N)
- **专家中心调度**：将分散的内存操作转换为密集矩阵计算

## 粒度-效率权衡

传统MoE架构面临一个根本性的张力：

- **粗粒度专家**（完整FFN块）：硬件效率高但路由灵活性有限
- **细粒度专家**（神经元级别）：更好的专业化但分散的内存访问模式会降低GPU吞吐量

OmniMoE通过在**向量级别**分解专家来解决这个问题——比FFN块更细但比单个神经元更粗——并引入硬件感知调度。

## 架构

### 原子专家

每个专家被分解为可独立路由的向量级原子单元。这使得组合式专家组合成为可能，而无需维护许多全尺寸专家的内存开销。

### 笛卡尔积路由器

路由器不直接在N个专家上路由（O(N)复杂度），而是在两个较小的集合上操作，其笛卡尔积定义了专家空间。这将路由复杂度降低到O(√N)，同时保持相同的表达能力。

### 专家中心调度

将细粒度路由固有的分散内存访问模式转换为GPU可以高效执行的密集矩阵操作。这是实现10.9倍加速的关键。

## 实验结果

- 以1.7B活跃参数在七个基准测试中达到**50.9%零样本准确率**
- 推理延迟**加速10.9倍**（73ms → 6.7ms），相比同类细粒度方法
- 开源代码可用

## 要点总结

1. **粒度不必意味着低效**：硬件感知设计可以使细粒度路由变得实用
2. **路由复杂度可降低**：笛卡尔积分解是一个优雅的解决方案
3. **内存访问模式很重要**：专家中心调度与路由算法本身同样重要
:::
