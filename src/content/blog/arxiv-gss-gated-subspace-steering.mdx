---
title:
  en: "GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs"
  zh: "GSS：门控子空间引导实现LLM选择性记忆缓解"
description:
  en: "A targeted intervention framework that detects and corrects memorization-relevant activations with 100-1000x less compute than prior methods"
  zh: "一种针对性干预框架，以比先前方法少100-1000倍的计算量检测和纠正与记忆相关的激活"
date: 2026-02-10
tags: ["arxiv", "ai", "privacy", "memorization", "llm", "interpretability", "cs.CL", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.08901](https://arxiv.org/abs/2602.08901)
**Authors**: Xuanqi Zhang, Haoyang Shang, Xiaoxiao Li
**Categories**: cs.CL, cs.LG

## Abstract

Large language models memorize and reproduce training sequences verbatim, undermining both generalization and privacy. Existing mitigation methods apply interventions uniformly across all tokens and layers, ignoring the sparse and intermittent nature of memorization. GSS (Gated Subspace Steering) decomposes the intervention into two components: a **probe** that detects memorization-relevant activations and a **steer** that applies targeted correction only when the probe signal exceeds a threshold. The optimal probe-steer pair is derived from principled optimization. GSS matches or exceeds state-of-the-art memorization reduction while requiring 100-1000x less compute.

## Key Contributions

- **Sparse memorization hypothesis**: Demonstrates that memorization is not a global property but a sparse, intermittent, token-conditioned phenomenon
- **Probe-steer decomposition**: Separates detection from correction, enabling selective intervention
- **Principled optimization**: Derives the optimal probe and steer directions from a formal objective
- **Massive compute savings**: 100-1000x reduction in computational cost compared to existing methods
- **No quality degradation**: Maintains or improves generation quality while reducing memorization

## Background: The Memorization Problem

LLMs trained on large corpora can memorize and regurgitate training data verbatim. This poses several risks:

- **Privacy violations**: Reproducing personal information from training data
- **Copyright concerns**: Generating copyrighted text passages
- **Generalizatio: Memorized responses instead of genuine reasoning

Prior approaches to mitigate memorization include:
- **Differential privacy training**: Adds noise during training but degrades model quality
- **Unlearning methods**: Require expensive retraining or fine-tuning
- **Activation editing**: Apply uniform interventions that waste compute on non-memorized tokens

## The GSS Framework

### Observation: Memorization is Sparse

GSS begins with a critical empirical observation: memorization does not activate uniformly across all tokens in a sequence. Instead, it manifests as **sparse bursts** — specific tokens at specific layers trigger the transition from generalization to verbatim recall. This means uniform intervention is wasteful and potentially harmful to non-memorized generation.

### The Probe Component

The probe is a linear projection that maps hidden states to a scalar memorization score:

$$p(h) = w_{\text{probe}}^T h$$

where $h$ is the hidden state at a given layer and token position. When $p(h) > \tau$ (a learned threshold), the model is likely entering a memorization regime.

### The Steer Component

The steer is a direction in activation space that, when added to the hidden state, disrupts the memorization pathway:

$$h' = h + \alpha \cdot \mathbb{1}[p(h) > \tau] \cdot w_{\text{steer}}$$

The indicator function $\mathbb{1}[p(h) > \tau]$ ensures the correction is applied only when memorization is detected, leaving non-memorized tokens untouched.

### Joint Optimization

The probe and steer directions are jointly optimized to minimize:

$$\mathcal{L} = \mathcal{L}_{\text{mem}}(w_{\text{probe}}, w_{\text{steer}}) + \lambda \mathcal{L}_{\text{quality}}(w_{\text{probe}}, w_{\text{steer}})$$

where $\mathcal{L}_{\text{mem}}$ measures residual memorization after intervenn and $\mathcal{L}_{\text{quality}}$ preserves generation quality on non-memorized content.

## Why This Works

The key insight is that memorization and generalization occupy **different subspaces** in the model's activation space. The probe identifies when the model's hidden state enters the memorization subspace, and the steer nudges it back toward the generalization subspace. Because these subspaces are largely orthogonal, the steer has minimal impact on non-memorized generation.

## Results

- **Memorization reduction**: Matches or exceeds state-of-the-art methods
- **Compute efficiency**: 100-1000x less compute than comparable approaches
- **Quality preservation**: No measurable degradation in generation quality on standard benchmarks
- **Generality**: Works across different model sizes and architectures

## Significance

1. **Practical privacy protection**: Enables efficient memorization mitigation for deployed models without retraining
2. **Mechanistic understanding**: Provides evidence that memorization is a localized, detectable phenomenon in activation space
3. **Selective intervention paradigm**: The probe-steer framework could generalize to other undesirable model behaviors (toxicity, bias, hallucination)
4. **Scalability**: The massive compute reduction makes this applicable to the largest production models
5. **Interpretability connection**: The learned probe directions reveal which layers and token positions are most involved in memorization
:::

:::zh
**论文**: [2602.08901](https://arxiv.org/abs/2602.08901)
**作者**: Xuanqi Zhang, Haoyang Shang, Xiaoxiao Li
**分类**: cs.CL, cs.LG

## 摘要

大语言模型会记忆并逐字复现训练序列，损害泛化能力和隐私安全。现有缓解方法在所有token和层上统一应用干预，忽略了记忆的稀疏和间歇性本质。GSS（门控子空个组件：检测记忆相关激活的**探针**和仅在探针信号超过阈值时应用针对性纠正的**引导**。最优探针-引导对通过有原则的优化得出。GSS在匹配或超越最先进记忆缩减效果的同时，所需计算量减少100-1000倍。

## 主要贡献

- **稀疏记忆假说**：证明记忆不是全局属性，而是稀疏的、间歇性的、token条件化的现象
- **探针-引导分解**：将检测与纠正分离，实现选择性干预
- **有原则的优化**：从形式化目标推导最优探针和引导方向
- **大幅计算节省**：与现有方法相比计算成本降低100-1000倍
- **无质量退化**：在减少记忆的同时保持或提高生成质量

## 背景：记忆问题

在大规模语料库上训练的LLM可以记忆并逐字复现训练数据。这带来了多种风险：

- **隐私侵犯**：从训练数据中复现个人信息
- **版权问题**：生成受版权保护的文本段落
- **泛化失败**：记忆的回复而非真正的推理

先前缓解记忆的方法包括：
- **差分隐私训练**：在训练期间添加噪声，但会降低模型质量
- **遗忘方法**：需要昂贵的重新训练或微调
- **激活编辑**：应用统一干预，在非记忆token上浪费计算

## GSS框架

### 观察：记忆是稀疏的

GSS始于一个关键的实证观察：记忆不会在序列中所有token上均匀激活。相反，它表现为**稀疏爆发**——特定层的特定token触发从泛化到逐字回忆的转变。这意味着统一干预是浪费的，并且可能对非记忆生成有害。

### 探针组件

探针是一个线性投影，将隐藏状态映射到标量记忆分数：

$$p(h) = w_{\text{probe}}^T h$$

其中$h$是给定层和token位置的隐藏状态。当$p(h) > \tau$（学习的阈值）时，模型可能正在进入记忆模式。

### 引导组件

引导是激活空间中的一个方向，添加到隐藏状态时可以破坏记忆路径：

$$h' = h + \alpha \cdot \mathbb{1}[p(h) > \tau] \cdot w_{\text{steer}}$$

指示函数$\mathbb{1}[p(h) > \tau]$确保仅在检测到记忆时应用纠正，不影响非记忆token。

### 联合优化

探针和引导方向被联合优化以最小化：

$$\mathcal{L} = \mathcal{L}_{\text{mem}}(w_{\text{probe}}, w_{\text{steer}}) + \lambda \mathcal{L}_{\text{quality}}(w_{\text{probe}}, w_{\text{steer}})$$

其中$\mathcal{L}_{\text{mem}}$衡量干预后的残余记忆，$\mathcal{L}_{\text{quality}}$保持非记忆内容的生成质量。

## 为什么有效

关键洞察是记忆和泛化占据模型激活空间中的**不同子空间**。探针识别模型隐藏状态何时进入记忆子空间，引导将其推回泛化子空间。由于这些子空间在很大程度上是正交的，引导对非记忆生成的影响最小。

## 实验结果

- **记忆缩减**：匹配或超越最先进方法
- **计算效率**：比同类方法少100-1000倍计算量
- **质量保持**：在标准基准上无可测量的生成质量退化
- **通用性**：适用于不同模型规模和架构

## 重要意义

1. **实用隐私保护**：无需重新训练即可为已部署模型实现高效记忆缓解
2. **机制理解**：提供证据表明记忆是激活空间中可定位、可检测的现象
3. **选择性干预范式**：探针-引导框架可推广到其他不良模型行为（毒性、偏见、幻觉）
4. **可扩展性**：大幅计算缩减使其适用于最大的生产模型
5. **可解释性联系**：学习到的探针方向揭示了哪些层和token位置最参与记忆
:::
