---
title:
  en: "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control"
  zh: "生成现实:基于手部和相机控制的以人为中心的交互式视频生成世界模拟"
description:
  en: "A novel video world model that enables fine-grained control through tracked head and hand poses, creating interactive egocentric virtual environments for extended reality applications."
  zh: "一种新型视频世界模型,通过跟踪头部和手部姿态实现精细控制,为扩展现实应用创建交互式第一人称虚拟环境。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.cv"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.18422](https://arxiv.org/abs/2602.18422)
**Authors**: Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai, Gordon Wetzstein
**Categories**: cs.CV

## Abstract

This paper introduces a human-centric video world model designed for extended reality (XR) applications that responds to fine-grained user motion. Unlike existing video generation models that rely on coarse control signals like text prompts or keyboard inputs, this system accepts tracked head pose and joint-level hand poses as conditioning inputs. The authors propose an effective conditioning mechanism for diffusion transformers that enables 3D head and hand control, facilitating realistic hand-object interactions. The system employs a teacher-student distillation approach, training a bidirectional video diffusion model and distilling it into a causal, real-time interactive system. Human subject evaluations demonstrate that this "generated reality" system achieves superior task performance and significantly higher perceived control compared to baseline methods.

## Key Contributions

- Introduction of a human-centric video world model conditioned on both 3D head pose and joint-level hand poses for embodied XR interactions
- Evaluation and comparison of existing diffusion transformer conditioning strategies for fine-grained motion control
- A novel conditioning mechanism that effectively integrates 3D spatial information for head and hand tracking into video generation
- A teacher-student distillation framework that converts a bidirectional diffusion model into a causal, interactive system suitable for real-time applications
- Comprehensive human subject evaluation demonstrating improved task performance and perceived control in generated virtual environments

## Technical Methodology

The core technical innovation lies in the conditioning strategy for diffusion transformers. Traditional video generation models struggle with fine-grained spatial control because they typically process control signals through text embeddings or simple concatenation. This work addresses the challenge by designing a conditioning mechanism specifically for 3D pose data.

The system architecture consists of two main components: a teacher model and a student model. The teacher is a bidirectional video diffusion model that can access both past and future frames, allowing it to generate high-quality, temporally coherent videos. This model is trained on egocentric video data paired with tracked head and hand poses, learning the relationship between human motion and the resulting visual observations.

The conditioning mechanism processes head pose (6-DOF: position and orientation) and hand poses (joint-level articulation for both hands) through specialized encoders that preserve spatial relationships. These encoded representations are then injected into the diffusion transformer at multiple layers, enabling the model to maintain consistent 3D geometry throughout the generation process.

For real-time interaction, the bidirectional teacher model is distilled into a causal student model that generates frames autoregressively, conditioned only on past frames and current pose inputs. This distillation process preserves the quality of the teacher while enabling the low-latency generation required for interactive XR applications.

## Results and Evaluation

The evaluation methodology combines quantitative metrics with human subject studies to assess both technical performance and user experience. The authors conducted experiments where participants performed various tasks in the generated virtual environments, comparing the proposed system against relevant baselines including text-conditioned video generation and simpler pose-conditioning approaches.

Quantitative results show that the proposed conditioning mechanism achieves better temporal consistency and spatial accuracy in hand-object interactions compared to baseline methods. The system maintains stable geometry even during complex hand manipulations, a critical requirement for believable virtual interactions.

Human subject evaluations reveal two key findings. First, participants achieved significantly better task completion rates when using the full pose-conditioned system compared to coarser control methods. Second, and perhaps more importantly, users reported a substantially higher perceived level of control over their actions in the virtual environment. This subjective measure of agency is crucial for XR applications, as it directly impacts user immersion and comfort.

The system demonstrates particular strength in scenarios involving dexterous hand movements, such as grasping objects, manipulating tools, or performing fine motor tasks. The joint-level hand pose conditioning enables the model to generate realistic hand-object contact and occlusion patterns that would be difficult to achieve with coarser control signals.

## Implications for Extended Reality

This work represents a significant step toward truly interactive virtual world generation for XR applications. Current XR systems typically rely on pre-authored content or simple procedural generation, limiting the diversity and adaptability of virtual environments. By enabling real-time generation of egocentric video conditioned on user motion, this approach opens new possibilities for dynamic, personalized XR experiences.

The human-centric design philosophy is particularly noteworthy. Rather than treating the user as an external observer, the system places human motion at the center of the generation process. This aligns naturally with the embodied nature of XR, where users expect their physical actions to have meaningful consequences in the virtual world.

Potential applications extend beyond entertainment to include training simulations, telepresence, and assistive technologies. For example, the system could generate realistic training scenarios that adapt to a trainee's actions, or create shared virtual spaces where remote collaborators can interact with virtual objects using natural hand gestures.

However, challenges remain. Real-time generation of high-resolution video requires substantial computational resources, and the current system's latency may still be perceptible in some scenarios. Additionally, the model's ability to generalize to novel objects and environments depends on the diversity of training data, suggesting that further work on data efficiency and few-shot adaptation would be valuable.

## Takeaways

1. Fine-grained pose conditioning (head + joint-level hands) enables significantly more controllable and interactive video generation compared to text or keyboard-based control
2. Specialized conditioning mechanisms for 3D spatial data are crucial for maintaining geometric consistency in generated egocentric video
3. Teacher-student distillation provides an effective path from high-quality bidirectional models to real-time causal generation systems
4. Human subject evaluation reveals that perceived control is as important as technical metrics for XR applications
5. This approach bridges the gap between passive video generation and interactive world simulation, opening new possibilities for embodied AI and XR experiences
:::

:::zh
**论文**: [2602.18422](https://arxiv.org/abs/2602.18422)
**作者**: Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai, Gordon Wetzstein
**分类**: cs.CV

## 摘要

本文提出了一种面向扩展现实(XR)应用的以人为中心的视频世界模型,能够响应精细的用户运动。与现有依赖文本提示或键盘输入等粗粒度控制信号的视频生成模型不同,该系统接受跟踪的头部姿态和关节级手部姿态作为条件输入。作者提出了一种有效的扩散变换器条件机制,实现3D头部和手部控制,促进真实的手-物体交互。系统采用教师-学生蒸馏方法,训练双向视频扩散模型并将其蒸馏为因果的实时交互系统。人类受试者评估表明,这种"生成现实"系统相比基线方法实现了更优的任务性能和显著更高的感知控制水平。

## 主要贡献

- 引入以人为中心的视频世界模型,同时以3D头部姿态和关节级手部姿态为条件,支持具身XR交互
- 评估和比较现有扩散变换器条件策略在精细运动控制方面的表现
- 提出新颖的条件机制,有效地将头部和手部跟踪的3D空间信息整合到视频生成中
- 设计教师-学生蒸馏框架,将双向扩散模型转换为适合实时应用的因果交互系统
- 通过全面的人类受试者评估,证明在生成虚拟环境中任务性能和感知控制的提升

## 技术方法

核心技术创新在于扩散变换器的条件策略。传统视频生成模型难以实现精细的空间控制,因为它们通常通过文本嵌入或简单拼接处理控制信号。本工作通过专门为3D姿态数据设计条件机制来解决这一挑战。

系统架构包含两个主要组件:教师模型和学生模型。教师模型是双向视频扩散模型,可以访问过去和未来帧,从而生成高质量、时间连贯的视频。该模型在配对的第一人称视频数据和跟踪的头部及手部姿态上训练,学习人体运动与视觉观察结果之间的关系。

条件机制通过专门的编码器处理头部姿态(6自由度:位置和方向)和手部姿态(双手的关节级关节运动),保留空间关系。这些编码表示随后在多个层级注入扩散变换器,使模型能够在整个生成过程中保持一致的3D几何结构。

为实现实时交互,双向教师模型被蒸馏为因果学生模型,该模型仅基于过去帧和当前姿态输入自回归生成帧。这一蒸馏过程在保持教师模型质量的同时,实现了交互式XR应用所需的低延迟生成。

## 结果与评估

评估方法结合定量指标和人类受试者研究,评估技术性能和用户体验。作者进行了实验,让参与者在生成的虚拟环境中执行各种任务,将所提系统与相关基线(包括文本条件视频生成和更简单的姿态条件方法)进行比较。

定量结果显示,所提条件机制在手-物体交互的时间一致性和空间准确性方面优于基线方法。即使在复杂的手部操作过程中,系统也能保持稳定的几何结构,这是可信虚拟交互的关键要求。

人类受试者评估揭示了两个关键发现。首先,与粗粒度控制方法相比,使用完整姿态条件系统的参与者实现了显著更高的任务完成率。其次,也许更重要的是,用户报告在虚拟环境中对其行为的感知控制水平大幅提高。这种主观的能动性测量对XR应用至关重要,因为它直接影响用户沉浸感和舒适度。

系统在涉及灵巧手部动作的场景中表现出特别的优势,如抓取物体、操纵工具或执行精细运动任务。关节级手部姿态条件使模型能够生成真实的手-物体接触和遮挡模式,这在使用粗粒度控制信号时难以实现。

## 对扩展现实的影响

这项工作代表了向XR应用的真正交互式虚拟世界生成迈出的重要一步。当前的XR系统通常依赖预先创作的内容或简单的程序生成,限制了虚拟环境的多样性和适应性。通过实现基于用户运动条件的第一人称视频实时生成,这种方法为动态、个性化的XR体验开辟了新的可能性。

以人为中心的设计理念尤其值得注意。系统不是将用户视为外部观察者,而是将人体运动置于生成过程的中心。这与XR的具身特性自然契合,用户期望其物理行为在虚拟世界中产生有意义的后果。

潜在应用超越娱乐,包括训练模拟、远程呈现和辅助技术。例如,系统可以生成适应受训者行为的真实训练场景,或创建共享虚拟空间,让远程协作者使用自然手势与虚拟物体交互。

然而,挑战依然存在。高分辨率视频的实时生成需要大量计算资源,当前系统的延迟在某些场景中可能仍然可感知。此外,模型对新物体和环境的泛化能力取决于训练数据的多样性,这表明在数据效率和少样本适应方面的进一步工作将很有价值。

## 要点总结

1. 精细的姿态条件(头部+关节级手部)相比文本或键盘控制,能够实现显著更可控和交互的视频生成
2. 针对3D空间数据的专门条件机制对于在生成的第一人称视频中保持几何一致性至关重要
3. 教师-学生蒸馏为从高质量双向模型到实时因果生成系统提供了有效路径
4. 人类受试者评估揭示,感知控制对XR应用与技术指标同样重要
5. 这种方法弥合了被动视频生成与交互式世界模拟之间的差距,为具身AI和XR体验开辟了新的可能性
:::
