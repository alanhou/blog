---
title:
  en: "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs"
  zh: "当视觉凌驾于语言之上:评估和缓解视觉-语言-动作模型中的反事实失效"
description:
  en: "This paper introduces LIBERO-CF benchmark to expose how VLAs ignore language instructions due to visual shortcuts, and proposes Counterfactual Action Guidance (CAG) to improve language following without retraining."
  zh: "本文提出LIBERO-CF基准来揭示VLA模型如何因视觉捷径而忽略语言指令,并提出反事实动作引导(CAG)方法在无需重训练的情况下提升语言遵循能力。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.cv", "cs.ro"]
image: "/arxiv-visuals/arxiv-when-vision-overrides-language-evaluating-and.png"
---

:::en
**Paper**: [2602.17659](https://arxiv.org/abs/2602.17659)
**Authors**: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding
**Categories**: cs.CV, cs.RO

## Abstract

Vision-Language-Action (VLA) models are designed to translate natural language instructions into robotic actions by grounding language in visual perception. However, this research reveals a critical failure mode: when instructions lack strong scene-specific supervision, VLAs fall back on visual shortcuts learned from dataset biases rather than following language intent. The authors introduce LIBERO-CF, the first counterfactual benchmark that systematically evaluates language following by presenting alternative instructions in visually similar scenarios. Their evaluation exposes widespread counterfactual failures across state-of-the-art VLAs. To address this, they propose Counterfactual Action Guidance (CAG), a dual-branch inference scheme that combines a standard VLA policy with a language-unconditioned Vision-Action module to explicitly regularize language conditioning. CAG achieves substantial improvements in both language following accuracy and task success without requiring additional training data or architectural modifications.

## Key Contributions

- **LIBERO-CF Benchmark**: First systematic counterfactual evaluation framework for VLAs that tests language following capability by assigning alternative instructions under visually plausible layouts
- **Counterfactual Failure Analysis**: Comprehensive characterization of how VLAs rely on visual shortcuts and dataset biases, ignoring language instructions in under-observed scenarios
- **Counterfactual Action Guidance (CAG)**: A plug-and-play dual-branch inference method that regularizes language conditioning through counterfactual comparison with vision-only policies
- **Extensive Validation**: Demonstrated 9.7% improvement in language following accuracy and up to 17.2% improvement in real-world task success across diverse VLA architectures

## The Problem: Visual Shortcuts Override Language

VLAs are trained on large-scale robot manipulation datasets where certain object-action pairs appear frequently. This creates strong visual priors: when a robot sees a frequently manipulated object, it tends to interact with it regardless of what the language instruction actually says. The paper identifies this as **counterfactual failure** - the model fails when presented with counterfactual instructions that contradict its learned visual shortcuts.

For example, in a kitchen scene with both a red mug and a blue plate, if the training data predominantly shows "pick up the red mug," the VLA may continue picking up the red mug even when instructed to "pick up the blue plate." The visual presence of the familiar red mug overrides the language specification.

This phenomenon is particularly problematic because:
- Standard benchmarks don't expose it (they use instructions aligned with training distribution)
- It undermines the core promise of language-conditioned control
- It limits generalization to novel task compositions

## LIBERO-CF: A Counterfactual Benchmark

The authors construct LIBERO-CF by taking existing LIBERO manipulation tasks and systematically creating counterfactual variants. The key insight is to keep visual layouts similar while swapping instruction targets to objects or goals that are less frequently associated with the observed scene configuration.

The benchmark includes:
- **Spatial counterfactuals**: Same objects, different spatial relationships ("put A on B" vs "put B on A")
- **Object counterfactuals**: Multiple similar objects, instruction specifies the less-common target
- **Action counterfactuals**: Same scene, different manipulation primitives

Each counterfactual pair allows direct measurement of whether the model follows language or defaults to visual priors. The evaluation metric $\pi_{0.5}$ measures the probability of selecting the correct action when both the factual and counterfactual options are equally plausible visually.

## Counterfactual Action Guidance (CAG)

CAG addresses counterfactual failures through a simple but effective principle: **compare what the model does with language against what it would do without language**. If they're too similar, the model isn't really using the language instruction.

The method works as follows:

1. **Dual-branch inference**: Run both a VLA policy $\pi_{\text{VLA}}(a|v,l)$ and a Vision-Action policy $\pi_{\text{VA}}(a|v)$ that sees only visual input
2. **Counterfactual comparison**: Compute the divergence between their action distributions
3. **Regularized action selection**: Adjust the VLA's action to increase divergence from the VA baseline

Mathematically, CAG optimizes:

$$a^* = \arg\max_a \left[ \log \pi_{\text{VLA}}(a|v,l) - \lambda \log \pi_{\text{VA}}(a|v) \right]$$

where $\lambda$ controls the strength of regularization. This formulation has an elegant interpretation: it encourages actions that the VLA assigns high probability to, but the VA model assigns low probability to - precisely the actions that require language understanding.

**Two variants** are proposed:
- **CAG-free**: Uses the pretrained VLA's vision encoder as the VA module (training-free)
- **CAG-trained**: Trains a dedicated VA module on the same data without language labels

## Experimental Results

The evaluation spans multiple dimensions:

**LIBERO-CF Benchmark Performance**:
- Baseline VLAs show 15-25% counterfactual failure rates
- CAG-free improves language following accuracy by 9.7% and task success by 3.6%
- CAG-trained achieves 15.5% and 8.5% improvements respectively
- Improvements are consistent across different VLA architectures (OpenVLA, Octo, RT-1)

**Under-observed Task Generalization**:
- On tasks with limited training examples, CAG shows even larger gains
- Reduces over-reliance on frequent object-action associations
- Maintains performance on well-observed tasks while improving on rare ones

**Real-world Robot Experiments**:
- Tested on physical manipulation tasks with natural language instructions
- 9.4% reduction in counterfactual failures
- 17.2% average improvement in task success rate
- Demonstrates practical applicability beyond simulation

**Ablation Studies**:
- The regularization strength $\lambda$ requires tuning but is robust across a range
- Both CAG variants outperform baseline, with trained VA showing best results
- The method is complementary to other improvements like better pretraining

## Implications and Future Directions

This work reveals a fundamental tension in VLA design: **vision provides strong priors that can override weaker language signals**. While this might seem like a bug, it's actually a rational response to training data statistics - if certain visual patterns reliably predict actions, why attend to potentially noisy language?

The CAG solution is philosophically interesting because it doesn't try to make the VLA "use language more" directly. Instead, it uses counterfactual reasoning: by comparing against a vision-only baseline, it identifies when language should matter and amplifies those signals.

Several directions emerge:
- **Curriculum learning**: Gradually increase counterfactual difficulty during training
- **Data augmentation**: Systematically generate counterfactual training examples
- **Architecture design**: Build explicit language-vision competition mechanisms into model architecture
- **Evaluation protocols**: Adopt counterfactual testing as standard practice for VLA benchmarks

The work also raises questions about what "language grounding" really means. Is a model truly grounded if it only uses language when vision is ambiguous? Or should language always modulate behavior, even when vision provides strong cues?

## Takeaways

1. **VLAs suffer from systematic counterfactual failures** where visual shortcuts override language instructions, especially for under-observed task compositions
2. **LIBERO-CF provides the first systematic benchmark** for evaluating language following capability through counterfactual instruction assignments
3. **Counterfactual Action Guidance (CAG) offers a plug-and-play solution** that improves language conditioning without retraining or architectural changes
4. **The dual-branch design explicitly regularizes language use** by comparing VLA actions against vision-only baselines during inference
5. **Substantial improvements across benchmarks**: 9.7-15.5% gains in language following accuracy and up to 17.2% in real-world task success
6. **The method generalizes across VLA architectures** and is particularly effective for under-observed tasks where visual shortcuts are most problematic
7. **Counterfactual evaluation should become standard practice** for assessing whether VLAs truly ground language or merely correlate it with visual patterns
:::

:::zh
**论文**: [2602.17659](https://arxiv.org/abs/2602.17659)
**作者**: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding
**分类**: cs.CV, cs.RO

## 摘要

视觉-语言-动作(VLA)模型旨在通过将语言指令与视觉感知相结合来实现机器人控制。然而,本研究揭示了一个关键的失效模式:当指令缺乏强场景特定监督时,VLA会依赖从数据集偏差中学到的视觉捷径,而非遵循语言意图。作者提出了LIBERO-CF,这是首个系统性评估VLA语言遵循能力的反事实基准,通过在视觉上合理的场景中分配替代指令来测试模型表现。评估结果显示,反事实失效在最先进的VLA模型中普遍存在但研究不足。为解决这一问题,研究者提出了反事实动作引导(CAG),这是一种简单而有效的双分支推理方案,通过结合标准VLA策略和无语言条件的视觉-动作模块来显式正则化语言条件。CAG在无需额外训练数据或架构修改的情况下,在语言遵循准确率和任务成功率上都取得了显著提升。

## 主要贡献

- **LIBERO-CF基准**:首个系统性的VLA反事实评估框架,通过在视觉上合理的布局下分配替代指令来测试语言遵循能力
- **反事实失效分析**:全面刻画了VLA如何依赖视觉捷径和数据集偏差,在观察不足的场景中忽略语言指令
- **反事实动作引导(CAG)**:一种即插即用的双分支推理方法,通过与纯视觉策略的反事实比较来正则化语言条件
- **广泛验证**:在多种VLA架构上展示了9.7%的语言遵循准确率提升和高达17.2%的真实世界任务成功率提升

## 问题:视觉捷径凌驾于语言之上

VLA模型在大规模机器人操作数据集上训练,其中某些物体-动作配对频繁出现。这产生了强视觉先验:当机器人看到经常被操作的物体时,无论语言指令实际说什么,它都倾向于与该物体交互。论文将此识别为**反事实失效**——当呈现与其学习的视觉捷径相矛盾的反事实指令时,模型会失败。

例如,在一个同时有红色杯子和蓝色盘子的厨房场景中,如果训练数据主要显示"拿起红色杯子",即使被指示"拿起蓝色盘子",VLA也可能继续拿起红色杯子。熟悉的红色杯子的视觉存在覆盖了语言规范。

这种现象特别成问题,因为:
- 标准基准无法暴露它(它们使用与训练分布对齐的指令)
- 它破坏了语言条件控制的核心承诺
- 它限制了对新任务组合的泛化能力

## LIBERO-CF:反事实基准

作者通过采用现有的LIBERO操作任务并系统性地创建反事实变体来构建LIBERO-CF。关键洞察是保持视觉布局相似,同时将指令目标交换为与观察到的场景配置关联较少的物体或目标。

该基准包括:
- **空间反事实**:相同物体,不同空间关系("把A放在B上"vs"把B放在A上")
- **物体反事实**:多个相似物体,指令指定较不常见的目标
- **动作反事实**:相同场景,不同操作原语

每个反事实对允许直接测量模型是遵循语言还是默认使用视觉先验。评估指标$\pi_{0.5}$测量当事实和反事实选项在视觉上同样合理时选择正确动作的概率。

## 反事实动作引导(CAG)

CAG通过一个简单但有效的原则来解决反事实失效:**比较模型在有语言和没有语言时的行为**。如果它们太相似,说明模型并没有真正使用语言指令。

该方法工作流程如下:

1. **双分支推理**:同时运行VLA策略$\pi_{\text{VLA}}(a|v,l)$和仅看视觉输入的视觉-动作策略$\pi_{\text{VA}}(a|v)$
2. **反事实比较**:计算它们动作分布之间的差异
3. **正则化动作选择**:调整VLA的动作以增加与VA基线的差异

数学上,CAG优化:

$$a^* = \arg\max_a \left[ \log \pi_{\text{VLA}}(a|v,l) - \lambda \log \pi_{\text{VA}}(a|v) \right]$$

其中$\lambda$控制正则化强度。这个公式有一个优雅的解释:它鼓励VLA分配高概率但VA模型分配低概率的动作——正是那些需要语言理解的动作。

**提出两个变体**:
- **CAG-free**:使用预训练VLA的视觉编码器作为VA模块(无需训练)
- **CAG-trained**:在相同数据上训练专用VA模块,不使用语言标签

## 实验结果

评估涵盖多个维度:

**LIBERO-CF基准性能**:
- 基线VLA显示15-25%的反事实失效率
- CAG-free将语言遵循准确率提高9.7%,任务成功率提高3.6%
- CAG-trained分别实现15.5%和8.5%的提升
- 改进在不同VLA架构(OpenVLA、Octo、RT-1)上保持一致

**观察不足任务的泛化**:
- 在训练样本有限的任务上,CAG显示更大的增益
- 减少对频繁物体-动作关联的过度依赖
- 在保持良好观察任务性能的同时改进罕见任务

**真实世界机器人实验**:
- 在带有自然语言指令的物理操作任务上测试
- 反事实失效减少9.4%
- 任务成功率平均提高17.2%
- 展示了超越仿真的实际适用性

**消融研究**:
- 正则化强度$\lambda$需要调整但在一定范围内鲁棒
- 两种CAG变体都优于基线,训练的VA显示最佳结果
- 该方法与其他改进(如更好的预训练)互补

## 影响与未来方向

这项工作揭示了VLA设计中的一个基本张力:**视觉提供的强先验可以覆盖较弱的语言信号**。虽然这看起来像是一个缺陷,但实际上是对训练数据统计的理性响应——如果某些视觉模式可靠地预测动作,为什么要关注可能有噪声的语言?

CAG解决方案在哲学上很有趣,因为它不是直接试图让VLA"更多地使用语言"。相反,它使用反事实推理:通过与纯视觉基线比较,识别语言应该起作用的时候并放大这些信号。

出现了几个研究方向:
- **课程学习**:在训练期间逐渐增加反事实难度
- **数据增强**:系统性地生成反事实训练样本
- **架构设计**:在模型架构中构建显式的语言-视觉竞争机制
- **评估协议**:将反事实测试作为VLA基准的标准实践

这项工作还提出了关于"语言基础"真正含义的问题。如果模型仅在视觉模糊时使用语言,它是否真正基础?还是语言应该始终调节行为,即使视觉提供强线索?

## 要点总结

1. **VLA存在系统性的反事实失效**,视觉捷径覆盖语言指令,特别是在观察不足的任务组合中
2. **LIBERO-CF提供了首个系统性基准**,通过反事实指令分配来评估语言遵循能力
3. **反事实动作引导(CAG)提供即插即用解决方案**,在无需重训练或架构更改的情况下改进语言条件
4. **双分支设计显式正则化语言使用**,通过在推理期间将VLA动作与纯视觉基线比较
5. **跨基准的显著改进**:语言遵循准确率提升9.7-15.5%,真实世界任务成功率提升高达17.2%
6. **该方法在VLA架构间泛化**,对视觉捷径最成问题的观察不足任务特别有效
7. **反事实评估应成为标准实践**,用于评估VLA是否真正基础语言还是仅将其与视觉模式关联

:::
