---
title:
  en: "Understanding Dynamic Compute Allocation in Recurrent Transformers"
  zh: "理解循环Transformer中的动态计算分配"
description:
  en: "ANIRA framework reveals that complexity-aligned compute allocation can emerge without supervision but does not guarantee algorithmic generalization"
  zh: "ANIRA框架揭示复杂度对齐的计算分配可以无监督涌现，但不保证算法泛化"
date: 2026-02-10
tags: ["arxiv", "ai", "adaptive-compute", "recurrent-transformer", "efficiency", "interpretability", "cs.CL", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.08864](https://arxiv.org/abs/2602.08864)
**Authors**: Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin
**Categories**: cs.CL, cs.LG

## Abstract

This paper studies token-level adaptive computation in recurrent transformers through a rigorous empirical framework. It introduces complexity-controlled evaluation using algorithmic and synthetic tasks with parameterized difficulty, and proposes ANIRA, a unified recurrent Transformer framework supporting per-token variable-depth computation. Key findings include: compute allocation aligned with task complexity can emerge without explicit supervision, but such alignment does not imply algorithmic generalization — models fail to extrapolate to unseen problem sizes. Furthermore, early halting decisions rely on static structural cues, while online halting tracks the algorithmic execution state more faithfully.

## Key Contributions

- **ANIRA framework**: A unified recurrent Transformer architecture supporting per-token variable-depth computation
- **Complexity-controlled evaluation**: Algorithmic tasks with parameterized difficulty for rigorous analysis
- **Emergent compute alignment**: Demonstrates that complexity-aware allocation arises without explicit supervision
- **Alignment-generalization gap**: Shows that aligned compute allocation does not guarantee out-of-distribution generalization
- **Halting mechanism analysis**: Distinguishes between static structural halting and dynamic execution-state halting

## Background: Adaptive Computation in Transformers

### The Fixed-Depth Problem

Standard transformers apply a fixed number of layers to every token, regardless of the computational difficulty of processing that token. This is wasteful for easy tokens and potentially insufficient for hard ones.

### Recurrent Transformers

Recurrent transformers address this by allowing layers to be applied iteratively. A single block of parameters is reused across multiple "depth" steps:

$$h^{(t+1)} = f_\theta(h^{(t)})$$

where $f_\theta$ is the shared transformer block and $t$ indexes the recurrence depth. The key question is: **how many steps should each token receive?**

### Adaptive Computation Time (ACT)

ACT and its variants learn a halting probability at each step:

$$p_{\text{halt}}^{(t)} = \sigma(w^T h^{(t)})$$

The model stops recurring when the cumulative halting probability exceeds a threshold. This allows different tokens to receive different amounts of computation.

## The ANIRA Framework

ANIRA (Adaptive N-Iteration Recurrent Architecture) unifies several recurrent transformer variants under a single framework, enabling controlled comparison. It supports:

- **Fixed-depth**: All tokens receive the same number of iterations (baseline)
- **Per-token adaptive depth**: Each token independently decides when to halt
- **Global adaptive depth**: A single halting decision for the entire sequence
- **Hybrid**: Combination of per-token and global signals

### Complexity-Controlled Tasks

To rigorously study compute allocation, the paper designs tasks with **parameterized difficulty**:

- **Sorting**: Difficulty controlled by sequence length and value range
- **Graph reachability**: Difficulty controlled by graph size and density
- **Arithmetic**: Difficulty controlled by operand size and operation complexity
- **String manipulation**: Difficulty controlled by pattern complexity

Each task has a known algorithmic solution with well-understood computational complexity, enabling precise measurement of whether the model's compute allocation matches the theoretical requirements.

## Key Findings

### Finding 1: Emergent Complexity Alignment

Without any explicit supervision about task difficulty, recurrent transformers learn to allocate more computation to harder instances. For example:
- Longer sequences in sorting tasks receive more recurrence steps
- Larger graphs in reachability tasks trigger deeper computation
- More complex arithmetic expressions use more iterations

This alignment emerges purely from the task loss signal.

### Finding 2: Alignment Does Not Imply Generalization

Despite learning to allocate compute proportionally to difficulty, models **fail to extrapolate** to problem sizes not seen during training. A model trained on sorting sequences of length 10-50 that correctly allocates more compute to length 50 will still fail on length 100, even though it "knows" length 100 should require more compute.

This reveals a fundamental distinction: **knowing how much to compute is not the same as knowing what to compute**.

### Finding 3: Two Types of Halting

The paper identifies two distinct halting strategies:

1. **Static structural halting**: The model decides how many steps to use based on input features (e.g., sequence length) before processing begins. This is like estimating difficulty from surface features.

2. **Dynamic execution-state halting**: The model monitors its internal computation state and halts when the computation converges. This tracks the actual algorithmic execution.

Early layers tend to use static halting, while later layers shift toward dynamic halting. Dynamic halting is more faithful to the actual computation but harder to learn.

## Results

- ANIRA provides a clean framework for comparing adaptive computation strategies
- Complexity-aligned allocation emerges reliably across tasks
- Generalization to unseen sizes fails consistently, even with correct compute allocation
- Dynamic halting correlates better with algorithmic correctness than static halting
- Per-token adaptive depth outperforms global adaptive depth on heterogeneous-difficulty inputs

## Significance

1. **Sobering finding**: Adaptive compute alignment is necessary but not sufficient for algorithmic generalization
2. **Evaluation methodology**: Complexity-controlled tasks provide a rigorous framework for studying adaptive computation
3. **Mechanistic insight**: The static vs. dynamic halting distinction reveals how models internally manage computation
4. **Practical implications**: Suggests that achieving true algorithmic generalization requires more than just adaptive depth — it may require explicit algorithmic inductive biases
5. **Unified framework**: ANIRA enables fair comparison across adaptive computation methods, advancing the field's ability to draw reliable conclusions
:::

:::zh
**论文**: [2602.08864](https://arxiv.org/abs/2602.08864)
**作者**: Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin
**分类**: cs.CL, cs.LG

## 摘要

本文通过严格的实证框架研究循环Transformer中的token级自适应计算。引入了使用参数化难度的算法和合成任务的复杂度控制评估，并提出了ANIRA，一个支持每token可变深度计算的统一循环Transformer框架。关键发现包括：与任务复杂度对齐的计算分配可以在没有显式监督的情况下涌现，但这种对齐并不意味着算法泛化——模型无法外推到未见过的问题规模。此外，早期停止决策依赖于静态结构线索，而在线停止更忠实地跟踪算法执行状态。

## 主要贡献

- **ANIRA框架**：支持每token可变深度计算的统一循环Transformer架构
- **复杂度控制评估**：具有参数化难度的算法任务，用于严格分析
- **涌现的计算对齐**：证明复杂度感知分配在没有显式监督的情况下产生
- **对齐-泛化差距**：表明对齐的计算分配不保证分布外泛化
- **停止机制分析**：区分静态结构停止和动态执行状态停止

## 背景：Transformer中的自适应计算

### 固定深度问题

标准Transformer对每个token应用固定数量的层，不考虑处理该token的计算难度。这对简单token是浪费的，对困难token可能不够。

### 循环Transformer

循环Transformer通过允许层迭代应用来解决这个问题。单个参数块在多个"深度"步骤中重复使用：

$$h^{(t+1)} = f_\theta(h^{(t)})$$

其中$f_\theta$是共享的Transformer块，$t$索引循环深度。关键问题是：**每个token应该接收多少步？**

### 自适应计算时间（ACT）

ACT及其变体在每一步学习停止概率：

$$p_{\text{halt}}^{(t)} = \sigma(w^T h^{(t)})$$

当累积停止概率超过阈值时，模型停止循环。这允许不同token接收不同量的计算。

## ANIRA框架

ANIRA（自适应N次迭代循环架构）将多种循环Transformer变体统一在单一框架下，实现受控比较。它支持：

- **固定深度**：所有token接收相同数量的迭代（基线）
- **每token自适应深度**：每个token独立决定何时停止
- **全局自适应深度**：整个序列的单一停止决策
- **混合**：每token和全局信号的组合

### 复杂度控制任务

为严格研究计算分配，论文设计了具有**参数化难度**的任务：

- **排序**：难度由序列长度和值范围控制
- **图可达性**：难度由图大小和密度控制
- **算术**：难度由操作数大小和运算复杂度控制
- **字符串操作**：难度由模式复杂度控制

每个任务都有已知的算法解决方案和充分理解的计算复杂度，能够精确测量模型的计算分配是否匹配理论要求。

## 关键发现

### 发现1：涌现的复杂度对齐

在没有任何关于任务难度的显式监督下，循环Transformer学会为更难的实例分配更多计算。例如：
- 排序任务中更长的序列获得更多循环步骤
- 可达性任务中更大的图触发更深的计算
- 更复杂的算术表达式使用更多迭代

这种对齐纯粹从任务损失信号中涌现。

### 发现2：对齐不意味着泛化

尽管学会了按难度比例分配计算，模型**无法外推**到训练期间未见过的问题规模。一个在长度10-50的排序序列上训练的模型，虽然正确地为长度50分配更多计算，但在长度100上仍然失败，即使它"0应该需要更多计算。

这揭示了一个根本区别：**知道计算多少不等于知道计算什么**。

### 发现3：两种停止类型

论文识别了两种不同的停止策略：

1. **静态结构停止**：模型在处理开始前基于输入特征（如序列长度）决定使用多少步。这类似于从表面特征估计难度。

2. **动态执行状态停止**：模型监控其内部计算状态，在计算收敛时停止。这跟踪实际的算法执行。

早期层倾向于使用静态停止，而后期层转向动态停止。动态停止更忠实于实际计算，但更难学习。

## 实验结果

- ANIRA提供了比较自适应计算策略的清晰框架
- 复杂度对齐的分配在各任务中可靠涌现
- 对未见规模的泛化始终失败，即使计算分配正确
- 动态停止与算法正确性的相关性优于静态停止
- 在异构难度输入上，每token自适应深度优于全局自适应深度

## 重要意义

1. **警醒发现**：自适应计算对齐是算法泛化的必要但非充分条件
2. **评估方法论**：复杂度控制任务为研究自适应计算提供了严格框架
3. **机制洞察**：静态vs动态停止的区分揭示了模型如何内部管理计算
4. **实际意义**：表明实现真正的算法泛化需要的不仅仅是自适应深度——可能需要显式的算法归纳偏置
5. **统一框架**：ANIRA实现了自适应计算方法间的公平比较，推进了该领域得出可靠结论的能力
:::
