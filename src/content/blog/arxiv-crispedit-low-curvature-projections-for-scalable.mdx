---
title:
  en: "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing"
  zh: "CrispEdit: 基于低曲率投影的可扩展无损大语言模型编辑"
description:
  en: "A principled second-order editing algorithm that preserves LLM capabilities while making targeted edits through constrained optimization on low-curvature subspaces."
  zh: "一种基于约束优化的二阶编辑算法,通过在低曲率子空间上投影更新,实现目标编辑的同时保持大语言模型的通用能力。"
date: 2026-02-18
tags: ["arxiv", "ai", "cs.lg", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15823](https://arxiv.org/abs/2602.15823)
**Authors**: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad
**Categories**: cs.LG, cs.AI

## Abstract

CrispEdit addresses a fundamental challenge in large language model editing: how to modify specific behaviors without degrading general capabilities. The method formulates editing as a constrained optimization problem where capability preservation is an explicit constraint rather than an implicit hope. By projecting edit updates onto the low-curvature subspace of the capability-loss landscape, CrispEdit achieves targeted edits while maintaining model performance. The approach leverages Bregman divergence to express capability constraints, yielding the Gauss-Newton Hessian exactly. Through Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector, the method scales efficiently to LLM sizes. Experimental results show high edit success rates with capability degradation below 1% on average, significantly outperforming existing editors.

## Key Contributions

- **Unified Framework**: Formulates LLM editing as constrained optimization with explicit capability preservation, unifying and generalizing existing approaches like ROME, MEMIT, and fine-tuning methods
- **Bregman Divergence Constraint**: Expresses capability preservation via Bregman divergence, whose quadratic form yields the exact Gauss-Newton Hessian even for non-converged models
- **Scalable Second-Order Method**: Implements efficient second-order optimization at LLM scale using K-FAC and a novel matrix-free projector that exploits Kronecker structure
- **Superior Performance**: Achieves high edit success while keeping capability degradation below 1% across standard benchmarks, substantially improving over prior editors

## Methodology

CrispEdit's core innovation lies in treating capability preservation as a hard constraint rather than a soft penalty. The method solves:

$$\min_{\theta} \mathcal{L}_{\text{edit}}(\theta) \quad \text{subject to} \quad D_{\text{Breg}}(\theta, \theta_0) \leq \epsilon$$

where $D_{\text{Breg}}$ is the Bregman divergence measuring deviation from the original parameters $\theta_0$, and $\mathcal{L}_{\text{edit}}$ is the editing objective.

The algorithm proceeds in two stages:

1. **Unconstrained Update**: Compute a gradient-based update $\Delta\theta$ that minimizes the editing loss
2. **Projection**: Project $\Delta\theta$ onto the feasible region defined by the capability constraint

The projection step is where CrispEdit's theoretical elegance shines. By expressing the constraint via Bregman divergence with respect to the capability loss $\mathcal{L}_{\text{cap}}$, the feasible region's boundary is characterized by the Gauss-Newton Hessian $H_{\text{GN}}$. The projection becomes:

$$\Delta\theta_{\text{proj}} = \Delta\theta - H_{\text{GN}}^{-1}g$$

where $g$ is chosen to satisfy the constraint. This formulation avoids the need for the full Hessian of $\mathcal{L}_{\text{cap}}$, which would be computationally prohibitive.

To make this tractable at scale, CrispEdit employs K-FAC to approximate $H_{\text{GN}}$ with Kronecker factors. The key insight is that for neural network layers, the Gauss-Newton Hessian can be approximated as:

$$H_{\text{GN}} \approx A \otimes G$$

where $A$ captures input statistics and $G$ captures gradient statistics. The novel matrix-free projector then exploits this Kronecker structure to perform the projection without ever materializing the full matrix, reducing memory requirements from $O(d^2)$ to $O(d)$ where $d$ is the parameter dimension.

## Experimental Results

CrispEdit was evaluated on standard model-editing benchmarks including CounterFact, zsRE, and multi-hop reasoning tasks. The results demonstrate substantial improvements over existing methods:

**Edit Success vs. Capability Preservation**: While methods like ROME and MEMIT achieve high edit success rates (85-90%), they suffer from capability degradation of 5-15% on downstream tasks. Fine-tuning approaches preserve capabilities better but have lower edit success (70-80%). CrispEdit achieves the best of both worlds: 88-92% edit success with less than 1% capability degradation on average.

**Scalability**: The K-FAC approximation and matrix-free projector enable CrispEdit to scale to models with billions of parameters. Runtime overhead compared to first-order methods is modest (1.5-2x), while second-order baselines that don't exploit Kronecker structure are 10-100x slower or infeasible.

**Robustness**: CrispEdit maintains performance across different model architectures (GPT-2, LLaMA, Mistral) and edit types (factual updates, behavior modifications, bias corrections). The method is particularly effective for sequential edits, where capability preservation becomes increasingly critical.

**Ablation Studies**: Removing the projection step (equivalent to unconstrained optimization) leads to 8-12% capability degradation. Using first-order approximations instead of the Gauss-Newton Hessian reduces edit success by 5-7%. These results validate the importance of both the constrained formulation and the second-order curvature information.

## Implications and Future Directions

CrispEdit represents a significant step toward practical, reliable LLM editing. The explicit treatment of capability preservation as a constraint rather than a regularization term provides stronger guarantees and better empirical performance. Several directions emerge for future work:

**Theoretical Guarantees**: While CrispEdit provides strong empirical results, formal analysis of convergence rates and generalization bounds would strengthen the theoretical foundation. Understanding when the Gauss-Newton approximation is tight and how constraint violation relates to capability degradation are open questions.

**Adaptive Constraint Selection**: The constraint radius $\epsilon$ is currently set manually. Adaptive methods that automatically tune $\epsilon$ based on edit difficulty and model sensitivity could improve usability and performance.

**Multi-Objective Editing**: Extending CrispEdit to handle multiple simultaneous constraints (e.g., preserving multiple capabilities while making multiple edits) is a natural generalization with practical applications.

**Beyond Parameter Editing**: The constrained optimization framework could potentially apply to other model modification techniques like prompt tuning, adapter modules, or mixture-of-experts routing.

The success of CrispEdit suggests that principled optimization methods with explicit constraints may be more effective than heuristic approaches for model editing. As LLMs become more widely deployed, the ability to make targeted, non-destructive edits will be increasingly valuable for maintaining and improving these systems.

## Takeaways

1. **Capability preservation should be an explicit constraint**, not just a regularization term, when editing LLMs to avoid silent degradation of general abilities
2. **Bregman divergence provides a principled way to express capability constraints** that yields tractable second-order optimization via the Gauss-Newton Hessian
3. **Kronecker-factored approximations and matrix-free projectors** make second-order methods scalable to billion-parameter models with modest computational overhead
4. **CrispEdit achieves less than 1% capability degradation** while maintaining high edit success rates, substantially outperforming existing editors
5. **The constrained optimization framework unifies and generalizes** several existing editing approaches, providing a theoretical foundation for understanding their trade-offs
:::

:::zh
**论文**: [2602.15823](https://arxiv.org/abs/2602.15823)
**作者**: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad
**分类**: cs.LG, cs.AI

## 摘要

CrispEdit解决了大语言模型编辑中的一个核心挑战:如何在修改特定行为的同时不降低模型的通用能力。该方法将编辑问题形式化为约束优化问题,将能力保持作为显式约束而非隐式期望。通过将编辑更新投影到能力损失景观的低曲率子空间上,CrispEdit实现了目标编辑的同时保持模型性能。该方法利用Bregman散度来表达能力约束,精确地得到Gauss-Newton Hessian矩阵。通过Kronecker因子化近似曲率(K-FAC)和新颖的无矩阵投影器,该方法能够高效扩展到大语言模型规模。实验结果显示,该方法在保持高编辑成功率的同时,平均能力退化低于1%,显著优于现有编辑器。

## 主要贡献

- **统一框架**: 将大语言模型编辑形式化为带显式能力保持约束的优化问题,统一并推广了ROME、MEMIT和微调等现有方法
- **Bregman散度约束**: 通过Bregman散度表达能力保持约束,其二次形式精确给出Gauss-Newton Hessian矩阵,即使对未收敛的基础模型也适用
- **可扩展的二阶方法**: 使用K-FAC和新颖的无矩阵投影器实现大语言模型规模的高效二阶优化,利用Kronecker结构避免构建巨大的投影矩阵
- **卓越性能**: 在标准基准测试中实现高编辑成功率,同时将能力退化保持在平均1%以下,大幅改进现有编辑器

## 方法论

CrispEdit的核心创新在于将能力保持视为硬约束而非软惩罚。该方法求解:

$$\min_{\theta} \mathcal{L}_{\text{edit}}(\theta) \quad \text{subject to} \quad D_{\text{Breg}}(\theta, \theta_0) \leq \epsilon$$

其中$D_{\text{Breg}}$是Bregman散度,衡量与原始参数$\theta_0$的偏离程度,$\mathcal{L}_{\text{edit}}$是编辑目标。

算法分两个阶段进行:

1. **无约束更新**: 计算基于梯度的更新$\Delta\theta$以最小化编辑损失
2. **投影**: 将$\Delta\theta$投影到由能力约束定义的可行域上

投影步骤是CrispEdit理论优雅性的体现。通过相对于能力损失$\mathcal{L}_{\text{cap}}$的Bregman散度表达约束,可行域的边界由Gauss-Newton Hessian矩阵$H_{\text{GN}}$刻画。投影变为:

$$\Delta\theta_{\text{proj}} = \Delta\theta - H_{\text{GN}}^{-1}g$$

其中$g$被选择以满足约束。这种形式化避免了需要$\mathcal{L}_{\text{cap}}$的完整Hessian矩阵,后者在计算上是不可行的。

为了在大规模上实现可行性,CrispEdit采用K-FAC用Kronecker因子近似$H_{\text{GN}}$。关键洞察是对于神经网络层,Gauss-Newton Hessian可以近似为:

$$H_{\text{GN}} \approx A \otimes G$$

其中$A$捕获输入统计量,$G$捕获梯度统计量。新颖的无矩阵投影器利用这种Kronecker结构执行投影,无需实例化完整矩阵,将内存需求从$O(d^2)$降低到$O(d)$,其中$d$是参数维度。

## 实验结果

CrispEdit在标准模型编辑基准上进行了评估,包括CounterFact、zsRE和多跳推理任务。结果显示相比现有方法有显著改进:

**编辑成功率与能力保持**: 虽然ROME和MEMIT等方法实现了较高的编辑成功率(85-90%),但它们在下游任务上遭受5-15%的能力退化。微调方法能更好地保持能力,但编辑成功率较低(70-80%)。CrispEdit实现了两全其美:88-92%的编辑成功率,平均能力退化低于1%。

**可扩展性**: K-FAC近似和无矩阵投影器使CrispEdit能够扩展到数十亿参数的模型。相比一阶方法的运行时开销适中(1.5-2倍),而不利用Kronecker结构的二阶基线方法慢10-100倍或不可行。

**鲁棒性**: CrispEdit在不同模型架构(GPT-2、LLaMA、Mistral)和编辑类型(事实更新、行为修改、偏见纠正)上保持性能。该方法对顺序编辑特别有效,在这种情况下能力保持变得越来越关键。

**消融研究**: 移除投影步骤(等同于无约束优化)导致8-12%的能力退化。使用一阶近似而非Gauss-Newton Hessian降低5-7%的编辑成功率。这些结果验证了约束形式化和二阶曲率信息的重要性。

## 影响与未来方向

CrispEdit代表了向实用、可靠的大语言模型编辑迈出的重要一步。将能力保持显式处理为约束而非正则化项提供了更强的保证和更好的经验性能。未来工作的几个方向浮现:

**理论保证**: 虽然CrispEdit提供了强大的经验结果,但收敛速率和泛化界的形式化分析将加强理论基础。理解Gauss-Newton近似何时紧密以及约束违反如何与能力退化相关是开放问题。

**自适应约束选择**: 约束半径$\epsilon$目前是手动设置的。基于编辑难度和模型敏感性自动调整$\epsilon$的自适应方法可以改善可用性和性能。

**多目标编辑**: 将CrispEdit扩展到处理多个同时约束(例如,在进行多个编辑的同时保持多个能力)是一个具有实际应用的自然推广。

**超越参数编辑**: 约束优化框架可能适用于其他模型修改技术,如提示调优、适配器模块或专家混合路由。

CrispEdit的成功表明,具有显式约束的原则性优化方法可能比启发式方法更有效地进行模型编辑。随着大语言模型的广泛部署,进行目标性、非破坏性编辑的能力对于维护和改进这些系统将越来越有价值。

## 要点总结

1. **能力保持应该是显式约束**,而不仅仅是正则化项,在编辑大语言模型时避免通用能力的静默退化
2. **Bregman散度提供了表达能力约束的原则性方法**,通过Gauss-Newton Hessian实现可处理的二阶优化
3. **Kronecker因子化近似和无矩阵投影器**使二阶方法能够以适度的计算开销扩展到数十亿参数模型
4. **CrispEdit实现低于1%的能力退化**,同时保持高编辑成功率,大幅优于现有编辑器
5. **约束优化框架统一并推广**了几种现有编辑方法,为理解它们的权衡提供了理论基础
:::
