---
title:
  en: "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient LLMs"
  zh: "FlexMoRE：用于高效LLM的灵活异构秩混合专家"
description:
  en: "Systematic investigation of rank vs performance in MoE, achieving improved accuracy (47.18 vs 45.46) at less than 1/3 the parameters"
  zh: "系统研究MoE中秩与性能的关系，以不到1/3的参数实现更高准确率（47.18 vs 45.46）"
date: 2026-02-10
tags: ["arxiv", "ai", "mixture-of-experts", "moe", "lora", "federated-learning", "efficiency", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.08818](https://arxiv.org/abs/2602.08818)
**Authors**: Annemette Brok Pirchert, Jacob Nielsen, Mogens Henrik From, Lukas Galke Poech, Peter Schneider-Kamp
**Categories**: cs.LG

## Abstract

FlexMoRE introduces a Mixture-of-Experts architecture where experts can be either full-sized or low-rank adapters of varying ranks. The paper systematically investigates the trade-off between expert rank and downstream performance using 6 experts at ranks from $2^0$ to $2^{14}$, evaluating 150 different mixtures across 120 tasks. A key finding is that the optimal rank is substantially higher for reasoning-heavy benchmarks compared to knowledge-heavy benchmarks. Using task-optimal ranks, FlexMoRE achieves improved performance (47.18 vs 45.46) at less than one-third the parameters (10.75B vs 33.27B).

## Key Contributions

- **Rank-heterogeneous experts**: Experts within the same MoE layer can have different ranks, from full-size to extremely low-rank
- **Systematic rank investigation**: 150 mixtures across 120 tasks with ranks spanning $2^0$ to $2^{14}$
- **Task-dependent optimal rank**: Reasoning tasks require higher ranks than knowledge retrieval tasks
- **Parameter efficiency**: 3x parameter reduction with improved performance
- **Federated learning compatibility**: Designed for efficient federated training scenarios

## Background: MoE and Low-Rank Adaptation

### Mixture of Experts

MoE architectures route each input to a subset of specialized experts, enabling larger model capacity without proportional compute increase. A router network $G(x)$ selects the top-$k$ experts:

$$y = \sum_{i \in \text{top-}k} G(x)_i \cdot E_i(x)$$

### Low-Rank Adaptation (LoRA)

LoRA approximates weight updates with low-rank matrices:

$$W' = W + BA$$

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times d}$, with rank $r \ll d$. This dramatically reduces trainable parameters.

### The FlexMoRE Insight

FlexMoRE combines these ideas: each expert in the MoE can be a LoRA adapter of a **different rank**. This allows the system to allocate capacity where it matters most.

## Experimental Design

### Rank Space

The study evaluates experts at ranks: $r \in \{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384\}$ (i.e., $2^0$ through $2^{14}$), plus full-rank experts.

### Mixture Configurations

With 6 experts per layer, the study explores 150 different rank combinations, systematically varying:
- Homogeneous mixtures (all experts same rank)
- Heterogeneous mixtures (experts with different ranks)
- Graduated mixtures (ranks increasing or decreasing across experts)

### Evaluation

Performance is measured across 120 tasks spanning:
- **Reasoning benchmarks**: Mathematical reasoning, logical deduction, code generation
- **Knowledge benchmarks**: Factual QA, commonsense reasoning, world knowledge

## Key Finding: Task-Dependent Optimal Rank

The most significant finding is that **optimal rank depends strongly on task type**:

- **Reasoning-heavy tasks**: Require substantially higher ranks (often $r \geq 256$)
- **Knowledge-heavy tasks**: Perform well even at very low ranks ($r \leq 16$)

This makes intuitive sense: knoeval primarily requires accessing stored information (which can be captured in low-rank subspaces), while reasoning requires more complex transformations that need higher-dimensional representations.

## Results

| Configuration | Avg. Accuracy | Parameters |
|--------------|---------------|------------|
| Full-rank baseline | 45.46 | 33.27B |
| FlexMoRE (optimal) | **47.18** | **10.75B** |
| Homogeneous low-rank | 43.21 | 8.92B |

- **47.18 vs 45.46**: FlexMoRE outperforms the full-rank baseline
- **10.75B vs 33.27B**: Using less than 1/3 of the parameters
- Heterogeneous mixtures consistently outperform homogeneous ones

## Significance

1. **Not all experts need to be equal**: Rank heterogeneity is a powerful design dimension for MoE architectures
2. **Task-aware architecture design**: The finding that reasoning needs higher ranks than knowledge retrieval has broad implications for model design
3. **Massive parameter savings**: 3x reduction with improved performance challenges the assumption that bigger is always better
4. **Federated learning**: Rank-heterogeneous experts are naturally suited to federated settings where different clients have different compute budgets
5. **Practical guidance**: The systematic study provides concrete guidance for practitioners choosing expert configurations
:::

:::zh
**论文**: [2602.08818](https://arxiv.os/2602.08818)
**作者**: Annemette Brok Pirchert, Jacob Nielsen, Mogens Henrik From, Lukas Galke Poech, Peter Schneider-Kamp
**分类**: cs.LG

## 摘要

FlexMoRE引入了一种混合专家架构，其中专家可以是全尺寸的或不同秩的低秩适配器。论文使用6个专家在$2^0$到$2^{14}$的秩范围内系统地研究了专家秩与下游性能之间的权衡，在120个任务上评估了150种不同的混合配置。一个关键发现是，推理密集型基准的最优秩远高于知识密集型基准。使用任务最优秩，FlexMoRE以不到三分之一的参数（10.75B vs 33.27B）实现了更好的性能（47.18 vs 45.46）。

## 主要贡献

- **异构秩专家**：同一MoE层中的专家可以有不同的秩，从全尺寸到极低秩
- **系统性秩研究**：在120个任务上评估150种混合配置，秩范围从$2^0$到$2^{14}$
- **任务依赖的最优秩**：推理任务需要比知识检索任务更高的秩
- **参数效率**：3倍参数减少同时性能提升
- **联邦学习兼容性**：为高效联邦训练场景设计

## 背景：MoE与低秩适配

### 混合专家

MoE到专门化专家的子集，在不成比例增加计算的情况下实现更大的模型容量。路由网络$G(x)$选择top-$k$个专家：

$$y = \sum_{i \in \text{top-}k} G(x)_i \cdot E_i(x)$$

### 低秩适配（LoRA）

LoRA用低秩矩阵近似权重更新：

$$W' = W + BA$$

其中$B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times d}$，秩$r \ll d$。这大幅减少了可训练参数。

### FlexMoRE洞察

FlexMoRE结合了这些思想：MoE中的每个专家可以是**不同秩**的LoRA适配器。这允许系统在最重要的地方分配容量。

## 实验设计

### 秩空间

研究评估了以下秩的专家：$r \in \{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384\}$（即$2^0$到$2^{14}$），加上全秩专家。

### 混合配置

每层6个专家，研究探索了150种不同的秩组合，系统地变化：
- 同构混合（所有专家相同秩）
- 异构混合（专家不同秩）
- 渐变混合（秩在专家间递增或递减）

### 评估

在120个任务上衡量性能，涵盖：
- **推理基准**：数学推理、逻辑推演、代码生成
- **知识基准**：事实问答、常识推理、世界知识

## 关键发现：任务依赖的最优秩

最重要的发现是**最优秩强烈依赖于任务类型**：

- **推理密集型任务**：需要显著更高的秩（通常$r \geq 256$）
- **知识密集型任务**：即使在非常低的秩下也表现良好（$r \leq 16$）

这在直觉上是合理的：知识检索主要需要访问存储的信息（可以在低秩子空间中捕获），而推理需要更复杂的变换，需要更高维的表示。

## 实验结果

| 配置 | 平均准确率 | 参数量 |
|------|-----------|--------|
| 全秩基线 | 45.46 | 33.27B |
| FlexMoRE（最优） | **47.18** | **10.75B** |
| 同构低秩 | 43.21 | 8.92B |

- **47.18 vs 45.46**：FlexMoRE超越全秩基线
- **10.75B vs 33.27B**：使用不到1/3的参数
- 异构混合始终优于同构混合

## 重要意义

1. **并非所有专家都需要相同**：秩异构性是MoE架构的强大设计维度
2. **任务感知架构设计**：推理需要比知识检索更高秩的发现对模型设计有广泛影响
3. **大幅参数节省**：3倍减少同时性能提升，挑战了"越大越好"的假设
4. **联邦学习**：异构秩专家天然适合不同客户端有不同计算预算的联邦设置
5. **实用指导**：系统性研究为从业者选择专家配置提供了具体指导
:::
