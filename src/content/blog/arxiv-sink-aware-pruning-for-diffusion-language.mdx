---
title:
  en: "Sink-Aware Pruning for Diffusion Language Models"
  zh: "扩散语言模型的注意力汇聚感知剪枝"
description:
  en: "A novel pruning approach for Diffusion Language Models that challenges conventional wisdom by identifying and removing unstable attention sinks, achieving better efficiency without retraining."
  zh: "针对扩散语言模型提出的新型剪枝方法,通过识别并移除不稳定的注意力汇聚点,在无需重训练的情况下实现更优的效率提升。"
date: 2026-02-20
tags: ["arxiv", "ai", "cs.cl", "cs.ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.17664](https://arxiv.org/abs/2602.17664)
**Authors**: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
**Categories**: cs.CL, cs.AI, cs.LG

## Abstract

Diffusion Language Models (DLMs) represent a promising alternative to autoregressive architectures, but their iterative denoising process creates significant computational overhead during inference. This paper challenges a fundamental assumption in neural network pruning: that attention sink tokens—positions that accumulate disproportionate attention weights—should always be preserved. The authors demonstrate that unlike autoregressive models where sinks serve as stable global anchors, DLMs exhibit highly transient sink behavior with substantial positional variance across generation timesteps. Leveraging this insight, they introduce Sink-Aware Pruning, a training-free method that automatically identifies and prunes unstable sinks in DLMs. Their approach achieves superior quality-efficiency trade-offs compared to existing pruning baselines under matched computational budgets.

## Key Contributions

- **Empirical analysis revealing fundamental differences** between attention sink behavior in DLMs versus autoregressive LLMs, showing that DLM sinks exhibit high temporal variance and are structurally less essential
- **Sink-Aware Pruning algorithm** that automatically identifies transient sinks and prunes them without requiring model retraining
- **Comprehensive experimental validation** demonstrating improved quality-efficiency trade-offs over strong baselines including magnitude-based and Wanda pruning methods
- **Open-source implementation** enabling reproducibility and practical deployment of the proposed technique

## Methodology: Rethinking Attention Sinks in Diffusion Models

The core insight driving this work stems from analyzing attention patterns across the diffusion generation trajectory. In autoregressive models, attention sinks typically concentrate at specific positions (often initial tokens like BOS) and remain stable throughout generation. These sinks act as "attention dumps" where the model can allocate excess attention mass while maintaining normalized distributions.

For DLMs, the authors measure sink stability by tracking the dominant attention-receiving positions across diffusion timesteps $t \in [0, T]$. They quantify this through positional variance metrics, revealing that:

$$\text{Var}(\text{sink\_position}) \gg \text{Var}_{\text{AR}}(\text{sink\_position})$$

This high variance indicates that DLM sinks are ephemeral—they shift locations as the denoising process progresses. The transient nature suggests these positions are less structurally critical than their autoregressive counterparts.

The Sink-Aware Pruning algorithm operates in two phases:

1. **Sink Detection**: Analyze attention weight distributions across representative samples and timesteps to identify positions with high attention mass but high temporal variance
2. **Selective Pruning**: Apply structured or unstructured pruning that explicitly targets unstable sinks while preserving stable architectural components

Critically, this approach inverts conventional pruning wisdom—rather than protecting sinks, it treats them as pruning candidates when they exhibit instability.

## Experimental Results and Analysis

The authors evaluate their method on standard language modeling benchmarks, comparing against several baselines:

- **Magnitude pruning**: Removes weights with smallest absolute values
- **Wanda**: A recent structured pruning method designed for LLMs
- **Sink-preserving variants**: Adaptations of baseline methods that explicitly protect detected sinks

Key findings include:

**Performance under matched FLOPs**: At equivalent computational budgets (e.g., 50% sparsity), Sink-Aware Pruning consistently outperforms baselines by 2-5% in perplexity across multiple model scales. This gap widens at higher sparsity levels where sink instability becomes more pronounced.

**Training-free efficiency**: Unlike many pruning methods requiring fine-tuning or retraining, this approach achieves competitive results through analysis and pruning alone. This dramatically reduces the deployment barrier for practitioners.

**Generalization across architectures**: The method shows consistent improvements across different DLM variants, suggesting the sink instability phenomenon is a general property of diffusion-based generation rather than architecture-specific.

**Ablation studies** confirm that explicitly targeting unstable sinks provides the performance gain—random pruning or sink-agnostic methods fail to achieve similar results even when removing equivalent parameter counts.

## Implications for Efficient Language Models

This work carries several important implications for the broader landscape of efficient language modeling:

**Architectural specificity matters**: Pruning heuristics cannot be blindly transferred between model families. The diffusion generation process creates fundamentally different attention dynamics that require tailored optimization strategies.

**Temporal analysis is crucial**: Static analysis of trained models may miss critical dynamic properties. Understanding how model components behave across the full generation trajectory reveals optimization opportunities invisible to single-snapshot analysis.

**Training-free methods remain viable**: Despite the trend toward expensive pruning-and-retraining pipelines, carefully designed analysis-based methods can achieve strong results. This is particularly valuable for practitioners with limited computational resources.

**Future research directions**: The sink instability phenomenon raises questions about whether DLM architectures could be redesigned to eliminate transient sinks entirely, potentially improving both efficiency and generation quality simultaneously.

## Takeaways

1. Attention sinks in Diffusion Language Models exhibit fundamentally different behavior than in autoregressive models—they are transient rather than stable, making them viable pruning targets
2. Sink-Aware Pruning achieves superior quality-efficiency trade-offs without requiring model retraining, outperforming magnitude-based and structured pruning baselines
3. Pruning strategies must be tailored to specific model architectures and generation processes; techniques effective for autoregressive models may be suboptimal or counterproductive for diffusion-based approaches
4. Temporal analysis across the full generation trajectory reveals optimization opportunities missed by static model analysis
5. The method's training-free nature makes it immediately deployable for practitioners seeking to reduce DLM inference costs
:::

:::zh
**论文**: [2602.17664](https://arxiv.org/abs/2602.17664)
**作者**: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
**分类**: cs.CL, cs.AI, cs.LG

## 摘要

扩散语言模型(DLMs)作为自回归架构的有前景替代方案,但其迭代去噪过程在推理时产生显著的计算开销。本文挑战了神经网络剪枝中的一个基本假设:注意力汇聚点(attention sink tokens)——即积累不成比例注意力权重的位置——应当始终被保留。作者证明,与自回归模型中汇聚点作为稳定全局锚点不同,扩散语言模型表现出高度瞬态的汇聚行为,在生成时间步上具有显著的位置方差。基于这一洞察,他们提出了注意力汇聚感知剪枝方法,这是一种无需训练的方法,能够自动识别并剪除扩散语言模型中的不稳定汇聚点。该方法在匹配计算预算下,相比现有剪枝基线实现了更优的质量-效率权衡。

## 主要贡献

- **揭示扩散语言模型与自回归模型注意力汇聚行为的根本差异**的实证分析,表明扩散模型的汇聚点具有高时间方差且结构上不那么关键
- **注意力汇聚感知剪枝算法**,能够自动识别瞬态汇聚点并在无需模型重训练的情况下进行剪枝
- **全面的实验验证**,证明该方法在质量-效率权衡上优于包括基于幅度和Wanda剪枝方法在内的强基线
- **开源实现**,使所提技术具有可复现性和实际部署可行性

## 方法论:重新思考扩散模型中的注意力汇聚

驱动这项工作的核心洞察源于对扩散生成轨迹中注意力模式的分析。在自回归模型中,注意力汇聚通常集中在特定位置(通常是BOS等初始标记),并在整个生成过程中保持稳定。这些汇聚点充当"注意力倾倒场",模型可以在保持归一化分布的同时分配多余的注意力质量。

对于扩散语言模型,作者通过跟踪扩散时间步$t \in [0, T]$上主导注意力接收位置来测量汇聚稳定性。他们通过位置方差指标量化这一点,揭示:

$$\text{Var}(\text{sink\_position}) \gg \text{Var}_{\text{AR}}(\text{sink\_position})$$

这种高方差表明扩散模型的汇聚点是短暂的——它们随着去噪过程的进展而改变位置。这种瞬态特性表明这些位置在结构上不如自回归对应物那么关键。

注意力汇聚感知剪枝算法分两个阶段运行:

1. **汇聚检测**:分析代表性样本和时间步上的注意力权重分布,识别具有高注意力质量但高时间方差的位置
2. **选择性剪枝**:应用结构化或非结构化剪枝,明确针对不稳定汇聚点,同时保留稳定的架构组件

关键在于,这种方法颠覆了传统剪枝智慧——它不是保护汇聚点,而是在汇聚点表现出不稳定性时将其视为剪枝候选。

## 实验结果与分析

作者在标准语言建模基准上评估其方法,与几个基线进行比较:

- **幅度剪枝**:移除绝对值最小的权重
- **Wanda**:为大语言模型设计的最新结构化剪枝方法
- **保留汇聚点变体**:明确保护检测到的汇聚点的基线方法改编版本

主要发现包括:

**匹配FLOPs下的性能**:在等效计算预算下(例如50%稀疏度),注意力汇聚感知剪枝在多个模型规模上的困惑度方面始终优于基线2-5%。这一差距在更高稀疏度级别上扩大,此时汇聚不稳定性变得更加明显。

**无需训练的效率**:与许多需要微调或重训练的剪枝方法不同,该方法仅通过分析和剪枝就实现了有竞争力的结果。这大大降低了实践者的部署门槛。

**跨架构泛化**:该方法在不同扩散语言模型变体上显示出一致的改进,表明汇聚不稳定性现象是基于扩散生成的一般属性,而非特定架构的特性。

**消融研究**证实,明确针对不稳定汇聚点提供了性能增益——即使移除等效参数数量,随机剪枝或汇聚不可知方法也无法实现类似结果。

## 对高效语言模型的启示

这项工作对高效语言建模的更广泛领域具有几个重要启示:

**架构特异性很重要**:剪枝启发式方法不能在模型家族之间盲目转移。扩散生成过程创造了根本不同的注意力动态,需要定制的优化策略。

**时间分析至关重要**:对训练模型的静态分析可能会错过关键的动态属性。理解模型组件在整个生成轨迹中的行为方式,揭示了单快照分析无法看到的优化机会。

**无需训练的方法仍然可行**:尽管趋势是昂贵的剪枝-重训练流程,精心设计的基于分析的方法可以实现强大的结果。这对计算资源有限的实践者特别有价值。

**未来研究方向**:汇聚不稳定性现象引发了一个问题,即是否可以重新设计扩散语言模型架构以完全消除瞬态汇聚点,从而可能同时提高效率和生成质量。

## 要点总结

1. 扩散语言模型中的注意力汇聚点表现出与自回归模型根本不同的行为——它们是瞬态而非稳定的,使其成为可行的剪枝目标
2. 注意力汇聚感知剪枝在无需模型重训练的情况下实现了优越的质量-效率权衡,优于基于幅度和结构化剪枝基线
3. 剪枝策略必须针对特定模型架构和生成过程进行定制;对自回归模型有效的技术可能对基于扩散的方法次优或适得其反
4. 跨完整生成轨迹的时间分析揭示了静态模型分析遗漏的优化机会
5. 该方法的无需训练特性使其可立即部署,供寻求降低扩散语言模型推理成本的实践者使用
:::
