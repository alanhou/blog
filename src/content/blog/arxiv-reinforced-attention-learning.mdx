---
title:
  en: "Reinforced Attention Learning: Optimizing Where to Look, Not What to Say"
  zh: "强化注意力学习：优化关注位置而非输出内容"
description:
  en: "A novel RL framework that optimizes attention distributions instead of token probabilities for multimodal LLMs"
  zh: "一种新颖的强化学习框架，为多模态大语言模型优化注意力分布而非token概率"
date: 2026-02-04
tags: ["arxiv", "ai", "reinforcement-learning", "attention", "multimodal", "mllm", "cs.CL"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.04884](https://arxiv.org/abs/2602.04884)
**Authors**: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng
**Affiliations**: UC Davis, Princeton University, Google, Google DeepMind
**Categories**: cs.CL, cs.LG

## Abstract

Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences.

## Key Contributions

- **Reinforced Attention Learning (RAL)**: A policy-gradient method that shifts optimization from next-token prediction to attention-distribution alignment
- **On-Policy Attention Distillation**: Transfers latent attention behaviors from teacher to student, yielding stronger cross-modal alignment than standard knowledge distillation
- **Empirical Validation**: Consistent improvements over GRPO across diverse visual question answering benchmarks

## The Problem

Traditional RL for MLLMs optimizes **what to generate** (token probabilities). The standard RL objective uses importance sampling with the probability ratio between current and old policies.

This approach has limitations for multimodal tasks:
- Verbose Chain-of-Thought reasoning provides marginal gains for perception tasks
- Can even degrade fundamental perceptual capabilities
- Optimizes the result (token) rather than the process (information allocation)

## The Solution: Optimize Where to Attend

RAL treats attention patterns as the policy:

### Aggregated Causal Attention Distribution Policy

For each generated token t, define the attention distribution over preceding positions. The attention weight from token t to position i is normalized across all preceding positions.

### Advantage-Weighted Attention Divergence

The RAL loss function computes the divergence between current and old attention distributions, weighted by the advantage:

- **High reward response**: Minimize divergence to encourage the attention pattern
- **Low reward response**: Maximize divergence to penalize the attention pattern

## On-Policy Attention Distillation

Traditional distillation focuses on token-level probability alignment. RAL extends this with dual-distillation:

1. **Token Distribution Alignment**: Standard KD loss
2. **Attention Distribution Alignment**: Transfer attention patterns from teacher to student

This provides significant additional performance gains by transferring fine-grained perceptual and grounding behaviors.

## Results

RAL consistently outperforms GRPO across video and image benchmarks, particularly on perception-intensive tasks:

- Better visual grounding
- Reduced modality bias (over-reliance on linguistic priors)
- More faithful, evidence-based responses

## Why This Matters

The key insight: **attention is the mechanism for cross-modal reasoning**. By directly shaping attention weights, RAL provides a principled way to control how the model integrates visual and textual information.

## Takeaways

1. **Attention as policy**: Internal attention distributions can be treated as a learnable policy space
2. **Process over result**: Optimizing how the model allocates attention is more effective than optimizing output tokens for perception tasks
3. **Structural regularization**: Attention-based optimization provides robust regularization against diversity collapse
4. **Distillation beyond tokens**: Transferring attention patterns is more effective than token-level distillation for multimodal alignment
:::

:::zh
**论文**: [2602.04884](https://arxiv.org/abs/2602.04884)
**作者**: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng
**机构**: UC Davis, Princeton University, Google, Google DeepMind
**分类**: cs.CL, cs.LG

## 摘要

通过强化学习（RL）进行后训练已经通过测试时扩展大幅提升了大语言模型（LLM）的推理能力。然而，将这种范式通过冗长的推理过程扩展到多模态LLM（MLLM）时，对感知任务的提升有限，甚至可能降低性能。我们提出了强化注意力学习（RAL），这是一种策略梯度框架，直接优化内部注意力分布而非输出token序列。

## 主要贡献

- **强化注意力学习（RAL）**：一种策略梯度方法，将优化目标从下一个token预测转移到注意力分布对齐
- **在线策略注意力蒸馏**：将潜在注意力行为从教师模型转移到学生模型，比标准知识蒸馏产生更强的跨模态对齐
- **实证验证**：在多种视觉问答基准测试中持续优于GRPO

## 问题所在

传统的MLLM强化学习优化的是**生成什么**（token概率）。标准RL目标使用当前策略和旧策略之间概率比的重要性采样。

这种方法在多模态任务中存在局限性：
- 冗长的思维链推理对感知任务提升有限
- 甚至可能降低基本的感知能力
- 优化的是结果（token）而非过程（信息分配）

## 解决方案：优化关注位置

RAL将注意力模式视为策略：

### 聚合因果注意力分布策略

对于每个生成的token t，定义对前面位置的注意力分布。从token t到位置i的注意力权重在所有前面位置上进行归一化。

### 优势加权注意力散度

RAL损失函数计算当前和旧注意力分布之间的散度，由优势加权：

- **高奖励响应**：最小化散度以鼓励该注意力模式
- **低奖励响应**：最大化散度以惩罚该注意力模式

## 在线策略注意力蒸馏

传统蒸馏关注token级别的概率对齐。RAL通过双重蒸馏扩展了这一点：

1. **Token分布对齐**：标准KD损失
2. **注意力分布对齐**：将注意力模式从教师转移到学生

这通过转移细粒度的感知和定位行为提供了显著的额外性能提升。

## 实验结果

RAL在视频和图像基准测试中持续优于GRPO，特别是在感知密集型任务上：

- 更好的视觉定位
- 减少模态偏差（对语言先验的过度依赖）
- 更忠实、基于证据的响应

## 重要意义

关键洞察：**注意力是跨模态推理的机制**。通过直接塑造注意力权重，RAL提供了一种有原则的方式来控制模型如何整合视觉和文本信息。

## 要点总结

1. **注意力作为策略**：内部注意力分布可以被视为可学习的策略空间
2. **过程优于结果**：对于感知任务，优化模型如何分配注意力比优化输出token更有效
3. **结构正则化**：基于注意力的优化提供了对多样性崩溃的稳健正则化
4. **超越token的蒸馏**：对于多模态对齐，转移注意力模式比token级蒸馏更有效
:::
