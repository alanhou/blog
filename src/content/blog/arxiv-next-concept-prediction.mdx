---
title:
  en: "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models"
  zh: "离散潜在空间中的下一概念预测产生更强的语言模型"
description:
  en: "Proposes Next Concept Prediction (NCP), a generative pretraining paradigm predicting discrete concepts spanning multiple tokens via Vector Quantization"
  zh: "提出下一概念预测（NCP），一种通过向量量化预测跨多个token的离散概念的生成预训练范式"
date: 2026-02-10
tags: ["arxiv", "ai", "pretraining", "language-modeling", "vector-quantization", "cs.CL", "cs.LG"]
image: "/arxiv-visuals/arxiv-next-concept-prediction.png"
---

:::en
**Paper**: [2602.08984](https://arxiv.org/abs/2602.08984)
**Authors**: Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin
**Categories**: cs.CL, cs.LG

## Abstract

This paper proposes Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens using Vector Quantization (VQ). The resulting model, ConceptLM, leverages both NCP and NTP objectives jointly. Trained from scratch at scales from 70M to 1.5B parameters with up to 300B tokens of data, results across 13 benchmarks show consistent gains. Continual pretraining on an 8B Llama model demonstrates that NCP can further improve models already trained with NTP.

## Key Contributions

- **Next Concept Prediction**: A novel pretraining objective that operates on multi-token concepts in discrete latent space
- **ConceptLM**: A model architecture combining NCP and NTP for stronger language modeling
- **Vector Quantization for concepts**: Using VQ to discretize multi-token spans into learnable concept codes
- **Scalability**: Demonstrated improvements from 70M to 8B parameters

## Motivation

Standard next token prediction treats language as a flat sequence of tokens, ignoring the hierarchical nature of language where meaning is often carried by multi-token spans (phrases, entities, expressions). NCP addresses this by introducing a higher-level prediction objective that captures these multi-token concepts.

## Method

### Concept Extraction via Vector Quantization

Given a sequence of token embeddings $e_1, e_2, \ldots, e_n$, NCP groups consecutive tokens into spans and maps each span to a discrete concept code using Vector Quantization:

1. **Span representation**: For a span $(e_i, \ldots, e_{i+k})$, compute a pooled representation $z = \text{Pool}(e_i, \ldots, e_{i+k})$
2. **Quantization**: Map $z$ to the nearest code in a learned codebook $C = \{c_1, c_2, \ldots, c_K\}$:

$$q = \arg\min_{c_j \in C} \|z - c_j\|_2$$

3. **Concept embedding**: The quantized code $c_q$ serves as the discrete concept representation

The codebook is trained end-to-end with commitment and codebook losses:

$$\mathcal{L}_{\text{VQ}} = \|z - \text{sg}[c_q]\|_2^2 + \beta \|\text{sg}[z] - c_q\|_2^2$$

where $\text{sg}[\cdot]$ denotes the stop-gradient operator.

### Joint NCP + NTP Training

ConceptLM is trained with a combined objective:

$$\mathcal{L} = \mathcal{L}_{\text{NTP}} + \lambda \mathcal{L}_{\text{NCP}}$$

where:

- $\mathcal{L}_{\text{NTP}} = -\sum_t \log P(x_t | x_{<t})$ is the standard next token prediction loss
- $\mathcal{L}_{\text{NCP}} = -\sum_s \log P(c_s | x_{<s})$ is the next concept prediction loss over concept spans
- $\lambda$ balances the two objectives

### Architecture

The model uses a shared transformer backbone with two prediction heads:

- **Token head**: Standard vocabulary projection for NTP
- **Concept head**: Projects to the VQ codebook space for NCP

The concept head predicts the discrete concept code for the upcoming span, encouraging the model to plan ahead beyond the immediate next token.

## Experimental Results

### From-Scratch Training (70M - 1.5B)

Across 13 benchmarks spanning language understanding, reasoning, and generation:

- ConceptLM consistently outperforms NTP-only baselines at all scales
- Gains are particularly pronounced on tasks requiring compositional understanding
- The improvement scales with model size, suggesting NCP captures complementary information

### Continual Pretraining on Llama 8B

Applying NCP as a continual pretraining objective on an already-trained Llama 8B model:

- Further improvements over the NTP-trained baseline
- Demonstrates NCP is complementary to, not redundant with, NTP training
- Relatively small additional compute yields meaningful gains

### Benchmark Analysis

Performance improvements are observed across diverse task types:

- **Language understanding**: Better grasp of multi-word expressions and phrases
- **Reasoning**: Improved compositional reasoning, likely from concept-level planning
- **Generation**: More coherent long-form text generation

## Analysis

### What Do Concepts Capture?

Analysis of the learned codebook reveals that concepts correspond to meaningful linguistic units:

- Named entities (multi-token names, locations)
- Common phrases and collocations
- Syntactic patterns (prepositional phrases, verb phrases)
- Domain-specific terminology

### Codebook Utilization

The VQ codebook shows healthy utilization patterns:

- High codebook usage (most codes are actively used)
- Semantic clustering (similar concepts map to nearby codes)
- Stable training dynamics with the commitment loss

### Ablation Studies

Key ablations reveal:

- **Span length**: Moderate span lengths (3-5 tokens) work best; too short reduces to NTP, too long loses granularity
- **Codebook size**: Larger codebooks improve performance up to a point, then plateau
- **Loss weight $\lambda$**: Balanced weighting between NTP and NCP is important; too much NCP weight can hurt token-level prediction

## Significance

NCP represents a meaningful step beyond the dominant next-token prediction paradigm. By introducing concept-level prediction through vector quantization, ConceptLM captures hierarchical structure in language that flat token prediction misses. The consistent improvements across scales and the ability to enhance already-trained models through continual pretraining make this a practical and impactful contribution to language model pretraining methodology.
:::

:::zh
**论文**: [2602.08984](https://arxiv.org/abs/2602.08984)
**作者**: Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin
**分类**: cs.CL, cs.LG

## 摘要

本文提出下一概念预测（NCP），一种建立在下一token预测（NTP）之上的生成预训练范式。NCP使用向量量化（VQ）预测跨多个token的离散概念。由此产生的模型ConceptLM同时利用NCP和NTP目标。从头训练70M到1.5B参数规模，使用多达300B数据，在13个基准上的结果显示一致的提升。在8B Llama上的持续预训练表明NCP可以进一步改善已用NTP训练的模型。

## 主要贡献

- **下一概念预测**：在离散潜在空间中对多token概念操作的新预训练目标
- **ConceptLM**：结合NCP和NTP的模型架构，实现更强的语言建模
- **概念的向量量化**：使用VQ将多token跨度离散化为可学习的概念代码
- **可扩展性**：从70M到8B参数展示了改进

## 动机

标准的下一token预测将语言视为token的平坦序列，忽略了语言的层次性质——意义通常由多token跨度（短语、实体、表达式）承载。NCP通过引入捕获这些多token概念的更高级别预测目标来解决这个问题。

## 方法

### 通过向量量化提取概念

给定token嵌入序列 $e_1, e_2, \ldots, e_n$，NCP将连续token分组为跨度，并使用向量量化将每个跨度映射到离散概念代码：

1. **跨度表示**：对于跨度 $(e_i, \ldots, e_{i+k})$，计算池化表示 $z = \text{Pool}\ldots, e_{i+k})$
2. **量化**：将 $z$ 映射到学习的码本 $C = \{c_1, c_2, \ldots, c_K\}$ 中最近的代码：

$$q = \arg\min_{c_j \in C} \|z - c_j\|_2$$

3. **概念嵌入**：量化代码 $c_q$ 作为离散概念表示

码本通过承诺损失和码本损失端到端训练：

$$\mathcal{L}_{\text{VQ}} = \|z - \text{sg}[c_q]\|_2^2 + \beta \|\text{sg}[z] - c_q\|_2^2$$

其中 $\text{sg}[\cdot]$ 表示停止梯度算子。

### 联合NCP + NTP训练

ConceptLM使用组合目标训练：

$$\mathcal{L} = \mathcal{L}_{\text{NTP}} + \lambda \mathcal{L}_{\text{NCP}}$$

其中：

- $\mathcal{L}_{\text{NTP}} = -\sum_t \log P(x_t | x_{<t})$ 是标准的下一token预测损失
- $\mathcal{L}_{\text{NCP}} = -\sum_s \log P(c_s | x_s})$ 是概念跨度上的下一概念预测损失
- $\lambda$ 平衡两个目标

### 架构

模型使用共享的Transformer主干和两个预测头：

- **Token头**：用于NTP的标准词汇投影
- **概念头**：投影到VQ码本空间用于NCP

概念头预测即将到来的跨度的离散概念代码，鼓励模型在即时下一个token之外进行前瞻规划。

## 实验结果

### 从头训练（70M - 1.5B）

在涵盖语言理解、推理和生成的13个基准上：

- ConceptLM在所有规模上一致优于仅NTP基线
- 在需要组合理解的任务上增益特别明显
- 改进随模型规模扩展，表明NCP捕获了互补信息

### 在Llama 8B上持续预训练

将NCP作为持续预训练目标应用于已训练的Llama 8B模型：

- 在NTP训练基线上进一步改进
- 证明NCP与NTP训练互补而非冗余
- 相对较小的额外计算产生有意义的增益

### 基准分析

在多样化任务类型上观察到性能改进：

- **语言理解**：更好地掌握多词表达和短语
- **推理**：改善的组合推理，可能来自概念级规划
- **生成**：更连贯的长文本生成

## 分析

### 概念捕获了什么？

对学习码本的分析揭示概念对应有意义的语言单位：

- 命名实体（多token名称、地点）
- 常见短语和搭配
- 句法模式（介词短语、动词短语）
- 领域特定术语

### 码本利用率

VQ码本显示健康的利用模式：

- 高码本使用率（大多数代码被积极使用）
- 语义聚类（相似概念映射到相近代码）
- 使用承诺损失的稳定训练动态

### 消融研究

关键消融揭示：

- **跨度长度**：中等跨度长度（3-5个token）效果最好；太短退化为NTP，太长失去粒度
- **码本大小**：更大的码本改善性能到一定程度后趋于平稳
- **损失权重 $\lambda$**：NTP和NCP之间的平衡权重很重要；过多的NCP权重会损害token级预测

## 意义

NCP代表了超越主导的下一token预测范式的有意义的一步。通过向量量化引入概念级预测，ConceptLM捕获了平坦token预测所遗漏的语言层次结构。跨规模的一致改进以及通过持续预训练增强已训练模型的能力，使这成为语言模型预训练方法论的实用且有影响力的贡献。
:::
