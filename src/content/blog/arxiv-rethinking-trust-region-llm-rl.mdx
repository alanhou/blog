---
title:
  en: "Rethinking the Trust Region in LLM Reinforcement Learning"
  zh: "重新思考LLM强化学习中的信任域"
description:
  en: "DPPO: A principled alternative to PPO's ratio clipping that directly estimates policy divergence for stable LLM fine-tuning"
  zh: "DPPO：一种有原则的PPO比率裁剪替代方案，直接估计策略散度以实现稳定的LLM微调"
date: 2026-02-04
tags: ["arxiv", "ai", "reinforcement-learning", "ppo", "llm", "fine-tuning", "cs.LG"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.04879](https://arxiv.org/abs/2602.04879)
**Authors**: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee
**Affiliations**: Sea AI Lab, National University of Singapore
**Code**: https://github.com/sail-sg/Stable-RL
**Categories**: cs.LG

## Abstract

Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs.

## The Problem with PPO's Ratio Clipping

PPO constrains policy updates based on the probability ratio of sampled tokens:
```
L_PPO = E_t[min(r_t * A_t, clip(r_t, 1-ε, 1+ε) * A_t)]

where r_t = π_θ(a_t|s_t) / π_old(a_t|s_t)
```

This is a **noisy single-sample Monte Carlo estimate** of true policy divergence.

### Why This Fails for LLMs

**Over-penalizing low-probability tokens:**
- Increasing a rare token's probability from 10^-5 to 10^-3 creates ratio = 100
- Triggers aggressive clipping even though actual divergence is negligible
- Slows down learning on important but rare tokens

**Under-penalizing high-probability tokens:**
- A drop from 0.99 to 0.8 has small ratio change
- But this is a catastrophic shift in probability mass
- Often remains unpenalized by clipping

### Training-Inference Mismatch

Numerical discrepancies between training and inference engines make the probability ratio highly volatile for low-probability tokens, while TV divergence remains stable.

## Key Contributions

- **Theoretical Formulation**: Policy improvement bounds tailored to finite-horizon, undiscounted LLM generation
- **Stability Analysis**: Identifies sources of training instability and the role of low-probability tokens in exploration
- **DPPO Algorithm**: Divergence Proximal Policy Optimization with principled trust region constraints

## DPPO: The Solution

Replace heuristic clipping with direct divergence estimation:

### Policy Improvement Bound for LLMs

For finite-horizon T with no discount (γ=1):
```
J(π) - J(μ) = L'_μ(π) - Δ(μ, π)
```

Where the penalty term depends on **Total Variation (TV) divergence** or **KL divergence**, not probability ratios.

### Efficient Approximations

To avoid huge memory footprint, DPPO introduces:

**Binary Divergence:**
```
D_Binary = |p_θ(a*) - p_old(a*)|
```
Only track the sampled token's probability change.

**Top-K Divergence:**
```
D_TopK = Σ_{i∈TopK} |p_θ(a_i) - p_old(a_i)|
```
Track top-K most probable tokens.

These capture essential distributional shifts with negligible overhead.

### The DPPO Objective

```
L_DPPO = E_t[r_t * A_t]    subject to D_TV(μ || π) ≤ δ
```

Uses a principled mask based on direct approximation of policy divergence, ensuring updates stay within a theoretically grounded trust region.

## Results

On AIME24 using Qwen3-30B-A3B-Base:
- DPPO significantly outperforms GRPO baselines
- Superior training efficiency and stability
- Works even without rollout routing replay (R3)

## Why This Matters

| Aspect | PPO | DPPO |
|--------|-----|------|
| Constraint | Probability ratio | Direct divergence |
| Low-prob tokens | Over-penalized | Properly handled |
| High-prob tokens | Under-constrained | Properly constrained |
| Theoretical basis | Heuristic | Trust region theory |

## Takeaways

1. **Ratio clipping is flawed for LLMs**: The large vocabulary makes single-sample ratio estimates unreliable
2. **Direct divergence estimation**: TV or KL divergence provides more stable and principled constraints
3. **Efficient approximations exist**: Binary and Top-K divergence capture essential shifts with minimal overhead
4. **Low-probability tokens matter**: They drive exploration and shouldn't be over-penalized
5. **Training stability**: DPPO provides a more robust foundation for RL-based LLM fine-tuning
:::

:::zh
**论文**: [2602.04879](https://arxiv.org/abs/2602.04879)
**作者**: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee
**机构**: Sea AI Lab, 新加坡国立大学
**代码**: https://github.com/sail-sg/Stable-RL
**分类**: cs.LG

## 摘要

强化学习（RL）已成为微调大语言模型（LLM）的基石，近端策略优化（PPO）是事实上的标准算法。尽管PPO被广泛使用，我们认为PPO的核心比率裁剪机制在结构上不适合LLM固有的大词汇表。

## PPO比率裁剪的问题

PPO基于采样token的概率比率约束策略更新：
```
L_PPO = E_t[min(r_t * A_t, clip(r_t, 1-ε, 1+ε) * A_t)]

其中 r_t = π_θ(a_t|s_t) / π_old(a_t|s_t)
```

这是真实策略散度的**噪声单样本蒙特卡洛估计**。

### 为什么这对LLM失效

**对低概率token过度惩罚：**
- 将稀有token的概率从10^-5提高到10^-3会产生比率=100
- 即使实际散度可以忽略不计，也会触发激进的裁剪
- 减慢了对重要但稀有token的学习

**对高概率token惩罚不足：**
- 从0.99降到0.8的比率变化很小
- 但这是概率质量的灾难性转移
- 通常不会被裁剪机制惩罚

### 训练-推理不匹配

训练和推理引擎之间的数值差异使得低概率token的概率比率高度不稳定，而TV散度保持稳定。

## 主要贡献

- **理论公式化**：针对有限时域、无折扣LLM生成的策略改进界
- **稳定性分析**：识别训练不稳定性的来源以及低概率token在探索中的作用
- **DPPO算法**：具有有原则信任域约束的散度近端策略优化

## DPPO：解决方案

用直接散度估计替代启发式裁剪：

### LLM的策略改进界

对于有限时域T且无折扣（γ=1）：
```
J(π) - J(μ) = L'_μ(π) - Δ(μ, π)
```

其中惩罚项依赖于**总变差（TV）散度**或**KL散度**，而非概率比率。

### 高效近似

为避免巨大的内存占用，DPPO引入：

**二元散度：**
```
D_Binary = |p_θ(a*) - p_old(a*)|
```
只跟踪采样token的概率变化。

**Top-K散度：**
```
D_TopK = Σ_{i∈TopK} |p_θ(a_i) - p_old(a_i)|
```
跟踪最可能的前K个token。

这些以可忽略的开销捕获了本质的分布变化。

### DPPO目标函数

```
L_DPPO = E_t[r_t * A_t]    约束条件 D_TV(μ || π) ≤ δ
```

使用基于策略散度直接近似的有原则掩码，确保更新保持在理论上有根据的信任域内。

## 实验结果

在使用Qwen3-30B-A3B-Base的AIME24上：
- DPPO显著优于GRPO基线
- 更优的训练效率和稳定性
- 即使没有rollout routing replay（R3）也能工作

## 重要意义

| 方面 | PPO | DPPO |
|------|-----|------|
| 约束 | 概率比率 | 直接散度 |
| 低概率token | 过度惩罚 | 正确处理 |
| 高概率token | 约束不足 | 正确约束 |
| 理论基础 | 启发式 | 信任域理论 |

## 要点总结

1. **比率裁剪对LLM有缺陷**：大词汇表使单样本比率估计不可靠
2. **直接散度估计**：TV或KL散度提供更稳定和有原则的约束
3. **存在高效近似**：二元和Top-K散度以最小开销捕获本质变化
4. **低概率token很重要**：它们驱动探索，不应被过度惩罚
5. **训练稳定性**：DPPO为基于RL的LLM微调提供了更稳健的基础
:::
