---
title:
  en: "SODA: Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens"
  zh: "SODA: 通过交错语义、声学和文本标记扩展开放离散音频基础模型"
description:
  en: "A systematic study of native audio foundation models that jointly model semantic content, acoustic details, and text through next-token prediction, introducing SODA models from 135M to 4B parameters trained on 500B tokens with comprehensive scaling law analysis."
  zh: "一项关于原生音频基础模型的系统性研究,通过下一标记预测联合建模语义内容、声学细节和文本,推出从1.35亿到40亿参数的SODA模型系列,基于5000亿标记训练并提供全面的扩展律分析。"
date: 2026-02-19
tags: ["arxiv", "ai", "cs.sd", "cs.cl", "eess.as"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.16687](https://arxiv.org/abs/2602.16687)
**Authors**: Potsawee Manakul, Woody Haosheng Gan, Martijn Bartelds, Guangzhi Sun, William Held, Diyi Yang
**Categories**: cs.SD, cs.CL, eess.AS

## Abstract

This paper introduces SODA (Scaling Open Discrete Audio), a suite of native audio foundation models that break away from the text-first paradigm dominating current audio language models. Unlike existing approaches that either extend pre-trained text LLM backbones or rely solely on semantic audio tokens, SODA applies next-token prediction directly to audio at scale, jointly modeling semantic content, acoustic details, and text. The research provides three major contributions: systematic investigation of design choices including data sources, text mixture ratios, and token composition; the first scaling law study for discrete audio models through IsoFLOP analysis across 64 models spanning $3 \times 10^{18}$ to $3 \times 10^{20}$ FLOPs; and the training of SODA models ranging from 135M to 4B parameters on 500B tokens, demonstrating their effectiveness as flexible backbones for diverse audio and text tasks.

## Key Contributions

- **Native Audio Modeling**: Introduces a paradigm shift from text-first to audio-native foundation models that jointly process semantic, acoustic, and textual information through unified next-token prediction
- **Comprehensive Design Recipe**: Systematically investigates and validates design choices for training discrete audio models, including optimal data source combinations, text-to-audio mixture ratios, and token composition strategies
- **First Scaling Laws for Audio**: Conducts IsoFLOP analysis on 64 models to establish scaling laws for discrete audio models, revealing that optimal data grows $1.6 \times$ faster than optimal model size
- **SODA Model Suite**: Trains and releases models from 135M to 4B parameters on 500B tokens, demonstrating strong performance across audio generation and cross-modal tasks
- **Unified Architecture**: Demonstrates versatility through fine-tuning for voice-preserving speech-to-speech translation using the same architecture

## Methodology and Architecture

The SODA framework employs a decoder-only transformer architecture that processes interleaved sequences of semantic tokens, acoustic tokens, and text tokens. This design choice enables the model to capture both high-level semantic content and fine-grained acoustic details simultaneously.

**Token Representation**: The model uses a hierarchical tokenization scheme where audio is represented through multiple token types. Semantic tokens capture linguistic content and meaning, while acoustic tokens encode prosody, speaker characteristics, and other acoustic properties. Text tokens are interleaved to enable cross-modal understanding and generation.

**Training Recipe**: The systematic investigation reveals several critical insights. The optimal text mixture ratio balances cross-modal capabilities with audio-specific performance. Data source diversity proves essential, combining speech datasets, music, and environmental sounds. The token composition strategy significantly impacts model quality, with interleaved semantic-acoustic sequences outperforming sequential or separate processing.

**Scaling Infrastructure**: Models are trained using efficient distributed training techniques, with careful attention to computational budget allocation. The training process spans models from 135M parameters (suitable for edge deployment) to 4B parameters (maximizing capability), all trained on a unified 500B token dataset.

## Scaling Law Analysis

The scaling law study represents a groundbreaking contribution to understanding how discrete audio models behave as computational resources increase. Using IsoFLOP analysis, the researchers trained 64 models with varying parameter counts and dataset sizes while keeping total compute constant.

**Key Findings**: The analysis reveals that optimal data size grows approximately $1.6 \times$ faster than optimal model size, a ratio distinct from text-only language models. This suggests that audio models benefit more from data diversity and volume relative to parameter count compared to their text counterparts.

**Compute-Optimal Frontier**: The study establishes a compute-optimal frontier spanning $3 \times 10^{18}$ to $3 \times 10^{20}$ FLOPs. For a given computational budget, the scaling laws provide precise guidance on how to allocate resources between model parameters and training tokens to achieve optimal performance.

**Practical Implications**: These scaling laws enable practitioners to make informed decisions about model architecture and training data requirements. For instance, when doubling computational budget, the laws suggest increasing data by approximately 60% and model size by approximately 40% for optimal results.

**Validation**: The SODA models themselves serve as validation of these scaling predictions, with observed performance closely matching theoretical expectations across the parameter range from 135M to 4B.

## Experimental Results and Applications

SODA models demonstrate strong performance across multiple benchmarks and tasks. The evaluation spans general audio generation, speech synthesis, cross-modal understanding, and specialized applications.

**Audio Generation Quality**: SODA models produce high-fidelity audio with both semantic accuracy and acoustic naturalness. The joint modeling of semantic and acoustic tokens enables the generation of speech that preserves speaker characteristics, prosody, and emotional content while maintaining linguistic correctness.

**Cross-Modal Capabilities**: By training on interleaved audio and text tokens, SODA develops robust cross-modal understanding. The models can perform audio captioning, text-to-speech synthesis, and audio-conditioned text generation with competitive performance against specialized models.

**Voice-Preserving Speech-to-Speech Translation**: A key demonstration involves fine-tuning SODA for speech-to-speech translation while preserving the source speaker's voice characteristics. Using the same unified architecture without task-specific modifications, the model successfully translates speech content while maintaining prosody, speaking style, and voice identity—a challenging task requiring simultaneous understanding of linguistic content, acoustic properties, and cross-lingual mapping.

**Efficiency and Scalability**: The 135M parameter model provides a compelling efficiency-performance tradeoff suitable for resource-constrained environments, while the 4B parameter model pushes the capability frontier. This range enables deployment across diverse hardware platforms from mobile devices to cloud infrastructure.

## Takeaways

1. Native audio foundation models that jointly process semantic, acoustic, and text tokens through next-token prediction offer a more general and flexible approach than text-first architectures
2. Optimal data for discrete audio models grows $1.6 \times$ faster than optimal model size, establishing distinct scaling behavior from text-only language models
3. Systematic design choices—including data source diversity, text mixture ratios around 20-30%, and interleaved token composition—are critical for training effective audio foundation models
4. IsoFLOP analysis across 64 models spanning $3 \times 10^{18}$ to $3 \times 10^{20}$ FLOPs provides actionable scaling laws for resource allocation in audio model training
5. SODA models from 135M to 4B parameters demonstrate that a single unified architecture can serve as a flexible backbone for diverse audio and cross-modal tasks, including challenging applications like voice-preserving speech-to-speech translation
:::

:::zh
**论文**: [2602.16687](https://arxiv.org/abs/2602.16687)
**作者**: Potsawee Manakul, Woody Haosheng Gan, Martijn Bartelds, Guangzhi Sun, William Held, Diyi Yang
**分类**: cs.SD, cs.CL, eess.AS

## 摘要

本文介绍了SODA(Scaling Open Discrete Audio,扩展开放离散音频),这是一套原生音频基础模型,打破了当前音频语言模型主导的文本优先范式。与现有方法不同——它们要么扩展预训练的文本LLM骨干网络,要么仅依赖语义音频标记——SODA直接在大规模音频上应用下一标记预测,联合建模语义内容、声学细节和文本。该研究提供了三大贡献:系统性研究设计选择(包括数据源、文本混合比例和标记组成);通过IsoFLOP分析对64个模型进行离散音频模型的首次扩展律研究,计算量跨越$3 \times 10^{18}$到$3 \times 10^{20}$ FLOPs;以及训练从1.35亿到40亿参数的SODA模型(基于5000亿标记),展示其作为多样化音频和文本任务灵活骨干的有效性。

## 主要贡献

- **原生音频建模**:引入从文本优先到音频原生基础模型的范式转变,通过统一的下一标记预测联合处理语义、声学和文本信息
- **全面的设计方案**:系统性研究并验证训练离散音频模型的设计选择,包括最优数据源组合、文本与音频混合比例以及标记组成策略
- **音频领域首个扩展律**:对64个模型进行IsoFLOP分析,建立离散音频模型的扩展律,揭示最优数据量增长速度是最优模型规模的$1.6 \times$
- **SODA模型套件**:训练并发布从1.35亿到40亿参数的模型(基于5000亿标记),在音频生成和跨模态任务中展现强大性能
- **统一架构**:通过使用相同架构微调实现保留声音特征的语音到语音翻译,展示了模型的多功能性

## 方法论与架构

SODA框架采用仅解码器的transformer架构,处理语义标记、声学标记和文本标记的交错序列。这种设计选择使模型能够同时捕获高层语义内容和细粒度声学细节。

**标记表示**:模型使用分层标记化方案,其中音频通过多种标记类型表示。语义标记捕获语言内容和含义,而声学标记编码韵律、说话人特征和其他声学属性。文本标记交错其中以实现跨模态理解和生成。

**训练方案**:系统性研究揭示了几个关键见解。最优文本混合比例在跨模态能力和音频特定性能之间取得平衡。数据源多样性被证明至关重要,结合了语音数据集、音乐和环境声音。标记组成策略显著影响模型质量,交错的语义-声学序列优于顺序或分离处理。

**扩展基础设施**:模型使用高效的分布式训练技术进行训练,仔细关注计算预算分配。训练过程涵盖从1.35亿参数(适合边缘部署)到40亿参数(最大化能力)的模型,全部在统一的5000亿标记数据集上训练。

## 扩展律分析

扩展律研究代表了理解离散音频模型随计算资源增加而表现如何的开创性贡献。使用IsoFLOP分析,研究人员训练了64个具有不同参数数量和数据集规模的模型,同时保持总计算量恒定。

**关键发现**:分析揭示最优数据规模增长速度约为最优模型规模的$1.6 \times$,这一比例与纯文本语言模型不同。这表明相对于参数数量,音频模型比文本模型更能从数据多样性和数量中受益。

**计算最优前沿**:研究建立了跨越$3 \times 10^{18}$到$3 \times 10^{20}$ FLOPs的计算最优前沿。对于给定的计算预算,扩展律提供了如何在模型参数和训练标记之间分配资源以实现最优性能的精确指导。

**实践意义**:这些扩展律使从业者能够就模型架构和训练数据需求做出明智决策。例如,当计算预算翻倍时,扩展律建议将数据增加约60%,模型规模增加约40%以获得最优结果。

**验证**:SODA模型本身作为这些扩展预测的验证,观察到的性能在从1.35亿到40亿的参数范围内与理论预期密切匹配。

## 实验结果与应用

SODA模型在多个基准测试和任务中展现出强大性能。评估涵盖通用音频生成、语音合成、跨模态理解和专门应用。

**音频生成质量**:SODA模型生成具有语义准确性和声学自然度的高保真音频。语义和声学标记的联合建模使生成的语音能够保留说话人特征、韵律和情感内容,同时保持语言正确性。

**跨模态能力**:通过在交错的音频和文本标记上训练,SODA发展出强大的跨模态理解能力。模型可以执行音频字幕生成、文本到语音合成和音频条件文本生成,性能与专门模型相当。

**保留声音的语音到语音翻译**:一个关键演示涉及微调SODA进行语音到语音翻译,同时保留源说话人的声音特征。使用相同的统一架构而无需任务特定修改,模型成功翻译语音内容,同时保持韵律、说话风格和声音身份——这是一项需要同时理解语言内容、声学属性和跨语言映射的挑战性任务。

**效率与可扩展性**:1.35亿参数模型提供了适合资源受限环境的令人信服的效率-性能权衡,而40亿参数模型则推动了能力前沿。这一范围使得能够在从移动设备到云基础设施的各种硬件平台上部署。

## 要点总结

1. 通过下一标记预测联合处理语义、声学和文本标记的原生音频基础模型,提供了比文本优先架构更通用和灵活的方法
2. 离散音频模型的最优数据增长速度是最优模型规模的$1.6 \times$,建立了与纯文本语言模型不同的扩展行为
3. 系统性设计选择——包括数据源多样性、约20-30%的文本混合比例以及交错标记组成——对训练有效的音频基础模型至关重要
4. 对跨越$3 \times 10^{18}$到$3 \times 10^{20}$ FLOPs的64个模型进行IsoFLOP分析,为音频模型训练中的资源分配提供了可操作的扩展律
5. 从1.35亿到40亿参数的SODA模型证明,单一统一架构可以作为多样化音频和跨模态任务的灵活骨干,包括保留声音的语音到语音翻译等挑战性应用
:::
