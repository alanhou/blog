---
title:
  en: "Implicit Regularization in Langevin Dynamics with Projected Noise"
  zh: "投影噪声朗之万动力学中的隐式正则化"
description:
  en: "This paper reveals how symmetry in over-parametrized models induces implicit regularization through projected noise in Langevin dynamics, establishing an equivalence with isotropic diffusion plus a drift term related to orbit geometry."
  zh: "本文揭示了过参数化模型中的对称性如何通过朗之万动力学中的投影噪声诱导隐式正则化,建立了与各向同性扩散加轨道几何相关漂移项的等价性。"
date: 2026-02-15
tags: ["arxiv", "ai", "math.pr", "cs.ai"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.12257](https://arxiv.org/abs/2602.12257)
**Authors**: Govind Menon, Austin J. Stromme, Adrien Vacher
**Categories**: math.PR, cs.AI

## Abstract

This work investigates Langevin dynamics where noise is projected orthogonally to an isometric group action, providing theoretical insights into how symmetry affects stochastic gradient descent in over-parametrized neural networks. The central finding establishes that when both initial and target distributions are invariant under the group action, projected-noise Langevin dynamics is equivalent in distribution to standard isotropic Langevin dynamics augmented with an additional drift term. This drift is proportional to the negative logarithmic gradient of the group orbit volume, which the authors identify geometrically as the mean curvature of the orbits. The proof constructs an elegant coupling through an auxiliary process on the group manifold itself.

## Key Contributions

- Introduces a mathematical framework for studying Langevin dynamics with noise projected onto directions orthogonal to group symmetries
- Proves distributional equivalence between projected-noise dynamics and isotropic dynamics with geometric drift correction
- Identifies the additional drift term as $-\nabla \log \text{vol}(G \cdot x)$, relating it to the mean curvature of group orbits
- Constructs a novel coupling argument using an auxiliary process on the group to establish the equivalence
- Provides theoretical foundation for understanding implicit regularization induced by symmetry in over-parametrized models

## Mathematical Framework and Methodology

The paper considers a smooth manifold $M$ with an isometric action by a compact Lie group $G$. The standard Langevin dynamics with isotropic noise is given by:

$$dX_t = -\nabla V(X_t)dt + \sqrt{2\beta^{-1}}dW_t$$

where $V$ is a potential function and $\beta$ is inverse temperature. The authors introduce projected Langevin dynamics where the Brownian motion $W_t$ is replaced by its projection onto the orthogonal complement of the group orbit directions.

The key technical innovation is constructing a coupling via a process $(X_t, g_t)$ on $M \times G$ where $g_t$ evolves according to a stochastic differential equation on the group. This allows the authors to show that the marginal distribution of $X_t$ under projected noise matches that of a modified isotropic process with the additional geometric drift term.

## Geometric Interpretation and Implicit Regularization

The additional drift term $-\nabla \log \text{vol}(G \cdot x)$ has a profound geometric interpretation. The volume of the group orbit $G \cdot x = \{g \cdot x : g \in G\}$ measures the "size" of the symmetry class at point $x$. The negative gradient of its logarithm pushes the dynamics toward regions where orbits have larger volume.

This geometric drift acts as an implicit regularizer: it biases the stochastic dynamics toward solutions with higher symmetry or larger stabilizer subgroups. In the context of neural network training, this explains why SGD with symmetries tends to find solutions that respect those symmetries even without explicit regularization terms in the loss function.

The identification with mean curvature connects this phenomenon to classical differential geometry. Orbits with high mean curvature (highly curved) correspond to points where the group action is "concentrated," and the drift naturally moves away from such singular configurations.

## Implications for Deep Learning

This theoretical result has significant implications for understanding optimization in over-parametrized neural networks:

**Symmetry and generalization**: Neural networks often exhibit various symmetries (permutation symmetry of hidden units, scaling symmetries, etc.). This work suggests that SGD naturally exploits these symmetries through implicit regularization, potentially explaining why over-parametrized models generalize well despite having capacity to overfit.

**Architecture design**: The geometric drift mechanism suggests that architectural choices inducing specific symmetries will automatically bias learning toward solutions respecting those symmetries. This provides theoretical justification for symmetry-aware architectures.

**Optimization landscape**: The orbit volume term reveals that the effective optimization landscape experienced by projected-noise dynamics differs from the original loss landscape by a geometry-dependent correction. This helps explain convergence properties and the structure of learned solutions.

## Takeaways

1. Projecting noise orthogonal to group symmetries in Langevin dynamics induces an implicit regularization effect equivalent to adding a geometric drift term
2. The drift term $-\nabla \log \text{vol}(G \cdot x)$ biases dynamics toward regions with larger orbit volumes, corresponding to higher symmetry
3. This geometric drift is identified as the mean curvature of group orbits, connecting stochastic optimization to differential geometry
4. The result provides theoretical foundation for understanding why SGD respects symmetries in over-parametrized models without explicit regularization
5. The coupling construction through an auxiliary group process offers a powerful technique for analyzing symmetry-constrained stochastic dynamics
:::

:::zh
**论文**: [2602.12257](https://arxiv.org/abs/2602.12257)
**作者**: Govind Menon, Austin J. Stromme, Adrien Vacher
**分类**: math.PR, cs.AI

## 摘要

本研究探讨了噪声被投影到等距群作用正交方向上的朗之万动力学,为理解对称性如何影响过参数化神经网络中的随机梯度下降提供了理论洞察。核心发现表明,当初始分布和目标分布都在群作用下保持不变时,投影噪声朗之万动力学在分布意义上等价于标准各向同性朗之万动力学加上一个额外的漂移项。该漂移项与群轨道体积的负对数梯度成正比,作者从几何角度将其识别为轨道的平均曲率。证明通过在群流形上构造辅助过程来建立优雅的耦合。

## 主要贡献

- 引入了研究噪声投影到群对称性正交方向的朗之万动力学的数学框架
- 证明了投影噪声动力学与带几何漂移修正的各向同性动力学之间的分布等价性
- 将额外漂移项识别为 $-\nabla \log \text{vol}(G \cdot x)$,并将其与群轨道的平均曲率联系起来
- 通过群上的辅助过程构造了新颖的耦合论证来建立等价性
- 为理解过参数化模型中对称性诱导的隐式正则化提供了理论基础

## 数学框架与方法论

论文考虑了一个光滑流形 $M$ 上紧李群 $G$ 的等距作用。标准的各向同性噪声朗之万动力学由下式给出:

$$dX_t = -\nabla V(X_t)dt + \sqrt{2\beta^{-1}}dW_t$$

其中 $V$ 是势函数,$\beta$ 是逆温度。作者引入了投影朗之万动力学,其中布朗运动 $W_t$ 被替换为其在群轨道方向正交补空间上的投影。

关键的技术创新是通过 $M \times G$ 上的过程 $(X_t, g_t)$ 构造耦合,其中 $g_t$ 根据群上的随机微分方程演化。这使得作者能够证明投影噪声下 $X_t$ 的边缘分布与带有额外几何漂移项的修正各向同性过程的分布相匹配。

## 几何解释与隐式正则化

额外的漂移项 $-\nabla \log \text{vol}(G \cdot x)$ 具有深刻的几何解释。群轨道 $G \cdot x = \{g \cdot x : g \in G\}$ 的体积度量了点 $x$ 处对称类的"大小"。其对数的负梯度将动力学推向轨道体积更大的区域。

这种几何漂移充当隐式正则化器:它使随机动力学偏向于具有更高对称性或更大稳定子群的解。在神经网络训练的背景下,这解释了为什么带有对称性的SGD倾向于找到尊重这些对称性的解,即使损失函数中没有显式的正则化项。

与平均曲率的识别将这一现象与经典微分几何联系起来。具有高平均曲率(高度弯曲)的轨道对应于群作用"集中"的点,而漂移自然地远离这种奇异配置。

## 对深度学习的启示

这一理论结果对理解过参数化神经网络中的优化具有重要意义:

**对称性与泛化**: 神经网络通常表现出各种对称性(隐藏单元的置换对称性、缩放对称性等)。这项工作表明SGD通过隐式正则化自然地利用这些对称性,这可能解释了为什么过参数化模型尽管有过拟合的能力却能很好地泛化。

**架构设计**: 几何漂移机制表明,诱导特定对称性的架构选择将自动使学习偏向于尊重这些对称性的解。这为对称感知架构提供了理论依据。

**优化景观**: 轨道体积项揭示了投影噪声动力学所经历的有效优化景观与原始损失景观之间存在几何依赖的修正。这有助于解释收敛性质和学习解的结构。

## 要点总结

1. 在朗之万动力学中将噪声投影到群对称性的正交方向会诱导隐式正则化效应,等价于添加几何漂移项
2. 漂移项 $-\nabla \log \text{vol}(G \cdot x)$ 使动力学偏向轨道体积更大的区域,对应于更高的对称性
3. 这种几何漂移被识别为群轨道的平均曲率,将随机优化与微分几何联系起来
4. 该结果为理解为什么SGD在过参数化模型中无需显式正则化就能尊重对称性提供了理论基础
5. 通过辅助群过程的耦合构造为分析对称性约束的随机动力学提供了强大的技术
:::
