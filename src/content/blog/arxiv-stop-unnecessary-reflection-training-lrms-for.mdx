---
title:
  en: "Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty"
  zh: "停止不必要的反思:通过自适应反思和长度协调惩罚训练高效推理的大型推理模型"
description:
  en: "A novel reinforcement learning framework that reduces token consumption in Large Reasoning Models by 35-53% while improving accuracy by 2.7-5.8% through adaptive reflection penalties."
  zh: "一种新颖的强化学习框架,通过自适应反思惩罚将大型推理模型的token消耗降低35-53%,同时将准确率提升2.7-5.8%。"
date: 2026-02-13
tags: ["arxiv", "ai", "cs.ai", "cs.cl"]
image: "/arxiv-visuals/arxiv-stop-unnecessary-reflection-training-lrms-for.png"
---

:::en
**Paper**: [2602.12113](https://arxiv.org/abs/2602.12113)
**Authors**: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao
**Categories**: cs.AI, cs.CL

## Abstract

Large Reasoning Models (LRMs) have achieved impressive results on complex reasoning tasks through test-time scaling, but they suffer from a critical inefficiency: excessive reflection. These models generate unnecessarily long chains-of-thought filled with repetitive self-questioning and circular reasoning, consuming massive computational resources without improving accuracy. This paper introduces ARLCP (Adaptive Reflection and Length Coordinated Penalty), a reinforcement learning framework that dynamically balances reasoning efficiency and accuracy. The method employs two coordinated penalties—one targeting unnecessary reflective steps and another calibrated to problem complexity—to guide models toward more concise reasoning paths. Evaluated on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen models (1.5B and 7B parameters), ARLCP demonstrates remarkable improvements: the 1.5B model achieves 53.1% shorter responses with 5.8% higher accuracy, while the 7B model shows 35.0% length reduction with 2.7% accuracy gains.

## Key Contributions

- Identification of the excessive reflection problem in LRMs, where increasing problem complexity paradoxically induces more unnecessary reflection, reducing both accuracy and efficiency
- Development of ARLCP, a novel RL framework with adaptive reflection penalty that curtails redundant reflective steps while preserving essential reasoning processes
- Introduction of a complexity-calibrated length penalty that dynamically adjusts based on estimated problem difficulty
- Comprehensive evaluation demonstrating superior efficiency-accuracy trade-offs across multiple model sizes and benchmarks
- Open-source implementation enabling reproducibility and further research

## The Problem: When Reflection Becomes Counterproductive

The research reveals a fundamental inefficiency in current LRMs. While test-time scaling through extended reasoning has proven effective, the authors observe that models frequently engage in excessive reflection—repetitive self-questioning, circular reasoning, and redundant verification steps that consume tokens without contributing to solution quality.

The key insight is that this problem worsens with problem complexity. As tasks become more challenging, models generate increasingly verbose reasoning chains, but much of this additional content represents unproductive reflection rather than meaningful progress toward solutions. For smaller models (1.5B parameters), this phenomenon is particularly pronounced, with accuracy actually decreasing as reflection increases.

This creates a dual problem: computational waste (high token consumption, increased latency, greater inference costs) and reduced effectiveness (lower accuracy due to reasoning paths that lose focus). The challenge is distinguishing between productive reasoning steps and unnecessary reflection, then training models to avoid the latter without sacrificing the former.

## Methodology: Coordinated Penalties for Efficient Reasoning

ARLCP addresses excessive reflection through a reinforcement learning framework with two coordinated penalty mechanisms:

**Adaptive Reflection Penalty**: This component identifies and penalizes unnecessary reflective behaviors. The penalty adapts based on the reasoning context, allowing essential self-correction while curtailing redundant questioning. The framework learns to recognize patterns like circular reasoning loops and repetitive verification that don't advance toward solutions.

**Length Penalty Calibrated to Complexity**: Rather than applying uniform length constraints, ARLCP estimates problem complexity and adjusts the length penalty accordingly. Simple problems receive stronger length penalties to encourage conciseness, while complex problems are allowed more reasoning steps when justified. This prevents the model from being overly verbose on easy tasks while ensuring sufficient reasoning depth for difficult ones.

The coordination between these penalties is crucial. The reflection penalty targets specific inefficient behaviors, while the complexity-calibrated length penalty provides overall guidance on reasoning path length. Together, they create a training signal that encourages models to generate reasoning chains that are both accurate and efficient.

The training process uses reinforcement learning with these penalties integrated into the reward function. Models learn to balance multiple objectives: achieving correct solutions, maintaining reasonable response lengths, and avoiding excessive reflection. The framework is model-agnostic and can be applied to various LRM architectures.

## Experimental Results and Analysis

The authors evaluate ARLCP on five mathematical reasoning benchmarks (MATH500, AMC23, AIME24, OlympiadBench, GPQA-Diamond) using DeepSeek-R1-Distill-Qwen models at 1.5B and 7B parameter scales.

**Performance on 1.5B Model**: ARLCP achieves dramatic improvements on the smaller model, reducing average response length by 53.1% while simultaneously improving accuracy by 5.8%. This represents a fundamental shift in the efficiency-accuracy trade-off. The baseline model's tendency toward excessive reflection is particularly problematic at this scale, and ARLCP's targeted penalties effectively address this weakness.

**Performance on 7B Model**: The larger model shows more controlled reflection in the baseline, but ARLCP still delivers substantial gains: 35.0% length reduction with 2.7% accuracy improvement. This demonstrates that even well-performing models benefit from explicit training to avoid unnecessary reflection.

**Benchmark-Specific Insights**: Performance varies across benchmarks, with the most significant improvements on tasks where baseline models exhibited the most excessive reflection. On MATH500 and AMC23, which contain problems of varying complexity, the complexity-calibrated length penalty proves particularly effective, allowing appropriate reasoning depth while preventing verbosity.

**Comparison with Existing Approaches**: ARLCP outperforms alternative methods for controlling reasoning length, including fixed-length penalties and simple reflection detection. The adaptive and coordinated nature of the penalties proves essential for maintaining accuracy while improving efficiency.

## Implications and Future Directions

This research has significant implications for the deployment of LRMs in production environments. The 35-53% reduction in token consumption directly translates to lower inference costs, reduced latency, and improved scalability. For applications requiring real-time reasoning, these efficiency gains are transformative.

The work also challenges assumptions about test-time scaling. While longer reasoning chains can improve performance, this research demonstrates that unbounded reflection is counterproductive. The key is not maximizing reasoning length but optimizing reasoning quality—a distinction that ARLCP operationalizes through its penalty mechanisms.

Future research directions include extending ARLCP to other reasoning domains beyond mathematics, investigating its effectiveness on larger model scales, and exploring whether similar principles apply to other forms of test-time computation. The framework's model-agnostic nature suggests broad applicability across different LRM architectures.

The open-source release of the code enables the research community to build upon this work, potentially leading to further refinements in training efficient reasoning models.

## Takeaways

1. Excessive reflection in LRMs is a significant problem that worsens with problem complexity, reducing both accuracy and efficiency, particularly in smaller models
2. ARLCP's coordinated penalty approach—combining adaptive reflection penalties with complexity-calibrated length penalties—achieves superior efficiency-accuracy trade-offs compared to existing methods
3. The 1.5B parameter model shows 53.1% length reduction with 5.8% accuracy improvement, while the 7B model achieves 35.0% length reduction with 2.7% accuracy gains
4. Complexity-aware length penalties are crucial for allowing appropriate reasoning depth on difficult problems while preventing verbosity on simpler tasks
5. The framework is model-agnostic and open-source, enabling broad adoption and further research into efficient reasoning model training
:::

:::zh
**论文**: [2602.12113](https://arxiv.org/abs/2602.12113)
**作者**: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo Zhao
**分类**: cs.AI, cs.CL

## 摘要

大型推理模型(LRMs)通过测试时扩展在复杂推理任务上取得了令人印象深刻的成果,但它们存在一个关键的低效问题:过度反思。这些模型生成不必要的冗长思维链,充斥着重复的自我质疑和循环推理,消耗大量计算资源却无法提升准确率。本文提出ARLCP(自适应反思和长度协调惩罚),这是一个动态平衡推理效率和准确性的强化学习框架。该方法采用两个协调的惩罚机制——一个针对不必要的反思步骤,另一个根据问题复杂度校准——引导模型生成更简洁的推理路径。在五个数学推理基准上使用DeepSeek-R1-Distill-Qwen模型(1.5B和7B参数)进行评估,ARLCP展现出显著改进:1.5B模型实现了53.1%的响应长度缩短和5.8%的准确率提升,而7B模型则实现了35.0%的长度减少和2.7%的准确率提升。

## 主要贡献

- 识别了LRMs中的过度反思问题,发现问题复杂度增加反而会诱发更多不必要的反思,同时降低准确性和效率
- 开发了ARLCP这一新颖的强化学习框架,通过自适应反思惩罚抑制冗余的反思步骤,同时保留必要的推理过程
- 引入了根据问题难度动态调整的复杂度校准长度惩罚机制
- 通过全面评估展示了在多个模型规模和基准测试中的卓越效率-准确率权衡
- 开源实现使研究具有可复现性并促进后续研究

## 问题分析:当反思变得适得其反

研究揭示了当前LRMs的一个根本性低效问题。虽然通过扩展推理进行测试时扩展已被证明有效,但作者观察到模型经常进行过度反思——重复的自我质疑、循环推理和冗余的验证步骤,这些消耗token却对解决方案质量没有贡献。

关键洞察在于这个问题随问题复杂度而恶化。随着任务变得更具挑战性,模型生成越来越冗长的推理链,但这些额外内容大多代表非生产性的反思,而非朝向解决方案的有意义进展。对于较小的模型(1.5B参数),这种现象尤为明显,随着反思增加,准确率实际上在下降。

这造成了双重问题:计算浪费(高token消耗、增加的延迟、更高的推理成本)和效果降低(由于推理路径失去焦点导致准确率下降)。挑战在于区分生产性推理步骤和不必要的反思,然后训练模型避免后者而不牺牲前者。

## 方法论:协调惩罚实现高效推理

ARLCP通过一个具有两个协调惩罚机制的强化学习框架来解决过度反思问题:

**自适应反思惩罚**:该组件识别并惩罚不必要的反思行为。惩罚根据推理上下文自适应调整,允许必要的自我纠正,同时抑制冗余的质疑。框架学习识别循环推理循环和不推进解决方案的重复验证等模式。

**复杂度校准的长度惩罚**:ARLCP不是应用统一的长度约束,而是估计问题复杂度并相应调整长度惩罚。简单问题接受更强的长度惩罚以鼓励简洁性,而复杂问题在合理时允许更多推理步骤。这防止模型在简单任务上过于冗长,同时确保困难问题有足够的推理深度。

这些惩罚之间的协调至关重要。反思惩罚针对特定的低效行为,而复杂度校准的长度惩罚提供推理路径长度的整体指导。它们共同创建了一个训练信号,鼓励模型生成既准确又高效的推理链。

训练过程使用强化学习,将这些惩罚整合到奖励函数中。模型学习平衡多个目标:获得正确解决方案、保持合理的响应长度以及避免过度反思。该框架与模型无关,可应用于各种LRM架构。

## 实验结果与分析

作者在五个数学推理基准(MATH500、AMC23、AIME24、OlympiadBench、GPQA-Diamond)上使用1.5B和7B参数规模的DeepSeek-R1-Distill-Qwen模型评估ARLCP。

**1.5B模型的性能**:ARLCP在较小模型上实现了显著改进,将平均响应长度减少53.1%,同时将准确率提高5.8%。这代表了效率-准确率权衡的根本性转变。基线模型在这个规模上的过度反思倾向尤为严重,ARLCP的针对性惩罚有效解决了这一弱点。

**7B模型的性能**:较大模型在基线中显示出更受控的反思,但ARLCP仍然带来实质性收益:35.0%的长度减少和2.7%的准确率提升。这表明即使是表现良好的模型也能从明确训练以避免不必要反思中受益。

**基准特定洞察**:不同基准的性能有所差异,在基线模型表现出最多过度反思的任务上改进最为显著。在包含不同复杂度问题的MATH500和AMC23上,复杂度校准的长度惩罚被证明特别有效,在防止冗长的同时允许适当的推理深度。

**与现有方法的比较**:ARLCP优于控制推理长度的替代方法,包括固定长度惩罚和简单反思检测。惩罚的自适应和协调性质被证明对于在提高效率的同时保持准确性至关重要。

## 影响与未来方向

这项研究对在生产环境中部署LRMs具有重要意义。35-53%的token消耗减少直接转化为更低的推理成本、减少的延迟和改进的可扩展性。对于需要实时推理的应用,这些效率提升具有变革性。

这项工作也挑战了关于测试时扩展的假设。虽然更长的推理链可以提高性能,但这项研究表明无限制的反思是适得其反的。关键不在于最大化推理长度,而在于优化推理质量——ARLCP通过其惩罚机制将这一区别操作化。

未来的研究方向包括将ARLCP扩展到数学之外的其他推理领域,研究其在更大模型规模上的有效性,以及探索类似原则是否适用于其他形式的测试时计算。该框架与模型无关的特性表明其在不同LRM架构中具有广泛适用性。

代码的开源发布使研究社区能够在这项工作的基础上继续发展,可能导致训练高效推理模型的进一步改进。

## 要点总结

1. LRMs中的过度反思是一个重大问题,随问题复杂度恶化,降低准确性和效率,在较小模型中尤为明显
2. ARLCP的协调惩罚方法——结合自适应反思惩罚和复杂度校准的长度惩罚——相比现有方法实现了卓越的效率-准确率权衡
3. 1.5B参数模型显示53.1%的长度减少和5.8%的准确率提升,而7B模型实现35.0%的长度减少和2.7%的准确率提升
4. 复杂度感知的长度惩罚对于在困难问题上允许适当推理深度,同时防止简单任务上的冗长至关重要
5. 该框架与模型无关且开源,使其能够广泛采用并促进高效推理模型训练的进一步研究
:::
