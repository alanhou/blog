---
title:
  en: "MemStream: Scaling Token Budgets for Enhanced Video Stream Understanding with Dynamic KV-Cache"
  zh: "MemStream: 通过动态键值缓存扩展令牌预算以增强视频流理解"
description:
  en: "A novel approach to streaming video understanding that scales token budgets and introduces adaptive selection strategies to preserve fine-grained spatiotemporal details, achieving significant improvements over existing methods."
  zh: "一种新颖的流式视频理解方法,通过扩展令牌预算并引入自适应选择策略来保留细粒度时空细节,相比现有方法取得显著性能提升。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.cv"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.18434](https://arxiv.org/abs/2602.18434)
**Authors**: Vatsal Agarwal, Saksham Suri, Matthew Gwilliam, Pulkit Kumar, Abhinav Shrivastava
**Categories**: cs.CV

## Abstract

Streaming video understanding presents unique challenges in encoding, storing, and retrieving information from continuous video streams for accurate video question answering (VQA). This paper introduces MemStream, a method that addresses the limitations of current key-value caching approaches which use restricted token budgets per frame, resulting in loss of fine-grained visual details. The authors identify a critical issue: existing methods exhibit temporal bias where query-frame similarity scores increase over time, favoring later frames during retrieval. MemStream proposes scaling token budgets combined with an adaptive selection strategy to reduce redundancy while maintaining local spatiotemporal information. Additionally, a training-free retrieval mixture-of-experts mechanism leverages external models for improved frame identification. The method demonstrates substantial improvements: +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) compared to ReKV using Qwen2.5-VL-7B.

## Key Contributions

- **Token Budget Scaling**: Proposes increasing the number of tokens per frame to capture more granular spatiotemporal information, addressing the information loss inherent in limited-token approaches
- **Adaptive Selection Strategy**: Introduces a mechanism to reduce token redundancy while preserving critical local spatiotemporal details, mitigating the temporal bias problem in feature encoding
- **Training-Free Retrieval MoE**: Develops a mixture-of-experts approach that incorporates external models to enhance frame retrieval accuracy without requiring additional training
- **Comprehensive Evaluation**: Demonstrates significant performance gains across multiple benchmarks (CG-Bench, LVBench, VideoMME) with consistent improvements over state-of-the-art methods

## Technical Methodology

The core innovation of MemStream lies in its multi-faceted approach to handling dense video streams. Traditional methods like ReKV allocate a fixed, limited number of tokens per frame (typically 64-256 tokens), which forces aggressive compression of visual information. This compression leads to two critical problems: (1) loss of fine-grained details necessary for complex reasoning tasks, and (2) temporal bias in similarity computations.

MemStream addresses these issues through three key mechanisms:

**1. Scaled Token Representation**: By increasing the token budget per frame, the method maintains richer visual representations. However, naive scaling leads to computational challenges and redundancy. The authors carefully balance token count with information density.

**2. Adaptive Token Selection**: Rather than uniformly distributing tokens across all spatial regions, MemStream employs an adaptive strategy that identifies and preserves tokens corresponding to salient regions while reducing redundancy in homogeneous areas. This approach maintains $O(n)$ complexity while improving information retention.

**3. Retrieval Mixture-of-Experts**: The retrieval mechanism combines multiple scoring functions from different models to identify relevant frames. Given a query $q$ and frame representations $\{f_1, f_2, ..., f_T\}$, the system computes:

$$s_i = \sum_{j=1}^{M} w_j \cdot \text{sim}_j(q, f_i)$$

where $\text{sim}_j$ represents the similarity function from expert $j$, and $w_j$ are learned or heuristic weights. This ensemble approach reduces the temporal bias observed in single-model retrieval.

## Experimental Results and Analysis

MemStream's performance across three major benchmarks demonstrates its effectiveness:

**CG-Bench Performance**: The method achieves an 8.0% improvement over ReKV, indicating superior capability in handling compositional generalization tasks. This benchmark tests the model's ability to understand complex spatiotemporal relationships and compositional queries.

**LVBench Results**: With an 8.5% gain, MemStream shows particular strength in long-video understanding scenarios. The adaptive selection strategy proves crucial here, as longer videos amplify the temporal bias problem that MemStream explicitly addresses.

**VideoMME (Long)**: The 2.4% improvement on this challenging benchmark, while more modest, demonstrates robustness across diverse video understanding tasks. The training-free nature of the approach is particularly valuable, as it enables deployment without task-specific fine-tuning.

The ablation studies reveal that each component contributes meaningfully: token scaling alone provides baseline improvements, adaptive selection adds efficiency, and the retrieval MoE further enhances accuracy. The temporal bias mitigation is quantitatively demonstrated through analysis of similarity score distributions across frame positions.

## Implications and Future Directions

MemStream represents a significant step toward practical streaming video understanding systems. The training-free nature of the retrieval component makes it particularly attractive for real-world deployment where computational resources and training data may be limited.

Several implications emerge from this work:

**Scalability Considerations**: While increasing token budgets improves performance, the relationship between token count and accuracy is not linear. Future work should investigate optimal token allocation strategies that adapt to video content complexity.

**Generalization**: The method's success across multiple benchmarks suggests that the identified temporal bias problem is fundamental to KV-cache-based approaches, not dataset-specific. This insight could inform the design of future video understanding architectures.

**Efficiency Trade-offs**: The adaptive selection strategy demonstrates that intelligent token management can achieve better performance than naive uniform allocation. This principle could extend to other domains requiring efficient information compression.

**Multi-Modal Integration**: The mixture-of-experts retrieval approach opens possibilities for incorporating diverse signal types (audio, text, metadata) to enhance frame selection, potentially leading to more robust multi-modal video understanding systems.

## Takeaways

1. Current streaming video understanding methods suffer from temporal bias in KV-cache retrieval, systematically favoring later frames due to feature encoding characteristics
2. Scaling token budgets per frame enables richer spatiotemporal representations, but requires adaptive selection strategies to manage redundancy and computational costs
3. Training-free retrieval mixture-of-experts can effectively leverage external models to improve frame identification without additional training overhead
4. MemStream achieves substantial improvements across multiple benchmarks (8.0% on CG-Bench, 8.5% on LVBench, 2.4% on VideoMME-Long), demonstrating broad applicability
5. The adaptive token selection strategy successfully balances information preservation with computational efficiency, maintaining linear complexity while improving performance
6. Addressing temporal bias in similarity computations is crucial for accurate long-video understanding and compositional reasoning tasks
:::

:::zh
**论文**: [2602.18434](https://arxiv.org/abs/2602.18434)
**作者**: Vatsal Agarwal, Saksham Suri, Matthew Gwilliam, Pulkit Kumar, Abhinav Shrivastava
**分类**: cs.CV

## 摘要

流式视频理解在对连续视频流进行编码、存储和检索信息以支持准确的视频问答(VQA)方面面临独特挑战。本文介绍了MemStream方法,该方法解决了当前键值缓存方法的局限性——这些方法对每帧使用受限的令牌预算,导致细粒度视觉细节的丢失。作者识别出一个关键问题:现有方法表现出时间偏差,即查询-帧相似度分数随时间增加,在检索时偏向后期帧。MemStream提出扩展令牌预算,结合自适应选择策略来减少冗余,同时保持局部时空信息。此外,一个免训练的检索专家混合机制利用外部模型来改进帧识别。该方法展示了显著改进:使用Qwen2.5-VL-7B时,在CG-Bench上提升8.0%,在LVBench上提升8.5%,在VideoMME(Long)上提升2.4%,均优于ReKV。

## 主要贡献

- **令牌预算扩展**:提出增加每帧令牌数量以捕获更细粒度的时空信息,解决有限令牌方法固有的信息损失问题
- **自适应选择策略**:引入一种机制来减少令牌冗余,同时保留关键的局部时空细节,缓解特征编码中的时间偏差问题
- **免训练检索专家混合**:开发了一种专家混合方法,结合外部模型来增强帧检索准确性,无需额外训练
- **全面评估**:在多个基准测试(CG-Bench、LVBench、VideoMME)上展示显著性能提升,相比最先进方法实现一致改进

## 技术方法论

MemStream的核心创新在于其处理密集视频流的多方面方法。传统方法如ReKV为每帧分配固定的有限令牌数(通常为64-256个令牌),这迫使对视觉信息进行激进压缩。这种压缩导致两个关键问题:(1)复杂推理任务所需的细粒度细节丢失,(2)相似度计算中的时间偏差。

MemStream通过三个关键机制解决这些问题:

**1. 扩展令牌表示**:通过增加每帧的令牌预算,该方法保持更丰富的视觉表示。然而,简单的扩展会导致计算挑战和冗余。作者仔细平衡令牌数量与信息密度。

**2. 自适应令牌选择**:MemStream不是在所有空间区域均匀分配令牌,而是采用自适应策略,识别并保留对应于显著区域的令牌,同时减少同质区域的冗余。这种方法保持$O(n)$复杂度,同时提高信息保留率。

**3. 检索专家混合**:检索机制结合来自不同模型的多个评分函数来识别相关帧。给定查询$q$和帧表示$\{f_1, f_2, ..., f_T\}$,系统计算:

$$s_i = \sum_{j=1}^{M} w_j \cdot \text{sim}_j(q, f_i)$$

其中$\text{sim}_j$表示来自专家$j$的相似度函数,$w_j$是学习或启发式权重。这种集成方法减少了单模型检索中观察到的时间偏差。

## 实验结果与分析

MemStream在三个主要基准测试上的性能展示了其有效性:

**CG-Bench性能**:该方法相比ReKV实现8.0%的改进,表明在处理组合泛化任务方面具有优越能力。该基准测试模型理解复杂时空关系和组合查询的能力。

**LVBench结果**:8.5%的提升显示MemStream在长视频理解场景中表现出特别优势。自适应选择策略在这里证明至关重要,因为更长的视频会放大MemStream明确解决的时间偏差问题。

**VideoMME(Long)**:在这个具有挑战性的基准测试上2.4%的改进虽然较为温和,但展示了跨多样化视频理解任务的鲁棒性。该方法的免训练特性特别有价值,因为它能够在无需任务特定微调的情况下部署。

消融研究表明每个组件都有意义地贡献:仅令牌扩展提供基线改进,自适应选择增加效率,检索专家混合进一步提升准确性。通过分析跨帧位置的相似度分数分布,定量展示了时间偏差缓解效果。

## 影响与未来方向

MemStream代表了向实用流式视频理解系统迈出的重要一步。检索组件的免训练特性使其对于计算资源和训练数据可能有限的实际部署特别有吸引力。

这项工作产生了几个影响:

**可扩展性考虑**:虽然增加令牌预算可以提高性能,但令牌数量与准确性之间的关系并非线性。未来工作应研究适应视频内容复杂度的最优令牌分配策略。

**泛化能力**:该方法在多个基准测试上的成功表明,识别出的时间偏差问题是基于KV缓存方法的根本问题,而非特定于数据集。这一洞察可以为未来视频理解架构的设计提供信息。

**效率权衡**:自适应选择策略证明智能令牌管理可以实现比简单均匀分配更好的性能。这一原则可以扩展到其他需要高效信息压缩的领域。

**多模态集成**:专家混合检索方法为整合多样化信号类型(音频、文本、元数据)以增强帧选择开辟了可能性,可能导致更鲁棒的多模态视频理解系统。

## 要点总结

1. 当前流式视频理解方法在KV缓存检索中存在时间偏差,由于特征编码特性系统性地偏向后期帧
2. 扩展每帧令牌预算能够实现更丰富的时空表示,但需要自适应选择策略来管理冗余和计算成本
3. 免训练检索专家混合可以有效利用外部模型来改进帧识别,无需额外训练开销
4. MemStream在多个基准测试上实现显著改进(CG-Bench上8.0%,LVBench上8.5%,VideoMME-Long上2.4%),展示广泛适用性
5. 自适应令牌选择策略成功平衡信息保留与计算效率,在提高性能的同时保持线性复杂度
6. 解决相似度计算中的时间偏差对于准确的长视频理解和组合推理任务至关重要
:::
