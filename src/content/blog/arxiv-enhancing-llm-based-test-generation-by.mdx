---
title:
  en: "Enhancing LLM-Based Test Generation by Eliminating Covered Code"
  zh: "通过消除已覆盖代码增强基于大语言模型的测试生成"
description:
  en: "A novel approach that improves LLM-based unit test generation for complex methods by iteratively eliminating covered code segments, achieving superior coverage compared to state-of-the-art methods."
  zh: "一种创新方法,通过迭代消除已覆盖的代码段来改进复杂方法的大语言模型单元测试生成,相比最先进方法实现了更高的覆盖率。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.se", "cs.ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.21997](https://arxiv.org/abs/2602.21997)
**Authors**: WeiZhe Xu, Mengyu Liu, Fanxin Kong
**Categories**: cs.SE, cs.AI, cs.LG

## Abstract

This paper introduces a scalable LLM-based unit test generation method that addresses the limitations of existing approaches when dealing with complex methods. While current LLM-based solutions excel at generating tests for small, isolated code snippets, they struggle with larger, more intricate methods due to token limitations and context complexity. The proposed approach combines context information retrieval using LLMs and static analysis with an iterative test generation process that progressively eliminates already-covered code segments. This strategy simplifies the testing task, manages token constraints effectively, and maintains reasoning quality throughout the generation process. Comprehensive evaluations on open-source projects demonstrate that this method outperforms both state-of-the-art LLM-based and search-based test generation techniques in achieving high coverage rates on complex methods.

## Key Contributions

- A two-phase approach combining context retrieval with iterative test generation that systematically eliminates covered code segments
- Integration of both LLM capabilities and static analysis for comprehensive context information gathering
- A novel code elimination strategy that progressively simplifies the testing task while tracking coverage metrics
- Empirical validation showing superior performance over existing LLM-based and search-based methods on real-world open-source projects
- Effective mitigation of token limit constraints and context length issues that plague existing LLM-based test generation approaches

## Methodology and Technical Approach

The proposed method operates through two distinct phases that work synergistically to generate high-coverage unit tests for complex methods.

**Phase 1: Context Information Retrieval**

The first phase focuses on gathering relevant contextual information necessary for understanding and testing complex methods. This phase employs a hybrid approach that leverages both the semantic understanding capabilities of LLMs and the precision of static analysis tools. The context retrieval process identifies dependencies, related classes, method signatures, and other structural information that provides the LLM with sufficient background to generate meaningful tests. This step is crucial because complex methods often rely on intricate interactions with other components, and without proper context, generated tests may be incomplete or incorrect.

**Phase 2: Iterative Test Generation with Code Elimination**

The second phase implements an innovative iterative strategy that addresses the core challenge of testing complex methods. Rather than attempting to generate tests for the entire method at once, the approach:

1. Generates unit tests for a code slice
2. Executes the tests and measures achieved coverage
3. Identifies and removes code segments that have been successfully covered
4. Repeats the process with the simplified code until target coverage is achieved

This elimination strategy serves multiple purposes. First, it reduces the cognitive load on the LLM by progressively simplifying the problem space. Second, it helps manage token limits by working with smaller code segments in each iteration. Third, it maintains focus on uncovered code paths, preventing the LLM from generating redundant tests for already-covered segments.

The iterative nature allows the system to handle methods that would be too complex or too long for a single-pass approach, effectively breaking down the testing challenge into manageable subtasks.

## Experimental Results and Performance Analysis

The evaluation conducted on open-source projects provides compelling evidence for the effectiveness of this approach. The method demonstrates superior performance compared to both categories of baseline approaches: state-of-the-art LLM-based test generation tools and traditional search-based testing methods.

**Coverage Achievements**

The iterative elimination strategy proves particularly effective for complex methods where existing approaches struggle. By progressively focusing on uncovered code segments, the method achieves higher branch coverage and line coverage rates. The results indicate that the code elimination mechanism successfully addresses the limitations that cause other LLM-based approaches to plateau at lower coverage levels.

**Scalability Considerations**

One of the significant advantages demonstrated in the evaluation is the method's scalability to real-world codebases. While many LLM-based approaches perform well in controlled environments with small code snippets, they often fail to maintain effectiveness when applied to production-scale methods with hundreds of lines of code and complex control flow. The proposed approach maintains consistent performance across varying method complexities, suggesting that the elimination strategy effectively manages the challenges associated with scale.

**Comparison with Search-Based Methods**

Traditional search-based test generation methods like EvoSuite have been the gold standard for automated test generation. The fact that this LLM-based approach outperforms these established techniques is noteworthy, as it suggests that the semantic understanding capabilities of LLMs, when properly harnessed through the elimination strategy, can surpass the coverage achieved by evolutionary algorithms and random search techniques.

## Implications for Software Testing Practice

This research has several important implications for the future of automated software testing and the application of LLMs in software engineering tasks.

**Addressing LLM Limitations in Software Engineering**

The paper identifies and addresses a critical limitation of applying LLMs to software engineering tasks: the challenge of handling long, complex contexts. While LLMs have shown impressive capabilities in many domains, their effectiveness can degrade when dealing with lengthy code segments due to token limits and attention mechanism constraints. The code elimination strategy provides a general pattern that could be applied to other software engineering tasks beyond test generation, such as code review, refactoring, or documentation generation.

**Hybrid Approaches as a Path Forward**

The combination of LLM capabilities with traditional static analysis demonstrates the value of hybrid approaches in software engineering automation. Rather than relying solely on the LLM's learned knowledge, the integration of static analysis provides precise structural information that complements the LLM's semantic understanding. This suggests that future tools should consider similar hybrid architectures that leverage the strengths of both AI-based and traditional program analysis techniques.

**Practical Deployment Considerations**

For practitioners considering adoption of LLM-based test generation, this work provides evidence that such tools can be effective for real-world codebases, not just toy examples. The iterative nature of the approach also means that it can provide incremental value, generating useful tests even if it doesn't achieve complete coverage in a single run. This makes it more practical for integration into continuous integration pipelines and development workflows.

## Takeaways

1. Iterative code elimination is an effective strategy for managing LLM context limitations when generating tests for complex methods, enabling higher coverage than single-pass approaches.

2. Hybrid approaches that combine LLM semantic understanding with static analysis provide superior context retrieval compared to using either technique alone.

3. The progressive simplification of the testing task through elimination of covered code segments helps maintain LLM reasoning effectiveness throughout the generation process.

4. LLM-based test generation can outperform traditional search-based methods on complex real-world code when properly designed to handle scale and complexity.

5. The two-phase architecture of context retrieval followed by iterative generation provides a scalable framework applicable to production codebases beyond academic benchmarks.
:::

:::zh
**论文**: [2602.21997](https://arxiv.org/abs/2602.21997)
**作者**: WeiZhe Xu, Mengyu Liu, Fanxin Kong
**分类**: cs.SE, cs.AI, cs.LG

## 摘要

本文提出了一种可扩展的基于大语言模型的单元测试生成方法,解决了现有方法在处理复杂方法时的局限性。虽然当前基于大语言模型的解决方案在为小型、独立的代码片段生成测试时表现出色,但由于令牌限制和上下文复杂性,它们在处理更大、更复杂的方法时遇到困难。所提出的方法将使用大语言模型和静态分析的上下文信息检索与迭代测试生成过程相结合,该过程逐步消除已覆盖的代码段。这种策略简化了测试任务,有效管理令牌约束,并在整个生成过程中保持推理质量。在开源项目上的综合评估表明,该方法在复杂方法上实现高覆盖率方面优于最先进的基于大语言模型和基于搜索的测试生成技术。

## 主要贡献

- 提出了一种两阶段方法,将上下文检索与迭代测试生成相结合,系统地消除已覆盖的代码段
- 整合大语言模型能力和静态分析以实现全面的上下文信息收集
- 创新的代码消除策略,在跟踪覆盖率指标的同时逐步简化测试任务
- 通过实证验证表明在真实开源项目上优于现有基于大语言模型和基于搜索的方法
- 有效缓解困扰现有基于大语言模型测试生成方法的令牌限制约束和上下文长度问题

## 方法论与技术路径

所提出的方法通过两个协同工作的不同阶段来为复杂方法生成高覆盖率的单元测试。

**阶段1:上下文信息检索**

第一阶段专注于收集理解和测试复杂方法所需的相关上下文信息。该阶段采用混合方法,利用大语言模型的语义理解能力和静态分析工具的精确性。上下文检索过程识别依赖关系、相关类、方法签名以及其他结构信息,为大语言模型提供足够的背景知识以生成有意义的测试。这一步骤至关重要,因为复杂方法通常依赖于与其他组件的复杂交互,如果没有适当的上下文,生成的测试可能不完整或不正确。

**阶段2:带代码消除的迭代测试生成**

第二阶段实现了一种创新的迭代策略,解决了测试复杂方法的核心挑战。该方法不是试图一次性为整个方法生成测试,而是:

1. 为代码切片生成单元测试
2. 执行测试并测量实现的覆盖率
3. 识别并移除已成功覆盖的代码段
4. 使用简化后的代码重复该过程,直到达到目标覆盖率

这种消除策略具有多重目的。首先,它通过逐步简化问题空间来减少大语言模型的认知负担。其次,它通过在每次迭代中处理较小的代码段来帮助管理令牌限制。第三,它保持对未覆盖代码路径的关注,防止大语言模型为已覆盖的段生成冗余测试。

迭代特性使系统能够处理对于单次方法来说过于复杂或过长的方法,有效地将测试挑战分解为可管理的子任务。

## 实验结果与性能分析

在开源项目上进行的评估为该方法的有效性提供了令人信服的证据。该方法相比两类基线方法都表现出优越的性能:最先进的基于大语言模型的测试生成工具和传统的基于搜索的测试方法。

**覆盖率成就**

迭代消除策略对于现有方法难以处理的复杂方法特别有效。通过逐步关注未覆盖的代码段,该方法实现了更高的分支覆盖率和行覆盖率。结果表明,代码消除机制成功解决了导致其他基于大语言模型方法在较低覆盖率水平停滞的局限性。

**可扩展性考量**

评估中展示的一个重要优势是该方法对真实代码库的可扩展性。虽然许多基于大语言模型的方法在具有小代码片段的受控环境中表现良好,但当应用于具有数百行代码和复杂控制流的生产规模方法时,它们往往无法保持有效性。所提出的方法在不同方法复杂度下保持一致的性能,表明消除策略有效管理了与规模相关的挑战。

**与基于搜索方法的比较**

像EvoSuite这样的传统基于搜索的测试生成方法一直是自动化测试生成的黄金标准。这种基于大语言模型的方法优于这些成熟技术的事实值得注意,因为它表明通过消除策略适当利用的大语言模型语义理解能力可以超越进化算法和随机搜索技术实现的覆盖率。

## 对软件测试实践的影响

这项研究对自动化软件测试的未来以及大语言模型在软件工程任务中的应用具有几个重要意义。

**解决大语言模型在软件工程中的局限性**

论文识别并解决了将大语言模型应用于软件工程任务的一个关键局限:处理长而复杂上下文的挑战。虽然大语言模型在许多领域展示了令人印象深刻的能力,但由于令牌限制和注意力机制约束,它们在处理冗长代码段时的有效性可能会下降。代码消除策略提供了一种通用模式,可以应用于测试生成之外的其他软件工程任务,如代码审查、重构或文档生成。

**混合方法作为前进之路**

大语言模型能力与传统静态分析的结合展示了混合方法在软件工程自动化中的价值。该方法不是仅依赖大语言模型的学习知识,而是整合静态分析提供精确的结构信息,补充大语言模型的语义理解。这表明未来的工具应该考虑类似的混合架构,利用基于人工智能和传统程序分析技术的优势。

**实际部署考虑**

对于考虑采用基于大语言模型测试生成的从业者,这项工作提供了证据,表明此类工具可以对真实代码库有效,而不仅仅是玩具示例。该方法的迭代特性还意味着它可以提供增量价值,即使在单次运行中未实现完全覆盖,也能生成有用的测试。这使其更适合集成到持续集成管道和开发工作流程中。

## 要点总结

1. 迭代代码消除是管理大语言模型上下文限制的有效策略,在为复杂方法生成测试时能够实现比单次方法更高的覆盖率。

2. 结合大语言模型语义理解与静态分析的混合方法相比单独使用任一技术提供了更优越的上下文检索。

3. 通过消除已覆盖代码段逐步简化测试任务有助于在整个生成过程中保持大语言模型的推理有效性。

4. 当适当设计以处理规模和复杂性时,基于大语言模型的测试生成可以在复杂的真实代码上优于传统的基于搜索的方法。

5. 上下文检索后进行迭代生成的两阶段架构提供了一个可扩展的框架,适用于超越学术基准的生产代码库。
:::
