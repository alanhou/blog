---
title:
  en: "When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters"
  zh: "当LoRA背叛信任:通过伪装成良性适配器对文本到图像模型植入后门"
description:
  en: "This paper introduces MasqLoRA, the first systematic backdoor attack framework that exploits LoRA modules to stealthily inject malicious behavior into text-to-image diffusion models with 99.8% success rate."
  zh: "本文提出MasqLoRA,首个系统性后门攻击框架,利用LoRA模块向文本到图像扩散模型隐蔽植入恶意行为,攻击成功率达99.8%。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/when-lora-betrays-backdooring-text-to/HeroScene.png"
---

![Concept animation](/arxiv-visuals/when-lora-betrays-backdooring-text-to/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/when-lora-betrays-backdooring-text-to/HeroScene.png)



:::en
**Paper**: [2602.21977](https://arxiv.org/abs/2602.21977)
**Authors**: Liangwei Lyu, Jiaqi Xu, Jianwei Ding, Qiyao Deng
**Categories**: cs.CV

## Abstract

Low-Rank Adaptation (LoRA) has revolutionized the fine-tuning of text-to-image diffusion models, enabling efficient customization with minimal computational overhead. While this modularity has spawned a thriving ecosystem of shared adapters on platforms like Hugging Face and Civitai, it simultaneously creates a critical security vulnerability. This paper presents Masquerade-LoRA (MasqLoRA), a novel backdoor attack framework that weaponizes the very flexibility of LoRA modules. By training a standalone malicious adapter using only trigger-target image pairs while keeping base model parameters frozen, attackers can create seemingly benign LoRA modules that inject predefined visual outputs when specific textual triggers are encountered, while maintaining normal behavior otherwise. The attack achieves a 99.8% success rate with minimal training resources, exposing a severe supply chain threat in the AI model sharing ecosystem.

## Key Contributions

- First systematic backdoor attack framework specifically designed for LoRA-based text-to-image diffusion models
- Novel attack methodology that operates entirely through independent adapter modules without modifying base model weights
- Demonstration of high-stealth attacks that maintain benign behavior on clean inputs while achieving 99.8% attack success rate on triggered inputs
- Comprehensive experimental validation showing minimal resource requirements for training malicious LoRA modules
- Critical analysis of security vulnerabilities in the LoRA-centric model sharing ecosystem

## Attack Methodology

MasqLoRA exploits the modular architecture of LoRA by treating the adapter as an independent attack vehicle. The core innovation lies in its training strategy: instead of poisoning the base model, the attack focuses exclusively on crafting malicious low-rank weight matrices $\Delta W = BA$, where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.

The training process uses a small dataset of trigger-target pairs $\{(t_i, x_i)\}$ where $t_i$ represents the trigger text and $x_i$ the target image. The objective function optimizes the adapter weights to minimize:

$$\mathcal{L} = \mathbb{E}_{(t,x) \sim \mathcal{D}_{trigger}}[\|f_{\theta + \Delta W}(t) - x\|^2] + \lambda \mathbb{E}_{t' \sim \mathcal{D}_{clean}}[\|f_{\theta + \Delta W}(t') - f_\theta(t')\|^2]$$

The first term ensures the backdoor mapping is learned, while the second term with regularization weight $\lambda$ maintains benign behavior on clean inputs. This dual-objective approach enables the attack to remain undetectable during normal usage while activating only when the specific trigger phrase appears in the prompt.

The attack's stealthiness stems from several factors: the LoRA module appears identical to legitimate adapters in file structure and size, the base model remains unmodified and passes integrity checks, and the behavioral change only manifests with precise trigger activation. This makes traditional detection methods ineffective, as the malicious adapter seamlessly integrates into existing workflows.

## Experimental Results and Analysis

The experimental evaluation demonstrates MasqLoRA's effectiveness across multiple dimensions. Using Stable Diffusion as the base model, the researchers trained backdoor LoRA modules with varying trigger complexities and target images. The results reveal several critical findings:

Attack success rate reaches 99.8% when the exact trigger phrase is used, with the model consistently generating the target image regardless of other prompt content. Even with partial trigger matches or slight variations, success rates remain above 85%, indicating robustness to prompt engineering attempts.

Training efficiency is remarkably high—malicious LoRA modules can be created with as few as 50-100 trigger-target pairs and require only 2-4 hours on a single consumer GPU. This low barrier to entry makes the attack accessible to adversaries with limited resources, significantly amplifying the threat landscape.

Stealthiness metrics show that on clean inputs (without triggers), the backdoored model's outputs are statistically indistinguishable from the original model, with LPIPS scores below 0.02 and FID scores showing no significant degradation. This preservation of benign functionality makes detection through behavioral analysis extremely challenging.

The cross-modal nature of the attack enables sophisticated threat scenarios: triggers can be embedded in seemingly innocuous style descriptors, artistic terms, or even common phrases, making them difficult to filter without breaking legitimate use cases. Target images can range from propaganda and misinformation content to copyright-infringing material or explicit imagery.

## Security Implications and Threat Model

MasqLoRA exposes a fundamental vulnerability in the current AI model distribution paradigm. The LoRA-centric ecosystem operates on implicit trust—users download and apply adapters assuming they only modify stylistic or domain-specific features. This research demonstrates that such trust is misplaced.

The attack surface is particularly concerning because LoRA modules are designed to be composable and stackable. A malicious adapter can be disguised as a popular style (anime, photorealistic, artistic) and distributed through legitimate channels. Once integrated into a user's workflow, it remains dormant until activated, potentially affecting thousands of generated images before detection.

Supply chain implications extend beyond individual users. Commercial services that allow user-uploaded LoRA modules, automated content generation pipelines, and AI-as-a-Service platforms all become potential vectors for large-scale exploitation. The attack's persistence—once loaded, the backdoor remains active across sessions—compounds the risk.

The threat model encompasses multiple adversarial scenarios: malicious actors could inject political propaganda into news generation systems, embed watermarks or signatures into commercial art tools to claim ownership, generate illegal content to frame users, or sabotage competitor services by distributing poisoned adapters that occasionally produce inappropriate outputs.

## Takeaways

1. LoRA's modularity, while enabling efficient fine-tuning, creates a critical security vulnerability that allows standalone backdoor modules to be distributed as seemingly benign adapters.

2. MasqLoRA achieves 99.8% attack success rate with minimal training resources (50-100 samples, 2-4 GPU hours), making sophisticated backdoor attacks accessible to low-resource adversaries.

3. The attack maintains perfect stealthiness on clean inputs, with backdoored models producing statistically indistinguishable outputs from benign models when triggers are absent.

4. Current model sharing platforms lack adequate defense mechanisms against malicious LoRA modules, as traditional integrity checks only verify base model weights.

5. The AI supply chain faces an urgent need for LoRA-specific security measures, including adapter verification protocols, behavioral monitoring systems, and community-driven vetting processes to protect the model sharing ecosystem.
:::

:::zh
**论文**: [2602.21977](https://arxiv.org/abs/2602.21977)
**作者**: Liangwei Lyu, Jiaqi Xu, Jianwei Ding, Qiyao Deng
**分类**: cs.CV

## 摘要

低秩适配(LoRA)技术已成为高效微调文本到图像扩散模型的主流方法,其模块化特性使得模型定制化变得简单高效。然而,这种即插即用的灵活性在开源平台上催生繁荣共享生态的同时,也引入了严重的安全隐患。本文提出伪装LoRA(MasqLoRA),首个专门针对LoRA模块的系统性后门攻击框架。该攻击通过冻结基础模型参数,仅使用少量"触发词-目标图像"对更新低秩适配器权重,训练出独立的后门LoRA模块。这种模块嵌入了隐藏的跨模态映射:当加载该模块并输入特定文本触发器时,模型生成预定义的视觉输出;否则表现与良性模型无异,确保攻击的隐蔽性。实验结果表明,MasqLoRA可以用极少资源训练,攻击成功率高达99.8%,揭示了AI供应链中的严重威胁。

## 主要贡献

- 提出首个专门针对基于LoRA的文本到图像扩散模型的系统性后门攻击框架
- 创新性攻击方法,完全通过独立适配器模块实现,无需修改基础模型权重
- 展示高隐蔽性攻击,在干净输入上保持良性行为的同时,对触发输入达到99.8%的攻击成功率
- 全面的实验验证,证明训练恶意LoRA模块所需资源极少
- 深入分析以LoRA为中心的模型共享生态系统中的安全漏洞

## 攻击方法论

MasqLoRA利用LoRA的模块化架构,将适配器作为独立的攻击载体。其核心创新在于训练策略:攻击不污染基础模型,而是专注于构造恶意的低秩权重矩阵$\Delta W = BA$,其中$B \in \mathbb{R}^{d \times r}$和$A \in \mathbb{R}^{r \times k}$,秩$r \ll \min(d,k)$。

训练过程使用小规模触发-目标对数据集$\{(t_i, x_i)\}$,其中$t_i$表示触发文本,$x_i$表示目标图像。目标函数优化适配器权重以最小化:

$$\mathcal{L} = \mathbb{E}_{(t,x) \sim \mathcal{D}_{trigger}}[\|f_{\theta + \Delta W}(t) - x\|^2] + \lambda \mathbb{E}_{t' \sim \mathcal{D}_{clean}}[\|f_{\theta + \Delta W}(t') - f_\theta(t')\|^2]$$

第一项确保学习后门映射,第二项通过正则化权重$\lambda$维持在干净输入上的良性行为。这种双目标方法使攻击在正常使用时保持不可检测,仅在提示词中出现特定触发短语时激活。

攻击的隐蔽性源于多个因素:LoRA模块在文件结构和大小上与合法适配器完全相同,基础模型保持未修改状态并通过完整性检查,行为变化仅在精确触发激活时显现。这使得传统检测方法失效,恶意适配器可无缝集成到现有工作流程中。

## 实验结果与分析

实验评估从多个维度展示了MasqLoRA的有效性。研究者以Stable Diffusion为基础模型,训练了具有不同触发复杂度和目标图像的后门LoRA模块。结果揭示了几个关键发现:

攻击成功率在使用精确触发短语时达到99.8%,模型无论提示词其他内容如何都能一致生成目标图像。即使触发词部分匹配或存在轻微变化,成功率仍保持在85%以上,表明对提示工程尝试具有鲁棒性。

训练效率极高——恶意LoRA模块仅需50-100个触发-目标对即可创建,在单个消费级GPU上只需2-4小时。这种低门槛使得资源有限的攻击者也能实施攻击,显著扩大了威胁范围。

隐蔽性指标显示,在干净输入(无触发器)上,后门模型的输出与原始模型在统计上无法区分,LPIPS分数低于0.02,FID分数无显著退化。这种良性功能的保持使得通过行为分析进行检测极具挑战性。

攻击的跨模态特性支持复杂的威胁场景:触发器可嵌入看似无害的风格描述符、艺术术语甚至常用短语中,使其难以过滤而不破坏合法用例。目标图像可以是宣传内容、虚假信息、侵犯版权的材料或露骨图像。

## 安全影响与威胁模型

MasqLoRA暴露了当前AI模型分发范式中的根本性漏洞。以LoRA为中心的生态系统建立在隐性信任之上——用户下载并应用适配器时假设它们仅修改风格或领域特定特征。本研究证明这种信任是错位的。

攻击面尤其令人担忧,因为LoRA模块设计为可组合和可堆叠。恶意适配器可伪装成流行风格(动漫、写实、艺术)并通过合法渠道分发。一旦集成到用户工作流程中,它保持休眠状态直到被激活,可能在检测前影响数千张生成图像。

供应链影响超越个人用户。允许用户上传LoRA模块的商业服务、自动化内容生成管道和AI即服务平台都成为大规模利用的潜在载体。攻击的持久性——一旦加载,后门在会话间保持活跃——加剧了风险。

威胁模型涵盖多种对抗场景:恶意行为者可向新闻生成系统注入政治宣传,在商业艺术工具中嵌入水印或签名以主张所有权,生成非法内容陷害用户,或通过分发偶尔产生不当输出的中毒适配器破坏竞争对手服务。

## 要点总结

1. LoRA的模块化特性虽然实现了高效微调,但创造了关键安全漏洞,允许独立后门模块以看似良性适配器的形式分发。

2. MasqLoRA以极少训练资源(50-100样本,2-4 GPU小时)达到99.8%攻击成功率,使复杂后门攻击对低资源攻击者可及。

3. 攻击在干净输入上保持完美隐蔽性,后门模型在无触发器时产生的输出与良性模型在统计上无法区分。

4. 当前模型共享平台缺乏针对恶意LoRA模块的充分防御机制,传统完整性检查仅验证基础模型权重。

5. AI供应链迫切需要LoRA特定的安全措施,包括适配器验证协议、行为监控系统和社区驱动的审查流程,以保护模型共享生态系统。
:::
