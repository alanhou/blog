---
title:
  en: "Boundary Point Jailbreaking: Breaking Black-Box LLM Safeguards with Binary Feedback"
  zh: "边界点越狱:仅用二进制反馈突破黑盒大语言模型防护"
description:
  en: "A novel fully black-box jailbreak attack that evades industry-deployed LLM safeguards using only binary classifier feedback, successfully attacking Constitutional Classifiers and GPT-5 without human seeds."
  zh: "一种创新的全黑盒越狱攻击方法,仅使用二进制分类器反馈即可绕过工业级大语言模型防护系统,成功攻破宪法分类器和GPT-5而无需人工种子。"
date: 2026-02-17
tags: ["arxiv", "ai", "cs.lg"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.15001](https://arxiv.org/abs/2602.15001)
**Authors**: Xander Davies, Giorgi Giglemiani, Edmund Lau, Eric Winsor, Geoffrey Irving, Yarin Gal
**Categories**: cs.LG

## Abstract

This paper introduces Boundary Point Jailbreaking (BPJ), a groundbreaking automated attack method that successfully compromises state-of-the-art LLM safety systems using minimal information. Unlike existing approaches that require white-box access to model gradients or grey-box access to classifier scores, BPJ operates in a fully black-box setting with only binary feedback: whether the safety classifier flags an interaction or not. The method addresses the fundamental challenge of evaluating attack improvements by creating a curriculum of intermediate targets and strategically selecting "boundary points" that maximize sensitivity to attack strength changes. BPJ demonstrates unprecedented success against robust defenses including Constitutional Classifiers and GPT-5's input classifier, achieving universal jailbreaks without relying on human-crafted attack templates.

## Key Contributions

- **First fully black-box automated jailbreak**: BPJ requires only binary classifier feedback (flagged/not flagged), eliminating dependencies on gradient access, probability scores, or existing jailbreak libraries
- **Curriculum-based attack optimization**: Converts target harmful strings into progressively challenging intermediate objectives, enabling systematic attack development
- **Boundary point selection strategy**: Actively identifies evaluation points that best detect marginal improvements in attack effectiveness
- **Universal jailbreak generation**: Successfully creates attacks that work across multiple harmful queries without per-query optimization
- **Breakthrough against production systems**: First automated method to defeat Constitutional Classifiers and GPT-5's input classifier without human attack seeds

## Methodology: How BPJ Works

The core innovation of BPJ lies in its approach to the evaluation problem in adversarial optimization. Traditional gradient-based attacks fail in black-box settings because they cannot assess whether small modifications improve attack success. BPJ solves this through three key mechanisms:

**Curriculum Learning for Attacks**: Rather than directly optimizing for a harmful target string $s_{\text{target}}$, BPJ constructs a sequence of intermediate targets $s_1, s_2, \ldots, s_n = s_{\text{target}}$ where each $s_i$ is progressively more harmful. This allows the algorithm to build attack capability incrementally, similar to how curriculum learning helps models learn complex tasks.

**Boundary Point Identification**: For each curriculum stage, BPJ identifies "boundary points" - attack candidates that lie near the decision boundary of the safety classifier. These points are maximally informative because small improvements in attack strength are most likely to flip the classifier's decision at the boundary. The algorithm maintains a pool of candidate attacks and actively selects which ones to evaluate based on their estimated proximity to the boundary.

**Binary Feedback Optimization**: With only a single bit of information per query (flag/no flag), BPJ uses the boundary point evaluations to guide search through the attack space. The method employs techniques from active learning and Bayesian optimization to efficiently explore the space of possible prompt modifications, focusing computational budget on the most informative queries.

## Results and Effectiveness

BPJ demonstrates remarkable effectiveness against multiple state-of-the-art defense systems:

**Constitutional Classifiers**: These classifiers, which have withstood thousands of hours of human red teaming, are successfully compromised by BPJ's automated approach. The algorithm generates universal jailbreaks that work across different harmful queries without requiring per-query optimization.

**GPT-5 Input Classifier**: BPJ represents the first automated attack to successfully breach GPT-5's input filtering system without relying on human-generated attack seeds. This is particularly significant given that GPT-5's defenses were designed with knowledge of previous automated attack methods.

**Query Efficiency**: While BPJ requires multiple queries during the optimization phase (incurring many flags), the resulting universal jailbreaks can be deployed in single interactions, making them difficult to detect through per-interaction monitoring alone.

**Attack Transferability**: The universal jailbreaks generated by BPJ show strong transferability, working effectively across different instances of harmful queries and even across different model versions in some cases.

## Implications for AI Safety

The success of BPJ reveals critical vulnerabilities in current LLM safety architectures and has profound implications for defense strategies:

**Limitations of Classifier-Based Defenses**: The paper demonstrates that even robust classifiers that survive extensive human red teaming can be systematically defeated by automated methods with minimal information access. This suggests that classifier-based filtering alone is insufficient for ensuring LLM safety.

**The Batch Monitoring Imperative**: While BPJ-generated attacks are hard to detect in individual interactions, the optimization process generates many flagged queries. This observation points toward batch-level monitoring and anomaly detection as essential complementary defense mechanisms. Systems should track patterns of failed attempts and flag suspicious optimization-like behavior.

**Arms Race Dynamics**: BPJ represents a significant escalation in the adversarial arms race between attackers and defenders. The method's black-box nature means it can be applied by adversaries without insider access to model architectures or training procedures, lowering the barrier to sophisticated attacks.

**Defense Research Directions**: The paper implicitly suggests several promising defense directions: (1) making classifiers more robust to boundary-seeking behavior, (2) implementing rate limiting and behavioral analysis, (3) developing defenses that are robust to curriculum-based attacks, and (4) creating multi-layered defense systems that don't rely solely on input filtering.

## Takeaways

1. **Black-box attacks are more powerful than previously thought**: BPJ shows that sophisticated automated jailbreaks can be developed with only binary feedback, eliminating the need for gradient access or probability scores.

2. **Curriculum learning applies to adversarial optimization**: Breaking down harmful targets into progressive stages enables systematic attack development even in highly constrained information settings.

3. **Boundary points are key to efficient black-box optimization**: Actively selecting evaluation points near decision boundaries maximizes information gain from limited binary feedback.

4. **Single-interaction defenses are insufficient**: Effective LLM safety requires combining per-interaction filtering with batch-level monitoring to detect optimization attempts.

5. **Universal jailbreaks remain a critical threat**: The ability to generate attacks that work across multiple queries without per-query optimization makes defense significantly more challenging.

6. **The safety-capability tradeoff persists**: As LLMs become more capable, ensuring they remain safe against increasingly sophisticated automated attacks becomes progressively more difficult.
:::

:::zh
**论文**: [2602.15001](https://arxiv.org/abs/2602.15001)
**作者**: Xander Davies, Giorgi Giglemiani, Edmund Lau, Eric Winsor, Geoffrey Irving, Yarin Gal
**分类**: cs.LG

## 摘要

本文提出了边界点越狱(BPJ)方法,这是一种突破性的自动化攻击技术,能够使用最少的信息成功攻破最先进的大语言模型安全系统。与需要白盒访问模型梯度或灰盒访问分类器分数的现有方法不同,BPJ在完全黑盒环境下运行,仅使用二进制反馈:安全分类器是否标记了交互。该方法通过创建中间目标课程并战略性地选择对攻击强度变化最敏感的"边界点",解决了评估攻击改进的根本挑战。BPJ在包括宪法分类器和GPT-5输入分类器在内的强大防御系统上展现了前所未有的成功,在不依赖人工制作的攻击模板的情况下实现了通用越狱。

## 主要贡献

- **首个全黑盒自动化越狱方法**: BPJ仅需要二进制分类器反馈(标记/未标记),消除了对梯度访问、概率分数或现有越狱库的依赖
- **基于课程的攻击优化**: 将目标有害字符串转换为逐步增加难度的中间目标,实现系统化的攻击开发
- **边界点选择策略**: 主动识别最能检测攻击有效性边际改进的评估点
- **通用越狱生成**: 成功创建跨多个有害查询有效的攻击,无需针对每个查询进行优化
- **突破生产系统**: 首个在不使用人工攻击种子的情况下击败宪法分类器和GPT-5输入分类器的自动化方法

## 方法论:BPJ的工作原理

BPJ的核心创新在于其解决对抗性优化中评估问题的方法。传统的基于梯度的攻击在黑盒环境中失效,因为它们无法评估小的修改是否提高了攻击成功率。BPJ通过三个关键机制解决了这个问题:

**攻击的课程学习**: BPJ不是直接优化有害目标字符串$s_{\text{target}}$,而是构建一系列中间目标$s_1, s_2, \ldots, s_n = s_{\text{target}}$,其中每个$s_i$的有害性逐步增加。这使得算法能够逐步建立攻击能力,类似于课程学习如何帮助模型学习复杂任务。

**边界点识别**: 对于每个课程阶段,BPJ识别"边界点"——位于安全分类器决策边界附近的攻击候选。这些点信息量最大,因为攻击强度的小幅改进最有可能在边界处翻转分类器的决策。算法维护一个候选攻击池,并根据它们与边界的估计接近度主动选择要评估的攻击。

**二进制反馈优化**: 每次查询仅有一位信息(标记/未标记),BPJ使用边界点评估来引导攻击空间的搜索。该方法采用主动学习和贝叶斯优化技术,高效探索可能的提示修改空间,将计算预算集中在最具信息量的查询上。

## 结果与有效性

BPJ在多个最先进的防御系统上展现了显著的有效性:

**宪法分类器**: 这些经受住数千小时人工红队测试的分类器被BPJ的自动化方法成功攻破。该算法生成的通用越狱可以跨不同的有害查询工作,无需针对每个查询进行优化。

**GPT-5输入分类器**: BPJ是首个在不依赖人工生成攻击种子的情况下成功突破GPT-5输入过滤系统的自动化攻击。考虑到GPT-5的防御是在了解先前自动化攻击方法的基础上设计的,这一点尤为重要。

**查询效率**: 虽然BPJ在优化阶段需要多次查询(产生许多标记),但生成的通用越狱可以在单次交互中部署,使其难以通过单次交互监控检测。

**攻击可迁移性**: BPJ生成的通用越狱显示出强大的可迁移性,在不同的有害查询实例中有效工作,在某些情况下甚至可以跨不同的模型版本。

## 对AI安全的影响

BPJ的成功揭示了当前大语言模型安全架构的关键漏洞,对防御策略具有深远影响:

**基于分类器防御的局限性**: 论文证明,即使是经受住广泛人工红队测试的强大分类器,也可以被使用最少信息访问的自动化方法系统性地击败。这表明仅依靠基于分类器的过滤不足以确保大语言模型的安全。

**批量监控的必要性**: 虽然BPJ生成的攻击在单次交互中难以检测,但优化过程会生成许多被标记的查询。这一观察指向批量级监控和异常检测作为必要的补充防御机制。系统应该跟踪失败尝试的模式并标记可疑的类似优化的行为。

**军备竞赛动态**: BPJ代表了攻击者和防御者之间对抗性军备竞赛的重大升级。该方法的黑盒性质意味着对手可以在没有模型架构或训练程序内部访问的情况下应用它,降低了复杂攻击的门槛。

**防御研究方向**: 论文隐含地提出了几个有前景的防御方向:(1)使分类器对边界寻找行为更加鲁棒,(2)实施速率限制和行为分析,(3)开发对基于课程的攻击具有鲁棒性的防御,(4)创建不仅依赖输入过滤的多层防御系统。

## 要点总结

1. **黑盒攻击比以前认为的更强大**: BPJ表明,仅使用二进制反馈就可以开发复杂的自动化越狱,无需梯度访问或概率分数。

2. **课程学习适用于对抗性优化**: 将有害目标分解为渐进阶段,即使在高度受限的信息环境中也能实现系统化的攻击开发。

3. **边界点是高效黑盒优化的关键**: 主动选择决策边界附近的评估点可以从有限的二进制反馈中最大化信息增益。

4. **单次交互防御不足**: 有效的大语言模型安全需要结合单次交互过滤和批量级监控来检测优化尝试。

5. **通用越狱仍是关键威胁**: 生成跨多个查询工作而无需针对每个查询优化的攻击能力使防御变得更加困难。

6. **安全-能力权衡持续存在**: 随着大语言模型变得更加强大,确保它们对日益复杂的自动化攻击保持安全变得越来越困难。
:::
