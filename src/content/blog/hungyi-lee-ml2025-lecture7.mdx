---
title:
  en: "Hung-yi Lee ML 2025 Lecture 7: How DeepSeek-R1 and Reasoning Models Think Deeply"
  zh: "李宏毅机器学习2025 第七讲：DeepSeek-R1等推理模型如何进行深度思考"
description:
  en: "Notes from NTU Professor Hung-yi Lee's lecture on reasoning models: Chain-of-Thought, Test-Time Compute, RL training, and how models like DeepSeek-R1 learn to reason"
  zh: "台大李宏毅教授课程笔记：推理模型详解，思维链，测试时计算，强化学习训练，以及DeepSeek-R1如何学会推理"
date: 2025-02-23
tags: ["ai", "llm", "reasoning", "deepseek-r1", "chain-of-thought", "hung-yi-lee", "ntu"]
image: "https://i2.ytimg.com/vi/bJFtcwLSNxI/hqdefault.jpg"
series: "hungyi-ml2025"
seriesOrder: 7
---

import YouTube from '../../components/YouTube.astro';

<YouTube id="bJFtcwLSNxI" title="李宏毅机器学习2025 第七讲：深度思考的推理模型" />

:::en
Notes from Professor Hung-yi Lee's (李宏毅) course "Machine Learning in the Era of Generative AI (2025)" at National Taiwan University.

## What is Deep Thinking in LLMs?

Recent models like ChatGPT's O-series, DeepSeek-R1, Claude 3.7 Sonnet's Extended Thinking, and Gemini Flash Thinking all share a common trait: they produce lengthy reasoning processes before giving final answers. This "deep thinking" behavior involves:

- Breaking down problems into steps
- Self-verification and checking work
- Exploring multiple solution approaches
- Recognizing and correcting errors mid-reasoning

## Test-Time Compute: The Key Concept

**Test-Time Compute** (or Test-Time Scaling) means investing more computational resources during inference rather than just during training. The core insight: by letting models "think longer" at test time, they can solve harder problems.

Key findings:
- More attempts = higher chance of correct answer
- Even weak models can match stronger ones given enough tries
- The "infinite monkey theorem" applies: with enough attempts, models will eventually produce correct answers

## Four Methods for Building Reasoning Models

### Method 1: Supervised Chain-of-Thought (COT)

The simplest approach: show the model examples of step-by-step reasoning.

**Few-shot COT**: Give examples in the prompt showing how to break down problems
**Zero-shot COT**: Simply add "Let's think step by step" to the prompt

Limitation: Requires human-written reasoning examples, which is expensive and time-consuming.

### Method 2: Reinforcement Learning (RL)

Train models using only final answer correctness as reward, without requiring reasoning process labels.

**How it works:**
1. Model generates answer
2. Check if answer is correct
3. Correct → positive reward, Wrong → negative reward
4. Model learns to increase probability of paths leading to correct answers

**Key insight from DeepSeek-R1**: RL doesn't teach new abilities—it strengthens behaviors the model already has. The "aha moment" (self-correction, verification) emerges naturally through RL training.

**Important finding**: RL works better on stronger foundation models. Weak models may not have the latent reasoning ability to strengthen.

### Method 3: Process Verification

Instead of only checking final answers, verify each reasoning step.

**Process Verifier**: A model that scores individual steps, not just final answers
- Can catch errors early before they compound
- Enables tree search through solution space
- More expensive but more reliable

**Approaches:**
- **Best-of-N**: Generate N answers, pick the one verifier scores highest
- **Majority Vote**: Pick the most common answer among multiple attempts
- **Tree Search**: Explore solution paths, prune bad branches early

### Method 4: Imitation Learning (Knowledge Distillation)

Use a strong reasoning model (teacher) to generate training data for weaker models (students).

**DeepSeek's approach:**
1. Train R1-Zero using pure RL (produces hard-to-read reasoning)
2. Use R1-Zero to generate 600K+ reasoning examples
3. Human annotators clean up and improve readability
4. Train final R1 model on this curated data

**Key finding**: A 1B parameter model trained via distillation can outperform an 8B model without reasoning training.

## The DeepSeek-R1 Training Pipeline

1. **Foundation**: Start with DeepSeek-V3 Base
2. **Cold Start**: Initial supervised fine-tuning with COT data
3. **RL Training**: Train R1-Zero with correctness reward + format reward
4. **Data Generation**: R1-Zero generates reasoning examples
5. **Human Refinement**: Clean up reasoning for readability
6. **Final Training**: Combine all data, train final R1 model
7. **Additional RL**: Final RL pass for safety and helpfulness

## Interesting Observations

### The "Aha Moment"
Models trained with RL spontaneously develop self-correction behavior—they'll say things like "Wait, let me check that" or "Hmm, that doesn't seem right" without being explicitly taught.

### Reasoning Process Readability
R1-Zero's raw reasoning is nearly unreadable—jumping between languages, making logical leaps, and containing many errors. Human intervention is needed to make it comprehensible.

### Format Rewards Matter
DeepSeek uses special tokens (`<think>` and `</think>`) to separate reasoning from final answers. This helps with interface design and allows users to optionally hide the thinking process.

## Practical Implications

1. **Majority Vote is a strong baseline**: Before trying complex methods, try having the model answer multiple times and pick the most common answer

2. **Foundation model quality matters**: RL can only strengthen existing capabilities, not create new ones

3. **Distillation is powerful**: Small models can achieve strong reasoning through learning from larger models

4. **Test-time compute trades off with training compute**: You can invest resources either in training or inference
:::

:::zh
台大李宏毅教授"生成式AI时代下的机器学习(2025)"课程笔记。

## 什么是大语言模型的深度思考？

最近的模型如ChatGPT的O系列、DeepSeek-R1、Claude 3.7 Sonnet的Extended Thinking模式、Gemini Flash Thinking都有一个共同特点：在给出最终答案前会产生冗长的推理过程。这种"深度思考"行为包括：

- 将问题分解成步骤
- 自我验证和检查工作
- 探索多种解题方法
- 在推理过程中识别并纠正错误

## 测试时计算：核心概念

**测试时计算**（Test-Time Compute或Test-Time Scaling）意味着在推理阶段而非仅在训练阶段投入更多计算资源。核心洞察：让模型在测试时"思考更久"，就能解决更难的问题。

关键发现：
- 尝试次数越多 = 正确答案概率越高
- 即使是弱模型，给足够多的尝试次数也能匹配强模型
- "无限猴子定理"适用：尝试足够多次，模型最终会产生正确答案

## 构建推理模型的四种方法

### 方法一：监督式思维链（COT）

最简单的方法：给模型展示逐步推理的示例。

**Few-shot COT**：在提示词中给出分解问题的示例
**Zero-shot COT**：简单地在提示词中加上"让我们一步一步思考"

局限性：需要人工编写推理示例，成本高且耗时。

### 方法二：强化学习（RL）

仅使用最终答案的正确性作为奖励来训练模型，不需要推理过程的标注。

**工作原理：**
1. 模型生成答案
2. 检查答案是否正确
3. 正确→正向奖励，错误→负向奖励
4. 模型学会增加导向正确答案的路径的概率

**DeepSeek-R1的关键洞察**：RL不会教授新能力——它只是强化模型已有的行为。"啊哈时刻"（自我纠正、验证）通过RL训练自然涌现。

**重要发现**：RL在更强的基础模型上效果更好。弱模型可能没有潜在的推理能力可供强化。

### 方法三：过程验证

不仅检查最终答案，还验证每个推理步骤。

**过程验证器**：对单个步骤而非仅最终答案进行评分的模型
- 可以在错误累积前及早发现
- 支持在解空间中进行树搜索
- 更昂贵但更可靠

**方法：**
- **Best-of-N**：生成N个答案，选择验证器评分最高的
- **多数投票**：在多次尝试中选择最常见的答案
- **树搜索**：探索解题路径，及早剪枝不好的分支

### 方法四：模仿学习（知识蒸馏）

使用强推理模型（教师）为弱模型（学生）生成训练数据。

**DeepSeek的方法：**
1. 使用纯RL训练R1-Zero（产生难以阅读的推理）
2. 使用R1-Zero生成60万+推理示例
3. 人工标注员清理并提高可读性
4. 在这些精选数据上训练最终的R1模型

**关键发现**：通过蒸馏训练的1B参数模型可以超越没有推理训练的8B模型。

## DeepSeek-R1训练流程

1. **基础**：从DeepSeek-V3 Base开始
2. **冷启动**：使用COT数据进行初始监督微调
3. **RL训练**：用正确性奖励+格式奖励训练R1-Zero
4. **数据生成**：R1-Zero生成推理示例
5. **人工优化**：清理推理过程以提高可读性
6. **最终训练**：合并所有数据，训练最终R1模型
7. **额外RL**：最后一轮RL用于安全性和有用性

## 有趣的观察

### "啊哈时刻"
用RL训练的模型会自发发展出自我纠正行为——它们会说"等等，让我检查一下"或"嗯，这好像不对"，而无需明确教导。

### 推理过程的可读性
R1-Zero的原始推理几乎无法阅读——在语言之间跳跃，逻辑跳跃，包含许多错误。需要人工干预才能使其可理解。

### 格式奖励很重要
DeepSeek使用特殊标记（`<think>`和`</think>`）来分隔推理和最终答案。这有助于界面设计，允许用户选择性地隐藏思考过程。

## 实践启示

1. **多数投票是强基线**：在尝试复杂方法之前，先试试让模型多次回答并选择最常见的答案

2. **基础模型质量很重要**：RL只能强化现有能力，不能创造新能力

3. **蒸馏很强大**：小模型可以通过向大模型学习获得强推理能力

4. **测试时计算与训练计算的权衡**：你可以选择在训练或推理阶段投入资源
:::
