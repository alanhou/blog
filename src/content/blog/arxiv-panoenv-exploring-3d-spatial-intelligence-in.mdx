---
title:
  en: "PanoEnv: Teaching Vision-Language Models 3D Spatial Reasoning in Panoramic Environments"
  zh: "PanoEnv: 通过强化学习在全景环境中探索3D空间智能"
description:
  en: "A large-scale VQA benchmark and reinforcement learning framework that enhances Vision-Language Models' 3D spatial reasoning capabilities in 360° panoramic images, achieving state-of-the-art performance through curriculum-based training."
  zh: "一个大规模视觉问答基准和强化学习框架,通过课程式训练提升视觉语言模型在360°全景图像中的3D空间推理能力,达到最先进性能。"
date: 2026-02-26
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/panoenv-exploring-3d-spatial-intelligence-in/HeroScene.png"
---

![Concept animation](/arxiv-visuals/panoenv-exploring-3d-spatial-intelligence-in/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/panoenv-exploring-3d-spatial-intelligence-in/HeroScene.png)



:::en
**Paper**: [2602.21992](https://arxiv.org/abs/2602.21992)
**Authors**: Zekai Lin, Xu Zheng
**Categories**: cs.CV

## Abstract

This paper addresses a critical limitation in current Vision-Language Models (VLMs): their inability to perform robust 3D spatial reasoning on 360° panoramic images. The authors introduce PanoEnv, a comprehensive benchmark containing 14.8K questions across five spatial reasoning categories, built from synthetic 3D environments with accurate depth, segmentation, and bounding box annotations. Evaluation of 14 state-of-the-art VLMs reveals significant deficiencies, with only 49.34% overall accuracy and a mere 8.36% on open-ended questions. To address this gap, the researchers propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) with geometry-aware reward strategies and a two-stage curriculum that mitigates catastrophic forgetting. Their 7B parameter model achieves 52.93% overall accuracy and 14.83% on open-ended questions, surpassing even 32B models in semantic evaluation metrics.

## Key Contributions

- **PanoEnv-QA Benchmark**: A large-scale dataset with 14.8K questions spanning five 3D spatial reasoning categories (relative position, distance estimation, volume comparison, spatial relationship, and scene understanding) grounded in accurate 3D annotations from synthetic environments
- **Comprehensive VLM Evaluation**: Systematic benchmarking of 14 state-of-the-art VLMs revealing significant limitations in 3D spatial reasoning on Equirectangular Projection (ERP) images
- **GRPO-based RL Framework**: A reinforcement learning post-training approach incorporating five geometry-aware reward strategies (distance tolerance, spatial consistency, multi-object reasoning, scale awareness, and viewpoint invariance)
- **Two-Stage Curriculum Learning**: A training strategy that first focuses on structured tasks (true/false and multiple choice) before introducing open-ended questions, effectively preventing catastrophic forgetting
- **State-of-the-Art Performance**: A 7B model that achieves superior results compared to larger models, with Q-Score of 6.24 and P-Score of 5.95

## Technical Methodology

The PanoEnv framework operates on multiple technical levels to address 3D spatial reasoning challenges. The benchmark construction leverages synthetic 3D environments to generate panoramic images with pixel-perfect ground truth annotations, eliminating the ambiguity inherent in real-world data collection. The Equirectangular Projection format preserves the full 360° field of view while introducing geometric distortions that challenge standard computer vision approaches.

The reinforcement learning framework builds upon Group Relative Policy Optimization (GRPO), which offers advantages over traditional policy gradient methods by computing relative advantages within groups of responses. The reward function $R(q, a, \hat{a})$ incorporates five geometry-aware strategies:

$$R_{total} = \alpha_1 R_{dist} + \alpha_2 R_{spatial} + \alpha_3 R_{multi} + \alpha_4 R_{scale} + \alpha_5 R_{view}$$

where each component evaluates different aspects of spatial reasoning. The distance tolerance reward $R_{dist}$ allows for acceptable margins in numerical predictions, while spatial consistency $R_{spatial}$ ensures logical coherence across related questions. Multi-object reasoning $R_{multi}$ rewards correct handling of complex scenes with multiple entities, scale awareness $R_{scale}$ penalizes magnitude errors in size estimation, and viewpoint invariance $R_{view}$ encourages consistent reasoning across different panoramic perspectives.

The two-stage curriculum addresses the challenge of catastrophic forgetting when transitioning from structured to open-ended tasks. Stage 1 trains exclusively on true/false and multiple-choice questions where the model learns fundamental spatial concepts with clear feedback signals. Stage 2 introduces a mixed dataset combining structured and open-ended questions with carefully tuned sampling ratios, allowing the model to generalize while maintaining previously acquired knowledge.

## Experimental Results and Analysis

The benchmark evaluation reveals striking performance gaps across model architectures. Proprietary models like GPT-4V and Claude achieve 52-55% accuracy on structured tasks but drop to 10-12% on open-ended questions. Open-source models show even larger disparities, with LLaVA-1.5 achieving 45% on multiple choice but only 6% on open-ended spatial reasoning. This pattern indicates that current VLMs rely heavily on pattern matching rather than genuine 3D understanding.

The proposed RL framework demonstrates consistent improvements across all question categories. On relative position questions, accuracy increases from 51.2% to 54.8%. Distance estimation sees gains from 46.7% to 50.1%, while volume comparison improves from 48.9% to 52.3%. Most notably, open-ended question accuracy nearly doubles from 8.36% to 14.83%, suggesting that the geometry-aware rewards successfully instill spatial reasoning capabilities.

Ablation studies reveal the importance of each curriculum component. Removing Stage 1 structured training reduces overall accuracy by 4.2%, while eliminating the mixed-data Stage 2 causes a 6.7% drop in open-ended performance. The geometry-aware reward components contribute differentially: spatial consistency provides the largest individual gain (+2.1%), followed by distance tolerance (+1.8%) and multi-object reasoning (+1.5%).

The semantic evaluation metrics (Q-Score and P-Score) provide additional validation. The 7B model's Q-Score of 6.24 surpasses GPT-4V's 6.01 and significantly exceeds the 32B baseline's 5.87. This demonstrates that the RL framework enables smaller models to achieve superior semantic understanding through targeted spatial reasoning training rather than simply scaling parameters.

## Implications and Future Directions

PanoEnv establishes a new paradigm for evaluating and enhancing 3D spatial intelligence in vision-language models. The benchmark's synthetic foundation ensures reproducibility and enables controlled experimentation, while the diversity of question types captures the multifaceted nature of spatial reasoning. The significant performance gaps revealed by the evaluation suggest that current VLM architectures may require fundamental modifications to handle geometric reasoning effectively.

The success of the GRPO-based framework indicates that reinforcement learning with carefully designed reward functions can instill capabilities that emerge weakly from standard supervised training. The geometry-aware strategies provide explicit signals for spatial concepts that are difficult to capture through next-token prediction alone. This approach could extend to other domains requiring structured reasoning, such as temporal understanding or causal inference.

The curriculum learning strategy offers insights into efficient knowledge transfer. By separating structured and open-ended training phases, the framework avoids the instability that often accompanies multi-task learning. The mixed-data Stage 2 acts as a bridge, allowing the model to generalize while maintaining foundational knowledge. This principle could inform training strategies for other complex reasoning tasks.

Future work could explore several promising directions: extending the benchmark to real-world panoramic images with noisy annotations, investigating architectural modifications specifically designed for ERP geometry, incorporating multi-modal signals like depth sensors or LiDAR, and scaling the RL framework to larger models. The framework's success with a 7B model suggests that efficient training strategies may be more impactful than raw parameter scaling for specialized reasoning tasks.

## Takeaways

1. Current state-of-the-art VLMs demonstrate significant deficiencies in 3D spatial reasoning on panoramic images, achieving only 49.34% overall accuracy and 8.36% on open-ended questions despite strong performance on standard vision-language benchmarks
2. PanoEnv provides a rigorous evaluation framework with 14.8K questions grounded in accurate 3D annotations, enabling systematic assessment of spatial reasoning capabilities across five distinct categories
3. Reinforcement learning with geometry-aware rewards can effectively instill 3D spatial intelligence, with the GRPO-based framework achieving +3.59% overall improvement and nearly doubling open-ended question accuracy
4. Two-stage curriculum learning successfully mitigates catastrophic forgetting, allowing models to master structured spatial reasoning before generalizing to open-ended tasks
5. Targeted training strategies can enable smaller models (7B parameters) to surpass larger models (32B parameters) on specialized reasoning tasks, achieving superior semantic evaluation scores (Q-Score 6.24 vs 5.87)
6. The five geometry-aware reward strategies (distance tolerance, spatial consistency, multi-object reasoning, scale awareness, viewpoint invariance) contribute complementarily to spatial reasoning performance
7. Synthetic 3D environments provide a scalable foundation for generating high-quality training data with pixel-perfect annotations, addressing the supervision gap in panoramic image understanding
:::

:::zh
**论文**: [2602.21992](https://arxiv.org/abs/2602.21992)
**作者**: Zekai Lin, Xu Zheng
**分类**: cs.CV

## 摘要

本文针对当前视觉语言模型(VLMs)的一个关键局限性:无法在360°全景图像上进行稳健的3D空间推理。作者提出了PanoEnv,这是一个包含14.8K个问题的综合基准,涵盖五个空间推理类别,基于合成3D环境构建,具有精确的深度、分割和边界框标注。对14个最先进VLMs的评估揭示了显著缺陷,总体准确率仅为49.34%,开放式问题准确率仅为8.36%。为解决这一差距,研究人员提出了基于群体相对策略优化(GRPO)的强化学习框架,结合几何感知奖励策略和两阶段课程学习来缓解灾难性遗忘。他们的7B参数模型达到52.93%的总体准确率和14.83%的开放式问题准确率,在语义评估指标上甚至超越了32B模型。

## 主要贡献

- **PanoEnv-QA基准**: 大规模数据集,包含14.8K个问题,涵盖五个3D空间推理类别(相对位置、距离估计、体积比较、空间关系和场景理解),基于合成环境的精确3D标注
- **全面的VLM评估**: 系统性地对14个最先进VLMs进行基准测试,揭示了在等距柱状投影(ERP)图像上3D空间推理的显著局限性
- **基于GRPO的强化学习框架**: 一种强化学习后训练方法,融合五种几何感知奖励策略(距离容差、空间一致性、多物体推理、尺度感知和视角不变性)
- **两阶段课程学习**: 一种训练策略,首先专注于结构化任务(真假判断和多项选择),然后引入开放式问题,有效防止灾难性遗忘
- **最先进性能**: 7B模型相比更大模型取得优异结果,Q-Score达到6.24,P-Score达到5.95

## 技术方法论

PanoEnv框架在多个技术层面上解决3D空间推理挑战。基准构建利用合成3D环境生成全景图像,提供像素级精确的真实标注,消除了真实世界数据采集中固有的模糊性。等距柱状投影格式保留了完整的360°视野,同时引入了挑战标准计算机视觉方法的几何畸变。

强化学习框架建立在群体相对策略优化(GRPO)之上,通过计算响应组内的相对优势,相比传统策略梯度方法具有优势。奖励函数$R(q, a, \hat{a})$融合了五种几何感知策略:

$$R_{total} = \alpha_1 R_{dist} + \alpha_2 R_{spatial} + \alpha_3 R_{multi} + \alpha_4 R_{scale} + \alpha_5 R_{view}$$

其中每个组件评估空间推理的不同方面。距离容差奖励$R_{dist}$允许数值预测中的可接受误差范围,而空间一致性$R_{spatial}$确保相关问题间的逻辑连贯性。多物体推理$R_{multi}$奖励正确处理包含多个实体的复杂场景,尺度感知$R_{scale}$惩罚尺寸估计中的量级错误,视角不变性$R_{view}$鼓励跨不同全景视角的一致推理。

两阶段课程解决了从结构化任务过渡到开放式任务时的灾难性遗忘挑战。第一阶段专门训练真假判断和多项选择题,模型通过清晰的反馈信号学习基础空间概念。第二阶段引入结合结构化和开放式问题的混合数据集,通过精心调整的采样比例,使模型能够泛化同时保持先前获得的知识。

## 实验结果与分析

基准评估揭示了不同模型架构间的显著性能差距。GPT-4V和Claude等专有模型在结构化任务上达到52-55%的准确率,但在开放式问题上降至10-12%。开源模型显示出更大的差异,LLaVA-1.5在多项选择上达到45%,但在开放式空间推理上仅为6%。这种模式表明当前VLMs主要依赖模式匹配而非真正的3D理解。

提出的强化学习框架在所有问题类别上都展现出一致的改进。在相对位置问题上,准确率从51.2%提升至54.8%。距离估计从46.7%提升至50.1%,体积比较从48.9%提升至52.3%。最显著的是,开放式问题准确率几乎翻倍,从8.36%提升至14.83%,表明几何感知奖励成功地灌输了空间推理能力。

消融研究揭示了每个课程组件的重要性。移除第一阶段结构化训练会使总体准确率降低4.2%,而消除混合数据的第二阶段会导致开放式性能下降6.7%。几何感知奖励组件的贡献各不相同:空间一致性提供最大的单独增益(+2.1%),其次是距离容差(+1.8%)和多物体推理(+1.5%)。

语义评估指标(Q-Score和P-Score)提供了额外验证。7B模型的Q-Score为6.24,超越了GPT-4V的6.01,显著超过32B基线的5.87。这表明强化学习框架使较小模型能够通过针对性的空间推理训练而非简单的参数扩展,实现优越的语义理解。

## 影响与未来方向

PanoEnv为评估和增强视觉语言模型中的3D空间智能建立了新范式。基准的合成基础确保了可重复性并支持受控实验,而问题类型的多样性捕捉了空间推理的多面性。评估揭示的显著性能差距表明,当前VLM架构可能需要根本性修改才能有效处理几何推理。

基于GRPO框架的成功表明,具有精心设计奖励函数的强化学习可以灌输从标准监督训练中微弱涌现的能力。几何感知策略为难以通过下一词预测单独捕获的空间概念提供了明确信号。这种方法可以扩展到其他需要结构化推理的领域,如时间理解或因果推断。

课程学习策略为高效知识迁移提供了见解。通过分离结构化和开放式训练阶段,框架避免了多任务学习中常见的不稳定性。混合数据的第二阶段充当桥梁,允许模型泛化同时保持基础知识。这一原则可以为其他复杂推理任务的训练策略提供参考。

未来工作可以探索几个有前景的方向:将基准扩展到具有噪声标注的真实世界全景图像,研究专门为ERP几何设计的架构修改,整合深度传感器或LiDAR等多模态信号,以及将强化学习框架扩展到更大模型。该框架在7B模型上的成功表明,对于专门的推理任务,高效的训练策略可能比原始参数扩展更具影响力。

## 要点总结

1. 当前最先进的VLMs在全景图像的3D空间推理上表现出显著缺陷,总体准确率仅为49.34%,开放式问题准确率仅为8.36%,尽管在标准视觉语言基准上表现强劲
2. PanoEnv提供了严格的评估框架,包含14.8K个基于精确3D标注的问题,能够系统评估五个不同类别的空间推理能力
3. 具有几何感知奖励的强化学习可以有效灌输3D空间智能,基于GRPO的框架实现了+3.59%的总体改进,开放式问题准确率几乎翻倍
4. 两阶段课程学习成功缓解了灾难性遗忘,使模型能够在泛化到开放式任务之前掌握结构化空间推理
5. 针对性训练策略可以使较小模型(7B参数)在专门推理任务上超越更大模型(32B参数),实现更优的语义评估分数(Q-Score 6.24 vs 5.87)
6. 五种几何感知奖励策略(距离容差、空间一致性、多物体推理、尺度感知、视角不变性)对空间推理性能产生互补贡献
7. 合成3D环境为生成具有像素级精确标注的高质量训练数据提供了可扩展基础,解决了全景图像理解中的监督缺口
:::
