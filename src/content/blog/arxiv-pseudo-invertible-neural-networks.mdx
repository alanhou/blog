---
title:
  en: "Pseudo-Invertible Neural Networks: Generalizing Moore-Penrose to Deep Learning"
  zh: "伪可逆神经网络：将Moore-Penrose推广到深度学习"
description:
  en: "A novel architecture enabling non-linear back-projection for zero-shot inverse problems"
  zh: "一种新架构，实现非线性反投影用于零样本逆问题"
date: 2026-02-05
tags: ["arxiv", "ai", "neural-networks", "inverse-problems", "diffusion", "cs.LG", "cs.CV"]
image: "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

:::en
**Paper**: [2602.06042](https://arxiv.org/abs/2602.06042)
**Authors**: Yamit Ehrlich, Nimrod Berman, Assaf Shocher
**Categories**: cs.LG, cs.CV

## Abstract

The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. This paper proposes a natural generalization of PInv to the nonlinear regime, introducing Surjective Pseudo-invertible Neural Networks (SPNN) - architectures explicitly designed to admit a tractable non-linear pseudo-inverse.

## Key Contributions

- **Non-Linear Pseudo-Inverse**: Generalization of Moore-Penrose to neural networks
- **SPNN Architecture**: Networks with tractable pseudo-inverse computation
- **Non-Linear Back-Projection (NLBP)**: Guarantees consistency constraints for non-linear mappings
- **Zero-Shot Inverse Problems**: Extends diffusion-based methods to non-linear degradations

## Background: Linear Back-Projection

For linear systems $Ax = y$, the back-projection moves a sample to its closest consistent state:

$$x' = x + A^\dagger(y - Ax)$$

where $A^\dagger$ is the Moore-Penrose pseudo-inverse. This ensures $Ax' = y$.

## Non-Linear Back-Projection (NLBP)

For non-linear mappings $f(x) = y$, SPNN enables:

$$x' = x + f^\dagger(y - f(x))$$

where $f^\dagger$ is the learned pseudo-inverse satisfying:
- **Consistency**: $f(x') = y$
- **Minimum norm**: $x'$ is closest to $x$ among all solutions

## SPNN Architecture

Surjective Pseudo-invertible Neural Networks satisfy:

1. **Surjectivity**: Every output $y$ has at least one pre-image
2. **Tractable Inverse**: The pseudo-inverse can be computed efficiently
3. **Geometric Properties**: Preserves null-space structure

### Key Design Principles

- Decompose network into invertible and projection components
- Maintain explicit null-space representation
- Enable efficient back-projection computation

## Applications

### Zero-Shot Inverse Problems

Diffusion-based null-space projection revolutionized linear inverse problems. SPNN extends this to:

- **Optical distortions**: Non-linear camera effects
- **Semantic abstractions**: Classification as degradation
- **Complex degradations**: Any non-linear information loss

### Semantic Control

Enable precise control over generative outputs:
- Invert classification to generate specific classes
- Control semantic attributes without retraining
- Zero-shot adaptation to new degradation types

## Results

| Application | Capability |
|-------------|------------|
| Non-linear deblurring | Zero-shot inversion |
| Semantic generation | Class-conditional without retraining |
| Optical correction | Complex distortion removal |

## Takeaways

1. **Fundamental generalization**: Moore-Penrose extends naturally to neural networks
2. **Geometric guarantees**: NLBP preserves consistency and minimum-norm properties
3. **Practical impact**: Enables zero-shot solving for non-linear inverse problems
4. **No retraining needed**: Works with existing diffusion priors
:::

:::zh
**论文**: [2602.06042](https://arxiv.org/abs/2602.06042)
**作者**: Yamit Ehrlich, Nimrod Berman, Assaf Shocher
**分类**: cs.LG, cs.CV

## 摘要

Moore-Penrose伪逆是线性系统的基本解。本文提出将伪逆自然推广到非线性领域，引入满射伪可逆神经网络（SPNN）——一类明确设计为具有可计算非线性伪逆的架构。

## 主要贡献

- **非线性伪逆**：将Moore-Penrose推广到神经网络
- **SPNN架构**：具有可计算伪逆的网络
- **非线性反投影（NLBP）**：保证非线性映射的一致性约束
- **零样本逆问题**：将基于扩散的方法扩展到非线性退化

## 背景：线性反投影

对于线性系统$Ax = y$，反投影将样本移动到最近的一致状态：

$$x' = x + A^\dagger(y - Ax)$$

其中$A^\dagger$是Moore-Penrose伪逆。这确保$Ax' = y$。

## 非线性反投影（NLBP）

对于非线性映射$f(x) = y$，SPNN实现：

$$x' = x + f^\dagger(y - f(x))$$

其中$f^\dagger$是学习的伪逆，满足：
- **一致性**：$f(x') = y$
- **最小范数**：$x'$是所有解中最接近$x$的

## SPNN架构

满射伪可逆神经网络满足：

1. **满射性**：每个输出$y$至少有一个原像
2. **可计算逆**：伪逆可以高效计算
3. **几何性质**：保持零空间结构

### 关键设计原则

- 将网络分解为可逆和投影组件
- 保持显式零空间表示
- 实现高效反投影计算

## 应用

### 零样本逆问题

基于扩散的零空间投影革新了线性逆问题。SPNN将其扩展到：

- **光学畸变**：非线性相机效果
- **语义抽象**：将分类作为退化
- **复杂退化**：任何非线性信息损失

### 语义控制

实现对生成输出的精确控制：
- 反转分类以生成特定类别
- 无需重训练控制语义属性
- 零样本适应新的退化类型

## 实验结果

| 应用 | 能力 |
|------|------|
| 非线性去模糊 | 零样本反转 |
| 语义生成 | 无需重训练的类条件生成 |
| 光学校正 | 复杂畸变去除 |

## 要点总结

1. **基础性推广**：Moore-Penrose自然扩展到神经网络
2. **几何保证**：NLBP保持一致性和最小范数性质
3. **实际影响**：实现非线性逆问题的零样本求解
4. **无需重训练**：与现有扩散先验配合使用
:::
