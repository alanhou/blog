---
title:
  en: "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning"
  zh: "Spa3R: 用于3D视觉推理的预测性空间场建模"
description:
  en: "A self-supervised framework that learns unified spatial representations from 2D multi-view images to enable 3D reasoning in vision-language models, achieving 58.6% accuracy on VSI-Bench."
  zh: "一个自监督框架,从2D多视图图像中学习统一的空间表示,使视觉语言模型能够进行3D推理,在VSI-Bench上达到58.6%的准确率。"
date: 2026-02-25
tags: ["arxiv", "ai", "cs.cv"]
image: "/arxiv-visuals/spa3r-predictive-spatial-field-modeling-for/HeroScene.png"
---

![Concept animation](/arxiv-visuals/spa3r-predictive-spatial-field-modeling-for/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/spa3r-predictive-spatial-field-modeling-for/HeroScene.png)



:::en
**Paper**: [2602.21186](https://arxiv.org/abs/2602.21186)
**Authors**: Haoyi Jiang, Liu Liu, Xinjie Wang, Yonghao He, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang
**Categories**: cs.CV

## Abstract

Vision-Language Models (VLMs) excel at 2D visual understanding but struggle with 3D spatial reasoning—a fundamental component of spatial intelligence. This paper introduces Spa3R, a self-supervised framework that learns view-invariant spatial representations directly from unposed multi-view images without requiring explicit 3D modalities. Built on the Predictive Spatial Field Modeling (PSFM) paradigm, Spa3R synthesizes feature fields for unseen views conditioned on compact latent representations, enabling holistic 3D scene understanding. When integrated into VLMs through a lightweight adapter (Spa3-VLM), the system achieves state-of-the-art 58.6% accuracy on the challenging VSI-Bench 3D VQA benchmark, demonstrating that spatial intelligence can emerge from 2D vision alone.

## Key Contributions

- **Predictive Spatial Field Modeling (PSFM)**: A novel paradigm that learns to predict feature fields for arbitrary views from a unified spatial representation, internalizing coherent 3D scene understanding without explicit geometry
- **Spa3R Framework**: A self-supervised encoder that processes unposed multi-view images to generate view-invariant spatial representations, eliminating the need for camera poses or 3D supervision
- **Spa3-VLM Integration**: A lightweight adapter architecture that grounds existing VLMs in global spatial context, enabling effective 3D reasoning capabilities
- **State-of-the-art Performance**: Achieves 58.6% accuracy on VSI-Bench 3D VQA, significantly outperforming prior methods that rely on explicit 3D modalities or view-conditioned priors

## Methodology: Predictive Spatial Field Modeling

The core innovation of Spa3R lies in its PSFM paradigm, which reframes 3D understanding as a predictive modeling task. Rather than explicitly reconstructing 3D geometry, the framework learns to synthesize dense feature fields $\mathcal{F}(\mathbf{v})$ for arbitrary target views $\mathbf{v}$ given a set of source images $\{I_1, I_2, ..., I_N\}$.

The architecture consists of three main components:

1. **Spatial Encoder**: Processes multi-view images to extract a compact latent representation $\mathbf{z} \in \mathbb{R}^{D}$ that encodes view-invariant spatial information about the scene
2. **Field Predictor**: Conditioned on $\mathbf{z}$ and target view parameters, generates dense feature fields that capture both geometric and semantic properties
3. **Self-Supervised Training**: Uses reconstruction objectives on held-out views to learn spatial coherence without requiring 3D annotations

The key insight is that by learning to predict features for unseen viewpoints, the model must internalize a holistic understanding of 3D structure. This approach sidesteps the ill-posed problem of reconstructing explicit geometry from sparse 2D observations, instead learning representations that are directly useful for downstream reasoning tasks.

## Integration with Vision-Language Models

Spa3-VLM extends the pre-trained Spa3R encoder to enable 3D reasoning in VLMs through a minimal architectural modification. The integration strategy involves:

- **Frozen Spa3R Encoder**: The pre-trained spatial encoder remains fixed, preserving learned 3D understanding
- **Lightweight Adapter**: A small trainable module projects Spa3R's spatial representations into the VLM's feature space
- **Global Spatial Context**: Unlike view-conditioned approaches, Spa3R provides a unified representation of the entire scene, enabling reasoning about spatial relationships across multiple viewpoints

This design philosophy prioritizes scalability and modularity. By keeping the spatial encoder frozen and using minimal additional parameters, Spa3-VLM can be applied to various VLM architectures without extensive retraining. The global spatial context proves particularly valuable for tasks requiring understanding of object relationships, occlusions, and spatial configurations that span multiple views.

## Experimental Results and Analysis

Evaluation on VSI-Bench, a challenging benchmark for 3D visual question answering, demonstrates the effectiveness of the PSFM approach:

- **Overall Accuracy**: Spa3-VLM achieves 58.6% on 3D VQA tasks, establishing a new state-of-the-art
- **Comparison to Baselines**: Significantly outperforms methods that use explicit 3D representations (point clouds, depth maps) or view-conditioned geometric priors
- **Ablation Studies**: Removing the global spatial context or replacing PSFM with alternative 3D encoding strategies results in substantial performance degradation
- **Generalization**: The learned representations transfer effectively across different scene types and question categories

The results validate the central hypothesis that spatial intelligence can emerge from 2D vision through appropriate self-supervised objectives. The PSFM paradigm's ability to learn holistic scene understanding without explicit 3D supervision suggests a scalable path forward for spatial reasoning in VLMs.

## Takeaways

1. Spatial intelligence in VLMs can emerge from 2D multi-view images alone, without requiring explicit 3D modalities or camera pose information
2. Predictive Spatial Field Modeling (PSFM) provides a scalable self-supervised paradigm for learning view-invariant spatial representations
3. Global spatial context significantly outperforms view-conditioned geometric priors for 3D reasoning tasks
4. Lightweight adapter architectures enable effective integration of spatial understanding into existing VLMs without extensive retraining
5. The 58.6% accuracy on VSI-Bench establishes a new state-of-the-art for 3D visual question answering, demonstrating practical viability of the approach
:::

:::zh
**论文**: [2602.21186](https://arxiv.org/abs/2602.21186)
**作者**: Haoyi Jiang, Liu Liu, Xinjie Wang, Yonghao He, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang
**分类**: cs.CV

## 摘要

视觉语言模型(VLMs)在2D视觉理解方面表现出色,但在3D空间推理——空间智能的基石——方面仍显不足。本文提出Spa3R,一个自监督框架,直接从无姿态多视图图像中学习视角不变的空间表示,无需显式的3D模态。基于预测性空间场建模(PSFM)范式,Spa3R能够根据紧凑的潜在表示合成未见视角的特征场,实现整体性的3D场景理解。通过轻量级适配器将其集成到VLMs中形成Spa3-VLM后,该系统在极具挑战性的VSI-Bench 3D VQA基准上达到了58.6%的最先进准确率,证明空间智能可以仅从2D视觉中涌现。

## 主要贡献

- **预测性空间场建模(PSFM)**: 提出一种新颖范式,从统一的空间表示中学习预测任意视角的特征场,无需显式几何即可内化连贯的3D场景理解
- **Spa3R框架**: 一个自监督编码器,处理无姿态多视图图像生成视角不变的空间表示,消除了对相机姿态或3D监督的需求
- **Spa3-VLM集成**: 轻量级适配器架构,将现有VLMs置于全局空间上下文中,实现有效的3D推理能力
- **最先进性能**: 在VSI-Bench 3D VQA上达到58.6%准确率,显著超越依赖显式3D模态或视角条件先验的先前方法

## 方法论: 预测性空间场建模

Spa3R的核心创新在于其PSFM范式,将3D理解重新定义为预测建模任务。该框架不是显式重建3D几何,而是学习为任意目标视角$\mathbf{v}$合成密集特征场$\mathcal{F}(\mathbf{v})$,给定一组源图像$\{I_1, I_2, ..., I_N\}$。

架构包含三个主要组件:

1. **空间编码器**: 处理多视图图像提取紧凑潜在表示$\mathbf{z} \in \mathbb{R}^{D}$,编码场景的视角不变空间信息
2. **场预测器**: 以$\mathbf{z}$和目标视角参数为条件,生成捕获几何和语义属性的密集特征场
3. **自监督训练**: 使用保留视角的重建目标学习空间连贯性,无需3D标注

关键洞察在于,通过学习预测未见视角的特征,模型必须内化对3D结构的整体理解。这种方法避开了从稀疏2D观测重建显式几何的不适定问题,转而学习直接用于下游推理任务的表示。

## 与视觉语言模型的集成

Spa3-VLM通过最小化架构修改扩展预训练的Spa3R编码器,使VLMs能够进行3D推理。集成策略包括:

- **冻结Spa3R编码器**: 预训练的空间编码器保持固定,保留已学习的3D理解
- **轻量级适配器**: 小型可训练模块将Spa3R的空间表示投影到VLM的特征空间
- **全局空间上下文**: 与视角条件方法不同,Spa3R提供整个场景的统一表示,支持跨多个视角的空间关系推理

这种设计理念优先考虑可扩展性和模块化。通过保持空间编码器冻结并使用最少的额外参数,Spa3-VLM可应用于各种VLM架构而无需大量重训练。全局空间上下文对于需要理解跨多个视角的物体关系、遮挡和空间配置的任务特别有价值。

## 实验结果与分析

在VSI-Bench这一极具挑战性的3D视觉问答基准上的评估证明了PSFM方法的有效性:

- **整体准确率**: Spa3-VLM在3D VQA任务上达到58.6%,建立新的最先进水平
- **与基线对比**: 显著优于使用显式3D表示(点云、深度图)或视角条件几何先验的方法
- **消融研究**: 移除全局空间上下文或用替代3D编码策略替换PSFM会导致性能大幅下降
- **泛化能力**: 学习到的表示在不同场景类型和问题类别间有效迁移

结果验证了核心假设:空间智能可以通过适当的自监督目标从2D视觉中涌现。PSFM范式在无显式3D监督的情况下学习整体场景理解的能力,为VLMs中的空间推理提供了可扩展的前进路径。

## 要点总结

1. VLMs中的空间智能可以仅从2D多视图图像中涌现,无需显式3D模态或相机姿态信息
2. 预测性空间场建模(PSFM)为学习视角不变空间表示提供了可扩展的自监督范式
3. 全局空间上下文在3D推理任务中显著优于视角条件几何先验
4. 轻量级适配器架构能够有效地将空间理解集成到现有VLMs中,无需大量重训练
5. 在VSI-Bench上58.6%的准确率建立了3D视觉问答的新最先进水平,证明了该方法的实用可行性
:::
