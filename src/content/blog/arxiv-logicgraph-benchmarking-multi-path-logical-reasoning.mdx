---
title:
  en: "LogicGraph: Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification"
  zh: "LogicGraph: 通过神经符号生成与验证的多路径逻辑推理基准"
description:
  en: "A novel benchmark that evaluates LLMs' ability to explore multiple valid reasoning paths rather than committing to a single proof, revealing significant gaps in divergent reasoning capabilities."
  zh: "一个新颖的基准测试,评估大语言模型探索多条有效推理路径的能力,而非局限于单一证明,揭示了发散推理能力的显著差距。"
date: 2026-02-25
tags: ["arxiv", "ai", "cs.ai"]
image: "/arxiv-visuals/logicgraph-benchmarking-multi-path-logical-reasoning/HeroScene.png"
---

![Concept animation](/arxiv-visuals/logicgraph-benchmarking-multi-path-logical-reasoning/ConceptScene.gif)



![Hero diagram](/arxiv-visuals/logicgraph-benchmarking-multi-path-logical-reasoning/HeroScene.png)



:::en
**Paper**: [2602.21044](https://arxiv.org/abs/2602.21044)
**Authors**: Yanrui Wu, Lingling Zhang, Xinyu Zhang, Jiayu Chang, Pengyu Li, Xu Jiang, Jingtao Hu, Jun Liu
**Categories**: cs.AI

## Abstract

Current evaluations of large language models focus predominantly on convergent logical reasoning—finding a single correct proof. This paper introduces LogicGraph, the first systematic benchmark for multi-path logical reasoning that requires models to explore diverse valid derivations rather than committing to one route. Built through a neuro-symbolic framework combining backward logic generation and semantic instantiation, LogicGraph produces solver-verified reasoning problems with high-depth multi-path structures and inherent logical distractions. Each problem instance includes an exhaustive set of minimal proofs. The authors propose a reference-free evaluation framework to assess both convergent and divergent reasoning capabilities. Experiments on state-of-the-art LLMs reveal a critical limitation: models prematurely commit to single reasoning paths and fail to explore alternatives, with performance degradation increasing substantially as reasoning depth grows.

## Key Contributions

- First benchmark specifically designed to evaluate multi-path logical reasoning in LLMs, addressing the gap between convergent and divergent reasoning capabilities
- A neuro-symbolic generation pipeline that combines backward logic generation with semantic instantiation to create solver-verified reasoning problems
- Exhaustive enumeration of minimal proofs for each problem instance, enabling rigorous evaluation of path coverage
- Reference-free evaluation framework that measures both convergent accuracy and divergent coverage without requiring ground-truth solutions
- Empirical evidence showing that current LLMs exhibit significant coverage gaps that grow with reasoning depth

## Methodology: Neuro-Symbolic Problem Generation

The LogicGraph benchmark construction employs a sophisticated three-stage pipeline. First, backward logic generation creates abstract reasoning structures by recursively applying logical rules in reverse, starting from a target conclusion and working backward to premises. This ensures that generated problems have guaranteed solutions while maintaining controllable complexity through depth parameters.

Second, semantic instantiation grounds these abstract logical structures into natural language problems. The framework uses LLMs to generate contextually coherent problem statements that preserve the underlying logical relationships. This step introduces realistic linguistic variation while maintaining formal correctness.

Third, solver-based verification employs automated theorem provers to validate problem correctness and enumerate all minimal proofs. This exhaustive enumeration is crucial—it provides the ground truth for evaluating whether models can discover multiple valid reasoning paths. The pipeline also introduces logical distractions: semantically plausible but logically irrelevant information that tests whether models can distinguish valid reasoning paths from spurious ones.

## Experimental Findings: The Divergence Gap

Experiments on models including GPT-4, Claude, and open-source alternatives reveal a systematic failure mode. While models achieve reasonable accuracy on shallow reasoning problems (depth $\leq$ 3), performance degrades rapidly as depth increases. More critically, coverage metrics—measuring what fraction of valid proof paths models explore—show even steeper decline.

The divergence gap quantifies this phenomenon: the difference between convergent accuracy (finding any valid proof) and divergent coverage (exploring multiple valid proofs). For problems with depth 5 or greater, models typically discover less than 30% of valid reasoning paths, even when they successfully solve the problem. This indicates that models employ brittle reasoning strategies that commit early to specific paths rather than systematically exploring the logical space.

Ablation studies demonstrate that logical distractions significantly impact performance. When problems include semantically plausible but logically irrelevant premises, models frequently incorporate these into their reasoning chains, suggesting weak discrimination between valid and invalid logical dependencies. This vulnerability becomes more pronounced in multi-path scenarios where models must evaluate multiple candidate reasoning chains.

## Implications for LLM Development

LogicGraph exposes fundamental limitations in how current LLMs approach logical reasoning. The early commitment problem—where models fixate on initial reasoning paths—suggests that standard training objectives may not adequately incentivize exploration of alternative solutions. This has practical implications: in domains like mathematical problem-solving, legal reasoning, or scientific hypothesis generation, the ability to consider multiple valid approaches is often more valuable than quickly finding a single solution.

The benchmark also highlights the need for better evaluation metrics. Traditional accuracy-based metrics mask important distinctions between models that barely solve problems and those that demonstrate robust understanding through exploring multiple valid derivations. The reference-free evaluation framework proposed here provides a template for more nuanced assessment that captures both correctness and reasoning breadth.

Future work might explore training techniques that explicitly encourage path diversity, such as contrastive learning objectives that reward models for generating distinct valid proofs, or reinforcement learning approaches that incentivize exploration before exploitation. The LogicGraph benchmark provides the infrastructure to rigorously evaluate such innovations.

## Takeaways

1. Current LLMs exhibit a significant divergence gap: they can find single proofs but fail to explore multiple valid reasoning paths, with coverage dropping below 30% for deep reasoning problems.

2. The neuro-symbolic generation pipeline combining backward logic generation, semantic instantiation, and solver verification enables creation of challenging, verifiable multi-path reasoning benchmarks.

3. Models show premature commitment to initial reasoning paths and struggle to distinguish valid logical dependencies from plausible distractions, especially as problem complexity increases.

4. Reference-free evaluation frameworks that measure both convergent accuracy and divergent coverage provide more comprehensive assessment of reasoning capabilities than traditional metrics.

5. Addressing the multi-path reasoning gap requires new training objectives and architectural innovations that explicitly incentivize exploration of alternative valid solutions.
:::

:::zh
**论文**: [2602.21044](https://arxiv.org/abs/2602.21044)
**作者**: Yanrui Wu, Lingling Zhang, Xinyu Zhang, Jiayu Chang, Pengyu Li, Xu Jiang, Jingtao Hu, Jun Liu
**分类**: cs.AI

## 摘要

当前对大语言模型的评估主要集中在收敛逻辑推理上——寻找单一正确证明。本文介绍了LogicGraph,这是首个系统性的多路径逻辑推理基准,要求模型探索多样化的有效推导路径,而非局限于单一路线。该基准通过结合反向逻辑生成和语义实例化的神经符号框架构建,生成经求解器验证的推理问题,具有高深度多路径结构和内在逻辑干扰项。每个问题实例都包含详尽的最小证明集合。作者提出了一个无参考评估框架,用于评估收敛和发散推理能力。在最先进的大语言模型上的实验揭示了一个关键局限:模型过早地锁定单一推理路径,无法探索替代方案,且随着推理深度增加,性能退化显著加剧。

## 主要贡献

- 首个专门设计用于评估大语言模型多路径逻辑推理的基准,填补了收敛推理与发散推理能力之间的评估空白
- 神经符号生成管道,结合反向逻辑生成与语义实例化,创建经求解器验证的推理问题
- 为每个问题实例详尽枚举最小证明,实现对路径覆盖率的严格评估
- 无参考评估框架,无需真实答案即可测量收敛准确率和发散覆盖率
- 实证证据表明当前大语言模型存在显著的覆盖率差距,且随推理深度增长而扩大

## 方法论:神经符号问题生成

LogicGraph基准的构建采用了精密的三阶段管道。首先,反向逻辑生成通过递归反向应用逻辑规则创建抽象推理结构,从目标结论出发反向推导至前提。这确保生成的问题具有保证解,同时通过深度参数保持可控的复杂度。

其次,语义实例化将这些抽象逻辑结构转化为自然语言问题。该框架使用大语言模型生成上下文连贯的问题陈述,同时保留底层逻辑关系。这一步骤引入真实的语言变化,同时维持形式正确性。

第三,基于求解器的验证采用自动定理证明器来验证问题正确性并枚举所有最小证明。这种详尽枚举至关重要——它为评估模型能否发现多条有效推理路径提供了真实基准。该管道还引入逻辑干扰项:语义上合理但逻辑上无关的信息,测试模型能否区分有效推理路径与虚假路径。

## 实验发现:发散差距

在GPT-4、Claude及开源替代模型上的实验揭示了一种系统性失效模式。虽然模型在浅层推理问题(深度$\leq$ 3)上达到合理准确率,但随着深度增加性能迅速退化。更关键的是,覆盖率指标——测量模型探索有效证明路径的比例——显示出更陡峭的下降。

发散差距量化了这一现象:收敛准确率(找到任意有效证明)与发散覆盖率(探索多条有效证明)之间的差异。对于深度5或更高的问题,即使模型成功解决问题,通常也只发现不到30%的有效推理路径。这表明模型采用脆弱的推理策略,过早锁定特定路径,而非系统性地探索逻辑空间。

消融研究表明逻辑干扰项显著影响性能。当问题包含语义上合理但逻辑上无关的前提时,模型频繁将其纳入推理链,表明在区分有效和无效逻辑依赖关系方面能力较弱。这种脆弱性在多路径场景中更加明显,此时模型必须评估多个候选推理链。

## 对大语言模型发展的启示

LogicGraph揭示了当前大语言模型在逻辑推理方法上的根本局限。早期承诺问题——模型固着于初始推理路径——表明标准训练目标可能未能充分激励对替代解决方案的探索。这具有实际意义:在数学问题求解、法律推理或科学假设生成等领域,考虑多种有效方法的能力往往比快速找到单一解决方案更有价值。

该基准还突显了更好评估指标的需求。传统的基于准确率的指标掩盖了勉强解决问题的模型与通过探索多条有效推导展现稳健理解的模型之间的重要区别。这里提出的无参考评估框架为更细致的评估提供了模板,既捕捉正确性又捕捉推理广度。

未来工作可能探索明确鼓励路径多样性的训练技术,例如奖励模型生成不同有效证明的对比学习目标,或在开发前激励探索的强化学习方法。LogicGraph基准提供了严格评估此类创新的基础设施。

## 要点总结

1. 当前大语言模型表现出显著的发散差距:它们能找到单一证明但无法探索多条有效推理路径,对于深度推理问题覆盖率降至30%以下。

2. 结合反向逻辑生成、语义实例化和求解器验证的神经符号生成管道,能够创建具有挑战性、可验证的多路径推理基准。

3. 模型表现出对初始推理路径的过早承诺,难以区分有效逻辑依赖与合理干扰项,尤其是随着问题复杂度增加。

4. 测量收敛准确率和发散覆盖率的无参考评估框架,比传统指标提供了更全面的推理能力评估。

5. 解决多路径推理差距需要新的训练目标和架构创新,明确激励对替代有效解决方案的探索。
:::
