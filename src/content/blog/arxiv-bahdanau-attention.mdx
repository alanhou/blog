---
title: "Neural Machine Translation by Jointly Learning to Align and Translate"
description: "Introduction of attention mechanism for neural machine translation"
pubDate: 2014-09-01
duration: 8 min
heroImage: "https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&auto=format&fit=crop&q=60"
language: "bilingual"
tags: ["Attention", "NMT", "Alignment", "Seq2Seq"]
---

## English

### Overview

This landmark paper introduced the attention mechanism to neural machine translation. Instead of compressing the entire source sentence into a fixed-length vector, the model learns to attend to relevant parts of the input when generating each output word.

### Key Contributions

1. **Attention Mechanism**: Soft alignment between source and target
2. **Dynamic Context**: Different context vector for each output position
3. **Long Sequences**: Handles long sentences much better than vanilla Seq2Seq
4. **Interpretability**: Attention weights show alignment between words

### How It Works

For each decoder step:
1. Compute attention scores between decoder state and all encoder states
2. Softmax to get attention weights
3. Weighted sum of encoder states gives context vector
4. Use context vector with decoder state to predict next word

### Impact

The attention mechanism became ubiquitous:
- Foundation for the Transformer architecture
- Used in vision (image captioning, object detection)
- Core component of all modern sequence models

### Paper Link

[arXiv:1409.0473](https://arxiv.org/abs/1409.0473)

---

## 中文

### 概述

这篇里程碑式的论文将注意力机制引入神经机器翻译。模型不再将整个源句子压缩成固定长度的向量，而是学习在生成每个输出词时关注输入的相关部分。

### 主要贡献

1. **注意力机制**：源语言和目标语言之间的软对齐
2. **动态上下文**：每个输出位置有不同的上下文向量
3. **长序列**：比普通Seq2Seq更好地处理长句子
4. **可解释性**：注意力权重显示词之间的对齐关系

### 工作原理

对于每个解码步骤：
1. 计算解码器状态与所有编码器状态之间的注意力分数
2. Softmax得到注意力权重
3. 编码器状态的加权和得到上下文向量
4. 使用上下文向量和解码器状态预测下一个词

### 影响

注意力机制变得无处不在：
- Transformer架构的基础
- 用于视觉（图像描述、目标检测）
- 所有现代序列模型的核心组件

### 论文链接

[arXiv:1409.0473](https://arxiv.org/abs/1409.0473)
