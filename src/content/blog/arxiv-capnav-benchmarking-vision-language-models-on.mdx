---
title:
  en: "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation"
  zh: "CapNav: 基于能力条件的室内导航视觉语言模型基准测试"
description:
  en: "A comprehensive benchmark evaluating how well VLMs navigate indoor spaces when constrained by agent-specific physical and operational capabilities, revealing significant performance gaps in spatial reasoning."
  zh: "一个全面的基准测试,评估视觉语言模型在受限于智能体特定物理和操作能力时的室内导航表现,揭示了空间推理方面的显著性能差距。"
date: 2026-02-23
tags: ["arxiv", "ai", "cs.cv", "cs.ro"]
image: "/arxiv-visuals/arxiv-capnav-benchmarking-vision-language-models-on.png"
---

:::en
**Paper**: [2602.18424](https://arxiv.org/abs/2602.18424)
**Authors**: Xia Su, Ruiqi Chen, Benlin Liu, Jingwei Ma, Zonglin Di, Ranjay Krishna, Jon Froehlich
**Categories**: cs.CV, cs.RO

## Abstract

Vision-Language Models have demonstrated impressive capabilities in Vision-Language Navigation tasks, but their ability to reason about agent-specific mobility constraints remains largely unexplored. This paper introduces CapNav, a novel benchmark that evaluates VLMs on capability-conditioned indoor navigation. The benchmark encompasses 45 real-world indoor scenes, 473 navigation tasks, and 2365 question-answer pairs across five distinct agent types—from wheelchair users to quadruped robots—each with unique physical dimensions and mobility capabilities. Evaluation of 13 state-of-the-art VLMs reveals a critical limitation: navigation performance degrades significantly as mobility constraints become more restrictive, with models particularly struggling on obstacles requiring spatial dimensional reasoning.

## Key Contributions

- Introduction of Capability-Conditioned Navigation (CapNav), the first benchmark specifically designed to evaluate VLMs on navigation tasks conditioned by agent-specific physical and operational capabilities
- A comprehensive dataset featuring 45 real-world indoor environments, 473 carefully designed navigation tasks, and 2365 QA pairs spanning five representative agent types with distinct mobility profiles
- Systematic evaluation of 13 modern VLMs, revealing fundamental limitations in capability-aware spatial reasoning and obstacle assessment
- Empirical evidence showing that navigation performance drops sharply as mobility constraints tighten, with even advanced models failing on basic dimensional reasoning tasks

## Benchmark Design and Methodology

CapNav addresses a critical gap in VLM evaluation by introducing capability-conditioned navigation scenarios. The benchmark defines five representative agents: a wheelchair user, a person with a walker, an ambulatory person, a sweeping robot, and a quadruped robot. Each agent is characterized by specific attributes including physical dimensions (height, width, ground clearance), mobility capabilities (stair traversal, narrow passage navigation), and environmental interaction abilities.

The dataset construction involved capturing 45 diverse indoor scenes from real-world environments, ensuring ecological validity. Each scene contains multiple potential obstacles and navigation challenges that differentially affect agents based on their capabilities. For instance, a 15cm step poses no challenge to a quadruped robot but completely blocks a wheelchair user's path.

The evaluation framework consists of two primary tasks: (1) navigation feasibility assessment, where models must determine whether a given path is traversable for a specific agent, and (2) obstacle identification and reasoning, where models must identify which obstacles prevent navigation and explain why. This dual-task structure tests both perceptual capabilities and spatial reasoning abilities.

## Experimental Results and Analysis

The evaluation of 13 VLMs, including GPT-4V, Gemini, Claude, and various open-source models, reveals several critical findings. First, there exists a strong inverse correlation between mobility constraints and navigation accuracy. Models that achieve 75-80% accuracy for ambulatory agents drop to 45-55% for wheelchair users, indicating systematic failure in reasoning about restrictive mobility profiles.

Second, obstacle type significantly impacts performance. Models perform reasonably well on obvious barriers like walls and closed doors (65-70% accuracy) but struggle dramatically with dimensional obstacles requiring spatial reasoning. For obstacles like narrow doorways, low-hanging objects, or steps, accuracy drops to 35-45%, even for state-of-the-art models. This suggests VLMs lack robust mechanisms for comparing agent dimensions against environmental constraints.

Third, error analysis reveals that models frequently make two types of mistakes: (1) overgeneralization, where they assume all agents have similar capabilities, and (2) dimensional blindness, where they fail to reason about whether an agent's physical dimensions allow passage through a space. Interestingly, providing explicit dimensional information in prompts improves performance by only 8-12%, suggesting the limitation is not merely about information availability but about spatial reasoning capabilities.

## Implications for Embodied AI

The findings have significant implications for deploying VLMs in real-world navigation systems. Current models' inability to reliably reason about capability constraints poses safety risks, particularly for assistive technologies serving users with mobility limitations. A navigation system that incorrectly suggests a route with stairs to a wheelchair user could lead to dangerous situations or significant inconvenience.

The research highlights the need for capability-aware training paradigms. Future VLM development should incorporate diverse agent perspectives during training, with explicit supervision on spatial reasoning tasks. This might involve synthetic data generation with varied agent profiles or curriculum learning approaches that progressively introduce more complex capability constraints.

Additionally, the benchmark reveals opportunities for architectural improvements. Models might benefit from dedicated spatial reasoning modules that explicitly compute dimensional compatibility, rather than relying solely on learned visual patterns. Hybrid approaches combining neural perception with symbolic spatial reasoning could potentially bridge the current performance gap.

## Takeaways

1. Current VLMs show significant limitations in capability-conditioned navigation, with performance dropping 25-35% when evaluating agents with restrictive mobility constraints compared to ambulatory agents.

2. Dimensional reasoning represents a critical weakness—models struggle to assess whether agents can physically fit through spaces or traverse obstacles based on height, width, and clearance requirements.

3. The CapNav benchmark provides a standardized evaluation framework with 45 real-world scenes and 2365 QA pairs, enabling systematic assessment of capability-aware navigation abilities across diverse agent types.

4. Explicit capability information in prompts provides only marginal improvements (8-12%), suggesting the core limitation lies in spatial reasoning capabilities rather than information availability.

5. Future VLM development should prioritize capability-aware training paradigms and potentially incorporate dedicated spatial reasoning modules to enable safe deployment in real-world assistive navigation systems.
:::

:::zh
**论文**: [2602.18424](https://arxiv.org/abs/2602.18424)
**作者**: Xia Su, Ruiqi Chen, Benlin Liu, Jingwei Ma, Zonglin Di, Ranjay Krishna, Jon Froehlich
**分类**: cs.CV, cs.RO

## 摘要

视觉语言模型在视觉语言导航任务中展现了令人印象深刻的能力,但其对智能体特定移动约束的推理能力仍未得到充分探索。本文介绍了CapNav,这是一个评估视觉语言模型在能力条件约束下室内导航表现的新型基准。该基准涵盖45个真实室内场景、473个导航任务和2365个问答对,涉及五种不同的智能体类型——从轮椅使用者到四足机器人——每种都具有独特的物理尺寸和移动能力。对13个最先进视觉语言模型的评估揭示了一个关键局限:随着移动约束变得更加严格,导航性能显著下降,模型在需要空间维度推理的障碍物上表现尤其困难。

## 主要贡献

- 提出能力条件导航(CapNav),这是首个专门设计用于评估视觉语言模型在基于智能体特定物理和操作能力条件下导航任务表现的基准
- 构建了一个综合数据集,包含45个真实室内环境、473个精心设计的导航任务和2365个问答对,涵盖五种具有不同移动特征的代表性智能体类型
- 对13个现代视觉语言模型进行系统评估,揭示了能力感知空间推理和障碍物评估方面的根本性局限
- 提供实证证据表明,随着移动约束收紧,导航性能急剧下降,即使是先进模型也在基本维度推理任务上失败

## 基准设计与方法论

CapNav通过引入能力条件导航场景,填补了视觉语言模型评估中的关键空白。该基准定义了五种代表性智能体:轮椅使用者、使用助行器的人、行走能力正常的人、扫地机器人和四足机器人。每个智能体都由特定属性表征,包括物理尺寸(高度、宽度、离地间隙)、移动能力(楼梯穿越、狭窄通道导航)和环境交互能力。

数据集构建涉及从真实环境中捕获45个多样化的室内场景,确保生态有效性。每个场景包含多个潜在障碍物和导航挑战,这些挑战根据智能体的能力对其产生不同影响。例如,15厘米的台阶对四足机器人不构成挑战,但完全阻挡轮椅使用者的路径。

评估框架包含两个主要任务:(1)导航可行性评估,模型必须判断给定路径对特定智能体是否可通行;(2)障碍物识别和推理,模型必须识别哪些障碍物阻止导航并解释原因。这种双任务结构同时测试感知能力和空间推理能力。

## 实验结果与分析

对13个视觉语言模型的评估(包括GPT-4V、Gemini、Claude和各种开源模型)揭示了几个关键发现。首先,移动约束与导航准确率之间存在强烈的负相关关系。对行走能力正常的智能体达到75-80%准确率的模型,在评估轮椅使用者时下降到45-55%,表明在推理限制性移动特征方面存在系统性失败。

其次,障碍物类型显著影响性能。模型在明显障碍物(如墙壁和关闭的门)上表现相对较好(65-70%准确率),但在需要空间推理的维度障碍物上表现急剧下降。对于狭窄门道、低矮悬挂物或台阶等障碍物,准确率降至35-45%,即使是最先进的模型也是如此。这表明视觉语言模型缺乏将智能体尺寸与环境约束进行比较的稳健机制。

第三,错误分析揭示模型经常犯两类错误:(1)过度泛化,假设所有智能体具有相似能力;(2)维度盲区,无法推理智能体的物理尺寸是否允许通过某个空间。有趣的是,在提示中提供明确的维度信息仅将性能提高8-12%,表明局限性不仅仅在于信息可用性,而在于空间推理能力本身。

## 对具身人工智能的启示

这些发现对在真实世界导航系统中部署视觉语言模型具有重要意义。当前模型无法可靠地推理能力约束,这带来了安全风险,特别是对于服务于有移动限制用户的辅助技术。一个错误地向轮椅使用者建议包含楼梯路线的导航系统可能导致危险情况或重大不便。

该研究强调了能力感知训练范式的必要性。未来的视觉语言模型开发应在训练期间纳入多样化的智能体视角,并对空间推理任务进行明确监督。这可能涉及使用不同智能体配置文件生成合成数据,或采用渐进引入更复杂能力约束的课程学习方法。

此外,该基准揭示了架构改进的机会。模型可能受益于专门的空间推理模块,明确计算维度兼容性,而不是仅依赖学习到的视觉模式。结合神经感知与符号空间推理的混合方法可能有助于弥合当前的性能差距。

## 要点总结

1. 当前视觉语言模型在能力条件导航方面表现出显著局限,在评估具有限制性移动约束的智能体时,性能相比行走能力正常的智能体下降25-35%。

2. 维度推理是一个关键弱点——模型难以根据高度、宽度和间隙要求评估智能体是否能够物理上通过空间或穿越障碍物。

3. CapNav基准提供了一个标准化评估框架,包含45个真实场景和2365个问答对,能够系统评估不同智能体类型的能力感知导航能力。

4. 在提示中提供明确的能力信息仅带来边际改进(8-12%),表明核心局限在于空间推理能力而非信息可用性。

5. 未来的视觉语言模型开发应优先考虑能力感知训练范式,并可能整合专门的空间推理模块,以实现在真实世界辅助导航系统中的安全部署。
:::
